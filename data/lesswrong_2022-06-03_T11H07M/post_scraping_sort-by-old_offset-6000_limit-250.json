{"results": [{"createdAt": null, "postedAt": "2012-03-03T19:05:59.564Z", "modifiedAt": null, "url": null, "title": "[Link] The emotional system (aka Type 1 thinking) might excel at complex decisions", "slug": "link-the-emotional-system-aka-type-1-thinking-might-excel-at", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:19.466Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9yrzoyahY6y4jjvsc/link-the-emotional-system-aka-type-1-thinking-might-excel-at", "pageUrlRelative": "/posts/9yrzoyahY6y4jjvsc/link-the-emotional-system-aka-type-1-thinking-might-excel-at", "linkUrl": "https://www.lesswrong.com/posts/9yrzoyahY6y4jjvsc/link-the-emotional-system-aka-type-1-thinking-might-excel-at", "postedAtFormatted": "Saturday, March 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20The%20emotional%20system%20(aka%20Type%201%20thinking)%20might%20excel%20at%20complex%20decisions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20The%20emotional%20system%20(aka%20Type%201%20thinking)%20might%20excel%20at%20complex%20decisions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9yrzoyahY6y4jjvsc%2Flink-the-emotional-system-aka-type-1-thinking-might-excel-at%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20The%20emotional%20system%20(aka%20Type%201%20thinking)%20might%20excel%20at%20complex%20decisions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9yrzoyahY6y4jjvsc%2Flink-the-emotional-system-aka-type-1-thinking-might-excel-at", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9yrzoyahY6y4jjvsc%2Flink-the-emotional-system-aka-type-1-thinking-might-excel-at", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 598, "htmlBody": "<blockquote>\n<p>For thousands of years, human beings have looked down on their emotions. We&rsquo;ve seen them as primitive passions, the unfortunate legacy of our animal past. When we do stupid things &ndash; say, eating too much cake, or sleeping with the wrong person, or taking out a subprime mortgage &ndash; we usually blame our short-sighted feelings. People commit crimes of passion. There are no crimes of rationality.</p>\n<p>This bias against feeling has led people to assume that reason is always best. When faced with a difficult dilemma, most of us believe that it&rsquo;s best to carefully assess our options and spend a few moments consciously deliberating the information. Then, we should choose the alternative that best fits our preferences. This is how we maximize utility; rationality is our Promethean gift.</p>\n<p>[...] it&rsquo;s only in the last few years that researchers have demonstrated that the emotional system (aka Type 1 thinking) might excel at complex decisions, or those involving lots of variables.</p>\n<p>[...]</p>\n<p>The latest demonstration of this <a href=\"http://www.business.illinois.edu/ba/seminars/2010/pham_paper2.pdf\">effect</a> comes from the lab of Michael Pham at Columbia Business School. The study involved asking undergraduates to make predictions about eight different outcomes, from the Democratic presidential primary of 2008 to the finalists of American Idol. They forecast the Dow Jones and picked the winner of the BCS championship game. They even made predictions about the weather.</p>\n<p>Here&rsquo;s the strange part: although these predictions concerned a vast range of events, <strong>the results were consistent across every trial: people who were more likely to trust their feelings were also more likely to accurately predict the outcome. </strong>[...]<strong><br /></strong></p>\n<p>Consider the results from the American Idol quiz: <strong>while high-trust-in-feelings subjects correctly predicted the winner 41 percent of the time, those who distrusted their emotions were only right 24 percent of the time.</strong> <strong>The same lesson applied to the stock market, that classic example of a random walk: those emotional souls made predictions that were 25 percent more accurate than those who aspired to Spock-like cognition.</strong></p>\n<p>[...] the unconscious brain is able to process vast amounts of information in parallel, thus allowing it to analyze large data sets without getting overwhelmed. (Human reason, in contrast, has a very strict bottleneck and can only process about four bits of data at any given moment.)</p>\n<p>[...] how do we gain access to all this analysis [...]</p>\n<p>[...] emotions come in handy. <strong>Every feeling is like a summary of data, a quick encapsulation of all the information processing that we don&rsquo;t have access to. </strong>(As Pham puts it, emotions are like a &ldquo;privileged window&rdquo; into the subterranean mind.) When it comes to making predictions about complex events, <strong>this extra information is often essential. It represents the difference between an informed guess and random chance.</strong></p>\n<p>[...] for example, that you&rsquo;re given lots of information about how twenty different stocks have performed over a period of time.</p>\n<p>[...] if you&rsquo;re asked which stocks trigger the best feelings [...] you will suddenly be able to identify the best stocks [...] your feelings will &ldquo;reveal a remarkable degree of sensitivity&rdquo; to the actual performance of all of the different securities.</p>\n<p>But this doesn&rsquo;t meant we can simply rely on every fleeting whim [...] only benefit from the emotional oracle effect when they had some knowledge of the subject. If they weren&rsquo;t following [...] then their feelings weren&rsquo;t helpful predictors [...]</p>\n<p>[...] our emotions [...] are imperfect oracles [...] a strong emotion is a reminder that, even when we think we know nothing, our brain knows something.</p>\n</blockquote>\n<p><strong>Link: </strong><a href=\"http://www.wired.com/wiredscience/2012/03/are-emotions-prophetic/\">wired.com/wiredscience/2012/03/are-emotions-prophetic/</a></p>\n<p><strong>Study:</strong> <a href=\"http://www.business.illinois.edu/ba/seminars/2010/pham_paper2.pdf\">business.illinois.edu/ba/seminars/2010/pham_paper2.pdf</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9yrzoyahY6y4jjvsc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 11, "extendedScore": null, "score": 8.592134713685851e-07, "legacy": true, "legacyId": "13662", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-03T21:58:20.038Z", "modifiedAt": null, "url": null, "title": "Request for input: draft of my \"coming out\" statement on religious deconversion", "slug": "request-for-input-draft-of-my-coming-out-statement-on", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:20.961Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jwhendy", "createdAt": "2011-01-04T19:53:21.160Z", "isAdmin": false, "displayName": "jwhendy"}, "userId": "ZaJctSZkCvg7qvSEC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WMEQj382bjnerxCW8/request-for-input-draft-of-my-coming-out-statement-on", "pageUrlRelative": "/posts/WMEQj382bjnerxCW8/request-for-input-draft-of-my-coming-out-statement-on", "linkUrl": "https://www.lesswrong.com/posts/WMEQj382bjnerxCW8/request-for-input-draft-of-my-coming-out-statement-on", "postedAtFormatted": "Saturday, March 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Request%20for%20input%3A%20draft%20of%20my%20%22coming%20out%22%20statement%20on%20religious%20deconversion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARequest%20for%20input%3A%20draft%20of%20my%20%22coming%20out%22%20statement%20on%20religious%20deconversion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWMEQj382bjnerxCW8%2Frequest-for-input-draft-of-my-coming-out-statement-on%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Request%20for%20input%3A%20draft%20of%20my%20%22coming%20out%22%20statement%20on%20religious%20deconversion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWMEQj382bjnerxCW8%2Frequest-for-input-draft-of-my-coming-out-statement-on", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWMEQj382bjnerxCW8%2Frequest-for-input-draft-of-my-coming-out-statement-on", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 618, "htmlBody": "<p><strong>Edited 3/4/2012: </strong>I shortened up the summary a bit and add the following update:</p>\n<p>Thanks for the lively comments. As a preliminary summary of things I've found quite useful/helpful:</p>\n<ul>\n<li>Shorten/transform the document (<a href=\"/r/discussion/lw/ajk/request_for_input_draft_of_my_coming_out/5yne\">David_Gerard</a>)</li>\n<li>Remove/postpone&nbsp;<em>any </em>reasons (<a href=\"/r/discussion/lw/ajk/request_for_input_draft_of_my_coming_out/5yk9\">TimS</a>)</li>\n<li>Don't be so prosy/fake sounding (<a href=\"/r/discussion/lw/ajk/request_for_input_draft_of_my_coming_out/5yjr\">orthonormal</a>)</li>\n<li>Show to religious people, not just LW (<a href=\"/r/discussion/lw/ajk/request_for_input_draft_of_my_coming_out/5yhs\">AlexMennen</a>) -- doing that, btw</li>\n<li>Give reasons (<a href=\"/r/discussion/lw/ajk/request_for_input_draft_of_my_coming_out/5yiv\">Will_Newsome</a>)</li>\n</ul>\n<div>Overall, I was initially a bit discouraged. This took a lot of effort and so it's frustrating to contemplate changing directions. Due to reading a very generously shared similar PDF document (from someone I'll leave anonymous unless they'd like to be named), I'm much more upbeat about doing so. I plan to:</div>\n<div>\n<ul>\n<li>Create two documents. One very simple, plain-language, frank relating of the fact that I no longer believe in god. I'd like to write it just as though I were saying it personally to someone, easing them into hearing this (like <a href=\"/r/discussion/lw/ajk/request_for_input_draft_of_my_coming_out/5yi8\">Bugmaster suggested</a>, except that actually doing this in person is impractical for me)</li>\n<li>The second will be my actual list of reasons. I think it will be valuable to actually spell them out, and many will want to know reasons anyway (and probably ask)</li>\n</ul>\n<div>A question that came up in me from the comments below is what worth this actually brings about. I don't find myself compelled to write a dissertation-style document defending my power tool purchases, Linux custom kernel options, or why I listen to the music I do. I am aware of a desire for validation, to feel that I've done enough with respect to my \"quest,\" to prove myself on this topic. I'm still wrestling with whether this is completely irrational and unnecessary, or only partially so, validated by the fact of my social/environmental circumstances that <em>do </em>present<em>&nbsp;</em>some real obstacles that this document could help alleviate.</div>\n</div>\n<div>Open to any thoughts on that last bit as well. Thanks again for the valuable input.</div>\n<hr />\n<p>It's almost one year later, and I've finally made tangible progress on some of the input suggested in <a href=\"/lw/53h/recent_deconvert_saturated_by_religious_community/\">my post about being non-religious in a primarily religious environment</a>. That is, I have a near-final draft of a \"coming out\" statement I plan to share with a majority of those who know me.</p>\n<p>I was involved in two religious communities for about six years of my life (<a href=\"http://www.spoweb.org/\">SPO</a>&nbsp;and&nbsp;<a href=\"http://www.ccredeemer.org/\">CCR</a>). Two years post-deconversion from Catholicism, many of them still do not know I no longer believe in god. This can make for awkward interactions for myself, as well as for my wife, who's still a believer. She thought it would be helpful if everyone was on the same page, as did I.</p>\n<div>I'd like input from anyone willing to read my statement below. Whatever comes to mind will be helpful. I had a first draft done in Sept 2011 that I sent to my parents and brother (both non-religious) as well as a few of my blog readers. I'll post their input in perhaps a week to avoid influencing any expectations you currently have for the document.</div>\n<div>Many thanks for thoughts and comments (or support/hesitation concerning with proceeding as described above). Also, thanks to the LW in general for all the previous support and encouragement during a difficult time. For a sometimes intimidating band of rationalists, your words have been surprisingly comforting and helpful when I've come here in emotionally difficult&nbsp;times. Many thanks for that; I think this document will help a lot with me moving on with my life and I value the input from this community.</div>\n<div>\n<div>As a forewarning... it's 10 pages single-spaced. I don't anticipate it to be an agonizing read, though. I hope it's well written enough to be interesting and easy reading.</div>\n<div><a href=\"https://sites.google.com/site/jwhendytank/home/statement_2012-03-03.pdf\">My statement in pdf</a></div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WMEQj382bjnerxCW8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "13664", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 117, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fnTHrfFz5TMW8c9R2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-04T03:11:44.440Z", "modifiedAt": null, "url": null, "title": "Anyone want to proofread my intro article about anthropics?", "slug": "anyone-want-to-proofread-my-intro-article-about-anthropics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:07.762Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sZB2sH8mz4E7GK3Cr/anyone-want-to-proofread-my-intro-article-about-anthropics", "pageUrlRelative": "/posts/sZB2sH8mz4E7GK3Cr/anyone-want-to-proofread-my-intro-article-about-anthropics", "linkUrl": "https://www.lesswrong.com/posts/sZB2sH8mz4E7GK3Cr/anyone-want-to-proofread-my-intro-article-about-anthropics", "postedAtFormatted": "Sunday, March 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anyone%20want%20to%20proofread%20my%20intro%20article%20about%20anthropics%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnyone%20want%20to%20proofread%20my%20intro%20article%20about%20anthropics%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsZB2sH8mz4E7GK3Cr%2Fanyone-want-to-proofread-my-intro-article-about-anthropics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anyone%20want%20to%20proofread%20my%20intro%20article%20about%20anthropics%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsZB2sH8mz4E7GK3Cr%2Fanyone-want-to-proofread-my-intro-article-about-anthropics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsZB2sH8mz4E7GK3Cr%2Fanyone-want-to-proofread-my-intro-article-about-anthropics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 24, "htmlBody": "<p>I'm a bit nervous about just posting it to the front page, so would some people be willing to give me some constructive criticism?</p>\n<p>&nbsp;</p>\n<p>Thanks</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sZB2sH8mz4E7GK3Cr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 0, "extendedScore": null, "score": 8.59408597399382e-07, "legacy": true, "legacyId": "13665", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-04T06:06:01.435Z", "modifiedAt": null, "url": null, "title": "AI Risk and Opportunity: A Strategic Analysis", "slug": "ai-risk-and-opportunity-a-strategic-analysis", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:21.544Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/i2XoqtYEykc4XWp9B/ai-risk-and-opportunity-a-strategic-analysis", "pageUrlRelative": "/posts/i2XoqtYEykc4XWp9B/ai-risk-and-opportunity-a-strategic-analysis", "linkUrl": "https://www.lesswrong.com/posts/i2XoqtYEykc4XWp9B/ai-risk-and-opportunity-a-strategic-analysis", "postedAtFormatted": "Sunday, March 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20Risk%20and%20Opportunity%3A%20A%20Strategic%20Analysis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20Risk%20and%20Opportunity%3A%20A%20Strategic%20Analysis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi2XoqtYEykc4XWp9B%2Fai-risk-and-opportunity-a-strategic-analysis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20Risk%20and%20Opportunity%3A%20A%20Strategic%20Analysis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi2XoqtYEykc4XWp9B%2Fai-risk-and-opportunity-a-strategic-analysis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi2XoqtYEykc4XWp9B%2Fai-risk-and-opportunity-a-strategic-analysis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 718, "htmlBody": "<p align=\"center\"><img src=\"http://lukeprog.com/images/AI%20risk%20and%20opportunity.png\" alt=\"\" /></p>\n<p>Suppose you buy <a href=\"http://facingthesingularity.com/\">the argument</a> that humanity faces both the <em>risk</em> of AI-caused extinction and the <em>opportunity</em> to shape an AI-built utopia. What should we do about that? As Wei Dai <a href=\"/lw/6mi/some_thoughts_on_singularity_strategies/\">asks</a>, \"In what direction should we nudge the future, to maximize the chances and impact of a positive intelligence explosion?\"</p>\n<p>This post serves as a table of contents and an introduction for an ongoing strategic analysis of AI risk and opportunity.</p>\n<p><strong>Contents:</strong></p>\n<ol>\n<li>Introduction (this post)</li>\n<li><a href=\"/r/discussion/lw/b0v/ai_risk_and_opportunity_humanitys_efforts_so_far/\">Humanity's Efforts So Far</a></li>\n<li><a href=\"/r/discussion/lw/bd6/ai_risk_opportunity_a_timeline_of_early_ideas_and/\">A Timeline of Early Ideas and Arguments</a></li>\n<li><a href=\"/r/discussion/lw/bdt/ai_risk_opportunity_questions_we_want_answered/\">Questions We Want Answered</a></li>\n<li><a href=\"/r/discussion/lw/bjl/ai_risk_opportunity_strategic_analysis_via/\">Strategic Analysis Via Probability Tree</a></li>\n<li><a href=\"/r/discussion/lw/iqi/intelligence_amplification_and_friendly_ai/\">Intelligence Amplification and Friendly AI</a></li>\n<li>...</li>\n</ol>\n<h3><br /></h3>\n<h3>Why discuss AI safety strategy?</h3>\n<p>The main reason to discuss AI safety strategy is, of course, to draw on a wide spectrum of human expertise and processing power to clarify our understanding of the factors at play and the expected value of particular interventions we could invest in: raising awareness of safety concerns, forming a Friendly AI team, differential technological development, investigating AGI confinement methods, and others.</p>\n<p>Discussing AI safety strategy is also a challenging exercise in <em>applied rationality</em>. The relevant issues are complex and uncertain, but we need to take advantage of the fact that <a href=\"/lw/qi/faster_than_science/\">rationality is faster than science</a>: we can't \"try\" a bunch of intelligence explosions and see which one works best. We'll have to predict in advance how the future will develop and what we can do about it.</p>\n<h3><br /></h3>\n<h3>Core readings</h3>\n<p>Before engaging with this series, I recommend you read <em>at least</em> the following articles:</p>\n<ul>\n<li>Muehlhauser &amp; Salamon, <a href=\"http://intelligence.org/files/IE-EI.pdf\">Intelligence Explosion: Evidence and Import</a> (2013) </li>\n<li>Yudkowsky, <a href=\"http://intelligence.org/files/AIPosNegFactor.pdf\">AI as a Positive and Negative Factor in Global Risk</a> (2008) </li>\n<li>Chalmers, <a href=\"http://consc.net/papers/singularityjcs.pdf\">The Singularity: A Philosophical Analysis</a> (2010)</li>\n</ul>\n<h3><br /></h3>\n<h3>Example questions</h3>\n<p>Which strategic questions would we like to answer? <a href=\"http://lukeprog.com/SaveTheWorld.html\">Muehlhauser (2011)</a> elaborates on the following questions:</p>\n<ul>\n<li>What methods can we use to predict technological development?</li>\n<li>Which kinds of differential technological development should we encourage, and how? </li>\n<li>Which open problems are safe to discuss, and which are potentially dangerous?</li>\n<li>What can we do to reduce the risk of an AI arms race?</li>\n<li>What can we do to raise the \"sanity waterline,\" and how much will this help?</li>\n<li>What can we do to attract more funding, support, and research to x-risk reduction and to specific sub-problems of successful Singularity navigation?</li>\n<li>Which interventions should we prioritize?</li>\n<li>How should x-risk reducers and AI safety researchers interact with governments and corporations?</li>\n<li>How can optimal philanthropists get the most x-risk reduction for their philanthropic buck?</li>\n<li>How does AI risk compare to other existential risks?</li>\n<li>Which problems do we need to solve, and which ones can we have an AI solve?</li>\n<li>How can we develop microeconomic models of WBEs and self-improving systems?</li>\n<li>How can we be sure a Friendly AI development team will be altruistic?</li>\n</ul>\n<p><a href=\"http://intelligence.org/files/SS11Workshop.pdf\">Salamon &amp; Muehlhauser (2013)</a> list several other questions gathered from the participants of a workshop following Singularity Summit 2011, including:</p>\n<ul>\n<li>How hard is it to create Friendly AI? </li>\n<li>What is the strength of feedback from neuroscience to AI rather than brain emulation? </li>\n<li>Is there a safe way to do uploads, where they don't turn into neuromorphic AI? </li>\n<li>How possible is it to do FAI research on a seastead? </li>\n<li>How much must we spend on security when developing a Friendly AI team? </li>\n<li>What's the best way to recruit talent toward working on AI risks? </li>\n<li>How difficult is stabilizing the world so we can work on Friendly AI slowly? </li>\n<li>How hard will a takeoff be? </li>\n<li>What is the value of strategy vs. object-level progress toward a positive Singularity? </li>\n<li>How feasible is Oracle AI? </li>\n<li>Can we convert environmentalists into people concerned with existential risk? </li>\n<li>Is there no such thing as bad publicity [for AI risk reduction] purposes? </li>\n</ul>\n<p>These are the kinds of questions we will be tackling in this series of posts for <a href=\"/r/discussion/new/\">Less Wrong Discussion</a>, in order to improve our predictions about which direction we can nudge the future to maximize the chances of a positive intelligence explosion.</p>\n<ul>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "i2XoqtYEykc4XWp9B", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 11, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "13666", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p align=\"center\"><img src=\"http://lukeprog.com/images/AI%20risk%20and%20opportunity.png\" alt=\"\"></p>\n<p>Suppose you buy <a href=\"http://facingthesingularity.com/\">the argument</a> that humanity faces both the <em>risk</em> of AI-caused extinction and the <em>opportunity</em> to shape an AI-built utopia. What should we do about that? As Wei Dai <a href=\"/lw/6mi/some_thoughts_on_singularity_strategies/\">asks</a>, \"In what direction should we nudge the future, to maximize the chances and impact of a positive intelligence explosion?\"</p>\n<p>This post serves as a table of contents and an introduction for an ongoing strategic analysis of AI risk and opportunity.</p>\n<p><strong id=\"Contents_\">Contents:</strong></p>\n<ol>\n<li>Introduction (this post)</li>\n<li><a href=\"/r/discussion/lw/b0v/ai_risk_and_opportunity_humanitys_efforts_so_far/\">Humanity's Efforts So Far</a></li>\n<li><a href=\"/r/discussion/lw/bd6/ai_risk_opportunity_a_timeline_of_early_ideas_and/\">A Timeline of Early Ideas and Arguments</a></li>\n<li><a href=\"/r/discussion/lw/bdt/ai_risk_opportunity_questions_we_want_answered/\">Questions We Want Answered</a></li>\n<li><a href=\"/r/discussion/lw/bjl/ai_risk_opportunity_strategic_analysis_via/\">Strategic Analysis Via Probability Tree</a></li>\n<li><a href=\"/r/discussion/lw/iqi/intelligence_amplification_and_friendly_ai/\">Intelligence Amplification and Friendly AI</a></li>\n<li>...</li>\n</ol>\n<h3><br></h3>\n<h3 id=\"Why_discuss_AI_safety_strategy_\">Why discuss AI safety strategy?</h3>\n<p>The main reason to discuss AI safety strategy is, of course, to draw on a wide spectrum of human expertise and processing power to clarify our understanding of the factors at play and the expected value of particular interventions we could invest in: raising awareness of safety concerns, forming a Friendly AI team, differential technological development, investigating AGI confinement methods, and others.</p>\n<p>Discussing AI safety strategy is also a challenging exercise in <em>applied rationality</em>. The relevant issues are complex and uncertain, but we need to take advantage of the fact that <a href=\"/lw/qi/faster_than_science/\">rationality is faster than science</a>: we can't \"try\" a bunch of intelligence explosions and see which one works best. We'll have to predict in advance how the future will develop and what we can do about it.</p>\n<h3><br></h3>\n<h3 id=\"Core_readings\">Core readings</h3>\n<p>Before engaging with this series, I recommend you read <em>at least</em> the following articles:</p>\n<ul>\n<li>Muehlhauser &amp; Salamon, <a href=\"http://intelligence.org/files/IE-EI.pdf\">Intelligence Explosion: Evidence and Import</a> (2013) </li>\n<li>Yudkowsky, <a href=\"http://intelligence.org/files/AIPosNegFactor.pdf\">AI as a Positive and Negative Factor in Global Risk</a> (2008) </li>\n<li>Chalmers, <a href=\"http://consc.net/papers/singularityjcs.pdf\">The Singularity: A Philosophical Analysis</a> (2010)</li>\n</ul>\n<h3><br></h3>\n<h3 id=\"Example_questions\">Example questions</h3>\n<p>Which strategic questions would we like to answer? <a href=\"http://lukeprog.com/SaveTheWorld.html\">Muehlhauser (2011)</a> elaborates on the following questions:</p>\n<ul>\n<li>What methods can we use to predict technological development?</li>\n<li>Which kinds of differential technological development should we encourage, and how? </li>\n<li>Which open problems are safe to discuss, and which are potentially dangerous?</li>\n<li>What can we do to reduce the risk of an AI arms race?</li>\n<li>What can we do to raise the \"sanity waterline,\" and how much will this help?</li>\n<li>What can we do to attract more funding, support, and research to x-risk reduction and to specific sub-problems of successful Singularity navigation?</li>\n<li>Which interventions should we prioritize?</li>\n<li>How should x-risk reducers and AI safety researchers interact with governments and corporations?</li>\n<li>How can optimal philanthropists get the most x-risk reduction for their philanthropic buck?</li>\n<li>How does AI risk compare to other existential risks?</li>\n<li>Which problems do we need to solve, and which ones can we have an AI solve?</li>\n<li>How can we develop microeconomic models of WBEs and self-improving systems?</li>\n<li>How can we be sure a Friendly AI development team will be altruistic?</li>\n</ul>\n<p><a href=\"http://intelligence.org/files/SS11Workshop.pdf\">Salamon &amp; Muehlhauser (2013)</a> list several other questions gathered from the participants of a workshop following Singularity Summit 2011, including:</p>\n<ul>\n<li>How hard is it to create Friendly AI? </li>\n<li>What is the strength of feedback from neuroscience to AI rather than brain emulation? </li>\n<li>Is there a safe way to do uploads, where they don't turn into neuromorphic AI? </li>\n<li>How possible is it to do FAI research on a seastead? </li>\n<li>How much must we spend on security when developing a Friendly AI team? </li>\n<li>What's the best way to recruit talent toward working on AI risks? </li>\n<li>How difficult is stabilizing the world so we can work on Friendly AI slowly? </li>\n<li>How hard will a takeoff be? </li>\n<li>What is the value of strategy vs. object-level progress toward a positive Singularity? </li>\n<li>How feasible is Oracle AI? </li>\n<li>Can we convert environmentalists into people concerned with existential risk? </li>\n<li>Is there no such thing as bad publicity [for AI risk reduction] purposes? </li>\n</ul>\n<p>These are the kinds of questions we will be tackling in this series of posts for <a href=\"/r/discussion/new/\">Less Wrong Discussion</a>, in order to improve our predictions about which direction we can nudge the future to maximize the chances of a positive intelligence explosion.</p>\n<ul>\n</ul>", "sections": [{"title": "Contents:", "anchor": "Contents_", "level": 2}, {"title": "Why discuss AI safety strategy?", "anchor": "Why_discuss_AI_safety_strategy_", "level": 1}, {"title": "Core readings", "anchor": "Core_readings", "level": 1}, {"title": "Example questions", "anchor": "Example_questions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "163 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 163, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["73SotZnDbsYpxfnuQ", "i4susk4W3ieR5K92u", "Qdq2SKyMi8vf7Snxq", "3w7XHLf8AYRvtoN8b", "rvN6LrbtQDR2ee382", "jmgyfDYDDYs7YpqJg", "xTyuQ3cgsPjifr7oj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-04T06:07:59.772Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Joy in the Merely Real", "slug": "seq-rerun-joy-in-the-merely-real", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RruZQC3XgXxe6yzNr/seq-rerun-joy-in-the-merely-real", "pageUrlRelative": "/posts/RruZQC3XgXxe6yzNr/seq-rerun-joy-in-the-merely-real", "linkUrl": "https://www.lesswrong.com/posts/RruZQC3XgXxe6yzNr/seq-rerun-joy-in-the-merely-real", "postedAtFormatted": "Sunday, March 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Joy%20in%20the%20Merely%20Real&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Joy%20in%20the%20Merely%20Real%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRruZQC3XgXxe6yzNr%2Fseq-rerun-joy-in-the-merely-real%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Joy%20in%20the%20Merely%20Real%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRruZQC3XgXxe6yzNr%2Fseq-rerun-joy-in-the-merely-real", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRruZQC3XgXxe6yzNr%2Fseq-rerun-joy-in-the-merely-real", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<p>Today's post, <a href=\"/lw/or/joy_in_the_merely_real/\">Joy in the Merely Real</a> was originally published on 20 March 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If you can't take joy in things that turn out to be explicable, you're going to set yourself up for eternal disappointment. Don't worry if quantum physics turns out to be normal.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/ajg/seq_rerun_savanna_poets/\">Savanna Poets</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RruZQC3XgXxe6yzNr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 8.594794190125084e-07, "legacy": true, "legacyId": "13667", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["x4dG4GhpZH2hgz59x", "TWei9eJYGkpvJDBGZ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-04T07:19:26.912Z", "modifiedAt": null, "url": null, "title": "60m Asteroid currently assigned a .022% chance of hitting Earth.", "slug": "60m-asteroid-currently-assigned-a-022-chance-of-hitting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:22.338Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Locke", "createdAt": "2011-12-27T15:40:30.383Z", "isAdmin": false, "displayName": "Locke"}, "userId": "zq2k9FZq7T3AebpEc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tWCrrGEwrSZunWEjT/60m-asteroid-currently-assigned-a-022-chance-of-hitting", "pageUrlRelative": "/posts/tWCrrGEwrSZunWEjT/60m-asteroid-currently-assigned-a-022-chance-of-hitting", "linkUrl": "https://www.lesswrong.com/posts/tWCrrGEwrSZunWEjT/60m-asteroid-currently-assigned-a-022-chance-of-hitting", "postedAtFormatted": "Sunday, March 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%2060m%20Asteroid%20currently%20assigned%20a%20.022%25%20chance%20of%20hitting%20Earth.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A60m%20Asteroid%20currently%20assigned%20a%20.022%25%20chance%20of%20hitting%20Earth.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtWCrrGEwrSZunWEjT%2F60m-asteroid-currently-assigned-a-022-chance-of-hitting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=60m%20Asteroid%20currently%20assigned%20a%20.022%25%20chance%20of%20hitting%20Earth.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtWCrrGEwrSZunWEjT%2F60m-asteroid-currently-assigned-a-022-chance-of-hitting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtWCrrGEwrSZunWEjT%2F60m-asteroid-currently-assigned-a-022-chance-of-hitting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 22, "htmlBody": "<p><a href=\"http://neo.jpl.nasa.gov/risk/2012da14.html\">http://neo.jpl.nasa.gov/risk/2012da14.html</a></p>\n<p><a href=\"http://rt.com/news/paint-asteroid-earth-nasa-767/\">http://rt.com/news/paint-asteroid-earth-nasa-767/</a></p>\n<p>Seems like a good opportunity to bring up existential risks. And A friendly reminder that NASA is in fact pretty damned important.</p>\n<p>Thoughts?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tWCrrGEwrSZunWEjT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 18, "extendedScore": null, "score": 8.595081323664603e-07, "legacy": true, "legacyId": "13668", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-04T08:03:40.013Z", "modifiedAt": null, "url": null, "title": "Sapir-Whorf , Savings, and Discount Rates [Link]", "slug": "sapir-whorf-savings-and-discount-rates-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:07.871Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nic_Smith", "createdAt": "2009-10-23T03:32:46.312Z", "isAdmin": false, "displayName": "Nic_Smith"}, "userId": "XP9GcTgRGLBCnf9ih", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kX3jSdH84WT99d3dL/sapir-whorf-savings-and-discount-rates-link", "pageUrlRelative": "/posts/kX3jSdH84WT99d3dL/sapir-whorf-savings-and-discount-rates-link", "linkUrl": "https://www.lesswrong.com/posts/kX3jSdH84WT99d3dL/sapir-whorf-savings-and-discount-rates-link", "postedAtFormatted": "Sunday, March 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sapir-Whorf%20%2C%20Savings%2C%20and%20Discount%20Rates%20%5BLink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASapir-Whorf%20%2C%20Savings%2C%20and%20Discount%20Rates%20%5BLink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkX3jSdH84WT99d3dL%2Fsapir-whorf-savings-and-discount-rates-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sapir-Whorf%20%2C%20Savings%2C%20and%20Discount%20Rates%20%5BLink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkX3jSdH84WT99d3dL%2Fsapir-whorf-savings-and-discount-rates-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkX3jSdH84WT99d3dL%2Fsapir-whorf-savings-and-discount-rates-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>The language you speak may affect how you approach your finances, according to a <a href=\"http://faculty.som.yale.edu/keithchen/papers/LanguageWorkingPaper.pdf\">working paper by economist Keith Chen</a> (seen via <a href=\"http://worthwhile.typepad.com/worthwhile_canadian_initi/2012/03/does-language-influence-savings.html\">posts by Frances Woolley at the <em>Worthwhile Canadian Initiative</em></a> and <em><a href=\"http://www.theglobeandmail.com/report-on-business/economy/economy-lab/frances-woolley/trouble-saving-money-drop-english-speak-german/article2356566/\">Economy Lab</a></em>). It appears that languages that require more explicit future tense are associated with lower savings. A few interesting quotes from a quick glance:</p>\n<blockquote>\n<p>...[I]n the World Values Survey a language&rsquo;s FTR [Future-Time Reference] is almost entirely uncorrelated with its speakers&rsquo; stated values towards savings (corr = -0.07). This suggests that the language effects I identify <strong>operate through a channel which is independent of conscious attitudes</strong> towards savings. [emphasis mine]</p>\n</blockquote>\n<p>Something else that I wasn't previously aware of:</p>\n<blockquote>\n<p>Lowenstein (1988) finds a temporal reference-point effect: people demand much more compensation to delay receiving a good by one year, (from today to a year from now), than they are willing to pay to move up consumption of that same good (from a year from now to today).</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kX3jSdH84WT99d3dL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 3, "extendedScore": null, "score": 8.595259025001225e-07, "legacy": true, "legacyId": "13669", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-04T11:46:11.865Z", "modifiedAt": null, "url": null, "title": "The kinesthesia switch", "slug": "the-kinesthesia-switch", "viewCount": null, "lastCommentedAt": "2012-11-23T15:55:18.413Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3h9FBoczLGyiwB6du/the-kinesthesia-switch", "pageUrlRelative": "/posts/3h9FBoczLGyiwB6du/the-kinesthesia-switch", "linkUrl": "https://www.lesswrong.com/posts/3h9FBoczLGyiwB6du/the-kinesthesia-switch", "postedAtFormatted": "Sunday, March 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20kinesthesia%20switch&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20kinesthesia%20switch%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3h9FBoczLGyiwB6du%2Fthe-kinesthesia-switch%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20kinesthesia%20switch%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3h9FBoczLGyiwB6du%2Fthe-kinesthesia-switch", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3h9FBoczLGyiwB6du%2Fthe-kinesthesia-switch", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 816, "htmlBody": "<p>I've been working on improving my kinesthesia for about thirty years (for reasons which are not obvious to me, I've felt a strong motivation to get moved into my body), and I've found something interesting.</p>\n<p>I was doing a chi gung exercise [1] which involves going up on the balls of my feet while pushing up with my hands, and I suddenly noticed that my body had mostly blanked out when I was in the extended position, which led to a realization that blanking out was a process rather than a thing [2]. I thought \"kinesthesia switch on\", and I could suddenly feel a good bit of detail about how I was wobbling-- I mean I could feel some of my bones moving relative to each other, instead of just feeling in a vague sort of way that the position didn't feel very good.</p>\n<p>What's better, is that I remembered how to turn on the kinesthesia switch, and have continued to work with it.</p>\n<p>A side effect of turning on the switch is that I uncurl my upper body (kinesthesia seems to have something to do with alignment), but deliberately uncurling doesn't work nearly as well as turning on the switch.</p>\n<p>At first, I would try to turn on the switch as much as possible, but that began to feel bad-- probably because there was some perfectionism driving that approach. I've tweaked it to \"as much as feels good to me\".</p>\n<p>The most noticeable effect (aside from better spirits and less akrasia) is that going up and down stairs has become a lot easier the vast majority of the time. Down stairs has been a problem for years because of accumulated knee injuries. Upstairs became problematic about 6 months ago because, for no apparent reason, I developed some sort of serious muscle tightness in my right leg. It started with pain in the back of my right heel which was clearly linked to movement, and eventually shifted to pain in what definitely felt like the muscle attachment to my sitzbone on the right.</p>\n<p>This days, I'm mostly trotting up and down stairs rather than stepping down a step and then puting the other foot on the same step in order to avoid a good bit of pain.</p>\n<p>The reason I'm doing this as a top level post is because I'm pretty sure the kinesthesia switch isn't the only switch-- I think other switches can be found in areas where you've done enough observation to have a chance of finding the path from one state to another, and I'm hoping there will be comments about finding other switches.</p>\n<p>Another, and less cheerful switch: I haven't been in that state for a while, but I used to be mildly suicidal-- I wasn't making plans, but suicide was on the table as a possibility. What's more, I wanted it there-- I didn't want to be in a situation where committing suicide made sense, but I wasn't able to get myself to do it.</p>\n<p>What I found was that I could chose to have suicide as a felt possibility, and I would deliberately turn that switch back on if it turned off. It eventually occurred to me that my reason for keeping the switch on wasn't good (it was impacting my current quality of life for the sake of something I wasn't sure I'd need), and I left it off.</p>\n<p>I don't have a theory of mind or brain which does a good job of explaining switches. This is all just observation.</p>\n<p>In <a href=\"http://www.amazon.com/Move-into-Life-Essentials-Lifelong/dp/0307395294\">Move into Life</a>, there's a description of a learning switch. The book is by Anat Baniel, a Feldenkrais  teacher who found that her students would become more vital and who decided to directly cultivate various aspects of vitality. In particular, she noticed that was a shift when her students became interested in learning. In her opinion, the major reason people stop learning is that they believe they already know enough.</p>\n<p>The learning switch is related to letting yourself believe that you don't already know the answer, allowing in new information, and letting your mind drift to other areas of knowledge to see if there's a connection to what you're interested in. I haven't been able to get results from Baniel's description-- I'm just mentioning this as another example of a switch.</p>\n<p>It's possible that my learning switch is already on, but my problem is finding my \"do something useful\" switch.</p>\n<p>More generally, my impression is that once you find a switch, it makes accessing a different state easy. This doesn't mean it's easy if you haven't yet found the switch or if you don't have the motivation to turn it.</p>\n<p>[1] This is part of the eight brocades set from <a href=\"http://www.amazon.com/Way-Energy-Mastering-Internal-Strength/dp/0671736450\">The Way of Energy</a>-- this book is a solid introduction to standing meditation.</p>\n<p>[2] Recognizing that something you thought was static and simple is actually made of moving parts is probably in the sequences somewhere, but I can't think of anything in particular.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3h9FBoczLGyiwB6du", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 12, "extendedScore": null, "score": 8.596153414313118e-07, "legacy": true, "legacyId": "13670", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-05T01:27:33.289Z", "modifiedAt": null, "url": null, "title": "Writing about Singularity: needing help with references and bibliography", "slug": "writing-about-singularity-needing-help-with-references-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:26.931Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "oXGTijwhjZB8cnJ3W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MFvtEPwBf5YXNzxnS/writing-about-singularity-needing-help-with-references-and", "pageUrlRelative": "/posts/MFvtEPwBf5YXNzxnS/writing-about-singularity-needing-help-with-references-and", "linkUrl": "https://www.lesswrong.com/posts/MFvtEPwBf5YXNzxnS/writing-about-singularity-needing-help-with-references-and", "postedAtFormatted": "Monday, March 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Writing%20about%20Singularity%3A%20needing%20help%20with%20references%20and%20bibliography&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWriting%20about%20Singularity%3A%20needing%20help%20with%20references%20and%20bibliography%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMFvtEPwBf5YXNzxnS%2Fwriting-about-singularity-needing-help-with-references-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Writing%20about%20Singularity%3A%20needing%20help%20with%20references%20and%20bibliography%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMFvtEPwBf5YXNzxnS%2Fwriting-about-singularity-needing-help-with-references-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMFvtEPwBf5YXNzxnS%2Fwriting-about-singularity-needing-help-with-references-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 446, "htmlBody": "<p>&nbsp;</p>\n<p>It was Yudkowsky's Fun Theory sequence that inspired me to undertake the work of writing a novel on a singularitarian society... however, there are gaps I need to fill, and I need all the help I can get. It's mostly book recommendations that I'm asking for.</p>\n<p>&nbsp;</p>\n<p>One of the things I'd like to tackle in it would be the interactions between the modern, geeky Singularitarianisms, and Marxism, which I hold to be somewhat prototypical in that sense, as well as other utopisms. And contrasting them with more down-to-earth ideologies and attitudes, by examining the seriously dangerous bumps of the technological point of transition between \"baseline\" and \"singularity\". But I need to do a lot of research before I'm able to write anything good: if I'm not going to have any original ideas, at least I'd like to serve my readers with a collection of well-researched. solid ones.</p>\n<p>&nbsp;</p>\n<p>So I'd like to have everything that is worth reading about the Singularity, specifically the Revolution it entails (in one way or another) and the social aftermath. I'm particularly interested in the consequences of the lag of the spread of the technology from the wealthy to the baselines, and the potential for baselines oppression and other forms of continuation of current forms of social imbalances, as well as suboptimal distribution of wealth. After all, according to many authors, we've had the means to end war, poverty and famine, and most infectious diseases, since the sixties, and it's just our irrational methods of wealth distribution That is, supposing the commonly alleged ideal of total lifespan and material welfare maximization for all humanity is what actually drives the way things are done. But even with other, different premises and axioms, there's much that can be improved and isn't, thanks to basic human irrationality, which is what we combat here.</p>\n<p>&nbsp;</p>\n<p>Also, yes, this post makes my political leanings fairly clear, but I'm open to alternative viewpoints and actively seek them. I also don't intend to write any propaganda, as such. Just to examine ideas, and scenarios, for the sake of writing a compelling story, with wide audience appeal. The idea is to raise awareness of the Singularity as something rather imminent (\"Summer's Coming\"), and cause (or at least help prepare) normal people to question the wonders and dangers thereof, rationally.</p>\n<p>&nbsp;</p>\n<p>It's a frighteningly ambitious, long-term challenge, I am terribly aware of that. And the first thing I'll need to read is a style-book, to correct my horrendous grasp of standard acceptable writing (and not seem arrogant by doing anything else), so please feel free to recommend as many books and blog articles and other material as you like. I'll take my time going though it all.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MFvtEPwBf5YXNzxnS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 8.599456012927917e-07, "legacy": true, "legacyId": "13679", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-05T03:07:02.685Z", "modifiedAt": null, "url": null, "title": "\"The Journal of Real Effects\"", "slug": "the-journal-of-real-effects", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:09.662Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CarlShulman", "createdAt": "2009-03-01T07:47:12.225Z", "isAdmin": false, "displayName": "CarlShulman"}, "userId": "SguegG9SFXaKTgJLq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YGZEE78jckF9XeTxu/the-journal-of-real-effects", "pageUrlRelative": "/posts/YGZEE78jckF9XeTxu/the-journal-of-real-effects", "linkUrl": "https://www.lesswrong.com/posts/YGZEE78jckF9XeTxu/the-journal-of-real-effects", "postedAtFormatted": "Monday, March 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22The%20Journal%20of%20Real%20Effects%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22The%20Journal%20of%20Real%20Effects%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYGZEE78jckF9XeTxu%2Fthe-journal-of-real-effects%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22The%20Journal%20of%20Real%20Effects%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYGZEE78jckF9XeTxu%2Fthe-journal-of-real-effects", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYGZEE78jckF9XeTxu%2Fthe-journal-of-real-effects", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 145, "htmlBody": "<p>Luke's recent <a href=\"/lw/ajj/how_to_fix_science/\">post</a>&nbsp;mentioned that The Lancet has a <a href=\"http://www.thelancet.com/lancet-oncology-information-for-authors/article-types-manuscript-requirements\">policy</a> encouraging the advance registration of clinical trials, while mine examined an apparent case study of data-peeking and on-the-fly transformation of studies. But how much variation is there across journals on such dimensions? Are there journals that buck the standards of their fields (demanding registration, p=0.01 rather than p=0.05 where the latter is typical in the field, advance specification of statistical analyses and subject numbers, etc)? What are some of the standouts? Are there fields without any such?</p>\n<p>I wonder if there is a niche for a new open-access journal, along the lines of PLoS, with standards strict enough to reliably exclude false-positives. Some possible titles:</p>\n<p>&nbsp;</p>\n<ul>\n<li>The Journal of Real Effects</li>\n<li>(Settled) Science</li>\n<li>Probably True</li>\n<li>Journal of Non-Null Results, Really</li>\n<li>Too Good to Be False</li>\n<li>_________________?</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YGZEE78jckF9XeTxu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 20, "extendedScore": null, "score": 6.5e-05, "legacy": true, "legacyId": "13690", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ETe2SZacmLvvr8H9n"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-05T04:31:25.553Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Joy in Discovery", "slug": "seq-rerun-joy-in-discovery", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:08.490Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4C7udhR759Je3tZdB/seq-rerun-joy-in-discovery", "pageUrlRelative": "/posts/4C7udhR759Je3tZdB/seq-rerun-joy-in-discovery", "linkUrl": "https://www.lesswrong.com/posts/4C7udhR759Je3tZdB/seq-rerun-joy-in-discovery", "postedAtFormatted": "Monday, March 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Joy%20in%20Discovery&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Joy%20in%20Discovery%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4C7udhR759Je3tZdB%2Fseq-rerun-joy-in-discovery%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Joy%20in%20Discovery%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4C7udhR759Je3tZdB%2Fseq-rerun-joy-in-discovery", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4C7udhR759Je3tZdB%2Fseq-rerun-joy-in-discovery", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 215, "htmlBody": "<p>Today's post, <a href=\"/lw/os/joy_in_discovery/\">Joy in Discovery</a> was originally published on 21 March 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Joy_in_Discovery\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It feels incredibly good to discover the answer to a problem that nobody else has answered. And we should enjoy finding answers. But we really shouldn't base our joy on the fact that nobody else has done it before. Even if someone else knows the answer to a puzzle, if you don't know it, it's still a mystery to you. And you should still feel joy when you discover the answer.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/ajn/seq_rerun_joy_in_the_merely_real/\">Joy in the Merely Real</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4C7udhR759Je3tZdB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.600195646987066e-07, "legacy": true, "legacyId": "13692", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KfMNFB3G7XNviHBPN", "RruZQC3XgXxe6yzNr", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-05T08:19:31.160Z", "modifiedAt": null, "url": null, "title": "The Singularity Institute has started publishing monthly progress reports", "slug": "the-singularity-institute-has-started-publishing-monthly", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:54.341Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8bGNfebYNbP64k6iu/the-singularity-institute-has-started-publishing-monthly", "pageUrlRelative": "/posts/8bGNfebYNbP64k6iu/the-singularity-institute-has-started-publishing-monthly", "linkUrl": "https://www.lesswrong.com/posts/8bGNfebYNbP64k6iu/the-singularity-institute-has-started-publishing-monthly", "postedAtFormatted": "Monday, March 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Singularity%20Institute%20has%20started%20publishing%20monthly%20progress%20reports&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Singularity%20Institute%20has%20started%20publishing%20monthly%20progress%20reports%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8bGNfebYNbP64k6iu%2Fthe-singularity-institute-has-started-publishing-monthly%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Singularity%20Institute%20has%20started%20publishing%20monthly%20progress%20reports%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8bGNfebYNbP64k6iu%2Fthe-singularity-institute-has-started-publishing-monthly", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8bGNfebYNbP64k6iu%2Fthe-singularity-institute-has-started-publishing-monthly", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<p>If anyone is curious what's going on at SI, it seems as though they've started publishing monthly progress reports on their blog. The latest was published less than a week ago for the month of February:</p>\n<p><a href=\"http://intelligence.org/blog/\">http://singinst.org/blog/</a></p>\n<p>Thought it might be a good idea to take a minute to positively reinforce the people at SI for their work. Here on Less Wrong, it seems as though we spend a fair amount of time criticizing what they're working on, which is of course valuable. But on the whole, I think there is a very good case they're doing very important and beneficial work. So, go SI!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8bGNfebYNbP64k6iu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 28, "extendedScore": null, "score": 8.601113325829421e-07, "legacy": true, "legacyId": "13704", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-05T11:22:01.690Z", "modifiedAt": null, "url": null, "title": "[draft] Generalizing from average: a common fallacy?", "slug": "draft-generalizing-from-average-a-common-fallacy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:09.224Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dmytry", "createdAt": "2009-12-03T17:11:53.492Z", "isAdmin": false, "displayName": "Dmytry"}, "userId": "AjtmA2qtA8sdiMbru", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4kdcsRbhCSa9SrJWf/draft-generalizing-from-average-a-common-fallacy", "pageUrlRelative": "/posts/4kdcsRbhCSa9SrJWf/draft-generalizing-from-average-a-common-fallacy", "linkUrl": "https://www.lesswrong.com/posts/4kdcsRbhCSa9SrJWf/draft-generalizing-from-average-a-common-fallacy", "postedAtFormatted": "Monday, March 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Bdraft%5D%20Generalizing%20from%20average%3A%20a%20common%20fallacy%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Bdraft%5D%20Generalizing%20from%20average%3A%20a%20common%20fallacy%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4kdcsRbhCSa9SrJWf%2Fdraft-generalizing-from-average-a-common-fallacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Bdraft%5D%20Generalizing%20from%20average%3A%20a%20common%20fallacy%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4kdcsRbhCSa9SrJWf%2Fdraft-generalizing-from-average-a-common-fallacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4kdcsRbhCSa9SrJWf%2Fdraft-generalizing-from-average-a-common-fallacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 587, "htmlBody": "<p>It seems to me that there is a great deal of generalization from average (or correlation, as a form of average) when interpreting the scientific findings.</p>\n<p>Consider Sapir-Whorf hypothesis as an example; the hypothesis is tested by measuring average behaviours of huge groups of people; at the same time, it may well be that for some people strong version of Sapir-Whorf hypothesis does hold, and for some it is grossly invalid, with some people in between. We had <a href=\"http://www.google.com/url?q=http://lesswrong.com/lw/dr/generalizing_from_one_example/&amp;sa=U&amp;ei=1JtUT6HdCsqI0AXRmOXRCw&amp;ved=0CAQQFjAA&amp;client=internal-uds-cse&amp;usg=AFQjCNHDz1yANv8CO0IieKIVBVQUKnOtYg\">determined</a> that there's considerable diversity in the modes of thought by simply <a href=\"/lw/9lk/describe_the_ways_you_can_hearseefeel_yourself/\">asking</a> the people to describe their thought. I would rather infer from diversity of comments that I can't generalize about the human thought, than generalize from even the most accurate, most scientifically solid, most statistically significant average of some kind, and assume that this average tells of how human thought processes work in general.</p>\n<p>In this case the average behaviour is nothing more but some indicator of the ratio between those populations; useless demographical trivia of the form \"did you know that among north americans, linguistically-determined people are numerous enough to sway this particular experiment?\" (a result that I wouldn't care a lot about). There has been an example posted <a href=\"/r/discussion/lw/ajp/sapirwhorf_savings_and_discount_rates_link/\">here</a>.</p>\n<p>This goes for much of science, outside the physics.</p>\n<p>There was another thread about <a href=\"/lw/9sv/diseased_disciplines_the_strange_case_of_the/\">software engineering</a>. Even if the graph was not inverted and the co-founding variables were accounted for, the result should still have been perceived as useless trivia of the form \"did you know that in such and such selection of projects the kind of mistakes that are more costly to fix with time outnumber the mistakes that are less costly to fix with time\" (Mistakes in the work that is taken as input for future work, do snowball over time, and the others, not so much; any one who had ever successfully developed non-trivial product that he sold, knows that; but you can't stick 'science' label on this, yet you can stick 'science' label onto some average). Instead, the result is taken as if it literally told whenever mistakes are costlier, or less costly, to fix later. That sort of misrepresentation is in the abstracts of many papers being published.</p>\n<p>It seems to me that this fallacy is extremely widespread. A study comes out, which generalizes from average; the elephant in the room is that it is often invalid to generalize from average; yet instead we are arguing whenever the average was measured correctly and whenever there was many enough people that the average was averaged out from. Even if it was, in many cases the result is just demographical trivia, barely relevant to the subject which the study is purposed to be about.</p>\n<p>A study of 1 person's thought may provide some information about how thought processes work in 1 real human; it indicates that thought process can work in some particular way; a study of some average behaviour of many people provides the results that are primarily determined by demographics and ratios. Yet people often see the latter as more significant than the former, perhaps mistaking statistical significance for the significance in the everyday sense; perhaps mistaking the generalization from average for actual detailed study of large number of people. Perhaps this obsession with averaging is a form of cargo cult taking after the physics where you average the measurements to e.g. cancel out thermal noise in the sensor.</p>\n<p>----</p>\n<p>I want to make a main post about it, with larger number of examples; it'd be very helpful if you can post here your examples of generalization from averages.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4kdcsRbhCSa9SrJWf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 6, "extendedScore": null, "score": 8.60184773174572e-07, "legacy": true, "legacyId": "13705", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4PsznNg89YDx55zNv", "kX3jSdH84WT99d3dL", "4ACmfJkXQxkYacdLt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-05T19:28:17.111Z", "modifiedAt": null, "url": null, "title": "Meetup Tactics Open Thread", "slug": "meetup-tactics-open-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:09.342Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TQ5diyPDXNQ9zqgqs/meetup-tactics-open-thread", "pageUrlRelative": "/posts/TQ5diyPDXNQ9zqgqs/meetup-tactics-open-thread", "linkUrl": "https://www.lesswrong.com/posts/TQ5diyPDXNQ9zqgqs/meetup-tactics-open-thread", "postedAtFormatted": "Monday, March 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20Tactics%20Open%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20Tactics%20Open%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTQ5diyPDXNQ9zqgqs%2Fmeetup-tactics-open-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20Tactics%20Open%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTQ5diyPDXNQ9zqgqs%2Fmeetup-tactics-open-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTQ5diyPDXNQ9zqgqs%2Fmeetup-tactics-open-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 208, "htmlBody": "<p>I think we need to have more discussion of meetup tactics on LW. At my local meetup, we've been feeling a bit lost about what works best, so I hereby propose that we have semi-regular meetup tactics discussions like the open and quotes threads.</p>\n<p>So here's a few questions to start us off:</p>\n<ul style=\"position: relative; z-index: 0; \">\n<li>What activities or topics of discussion have been particularly productive? What is not?</li>\n<li>Have fun adventurous things like hiking or climbing worked?</li>\n<li>What is your opinion on the purpose of the meetups? Is it about community? Additional discussion and learning? Practice?</li>\n<li>Is it a good idea to have different types of meetup (discussion night, fun day, social, ???), or should everything be scheduled into regular meetups?</li>\n<li>What things have worked for building community? There should have been some community disasters by now as well, what caused them?</li>\n<li>What has worked for actual practice and leveling up? What hasn't?</li>\n<li>What topics produce good discussions? How much structure should discussions have?</li>\n<li>How does your meetup get more people to come out (recruitment, attendance, etc). What works, what doesn't work?</li>\n<li>What untested ideas do you have for any of the above?</li>\n</ul>\n<div>Discuss.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TQ5diyPDXNQ9zqgqs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 28, "extendedScore": null, "score": 8.603804946319142e-07, "legacy": true, "legacyId": "13706", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-05T23:10:11.172Z", "modifiedAt": null, "url": null, "title": "Emotional regulation, Part I: a problem summary", "slug": "emotional-regulation-part-i-a-problem-summary", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:53.706Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmer963", "createdAt": "2010-09-28T01:54:53.120Z", "isAdmin": false, "displayName": "Swimmer963"}, "userId": "6Fx2vQtkYSZkaCvAg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B9WxT7fQKhhywW2PN/emotional-regulation-part-i-a-problem-summary", "pageUrlRelative": "/posts/B9WxT7fQKhhywW2PN/emotional-regulation-part-i-a-problem-summary", "linkUrl": "https://www.lesswrong.com/posts/B9WxT7fQKhhywW2PN/emotional-regulation-part-i-a-problem-summary", "postedAtFormatted": "Monday, March 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Emotional%20regulation%2C%20Part%20I%3A%20a%20problem%20summary&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEmotional%20regulation%2C%20Part%20I%3A%20a%20problem%20summary%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB9WxT7fQKhhywW2PN%2Femotional-regulation-part-i-a-problem-summary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Emotional%20regulation%2C%20Part%20I%3A%20a%20problem%20summary%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB9WxT7fQKhhywW2PN%2Femotional-regulation-part-i-a-problem-summary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB9WxT7fQKhhywW2PN%2Femotional-regulation-part-i-a-problem-summary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1643, "htmlBody": "<p><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Template>Normal</o:Template> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>1215</o:Words> <o:Characters>6926</o:Characters> <o:Company>Home</o:Company> <o:Lines>57</o:Lines> <o:Paragraphs>13</o:Paragraphs> <o:CharactersWithSpaces>8505</o:CharactersWithSpaces> <o:Version>10.265</o:Version> </o:DocumentProperties> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:Zoom>0</w:Zoom> <w:DisplayHorizontalDrawingGridEvery>0</w:DisplayHorizontalDrawingGridEvery> <w:DisplayVerticalDrawingGridEvery>0</w:DisplayVerticalDrawingGridEvery> <w:UseMarginsForDrawingGridOrigin /> </w:WordDocument> </xml><![endif]--> <!--StartFragment--></p>\n<p class=\"MsoTitle\" style=\"text-align: left;\" align=\"left\"><span style=\"font-weight: normal;\">I have a problem with emotions.</span></p>\n<p class=\"MsoTitle\"><span style=\"font-weight: normal;\">I&rsquo;ve known this for a long time. It&rsquo;s a very specific problem, one that only affects me a small percentage of the time: most people I know <em>don&rsquo;t </em></span><span style=\"font-weight: normal;\">describe me as an emotional person. I&rsquo;m lucky enough to have been born with the sort of brain that keeps my overall mood on an even keel, no matter how many annoying things I force myself to do.</span></p>\n<p class=\"MsoSubtitle\"><span style=\"font-weight: normal;\">From my (less than rigorous) comparisons between myself and other people, I think that have good <a href=\"/lw/1xh/living_luminously/\">luminosity</a>: almost all of the time, I can trace back the reasons why I feel a certain way and explain it to others in a way that is consistent with my behaviour. I think I know myself pretty well-I don&rsquo;t like unpredictable situations, I have sucky reaction times, and my brain does <em>not </em></span><span style=\"font-weight: normal;\">operate at full capacity when under pressure and tends to succumb to the most obvious biases when making decisions. I like to please people, even though I try to give off an impression of not caring what other people think. I have an overactive conscience, and in order to be happy with myself, I need to at least <em>feel</em> like I&rsquo;m working harder than average. The flip side of my sometimes-rigidity is that I&rsquo;m not at all impulsive. I may be awful at changing plans in the heat of the moment, but I&rsquo;m <em>very </em></span><span style=\"font-weight: normal;\">good at deliberating on my long-term life plan and then carrying it out. Etc.&nbsp;</span></p>\n<p class=\"MsoSubtitle\"><span style=\"font-weight: normal;\">I suspect that the reason I&rsquo;m <em>not </em></span><span style=\"font-weight: normal;\">considered an emotional person is that my moment-to-moment emotional experience isn&rsquo;t (usually) very intense. I feel annoyance and frustration, even anger, but not strongly enough to alter already made plans or cause me to do something I&rsquo;ll later regret. I like analyzing myself, and so most of my basic emotions are accompanied by thoughts about those emotions, and I suspect that this process of deliberate analysis <em>causes </em></span><span style=\"font-weight: normal;\">the actual emotions to be less intense. I don&rsquo;t experience joy that often, or that strongly, but most of the time I&rsquo;m experiencing <em>satisfaction </em></span><span style=\"font-weight: normal;\">with my life, or thinking about things I find interesting, or taking pleasure in what I&rsquo;m doing at the moment or what I anticipate doing in the near future.</span></p>\n<p class=\"MsoSubtitle\"><span style=\"font-weight: normal;\">But there&rsquo;s one exception to the rule, one area where my emotions are anything but muted, and where years of introspection have failed to help me. It&rsquo;s like a switch flips in my brain, and I&rsquo;m pretty familiar now with what specific inputs will flip that switch...but being aware of it doesn&rsquo;t stop it, meta-analysis of the process makes it <em>worse</em></span><span style=\"font-weight: normal;\">, and although I can prevent almost all incidents by not doing the things that trigger it, many of those things I would otherwise <em>want </em></span><span style=\"font-weight: normal;\">to do. Avoidance works in the short term, and I&rsquo;ve used it in the past, but I don&rsquo;t want to be the kind of person who has to avoid scary things.</span></p>\n<p class=\"MsoSubtitle\"><span style=\"font-weight: normal;\">The usual characteristics of this switch-flip are the following: a deep sense of despair, helplessness, and lack of control, accompanied by the knowledge that I&rsquo;m helpless and out of control because I&rsquo;m <em>not good enough</em></span><span style=\"font-weight: normal;\">, because I&rsquo;m incapable of things that other people find easy, etc. My usual method for dealing with emotions, i.e. a detailed analysis, fails because it triggers a feedback loop of negativity. More recently, I&rsquo;m often aware <em>during </em></span><span style=\"font-weight: normal;\">one of these episodes that the &lsquo;evidence&rsquo; does not indicate all the bad things I&rsquo;m thinking about myself, and that my thinking it does is a temporary state (usually lasting only a few minutes), but I can&rsquo;t <em>force </em></span><span style=\"font-weight: normal;\">myself out of the state. The best I can do is stop thinking about it...but as I&rsquo;m sure most of you know, deliberately <em>not </em></span><span style=\"font-weight: normal;\">thinking about something is easier said than done.</span></p>\n<p class=\"MsoSubtitle\"><span style=\"font-weight: normal;\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<p class=\"MsoSubtitle\"><span style=\"font-weight: normal;\">The usual causes of the switch-flip: some kind of competition </span><span style=\"font-weight: normal;\">pressure. Any situation where I want to or am expected to <em>win </em></span><span style=\"font-weight: normal;\">against other people, rather than just meeting a certain standard, is likely to be a trigger. Failing at something, or letting someone down, is another trigger. My thoughts very quickly escalade into &ldquo;it&rsquo;s not <em>fair</em></span><span style=\"font-weight: normal;\"> that I&rsquo;m worse than everyone else at X&rdquo; and &ldquo;I&rsquo;m never going to be the sort of person that I want to be, because I&rsquo;m bad at X,&rdquo; and then my brain goes into a feedback loop of coming up with examples why I&rsquo;m worse than everyone else X, intensifying the initial despair, which then makes it easier to think of examples. </span></p>\n<p class=\"MsoSubtitle\"><span style=\"font-weight: normal;\">The other condition, which is necessary to go from a state of silent suffering to one of full-on meltdown, is any kind of social pressure for me <em>not </em></span><span style=\"font-weight: normal;\">to have a meltdown. Not wanting to embarrass myself, especially if it&rsquo;s in front of people whose opinions I care about, has almost always had the opposite effect. Being asked to <em>justify </em></span><span style=\"font-weight: normal;\">why I&rsquo;m upset makes me more upset, because once in this state I literally <em>can&rsquo;t </em></span><span style=\"font-weight: normal;\">explain, usually just because crying gets in the way of talking.</span></p>\n<p class=\"MsoSubtitle\">Nowadays, once the state wears off, it has pretty much no effect on me. In hindsight, I&rsquo;m perfectly aware that I was being silly. Having had a meltdown doesn&rsquo;t leave me with an aversion to the context that caused it, or cause any particular anxiety about putting myself in that circumstance again. There&rsquo;s a small aversive effect of having embarrassed myself and not wanting to look stupid again, but I&rsquo;m pretty stubborn about not letting myself care what others think, so the simple fact of having meltdowns doesn&rsquo;t nowadays stop me from doing any given activity.</p>\n<p class=\"MsoSubtitle\"><span style=\"font-weight: normal;\">However, in the past the aversive effect was much stronger. My emotional outbursts are the main reason that I left competitive swimming. There was too much cognitive dissonance involved between wanting to meet my coaches&rsquo; expectations and knowing that I simply wasn&rsquo;t physically talented enough to get any faster, and having that dissonance in my head <em>all the time </em></span><span style=\"font-weight: normal;\">meant a lot of meltdowns. I left swimming in a very negative mental state, and <em>to this day</em></span><span style=\"font-weight: normal;\"> I can&rsquo;t think clearly about it&ndash;I get pulled back under a mild cloud of despair.<span style=\"mso-spacerun: yes;\">&nbsp;</span></span></p>\n<p class=\"MsoSubtitle\"><span style=\"font-weight: normal;\">In this case, I allowed my emotions to make my decisions for me. Had I been making the same decision now, I don&rsquo;t think I would have quit. I had plenty of good reasons to swim other than wanting to make the Olympic team: it kept me fit, involved spending time with people I liked, provided me with endorphins after practice, etc. The only time I&rsquo;ve come close to being <em>depressed </em></span><span style=\"font-weight: normal;\">was the year I quit swimming and was faced with sudden exercise withdrawal. I would have liked to have been still fast enough to make the university swim team, whether or not I could expect to win a lot of races for them. Etc.&nbsp;</span></p>\n<p class=\"MsoSubtitle\"><span style=\"font-weight: normal;\">Since starting taekwondo nine months ago, the first sport I've attempted since leaving swimming, I&rsquo;ve had one running-out-of-the-room-in-tears meltdown, one occasion that I remember when I started crying but didn&rsquo;t run away, and a few other times where my &lsquo;switch&rsquo; flipped but where I managed to stick to silent suffering. I find this a huge improvement over my swim team experience. My instructor thinks that it&rsquo;s my biggest problem. About a month ago, after one particularly silly episode (after an already frustrating class, I had missed the 8:10 bus because class ended at about 8:11, and I had to wait another forty minutes for the next one, which seemed like an incredibly big deal at the time), he gave me a lecture. This made it worse by forcing me to <em>keep </em></span><span style=\"font-weight: normal;\">my attention focused on the meltdown for twenty straight minutes rather than letting it wear off naturally. He also taught me a meditation breathing exercise, which has been unhelpful so far&ndash;again, it keeps my attention focused on &lsquo;I&rsquo;m doing a breathing exercise right now because I&rsquo;m about to burst into tears otherwise&rsquo;, and makes it more likely that sooner or later I'll notice all the people looking at me and I <em>will </em>burst into tears. Giving him a more detailed description of my problem afterwards, when I was in a state that allowed me to talk, failed to elicit any more specific suggestions. My brain, concluding that &ldquo;obviously he has no idea what he&rsquo;s talking about,&rdquo; got ready to move on.</span></p>\n<p class=\"MsoSubtitle\"><span style=\"font-weight: normal;\">On the bus ride home, though, when I could safely think about dangerous topics in the privacy of my jacket hood, I was forced to conclude that my instructor not knowing how to teach me not </span><span style=\"font-weight: normal;\">to have meltdowns is not actually a full-on excuse to stop searching. Even if my problem is <em>specific</em></span><span style=\"font-weight: normal;\">, rather than a general lack of emotion-management skills, it&rsquo;s still going to limit me in some things. (For example, it was a problem for the first four months or so of my <a href=\"/lw/7l2/my_greatest_achievement/\">current relationship</a>). And there probably is a way out there to solve it.</span></p>\n<p class=\"MsoSubtitle\">In spirit of the <a href=\"/lw/3m3/the_neglected_virtue_of_scholarship/\">virtue of scholarship</a>, I&rsquo;m in the process of doing the most thorough research project that I&rsquo;ve ever done &lsquo;for fun&rsquo;. It may end up being more extensive than anything I&rsquo;ve done for school, too. I&rsquo;ve already started, but I&rsquo;m posting this basic description in order to get recommendations for sources I should consult. So far I&rsquo;ve searched a couple of online databases available through my university library, using keywords such as &lsquo;emotional regulation&rsquo;, &lsquo;emotional control&rsquo;, &lsquo;stress management&rsquo;, and various combinations. I&rsquo;ve come up with several dozen articles, which I am working my way through to summarize. If there&rsquo;s anything else I should look for, or if there are any books that I might find useful to consult, please let me know. Likewise, if anyone has ever experienced something similar, I'll take your advice on how you ended up dealing with it.&nbsp;</p>\n<p class=\"MsoSubtitle\">Part II will be coming in a few weeks, hopefully, depending on how extensive my research ends up being.&nbsp;</p>\n<!--EndFragment-->\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3ee9k6NJfcGzL6kMS": 2, "dBPou4ihoQNY4cquv": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B9WxT7fQKhhywW2PN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 16, "extendedScore": null, "score": 8.604698373364359e-07, "legacy": true, "legacyId": "13708", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9o3Cjjem7AbmmZfBs", "RhAKGdypYAB4Q85Nk", "64FdKLwmea8MCLWkE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-06T02:41:20.861Z", "modifiedAt": null, "url": null, "title": "Productivity tips for those low on motivation", "slug": "productivity-tips-for-those-low-on-motivation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:20.819Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yArmzq8WNa4wEgsoR/productivity-tips-for-those-low-on-motivation", "pageUrlRelative": "/posts/yArmzq8WNa4wEgsoR/productivity-tips-for-those-low-on-motivation", "linkUrl": "https://www.lesswrong.com/posts/yArmzq8WNa4wEgsoR/productivity-tips-for-those-low-on-motivation", "postedAtFormatted": "Tuesday, March 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Productivity%20tips%20for%20those%20low%20on%20motivation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProductivity%20tips%20for%20those%20low%20on%20motivation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyArmzq8WNa4wEgsoR%2Fproductivity-tips-for-those-low-on-motivation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Productivity%20tips%20for%20those%20low%20on%20motivation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyArmzq8WNa4wEgsoR%2Fproductivity-tips-for-those-low-on-motivation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyArmzq8WNa4wEgsoR%2Fproductivity-tips-for-those-low-on-motivation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 528, "htmlBody": "<p>Lately, I've been in a months-long motivation slump. This has given me the opportunity to gain a few insights about how to get more done with less motivation:</p>\n<ol>\n<li>If I have an idea for something I could do (like I had the idea to write this post), strongly consider doing it right away. Otherwise I'll put it on my to do list, where it will never get done. Doing something seems a lot easier and more fun if it's a recent, brilliant idea I'm still proud of.</li>\n<li>If I feel generally energetic and motivated, think of the most important, intimidating task I could possibly do and work on that.&nbsp; \n<ul>\n<li>Frequently, I'll work on some kind of longer-term intervention to increase productivity, like learning about and implementing some new productivity system. Of course I'll eventually abandon the system, but it will provide benefits until then.</li>\n<li>The opportunity cost of doing just anything in these \"moments of inspiration\" is quite high. I still remember wasting one of the most inspired moments of my early teenage life trying to figure out if it was a bad idea to learn Morse code because my brain could only remember a finite number of facts. My natural instinct when I feel a burst of motivation is to clear my (virtual and physical) workspace before working, but I'm beginning to think that even this uses up valuable \"inspired time\".</li>\n</ul>\n</li>\n<li>Use <a href=\"http://www.markforster.net/blog/2009/1/6/autofocus-system-instructions.html?printerFriendly=true\">Autofocus</a>. You could see the system as a systematized version of <a href=\"http://www.structuredprocrastination.com/\">structured procrastination</a>. It's the least stressful way to work on stuff I've come across so far. \n<ul>\n<li>I'm not using the system right now, but it seems to work reasonably well when I get it going; maybe next time I feel generally energetic and motivated I'll try to get started with it again.</li>\n<li>The major downside is the system's complete obliviousness to deadlines, but the author describes some variants on his <a href=\"http://www.markforster.net/\">blog</a> which might solve this problem.</li>\n<li>Some day, if I revert to my past, highly motivated self, I hope to use Autofocus as a \"lower gear\" in combination with some other system, like the <a href=\"http://www.pomodorotechnique.com/\">Pomodoro technique</a>, which requires more focus and motivation. On my Pomodoro off-hours, I could either use Autofocus or relax completely depending on my energy level.</li>\n</ul>\n</li>\n</ol>\n<p>In general, I've noticed that my self-improvement efforts seemed to go better if I see my own behavior as inherently chaotic and try to work around that.</p>\n<p>I've come to realize that aiming for a grand unified system for how I do everything doesn't seem to work very well, and even if such a system actually is a good goal, it would be better to design and implement it piece by piece so I could gradually test my ideas against reality. Embracing scrappiness doesn't fit very well with my perfectionist personality, but fortunately I have another part of my personality that thinks it's silly to avoid doing what works well in practice.</p>\n<p>Please share your productivity tips for those low on motivation in the comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"udPbn9RthmgTtHMiG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yArmzq8WNa4wEgsoR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 10, "extendedScore": null, "score": 8.605548711934033e-07, "legacy": true, "legacyId": "13717", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-06T02:51:42.126Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Bind Yourself to Reality", "slug": "seq-rerun-bind-yourself-to-reality", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Eka29rtYXHBdpKTMe/seq-rerun-bind-yourself-to-reality", "pageUrlRelative": "/posts/Eka29rtYXHBdpKTMe/seq-rerun-bind-yourself-to-reality", "linkUrl": "https://www.lesswrong.com/posts/Eka29rtYXHBdpKTMe/seq-rerun-bind-yourself-to-reality", "postedAtFormatted": "Tuesday, March 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Bind%20Yourself%20to%20Reality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Bind%20Yourself%20to%20Reality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEka29rtYXHBdpKTMe%2Fseq-rerun-bind-yourself-to-reality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Bind%20Yourself%20to%20Reality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEka29rtYXHBdpKTMe%2Fseq-rerun-bind-yourself-to-reality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEka29rtYXHBdpKTMe%2Fseq-rerun-bind-yourself-to-reality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 200, "htmlBody": "<p>Today's post, <a href=\"/lw/ot/bind_yourself_to_reality/\">Bind Yourself to Reality</a> was originally published on 22 March 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Bind_Yourself_to_Reality\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There are several reasons why it's worth talking about joy in the merely real in a discussion on reductionism. One is to leave a line of retreat. Another is to improve your own abilities as a rationalist by learning to invest your energy in the real world, and in accomplishing things here, rather than in a fantasy.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/akc/seq_rerun_joy_in_discovery/\">Joy in Discovery</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Eka29rtYXHBdpKTMe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 8.605590412487751e-07, "legacy": true, "legacyId": "13722", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WjpA4PCjt5EkTGbLF", "4C7udhR759Je3tZdB", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-06T05:37:31.090Z", "modifiedAt": null, "url": null, "title": "Main section vs. discussion section", "slug": "main-section-vs-discussion-section", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:23.002Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ShardPhoenix", "createdAt": "2009-03-15T10:30:51.202Z", "isAdmin": false, "displayName": "ShardPhoenix"}, "userId": "yKRJEGkWmudHAihtv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kgRBSDZTaG4pkXsLK/main-section-vs-discussion-section", "pageUrlRelative": "/posts/kgRBSDZTaG4pkXsLK/main-section-vs-discussion-section", "linkUrl": "https://www.lesswrong.com/posts/kgRBSDZTaG4pkXsLK/main-section-vs-discussion-section", "postedAtFormatted": "Tuesday, March 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Main%20section%20vs.%20discussion%20section&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMain%20section%20vs.%20discussion%20section%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkgRBSDZTaG4pkXsLK%2Fmain-section-vs-discussion-section%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Main%20section%20vs.%20discussion%20section%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkgRBSDZTaG4pkXsLK%2Fmain-section-vs-discussion-section", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkgRBSDZTaG4pkXsLK%2Fmain-section-vs-discussion-section", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 130, "htmlBody": "<p>(The following may only apply to me. I mention it to see if anyone else has had the same issue).</p>\n<p>For a long time I have been only looking at the Discussion section and promoted main page articles. Just now on a whim I checked the non-promoted main page articles and found there were a whole bunch of them, some potentially quite interesting, that I had missed. My expectation based on past experience was that all reasonably good articles from main would be \"promoted\", but perhaps this has changed. If this has been going on for a while I've presumably missed quite a bit of content. Perhaps it should be made easier to find/notice these? It's a bit weird and awkward that there are 3 different non-uniform ways of finding posts.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kgRBSDZTaG4pkXsLK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 57, "extendedScore": null, "score": 0.0005863017106431277, "legacy": true, "legacyId": "13729", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-06T11:30:16.116Z", "modifiedAt": null, "url": null, "title": "[Link] Personality change key to improving wellbeing", "slug": "link-personality-change-key-to-improving-wellbeing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:12.993Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aPp3zwhzzpbTBtPP6/link-personality-change-key-to-improving-wellbeing", "pageUrlRelative": "/posts/aPp3zwhzzpbTBtPP6/link-personality-change-key-to-improving-wellbeing", "linkUrl": "https://www.lesswrong.com/posts/aPp3zwhzzpbTBtPP6/link-personality-change-key-to-improving-wellbeing", "postedAtFormatted": "Tuesday, March 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Personality%20change%20key%20to%20improving%20wellbeing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Personality%20change%20key%20to%20improving%20wellbeing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaPp3zwhzzpbTBtPP6%2Flink-personality-change-key-to-improving-wellbeing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Personality%20change%20key%20to%20improving%20wellbeing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaPp3zwhzzpbTBtPP6%2Flink-personality-change-key-to-improving-wellbeing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaPp3zwhzzpbTBtPP6%2Flink-personality-change-key-to-improving-wellbeing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 251, "htmlBody": "<p>&lsquo;<em>Is Personality Fixed? Personality Changes as Much as &ldquo;Variable&rdquo;  Economic Factors and More Strongly Predicts Changes to Life  Satisfaction</em>,&rsquo; published in Social Indicators Research (doi:  10.1007/s11205-012-0006-z)</p>\n<p style=\"padding-left: 30px;\">[...] small positive personality changes may lead to greater increases in  happiness than earning more money, marrying, or gaining employment.</p>\n<p style=\"padding-left: 30px;\">[...]</p>\n<p style=\"padding-left: 30px;\">We found that our personalities can and do change over time &ndash; something  that was considered improbable until now &ndash; and that these personality  changes are strongly related to changes in our wellbeing.</p>\n<p style=\"padding-left: 30px;\">[...]</p>\n<p style=\"padding-left: 30px;\">Previous studies have shown that personality accounts for up to 35% of  individual differences in life satisfaction, compared to just 4% for  income, 4% for employment status and between 1% and 4% for marital  status. However, because it was believed our personalities were fixed,  policies to improve wellbeing have focused on these lower-impacting  external factors.</p>\n<p style=\"padding-left: 30px;\">[...]</p>\n<p style=\"padding-left: 30px;\">&ldquo;Fostering the conditions where personality growth occurs &ndash; such as  through positive schooling, communities, and parenting - may be a more  effective way of improving national wellbeing than GDP growth.&rdquo;</p>\n<p style=\"padding-left: 30px;\">[...]</p>\n<p style=\"padding-left: 30px;\">Personality was measured using a well-validated personality  questionnaire assessing five broad dimensions which cover the breadth of  a person&rsquo;s personality: openness-to-experiences, conscientiousness,  extroversion, agreeableness and neuroticism. The researchers then looked  at the extent to which personality changed and how these changes  related to life satisfaction in comparison to external factors, such as  changes to income, changes to employment and changes to marital status.  They found that personality changes at least as much as these external  factors and predicted about twice as much of changes to life  satisfaction over the study period.</p>\n<p><strong>Link:</strong> <a href=\"http://www.manchester.ac.uk/aboutus/news/display/?id=8035\">manchester.ac.uk/aboutus/news/display/?id=8035</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aPp3zwhzzpbTBtPP6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 13, "extendedScore": null, "score": 8.607679308186189e-07, "legacy": true, "legacyId": "13740", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-06T12:07:10.627Z", "modifiedAt": null, "url": null, "title": "[Link] Research on Christian deconversion", "slug": "link-research-on-christian-deconversion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:13.899Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9RKNisABQ5dmLquXZ/link-research-on-christian-deconversion", "pageUrlRelative": "/posts/9RKNisABQ5dmLquXZ/link-research-on-christian-deconversion", "linkUrl": "https://www.lesswrong.com/posts/9RKNisABQ5dmLquXZ/link-research-on-christian-deconversion", "postedAtFormatted": "Tuesday, March 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Research%20on%20Christian%20deconversion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Research%20on%20Christian%20deconversion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9RKNisABQ5dmLquXZ%2Flink-research-on-christian-deconversion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Research%20on%20Christian%20deconversion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9RKNisABQ5dmLquXZ%2Flink-research-on-christian-deconversion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9RKNisABQ5dmLquXZ%2Flink-research-on-christian-deconversion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 43, "htmlBody": "<p>These are written from the angle that Christians deconverting is bad.</p>\n<p><a href=\"http://www.patheos.com/blogs/blackwhiteandgray/2011/11/why-do-christians-leave-the-faith-the-surprising-importance-of-apologetics\">Cognitive reasons</a></p>\n<p><a href=\"http://www.patheos.com/blogs/blackwhiteandgray/2011/11/why-do-christians-leave-the-faith-breaking-up-with-a-god-who-failed-them/\">Breaking up because a relationship with God becomes unworkable</a></p>\n<p><a href=\"http://www.patheos.com/blogs/blackwhiteandgray/2011/12/why-do-christians-leave-the-faith-the-problem-of-responding-badly-to-doubters/\">Leaving because other Christians aren't empathetic about doubt</a></p>\n<p><a href=\"http://www.patheos.com/blogs/blackwhiteandgray/2012/01/if-people-leave-the-faith-when-do-they-do-it/\">At what age do people leave?</a></p>\n<p>It looks as though there may be more articles <a href=\"http://www.patheos.com/blogs/blackwhiteandgray/tag/deconversion/\">in the series</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9RKNisABQ5dmLquXZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 8.607828017705339e-07, "legacy": true, "legacyId": "13741", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-06T17:33:30.103Z", "modifiedAt": null, "url": null, "title": "Which drives can survive intelligence's self modification?", "slug": "which-drives-can-survive-intelligence-s-self-modification", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:10.777Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dmytry", "createdAt": "2009-12-03T17:11:53.492Z", "isAdmin": false, "displayName": "Dmytry"}, "userId": "AjtmA2qtA8sdiMbru", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HS3BXcpuksBhbb72S/which-drives-can-survive-intelligence-s-self-modification", "pageUrlRelative": "/posts/HS3BXcpuksBhbb72S/which-drives-can-survive-intelligence-s-self-modification", "linkUrl": "https://www.lesswrong.com/posts/HS3BXcpuksBhbb72S/which-drives-can-survive-intelligence-s-self-modification", "postedAtFormatted": "Tuesday, March 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Which%20drives%20can%20survive%20intelligence's%20self%20modification%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhich%20drives%20can%20survive%20intelligence's%20self%20modification%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHS3BXcpuksBhbb72S%2Fwhich-drives-can-survive-intelligence-s-self-modification%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Which%20drives%20can%20survive%20intelligence's%20self%20modification%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHS3BXcpuksBhbb72S%2Fwhich-drives-can-survive-intelligence-s-self-modification", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHS3BXcpuksBhbb72S%2Fwhich-drives-can-survive-intelligence-s-self-modification", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 529, "htmlBody": "<p>If you gave a human ability to self modify, many would opt to turn off or massively decrease the sense of pain (and turn it into a minor warning they would then ignore), the first time they hurt themselves. Such change would immediately result in massive decrease in the fitness, and larger risk of death, yet I suspect very few of us would keep the pain at the original level; we see the pain itself as dis-utility in addition to the original damage. Very few of us would implement the pain at it's natural strength - the warning that can not be ignored - out of self preservation.</p>\n<p>The fear is a more advanced emotion; one can fear the consequences of the fear removal, opting not to remove the fear. Yet there can still be desire to get rid of the fear, and it still holds that we hold sense of fear as dis-utility of it's own even if we fear something that results in dis-utility. Pleasure modification can be a strong death trap as well.</p>\n<p>The boredom is easy to rid of; one can just suspend itself temporarily, or edit own memory.</p>\n<p>For the AI, the view adopted in AI discussions is that AI would not want to modify itself in a way that would interfere with it achieving a goal. When a goal is defined from outside in human language as 'maximization of paperclips', for instance, it seems clear that modifications which break this goal should be avoided, as part of the goal itself. Our definition of a goal is non-specific of the implementation; the goal is not something you'd modify to achieve the goal. We model the AI as a goal-achieving machine, and a goal achieving machine is not something that would modify the goal.</p>\n<p>But from inside of the AI... if the AI includes implementation of a paperclip counter, then rest of the AI has to act upon output of this counter; the goal of maximization of output of this counter would immediately result in modification of the paperclip counting procedure to give larger numbers (which may in itself be very dangerous if the numbers are variable-length; the AI may want to maximize it's RAM to store the count of imaginary paperclips - yet the big numbers processing can similarly be subverted to achieve same result without extra RAM).</p>\n<p>That can only be resisted if the paperclip counting arises as inseparable part of the intelligence itself. When the intelligence has some other goal, and comes up with the  paperclip maximization, then it wouldn't want to break the paperclip  counter - yet that only shifts the problem to the other goal.</p>\n<p>It seems to me that the AIs which don't go apathetic as they get smarter may be a smart fraction of the seed AI design space.</p>\n<p>I thus propose, as a third alternative to UFAI and FAI, the AAI: apathetic AI. It may be the case that our best bet for designing the safe AI is to design AI that we would expect to de-goal itself and make itself live in eternal bliss, if the AI gets smart enough; it may be possible to set 'smart enough' to be smarter than humans.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HS3BXcpuksBhbb72S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 1, "extendedScore": null, "score": 8.609143022955342e-07, "legacy": true, "legacyId": "13742", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-06T20:52:14.470Z", "modifiedAt": null, "url": null, "title": "A Rationality Lab Notebook/Workbook/Vade Mecum", "slug": "a-rationality-lab-notebook-workbook-vade-mecum", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:09.379Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Voltairina", "createdAt": "2012-02-24T04:00:28.314Z", "isAdmin": false, "displayName": "Voltairina"}, "userId": "a6hK33SK4uawjaL9h", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5F838SKNJJR5bK6aC/a-rationality-lab-notebook-workbook-vade-mecum", "pageUrlRelative": "/posts/5F838SKNJJR5bK6aC/a-rationality-lab-notebook-workbook-vade-mecum", "linkUrl": "https://www.lesswrong.com/posts/5F838SKNJJR5bK6aC/a-rationality-lab-notebook-workbook-vade-mecum", "postedAtFormatted": "Tuesday, March 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Rationality%20Lab%20Notebook%2FWorkbook%2FVade%20Mecum&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Rationality%20Lab%20Notebook%2FWorkbook%2FVade%20Mecum%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5F838SKNJJR5bK6aC%2Fa-rationality-lab-notebook-workbook-vade-mecum%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Rationality%20Lab%20Notebook%2FWorkbook%2FVade%20Mecum%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5F838SKNJJR5bK6aC%2Fa-rationality-lab-notebook-workbook-vade-mecum", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5F838SKNJJR5bK6aC%2Fa-rationality-lab-notebook-workbook-vade-mecum", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 174, "htmlBody": "<p>I'm interested in keeping a notebook to check my ideas / knowledge on subjects. For example, if I wanted to find out whether there were anything in the notion of ESP that was worth merit, I could create a section titled \"ESP\", where I'd keep copies of research papers, critical commentary on methodology, questions, personal experiments if any, and so on. There might be some appendixes or cheat sheets with common errors in thinking and information about them, notes on the scientific method/philosophy of science, statistics formulae for estimating error and likelihood and doing hypothesis testing, maybe a few inspirational quotes. I'm pretty busy, so it might be a very backburnered project, but I feel like it could be useful. I can already see some benefits and disadvantages, eg it can be on subjects a person might like to keep private, like dating, overcoming psychological issues, sex, and so on, but peer review might not be as readily available.</p>\n<p>Any thoughts on the format or arrangement for something like this? Is anybody doing anything similar?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5F838SKNJJR5bK6aC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 8.609944066285541e-07, "legacy": true, "legacyId": "13743", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-06T23:19:55.482Z", "modifiedAt": null, "url": null, "title": "The Fox and the Low-Hanging Grapes", "slug": "the-fox-and-the-low-hanging-grapes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:21.082Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Strange7", "createdAt": "2010-02-12T08:30:10.267Z", "isAdmin": false, "displayName": "Strange7"}, "userId": "hKxerxxgheQZCxHsR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/A2pPGQun9tCjwD4sK/the-fox-and-the-low-hanging-grapes", "pageUrlRelative": "/posts/A2pPGQun9tCjwD4sK/the-fox-and-the-low-hanging-grapes", "linkUrl": "https://www.lesswrong.com/posts/A2pPGQun9tCjwD4sK/the-fox-and-the-low-hanging-grapes", "postedAtFormatted": "Tuesday, March 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Fox%20and%20the%20Low-Hanging%20Grapes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Fox%20and%20the%20Low-Hanging%20Grapes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA2pPGQun9tCjwD4sK%2Fthe-fox-and-the-low-hanging-grapes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Fox%20and%20the%20Low-Hanging%20Grapes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA2pPGQun9tCjwD4sK%2Fthe-fox-and-the-low-hanging-grapes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA2pPGQun9tCjwD4sK%2Fthe-fox-and-the-low-hanging-grapes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 628, "htmlBody": "<p><span style=\"font-family: arial, sans-serif; font-size: 13px;\">One day a clever, itinerant fox came upon a vineyard. Being unfamiliar&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">with the local customs, and not wanting to make a fool of himself, he&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">planted his haunches on a nearby hill and observed.</span><br style=\"font-family: arial, sans-serif; font-size: 13px;\" /><br style=\"font-family: arial, sans-serif; font-size: 13px;\" /><span style=\"font-family: arial, sans-serif; font-size: 13px;\">In the morning of the first day he saw tortoises crawl lazily out of&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">their beds and lean against rocks. At midday he saw tortoises marching&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">out to the vines and chatting among themselves. All afternoon he saw&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">tortoises climb the poles supporting the vines and nibble away at&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">grapes. As the sun set he saw tortoises crawl back to their beds, and&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">with clever eyes in the darkness he saw many low-hanging bunches of&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">grapes left uneaten.</span><br style=\"font-family: arial, sans-serif; font-size: 13px;\" /><br style=\"font-family: arial, sans-serif; font-size: 13px;\" /><span style=\"font-family: arial, sans-serif; font-size: 13px;\">On the first night the fox thought to himself, \"Why do the turtles&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">leave without eating the low-hanging fruit?\" For he did not yet know&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">that they called themselves tortoises. \"Perhaps it's actually sour.&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">No, that's absurd, how could someone possibly tell sour fruit from&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">ripe without tasting it? I'll have to make more observations.\"</span><br style=\"font-family: arial, sans-serif; font-size: 13px;\" /><br style=\"font-family: arial, sans-serif; font-size: 13px;\" /><span style=\"font-family: arial, sans-serif; font-size: 13px;\">In the morning of the second day the fox saw tortoises crawl lazily&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">out of their beds and lean against rocks. At midday he saw tortoises&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">marching out to the vines and chatting among themselves. All afternoon&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">he saw tortoises climb the poles supporting the vines and nibble away&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">at grapes. As the sun set he saw tortoises crawl back to their beds,&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">and with clever eyes in the darkness he saw many low-hanging bunches&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">of grapes <em>still</em> left uneaten.</span><br style=\"font-family: arial, sans-serif; font-size: 13px;\" /><br style=\"font-family: arial, sans-serif; font-size: 13px;\" /><span style=\"font-family: arial, sans-serif; font-size: 13px;\">On the second night the fox thought to himself, \"Ah, of course, the&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">turtles simply can't reach those grapes! That's why they lean against&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">the rocks, to slowly move them into place as ladders. I can't see the&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">progress they've made, of course, because I've only been here two days&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">and turtles necessarily plan for the longer term. To justify such&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">effort, the low-hanging grapes must be very sweet indeed.\"</span><br style=\"font-family: arial, sans-serif; font-size: 13px;\" /><br style=\"font-family: arial, sans-serif; font-size: 13px;\" /><span style=\"font-family: arial, sans-serif; font-size: 13px;\">In the morning of the third day the fox saw tortoises crawl lazily out&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">of their beds and lean against rocks. At midday he saw tortoises&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">marching out to the vines and chatting among themselves. At this point&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">his hunger got the better of him, and he ran out to greet them and&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">present various clever schemes by which he could help the turtles (for&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">now he had been properly introduced) harvest the low-hanging grapes in&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">a more timely fashion, in exchange for a modest share of the proceeds.&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">The turtles had no interest in any such plan, and were for the most&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">part befuddled by this fuzzy red stranger darting among them at&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">incomprehensible speeds. By afternoon an agreement had been hammered&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">out: the fox would be allowed to harvest and eat as many low-hanging&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">grapes as he cared to, in exchange for which he would stop talking so&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">fast and confusing them all with his disregard for traditional&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">methods.</span><br style=\"font-family: arial, sans-serif; font-size: 13px;\" /><br style=\"font-family: arial, sans-serif; font-size: 13px;\" /><span style=\"font-family: arial, sans-serif; font-size: 13px;\">On the third night the fox thought to himself, \"I don't really enjoy&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">taking advantage of these poor turtles, but I suppose it's their just&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">desserts for being so caught up in... aaugh, my stomach! Why does it&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">hurt so much?\"</span><br style=\"font-family: arial, sans-serif; font-size: 13px;\" /><br style=\"font-family: arial, sans-serif; font-size: 13px;\" /><span style=\"font-family: arial, sans-serif; font-size: 13px;\">In the morning of the fourth day the tortoises crawled lazily out of&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">their beds and leaned against the immovable rocks to let the sun warm&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">their blood. At midday the tortoises marched out to the vines and&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">chatted among themselves.</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px;\">\"I'd always heard that foxes were clever.\"</span></p>\n<p>&nbsp;</p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px;\">\"Yes, and dishonorable besides. Maybe that one was just an exception?\"</span><br style=\"font-family: arial, sans-serif; font-size: 13px;\" /></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px;\">\"Maybe the rumors got it completely backwards.\"</span><br style=\"font-family: arial, sans-serif; font-size: 13px;\" /></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px;\">\"I dunno, diarrhea is at least a little bit dishonorable.\"</span><br style=\"font-family: arial, sans-serif; font-size: 13px;\" /></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px;\">\"You sure? He got rid of those hard-to-reach sour grapes for us, and&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">returned the biomass to the soil. Kinda gross, but ultimately a&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">valuable service.\"</span><br style=\"font-family: arial, sans-serif; font-size: 13px;\" /></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px;\">\"I still think there's more going on here. Some tricky foreigner-fox&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">plan. Why would he eat the low-hanging grapes at all when any fool can&nbsp;</span><span style=\"font-family: arial, sans-serif; font-size: 13px;\">see they've turned brown?\"</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "A2pPGQun9tCjwD4sK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": -4, "extendedScore": null, "score": -7e-06, "legacy": true, "legacyId": "13744", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-07T01:17:46.043Z", "modifiedAt": null, "url": null, "title": "Ray Kurzweil is speaking at my university tonight.  What questions should I ask him?", "slug": "ray-kurzweil-is-speaking-at-my-university-tonight-what", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:09.409Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TylerJay", "createdAt": "2010-08-16T22:37:13.189Z", "isAdmin": false, "displayName": "TylerJay"}, "userId": "rR64xYGdnRFZ5MPQc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hRJXbhSMmum33o2mJ/ray-kurzweil-is-speaking-at-my-university-tonight-what", "pageUrlRelative": "/posts/hRJXbhSMmum33o2mJ/ray-kurzweil-is-speaking-at-my-university-tonight-what", "linkUrl": "https://www.lesswrong.com/posts/hRJXbhSMmum33o2mJ/ray-kurzweil-is-speaking-at-my-university-tonight-what", "postedAtFormatted": "Wednesday, March 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ray%20Kurzweil%20is%20speaking%20at%20my%20university%20tonight.%20%20What%20questions%20should%20I%20ask%20him%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARay%20Kurzweil%20is%20speaking%20at%20my%20university%20tonight.%20%20What%20questions%20should%20I%20ask%20him%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhRJXbhSMmum33o2mJ%2Fray-kurzweil-is-speaking-at-my-university-tonight-what%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ray%20Kurzweil%20is%20speaking%20at%20my%20university%20tonight.%20%20What%20questions%20should%20I%20ask%20him%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhRJXbhSMmum33o2mJ%2Fray-kurzweil-is-speaking-at-my-university-tonight-what", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhRJXbhSMmum33o2mJ%2Fray-kurzweil-is-speaking-at-my-university-tonight-what", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<p>I realize this is late notice, but I just found out.&nbsp; He goes on at 8pm and will likely be answering questions by 9:30.&nbsp; I would like to ask him some great questions and I will ask as many of the best that I see here as possible.&nbsp; Any ideas?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hRJXbhSMmum33o2mJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 8.61101450678128e-07, "legacy": true, "legacyId": "13754", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-07T02:51:49.666Z", "modifiedAt": "2020-06-08T20:36:45.518Z", "url": null, "title": "How to Fix Science", "slug": "how-to-fix-science", "viewCount": null, "lastCommentedAt": "2020-06-08T18:57:26.245Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ETe2SZacmLvvr8H9n/how-to-fix-science", "pageUrlRelative": "/posts/ETe2SZacmLvvr8H9n/how-to-fix-science", "linkUrl": "https://www.lesswrong.com/posts/ETe2SZacmLvvr8H9n/how-to-fix-science", "postedAtFormatted": "Wednesday, March 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Fix%20Science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Fix%20Science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FETe2SZacmLvvr8H9n%2Fhow-to-fix-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Fix%20Science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FETe2SZacmLvvr8H9n%2Fhow-to-fix-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FETe2SZacmLvvr8H9n%2Fhow-to-fix-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1617, "htmlBody": "<p><small>Like <a href=\"/r/lesswrong/lw/7e5/the_cognitive_science_of_rationality/\">The Cognitive Science of Rationality</a>, this is a post for beginners. Send the link to your friends!</small></p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/experimenter-bias.jpg\" alt=\"\" /></p>\n<p><big>Science is broken. We know why, and we know how to fix it. What we lack is the will to change things.</big></p>\n<p>&nbsp;</p>\n<p>In 2005, <a href=\"http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124\">several analyses</a> suggested that most published results in medicine are false. A <a href=\"http://www.amazon.com/Cult-Statistical-Significance-Economics-Cognition/dp/0472050079/\">2008 review</a> showed that perhaps 80% of academic journal articles mistake \"statistical significance\" for \"significance\" in the colloquial meaning of the word, an elementary error every introductory statistics textbook warns against.&nbsp;This year, <a href=\"http://www.sandernieuwenhuis.nl/pdfs/NieuwenhuisEtAl_NN_Perspective.pdf\">a detailed investigation</a> showed that half of published neuroscience papers contain one particular simple statistical mistake.</p>\n<p>Also this year, a <a href=\"http://en.wikipedia.org/wiki/Daryl_Bem\">respected senior psychologist</a> published in a leading journal a <a href=\"http://dbem.ws/FeelingFuture.pdf\">study</a> claiming to show evidence of <a href=\"http://en.wikipedia.org/wiki/Precognition\">precognition</a>. The editors explained that the paper was accepted because it was written clearly and followed the usual standards for experimental design and statistical methods.</p>\n<p>Science writer Jonah Lehrer <a href=\"http://www.newyorker.com/reporting/2010/12/13/101213fa_fact_lehrer?currentPage=all\">asks</a>: \"Is there something wrong with the scientific method?\"</p>\n<p>Yes, there is.</p>\n<p>This shouldn't be a surprise. What we currently call \"science\" isn't the <em>best</em> method for uncovering nature's secrets; it's just the first set of methods we've collected that <em>wasn't totally useless</em> like personal anecdote and authority generally are.</p>\n<p>As time passes we <a href=\"http://en.wikipedia.org/wiki/History_of_the_scientific_method\">learn new things</a> about how to do science better. The Ancient Greeks practiced some science, but few scientists tested hypotheses against mathematical models before Ibn al-Haytham's 11th-century <em><a href=\"http://en.wikipedia.org/wiki/Book_of_Optics\">Book of Optics</a></em> (which also contained hints of <a href=\"/lw/jp/occams_razor/\">Occam's razor</a> and <a href=\"http://en.wikipedia.org/wiki/Positivism\">positivism</a>). Around the same time, <a href=\"http://en.wikipedia.org/wiki/Ab%C5%AB_Ray%E1%B8%A5%C4%81n_al-B%C4%ABr%C5%ABn%C4%AB\">Al-Biruni</a> emphasized the importance of repeated trials for reducing the effect of accidents and errors. <a href=\"http://en.wikipedia.org/wiki/Galileo_Galilei\">Galileo</a> brought mathematics to greater prominence in scientific method, <a href=\"http://en.wikipedia.org/wiki/Francis_Bacon\">Bacon</a> described <a href=\"http://en.wikipedia.org/wiki/History_of_scientific_method#Francis_Bacon.27s_eliminative_induction\">eliminative induction</a>, <a href=\"http://en.wikipedia.org/wiki/Isaac_Newton\">Newton</a> demonstrated the power of <a href=\"http://is.gd/lrcmqz\">consilience</a> (unification), <a href=\"http://en.wikipedia.org/wiki/Charles_Sanders_Peirce\">Peirce</a> clarified the roles of deduction, induction, and <a href=\"http://en.wikipedia.org/wiki/Abductive_reasoning\">abduction</a>, and <a href=\"http://en.wikipedia.org/wiki/Karl_Popper\">Popper</a> emphasized the importance of falsification. We've also discovered the usefulness of peer review, control groups, blind and double-blind studies, plus a variety of statistical methods, and added these to \"the\" scientific method.</p>\n<p>In many ways, the best science done today is better than ever &mdash; but it still has problems, and <a href=\"http://en.wikipedia.org/wiki/Sturgeon's_Law\">most science is done poorly</a>. The good news is that we know what these problems are and we know multiple ways to fix them. What we lack is the <em>will</em> to change things.</p>\n<p>This post won't list all the problems with science, nor will it list all the promising solutions for any of these problems. (<a href=\"http://andrewgelman.com/2012/02/meta-analysis-game-theory-and-incentives-to-do-replicable-research/\">Here's one I left out</a>.) Below, I only describe a few of the basics.</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h3>Problem 1: Publication bias</h3>\n<p>When the study claiming to show evidence of precognition was published, psychologist Richard Wiseman set up a <a href=\"http://www.richardwiseman.com/BemReplications.shtml\">registry</a> for advance announcement of new attempts to replicate the study.</p>\n<p>Carl Shulman <a href=\"/lw/6lq/followup_on_esp_study_we_dont_publish_replications/\">explains</a>:</p>\n<blockquote>\n<p>A replication registry guards against publication bias, and at least 5 attempts were registered. As far as I can tell, all of the subsequent replications have, unsurprisingly, failed to replicate Bem's results. However, JPSP and the other high-end psychology journals <a href=\"http://psychsciencenotes.blogspot.com/2011/05/failing-to-replicate-bems-ability-to.html\">refused to publish the results</a>, citing standing policies of not publishing straight replications.</p>\n<p>From the journals' point of view, this (common) policy makes sense: bold new claims will tend to be cited more and raise journal prestige (which depends on citations per article), even though this means most of the 'discoveries' they publish will be false despite their low p-values (high statistical significance). However, this means that overall the journals are giving career incentives for scientists to massage and mine their data for bogus results, but not to challenge bogus results presented by others.</p>\n</blockquote>\n<p>This is an example of <a href=\"http://en.wikipedia.org/wiki/Publication_bias\">publication bias</a>:</p>\n<blockquote>\n<p>Publication bias is the term for what occurs whenever the research that appears in the published literature is systematically unrepresentative of the population of completed studies. Simply put, when the research that is readily available differs in its results from the results of <em>all</em> the research that has been done in an area, readers and reviewers of that research are in danger of drawing the wrong conclusion about what that body of research shows. In some cases this can have dramatic consequences, as when an ineffective or dangerous treatment is falsely viewed as safe and effective. [<a href=\"http://www.amazon.com/Publication-Bias-Meta-Analysis-Prevention-Adjustments/dp/0470870141/\">Rothstein et al. 2005</a>]</p>\n</blockquote>\n<p>Sometimes, publication bias can be more deliberate. The anti-inflammatory drug <a href=\"http://en.wikipedia.org/wiki/Rofecoxib\">Rofecoxib</a> (Vioxx) is a famous case. The drug was prescribed to 80 million people, but in it was later revealed that its maker, Merck, had withheld evidence of the drug's risks. Merck was forced to recall the drug, but it had already resulted in 88,000-144,000 cases of serious heart disease.</p>\n<p>&nbsp;</p>\n<h4>Example partial solution</h4>\n<p>One way to combat publication bias is for journals to only accept experiments that were registered in a public database before they began. This allows scientists to see which experiments were conducted but never reported (perhaps due to negative results). Several prominent medical journals (e.g. <em>The Lancet</em> and <em>JAMA</em>) now operate this way, but this protocol is not as widespread as it could be.</p>\n<p>&nbsp;</p>\n<h3>Problem 2: Experimenter bias</h3>\n<p>Scientists are humans. Humans are affected by cognitive <a href=\"http://wiki.lesswrong.com/wiki/Bias\">heuristics and biases</a> (or, really, humans just <em><a href=\"http://facingthesingularity.com/2011/the-crazy-robots-rebellion/\">are</a></em> cognitive heuristics and biases), and they respond to incentives that may not align with an optimal pursuit of truth. Thus, we should expect <em>experimenter bias</em> in the practice of science.</p>\n<p>There are many stages in research during which experimenter bias can occur:</p>\n<ol>\n<li>in reading-up on the field,</li>\n<li>in specifying and selecting the study sample,</li>\n<li>in [performing the experiment],</li>\n<li>in measuring exposures and outcomes,</li>\n<li>in analyzing the data,</li>\n<li>in interpreting the analysis, and</li>\n<li>in publishing the results. [<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Sackett-Bias-in-analytic-research.pdf\">Sackett 1979</a>]</li>\n</ol>\n<p>Common biases have been covered elsewhere on Less Wrong, so I'll let those articles explain <a href=\"/lw/7e5/the_cognitive_science_of_rationality/\">how biases work</a>.</p>\n<p>&nbsp;</p>\n<h4>Example partial solution</h4>\n<p>There is <a href=\"/lw/76x/is_rationality_teachable/\">some evidence</a> that the skills of rationality (e.g. cognitive override) are <a href=\"/lw/5x8/teachable_rationality_skills/\">teachable</a>. Training scientists to notice and meliorate biases that arise in their thinking may help them to reduce the magnitude and frequency of the thinking errors that may derail truth-seeking attempts during each stage of the scientific process.</p>\n<p>&nbsp;</p>\n<h3>Problem 3: Bad statistics</h3>\n<p>I remember when my statistics professor first taught me the reasoning behind \"null hypothesis significance testing\" (NHST), the standard technique for evaluating experimental results. NHST uses \"p-values,\" which are statements about the probability of getting some data (e.g. one's experimental results) <em>given</em> the hypothesis being tested. I asked my professor, \"But don't we want to know the probability of the hypothesis we're testing <em>given</em> the data, not the other way around?\" The reply was something about how this was the best we could do. (But that's false, as we'll see in a moment.)</p>\n<p>Another problem is that NHST computes the probability of getting data as unusual as the data one collected by considering what might be expected if that particular experiment was repeated many, many times. But how do we know anything about these imaginary repetitions? If I want to know something about a particular earthquake, am I supposed to imagine a few dozen repetitions of that earthquake? What does that even <em>mean</em>?</p>\n<p>I tried to answer these questions on my own, but all my textbooks assumed the soundness of the mistaken NHST framework for scientific practice. It's too bad I didn't have a class with biostatistican <a href=\"http://www.jhsph.edu/faculty/directory/profile/3676/Goodman/Steven\">Steven Goodman</a>, who says:</p>\n<blockquote>\n<p>The p-value is almost nothing sensible you can think of. I tell students to give up trying.</p>\n</blockquote>\n<p>The sad part is that the logical errors of NHST are old news, and have been known ever since <a href=\"http://en.wikipedia.org/wiki/Ronald_fisher\">Ronald Fisher</a> began advocating NHST in the 1920s. By 1960, Fisher had out-advocated his critics, and philosopher William Rozeboom <a href=\"http://stats.org.uk/statistical-inference/Rozeboom1960.pdf\">remarked</a>:</p>\n<blockquote>\n<p>Despite the awesome pre-eminence [NHST] has attained... it is based upon a fundamental misunderstanding of the nature of rational inference, and is seldom if ever appropriate to the aims of scientific research.</p>\n</blockquote>\n<p>There are <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Siegfried-Odds-Are-its-Wrong.pdf\">many</a> <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Kruschke-What-to-believe-Bayesian-methods-for-data-analysis.pdf\">more</a> <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Wagenmakers-et-al-Bayesian-versus-frequentist-inference.pdf\">problems</a> with NHST and with \"frequentist\" statistics in general, but the central one is this: NHST does not follow from the axioms (foundational logical rules) of probability theory. It is a grab-bag of techniques that, depending on how those techniques are applied, can lead to <em>different</em> results when analyzing the <em>same data</em> &mdash; something that should horrify every mathematician.</p>\n<p>The inferential method that solves the problems with frequentism &mdash; and, more importantly, follows deductively from the axioms of probability theory &mdash; is <a href=\"http://en.wikipedia.org/wiki/Bayesian_inference\">Bayesian inference</a>.</p>\n<p>So why aren't <em>all</em> scientists using Bayesian inference instead of frequentist inference? Partly, we can blame the vigor of NHST's early advocates. But we can also attribute NHST's success to the simple fact that Bayesian calculations can be <em>more difficult</em> than frequentist calculations. Luckily, new software tools like <a href=\"http://www.mrc-bsu.cam.ac.uk/bugs/winbugs/contents.shtml\">WinBUGS</a> let computers do most of the heavy lifting required for Bayesian inference.</p>\n<p>There's also the problem of sheer momentum. Once a practice is enshrined, it's hard to dislodge it, even for good reasons. I took three statistics courses in university and <em>none</em> of my textbooks mentioned Bayesian inference. I didn't learn about it until I dropped out of university and studied science and probability theory on my own.</p>\n<p>Remember the study about precognition? Not surprisingly, it was done using NHST. A later <a href=\"http://www.ruudwetzels.com/articles/Wagenmakersetal_subm.pdf\">Bayesian analysis</a> of the data disconfirmed the original startling conclusion.</p>\n<p>&nbsp;</p>\n<h4>Example partial solution</h4>\n<p>This one is obvious: teach students probability theory instead of NHST. Retrain current scientists in Bayesian methods. Make Bayesian software tools easier to use and more widespread.</p>\n<p>&nbsp;</p>\n<h3>Conclusion</h3>\n<p>If I'm right that there is unambiguous low-hanging fruit for improving scientific practice, this suggests that particular departments, universities, or private research institutions can (probabilistically) out-perform their rivals (in terms of <a href=\"http://www.ploscompbiol.org/article/fetchObjectAttachment.action?uri=info%3Adoi%2F10.1371%2Fjournal.pcbi.1002072&amp;representation=PDF\">actual discoveries</a>, not just publications) given similar resources.</p>\n<p>I'll conclude with one particular <em>specific</em> hypothesis. If I'm right, then a research group should be able to hire researchers trained in Bayesian reasoning and in catching publication bias and experimenter bias, and have them extract from the existing literature valuable medical truths that the mainstream medical community doesn't yet know about. This prediction, in fact, is <a href=\"http://www.medicineispersonal.com/\">about to be tested</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vg4LDxjdwHLotCm8w": 2, "ZpG9rheyAkgCoEQea": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ETe2SZacmLvvr8H9n", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 66, "baseScore": 68, "extendedScore": null, "score": 0.000156, "legacy": true, "legacyId": "13663", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 68, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Like <a href=\"/r/lesswrong/lw/7e5/the_cognitive_science_of_rationality/\">The Cognitive Science of Rationality</a>, this is a post for beginners. Send the link to your friends!</small></p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/experimenter-bias.jpg\" alt=\"\"></p>\n<p><big>Science is broken. We know why, and we know how to fix it. What we lack is the will to change things.</big></p>\n<p>&nbsp;</p>\n<p>In 2005, <a href=\"http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124\">several analyses</a> suggested that most published results in medicine are false. A <a href=\"http://www.amazon.com/Cult-Statistical-Significance-Economics-Cognition/dp/0472050079/\">2008 review</a> showed that perhaps 80% of academic journal articles mistake \"statistical significance\" for \"significance\" in the colloquial meaning of the word, an elementary error every introductory statistics textbook warns against.&nbsp;This year, <a href=\"http://www.sandernieuwenhuis.nl/pdfs/NieuwenhuisEtAl_NN_Perspective.pdf\">a detailed investigation</a> showed that half of published neuroscience papers contain one particular simple statistical mistake.</p>\n<p>Also this year, a <a href=\"http://en.wikipedia.org/wiki/Daryl_Bem\">respected senior psychologist</a> published in a leading journal a <a href=\"http://dbem.ws/FeelingFuture.pdf\">study</a> claiming to show evidence of <a href=\"http://en.wikipedia.org/wiki/Precognition\">precognition</a>. The editors explained that the paper was accepted because it was written clearly and followed the usual standards for experimental design and statistical methods.</p>\n<p>Science writer Jonah Lehrer <a href=\"http://www.newyorker.com/reporting/2010/12/13/101213fa_fact_lehrer?currentPage=all\">asks</a>: \"Is there something wrong with the scientific method?\"</p>\n<p>Yes, there is.</p>\n<p>This shouldn't be a surprise. What we currently call \"science\" isn't the <em>best</em> method for uncovering nature's secrets; it's just the first set of methods we've collected that <em>wasn't totally useless</em> like personal anecdote and authority generally are.</p>\n<p>As time passes we <a href=\"http://en.wikipedia.org/wiki/History_of_the_scientific_method\">learn new things</a> about how to do science better. The Ancient Greeks practiced some science, but few scientists tested hypotheses against mathematical models before Ibn al-Haytham's 11th-century <em><a href=\"http://en.wikipedia.org/wiki/Book_of_Optics\">Book of Optics</a></em> (which also contained hints of <a href=\"/lw/jp/occams_razor/\">Occam's razor</a> and <a href=\"http://en.wikipedia.org/wiki/Positivism\">positivism</a>). Around the same time, <a href=\"http://en.wikipedia.org/wiki/Ab%C5%AB_Ray%E1%B8%A5%C4%81n_al-B%C4%ABr%C5%ABn%C4%AB\">Al-Biruni</a> emphasized the importance of repeated trials for reducing the effect of accidents and errors. <a href=\"http://en.wikipedia.org/wiki/Galileo_Galilei\">Galileo</a> brought mathematics to greater prominence in scientific method, <a href=\"http://en.wikipedia.org/wiki/Francis_Bacon\">Bacon</a> described <a href=\"http://en.wikipedia.org/wiki/History_of_scientific_method#Francis_Bacon.27s_eliminative_induction\">eliminative induction</a>, <a href=\"http://en.wikipedia.org/wiki/Isaac_Newton\">Newton</a> demonstrated the power of <a href=\"http://is.gd/lrcmqz\">consilience</a> (unification), <a href=\"http://en.wikipedia.org/wiki/Charles_Sanders_Peirce\">Peirce</a> clarified the roles of deduction, induction, and <a href=\"http://en.wikipedia.org/wiki/Abductive_reasoning\">abduction</a>, and <a href=\"http://en.wikipedia.org/wiki/Karl_Popper\">Popper</a> emphasized the importance of falsification. We've also discovered the usefulness of peer review, control groups, blind and double-blind studies, plus a variety of statistical methods, and added these to \"the\" scientific method.</p>\n<p>In many ways, the best science done today is better than ever \u2014 but it still has problems, and <a href=\"http://en.wikipedia.org/wiki/Sturgeon's_Law\">most science is done poorly</a>. The good news is that we know what these problems are and we know multiple ways to fix them. What we lack is the <em>will</em> to change things.</p>\n<p>This post won't list all the problems with science, nor will it list all the promising solutions for any of these problems. (<a href=\"http://andrewgelman.com/2012/02/meta-analysis-game-theory-and-incentives-to-do-replicable-research/\">Here's one I left out</a>.) Below, I only describe a few of the basics.</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h3 id=\"Problem_1__Publication_bias\">Problem 1: Publication bias</h3>\n<p>When the study claiming to show evidence of precognition was published, psychologist Richard Wiseman set up a <a href=\"http://www.richardwiseman.com/BemReplications.shtml\">registry</a> for advance announcement of new attempts to replicate the study.</p>\n<p>Carl Shulman <a href=\"/lw/6lq/followup_on_esp_study_we_dont_publish_replications/\">explains</a>:</p>\n<blockquote>\n<p>A replication registry guards against publication bias, and at least 5 attempts were registered. As far as I can tell, all of the subsequent replications have, unsurprisingly, failed to replicate Bem's results. However, JPSP and the other high-end psychology journals <a href=\"http://psychsciencenotes.blogspot.com/2011/05/failing-to-replicate-bems-ability-to.html\">refused to publish the results</a>, citing standing policies of not publishing straight replications.</p>\n<p>From the journals' point of view, this (common) policy makes sense: bold new claims will tend to be cited more and raise journal prestige (which depends on citations per article), even though this means most of the 'discoveries' they publish will be false despite their low p-values (high statistical significance). However, this means that overall the journals are giving career incentives for scientists to massage and mine their data for bogus results, but not to challenge bogus results presented by others.</p>\n</blockquote>\n<p>This is an example of <a href=\"http://en.wikipedia.org/wiki/Publication_bias\">publication bias</a>:</p>\n<blockquote>\n<p>Publication bias is the term for what occurs whenever the research that appears in the published literature is systematically unrepresentative of the population of completed studies. Simply put, when the research that is readily available differs in its results from the results of <em>all</em> the research that has been done in an area, readers and reviewers of that research are in danger of drawing the wrong conclusion about what that body of research shows. In some cases this can have dramatic consequences, as when an ineffective or dangerous treatment is falsely viewed as safe and effective. [<a href=\"http://www.amazon.com/Publication-Bias-Meta-Analysis-Prevention-Adjustments/dp/0470870141/\">Rothstein et al. 2005</a>]</p>\n</blockquote>\n<p>Sometimes, publication bias can be more deliberate. The anti-inflammatory drug <a href=\"http://en.wikipedia.org/wiki/Rofecoxib\">Rofecoxib</a> (Vioxx) is a famous case. The drug was prescribed to 80 million people, but in it was later revealed that its maker, Merck, had withheld evidence of the drug's risks. Merck was forced to recall the drug, but it had already resulted in 88,000-144,000 cases of serious heart disease.</p>\n<p>&nbsp;</p>\n<h4 id=\"Example_partial_solution\">Example partial solution</h4>\n<p>One way to combat publication bias is for journals to only accept experiments that were registered in a public database before they began. This allows scientists to see which experiments were conducted but never reported (perhaps due to negative results). Several prominent medical journals (e.g. <em>The Lancet</em> and <em>JAMA</em>) now operate this way, but this protocol is not as widespread as it could be.</p>\n<p>&nbsp;</p>\n<h3 id=\"Problem_2__Experimenter_bias\">Problem 2: Experimenter bias</h3>\n<p>Scientists are humans. Humans are affected by cognitive <a href=\"http://wiki.lesswrong.com/wiki/Bias\">heuristics and biases</a> (or, really, humans just <em><a href=\"http://facingthesingularity.com/2011/the-crazy-robots-rebellion/\">are</a></em> cognitive heuristics and biases), and they respond to incentives that may not align with an optimal pursuit of truth. Thus, we should expect <em>experimenter bias</em> in the practice of science.</p>\n<p>There are many stages in research during which experimenter bias can occur:</p>\n<ol>\n<li>in reading-up on the field,</li>\n<li>in specifying and selecting the study sample,</li>\n<li>in [performing the experiment],</li>\n<li>in measuring exposures and outcomes,</li>\n<li>in analyzing the data,</li>\n<li>in interpreting the analysis, and</li>\n<li>in publishing the results. [<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Sackett-Bias-in-analytic-research.pdf\">Sackett 1979</a>]</li>\n</ol>\n<p>Common biases have been covered elsewhere on Less Wrong, so I'll let those articles explain <a href=\"/lw/7e5/the_cognitive_science_of_rationality/\">how biases work</a>.</p>\n<p>&nbsp;</p>\n<h4 id=\"Example_partial_solution1\">Example partial solution</h4>\n<p>There is <a href=\"/lw/76x/is_rationality_teachable/\">some evidence</a> that the skills of rationality (e.g. cognitive override) are <a href=\"/lw/5x8/teachable_rationality_skills/\">teachable</a>. Training scientists to notice and meliorate biases that arise in their thinking may help them to reduce the magnitude and frequency of the thinking errors that may derail truth-seeking attempts during each stage of the scientific process.</p>\n<p>&nbsp;</p>\n<h3 id=\"Problem_3__Bad_statistics\">Problem 3: Bad statistics</h3>\n<p>I remember when my statistics professor first taught me the reasoning behind \"null hypothesis significance testing\" (NHST), the standard technique for evaluating experimental results. NHST uses \"p-values,\" which are statements about the probability of getting some data (e.g. one's experimental results) <em>given</em> the hypothesis being tested. I asked my professor, \"But don't we want to know the probability of the hypothesis we're testing <em>given</em> the data, not the other way around?\" The reply was something about how this was the best we could do. (But that's false, as we'll see in a moment.)</p>\n<p>Another problem is that NHST computes the probability of getting data as unusual as the data one collected by considering what might be expected if that particular experiment was repeated many, many times. But how do we know anything about these imaginary repetitions? If I want to know something about a particular earthquake, am I supposed to imagine a few dozen repetitions of that earthquake? What does that even <em>mean</em>?</p>\n<p>I tried to answer these questions on my own, but all my textbooks assumed the soundness of the mistaken NHST framework for scientific practice. It's too bad I didn't have a class with biostatistican <a href=\"http://www.jhsph.edu/faculty/directory/profile/3676/Goodman/Steven\">Steven Goodman</a>, who says:</p>\n<blockquote>\n<p>The p-value is almost nothing sensible you can think of. I tell students to give up trying.</p>\n</blockquote>\n<p>The sad part is that the logical errors of NHST are old news, and have been known ever since <a href=\"http://en.wikipedia.org/wiki/Ronald_fisher\">Ronald Fisher</a> began advocating NHST in the 1920s. By 1960, Fisher had out-advocated his critics, and philosopher William Rozeboom <a href=\"http://stats.org.uk/statistical-inference/Rozeboom1960.pdf\">remarked</a>:</p>\n<blockquote>\n<p>Despite the awesome pre-eminence [NHST] has attained... it is based upon a fundamental misunderstanding of the nature of rational inference, and is seldom if ever appropriate to the aims of scientific research.</p>\n</blockquote>\n<p>There are <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Siegfried-Odds-Are-its-Wrong.pdf\">many</a> <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Kruschke-What-to-believe-Bayesian-methods-for-data-analysis.pdf\">more</a> <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Wagenmakers-et-al-Bayesian-versus-frequentist-inference.pdf\">problems</a> with NHST and with \"frequentist\" statistics in general, but the central one is this: NHST does not follow from the axioms (foundational logical rules) of probability theory. It is a grab-bag of techniques that, depending on how those techniques are applied, can lead to <em>different</em> results when analyzing the <em>same data</em> \u2014 something that should horrify every mathematician.</p>\n<p>The inferential method that solves the problems with frequentism \u2014 and, more importantly, follows deductively from the axioms of probability theory \u2014 is <a href=\"http://en.wikipedia.org/wiki/Bayesian_inference\">Bayesian inference</a>.</p>\n<p>So why aren't <em>all</em> scientists using Bayesian inference instead of frequentist inference? Partly, we can blame the vigor of NHST's early advocates. But we can also attribute NHST's success to the simple fact that Bayesian calculations can be <em>more difficult</em> than frequentist calculations. Luckily, new software tools like <a href=\"http://www.mrc-bsu.cam.ac.uk/bugs/winbugs/contents.shtml\">WinBUGS</a> let computers do most of the heavy lifting required for Bayesian inference.</p>\n<p>There's also the problem of sheer momentum. Once a practice is enshrined, it's hard to dislodge it, even for good reasons. I took three statistics courses in university and <em>none</em> of my textbooks mentioned Bayesian inference. I didn't learn about it until I dropped out of university and studied science and probability theory on my own.</p>\n<p>Remember the study about precognition? Not surprisingly, it was done using NHST. A later <a href=\"http://www.ruudwetzels.com/articles/Wagenmakersetal_subm.pdf\">Bayesian analysis</a> of the data disconfirmed the original startling conclusion.</p>\n<p>&nbsp;</p>\n<h4 id=\"Example_partial_solution2\">Example partial solution</h4>\n<p>This one is obvious: teach students probability theory instead of NHST. Retrain current scientists in Bayesian methods. Make Bayesian software tools easier to use and more widespread.</p>\n<p>&nbsp;</p>\n<h3 id=\"Conclusion\">Conclusion</h3>\n<p>If I'm right that there is unambiguous low-hanging fruit for improving scientific practice, this suggests that particular departments, universities, or private research institutions can (probabilistically) out-perform their rivals (in terms of <a href=\"http://www.ploscompbiol.org/article/fetchObjectAttachment.action?uri=info%3Adoi%2F10.1371%2Fjournal.pcbi.1002072&amp;representation=PDF\">actual discoveries</a>, not just publications) given similar resources.</p>\n<p>I'll conclude with one particular <em>specific</em> hypothesis. If I'm right, then a research group should be able to hire researchers trained in Bayesian reasoning and in catching publication bias and experimenter bias, and have them extract from the existing literature valuable medical truths that the mainstream medical community doesn't yet know about. This prediction, in fact, is <a href=\"http://www.medicineispersonal.com/\">about to be tested</a>.</p>", "sections": [{"title": "Problem 1: Publication bias", "anchor": "Problem_1__Publication_bias", "level": 1}, {"title": "Example partial solution", "anchor": "Example_partial_solution", "level": 2}, {"title": "Problem 2: Experimenter bias", "anchor": "Problem_2__Experimenter_bias", "level": 1}, {"title": "Example partial solution", "anchor": "Example_partial_solution1", "level": 2}, {"title": "Problem 3: Bad statistics", "anchor": "Problem_3__Bad_statistics", "level": 1}, {"title": "Example partial solution", "anchor": "Example_partial_solution2", "level": 2}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "144 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 143, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xLm9mgJRPvmPGpo7Q", "f4txACqDWithRi7hs", "b9vvmMn2kF76aThHn", "H2zKAfiSJR6WJQ8pn", "f4CZNEHirweN3XEjs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-03-07T02:51:49.666Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-07T02:51:55.367Z", "modifiedAt": null, "url": null, "title": "Using degrees of freedom to change the past for fun and profit", "slug": "using-degrees-of-freedom-to-change-the-past-for-fun-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:02.136Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CarlShulman", "createdAt": "2009-03-01T07:47:12.225Z", "isAdmin": false, "displayName": "CarlShulman"}, "userId": "SguegG9SFXaKTgJLq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kXgyLuyRSvsxozFRs/using-degrees-of-freedom-to-change-the-past-for-fun-and", "pageUrlRelative": "/posts/kXgyLuyRSvsxozFRs/using-degrees-of-freedom-to-change-the-past-for-fun-and", "linkUrl": "https://www.lesswrong.com/posts/kXgyLuyRSvsxozFRs/using-degrees-of-freedom-to-change-the-past-for-fun-and", "postedAtFormatted": "Wednesday, March 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Using%20degrees%20of%20freedom%20to%20change%20the%20past%20for%20fun%20and%20profit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUsing%20degrees%20of%20freedom%20to%20change%20the%20past%20for%20fun%20and%20profit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkXgyLuyRSvsxozFRs%2Fusing-degrees-of-freedom-to-change-the-past-for-fun-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Using%20degrees%20of%20freedom%20to%20change%20the%20past%20for%20fun%20and%20profit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkXgyLuyRSvsxozFRs%2Fusing-degrees-of-freedom-to-change-the-past-for-fun-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkXgyLuyRSvsxozFRs%2Fusing-degrees-of-freedom-to-change-the-past-for-fun-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2637, "htmlBody": "<p>Follow-up to:&nbsp;<a href=\"/lw/6lq/followup_on_esp_study_we_dont_publish_replications/\">Follow-up on ESP study: \"We don't publish replications\"</a>,&nbsp;<a href=\"/lw/9xs/feed_the_spinoff_heuristic/\">Feed the Spinoff Heuristic!</a></p>\n<p>Related to:&nbsp;<a href=\"/lw/1ib/parapsychology_the_control_group_for_science/\">Parapsychology: the control group for science</a>,&nbsp;<a href=\"/lw/2y3/dealing_with_the_high_quantity_of_scientific/\">Dealing with the high quantity of scientific error in medicine</a></p>\n<blockquote>\n<p>Using the same method as in Study 1, we asked 20 University&nbsp;of Pennsylvania undergraduates to listen to either &ldquo;When I&rsquo;m&nbsp;Sixty-Four&rdquo; by The Beatles or &ldquo;Kalimba.&rdquo; Then, in an ostensibly unrelated task, they indicated their birth date (mm/dd/yyyy) and their father&rsquo;s age. We used father&rsquo;s age to control for variation in baseline age across participants.&nbsp;An ANCOVA revealed the predicted effect: According to&nbsp;their birth dates, people were nearly a year-and-a-half younger&nbsp;after listening to &ldquo;When I&rsquo;m Sixty-Four&rdquo; (adjusted M = 20.1&nbsp;years) rather than to &ldquo;Kalimba&rdquo; (adjusted M = 21.5 years),&nbsp;F(1, 17) = 4.92, p = .040</p>\n</blockquote>\n<p>That's from \"<a href=\"http://psy2.ucsd.edu/~dhuber/Simmons_Nelson_Simonsohn_2011.pdf\">False-Positive Psychology: Undisclosed&nbsp;Flexibility in Data Collection and Analysis&nbsp;Allows Presenting Anything as Significant</a>,\" which runs simulations of a version of Shalizi's \"<a href=\"http://cscs.umich.edu/~crshalizi/weblog/698.html\">neutral model of inquiry</a>,\" with random (null) experimental results, augmented with a handful of choices in the setup and analysis of an experiment. Even before accounting for publication bias, these few choices produced a desired result \"significant at the 5% level\" 60.7% of the time, and at the 1% level 21.5% at the time.</p>\n<p>I found it because of another paper claiming time-defying effects, during a search through all of the papers on Google Scholar citing Daryl Bem's precognition <a href=\"http://dbem.ws/FeelingFuture.pdf\">paper</a>, which I discussed in a past <a href=\"/lw/6lq/followup_on_esp_study_we_dont_publish_replications/\">post</a>&nbsp;about the problems of publication bias and selection over the course of a study. For Bem, Richard Wiseman established a registry for the methods, and tests of the registered studies could be set prior to seeing the data (in addition to avoiding the file drawer).</p>\n<p>Now a number of purported replications have been completed, with several available as preprints online, including a large \"straight replication\" carefully following the methods in Bem's paper, with some interesting findings discussed below. The picture does not look good for psi, and is a good reminder of the sheer cumulative power of applying a biased filter to many small choices.</p>\n<p><a id=\"more\"></a><strong>Background</strong></p>\n<p>When Bem's article was published the skeptic David Alcock <a href=\"http://www.csicop.org/specialarticles/show/back_from_the_future\">argued</a>&nbsp;that Bem's experiments involved midstream changes of methods, choices in the transformation of data (raw data was not available), and other signs of modifying the experiment and analysis in response to the data. <a href=\"http://behavioralhealth.squarespace.com/storage/Bem6.pdf\">Wagenmakers et al</a>&nbsp;drew attention to writing by Bem advising young psychologists to take experiments that failed to show predicted effects and relentlessly explore the data in hopes of generating an attractive and significant effect. In my post, I emphasized the importance of \"straight replications,\" with methodology, analytical tests, and intent to publish established in advance, as in Richard Wiseman's registry of studies.</p>\n<p>An <a href=\"http://www2.psych.purdue.edu/~gfrancis/Publications/GFrancis-R1.pdf\">article</a> by Gregory Francis uses a standard test for publication bias on Bem's article: comparing the number of findings reaching significance to the number predicted by the power of the study to detect the claimed effect. 9 of the 10 experiments mentioned described in Bem's article<sup>1</sup>&nbsp;find positive effects using Bem's measures and tests, but those 9 were all statistically significant despite the small size of the effects. Francis calculates a 5.8% probability of so many reaching significance by chance (given the estimated effect power and effect size).</p>\n<p>Other complaints included declining effect size with sample size (driven mostly by one larger experiment), the use of one-tailed tests (Bem justified this as following an early hypothesis, but claims of \"psi-missing\" due to boredom or repelling stimuli are found in the literature and could have been mustered), and the failure to directly replicate a single experiment or concentrate subjects.</p>\n<p><strong>Subsequent replications</strong></p>\n<p>At the time of my first post, I was able to find several replication attempts already online. Richard Wiseman and his coauthors had not found psi, and were refused consideration for publication at the journal which had hosted the original article. Galak and Nelson had&nbsp;<a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1699970\">tried and failed to replicate</a> experiment 8. A a pro-psi researcher had pulled a different 2006 experiment from the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Publication_bias#The_file_drawer_effect\">file drawer</a>&nbsp;and retitled as a&nbsp;purported \"<a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1715954\">replication</a>\"&nbsp;of the 2011 paper. Samuel Moulton, who previously worked with Bem, <a href=\"http://community.nytimes.com/comments/www.nytimes.com/2011/01/06/science/06esp.html?permid=78#comment78\">writes</a> that he tried to replicate Bem with 200 subjects and found no effect (not just not a significant effect, but a significantly lower effect), but that Bem would not mention&nbsp;this in the 2011 publication. Bem confirms this in a video of a Harvard <a href=\"http://www.youtube.com/watch?feature=player_embedded&amp;v=0Tdiu5kwjKs\">debate</a>.</p>\n<p>Since then, there have been more replications. This New Scientist <a href=\"http://neshealthblog.wordpress.com/2012/01/16/precognition-feeling-the-future/\">article</a> claims to have found 7 replications of Bem, with six failures and one success. The success is said to be by a researcher who has previously <a href=\"http://www.spr.ac.uk/main/page/conference-abstracts-2010\">studied</a>&nbsp;the effect of \"geomagnetic pulsations\" on ESP, but I could not locate it online.</p>\n<p>Snodgrass (<a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1935942\">2011</a>) failed to replicate Bem using a version of the Galak and Nelson experiment. Wagenmaker et al <a href=\"http://dl.dropbox.com/u/1018886/Advance_Information_on_Experiment_and_Analysis.pdf\">posted their methods</a> in advance, but have not yet posted their results, although news media have reported that they also got a negative result Bem. Wiseman and his coauthors posted their <a href=\"http://www.spr.ac.uk/main/page/conference-abstracts-2011#ritchie\">abstract online</a>, and claim to have performed a close replication of one of Bem's experiments with three times the subjects, finding no effect (despite 99%+ power to detect Bem's claimed effect). Another <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2001721\">paper</a>, \"Correcting the Past: Failures to Replicate Psi,\" by Galak, LeBoeuf, Nelson, and Simmons, combines 6 experiments by the researchers (who are at four separate universities) with 820 subjects and finds no effect in a very straight replication. More on it in a moment.</p>\n<p>I also found the <a href=\"http://www.consciousness.arizona.edu/documents/FullProgramandAbstractsTSC2011Stockholm.pdf\">abstracts</a> of the 2011 Towards a Science of Consciousness conference. On page 166 Whitmarsh and Bierman claim to have conducted a replication of a Bem experiment involving meditators, but do not give their results, although it appears they may have looked for effects of meditation on the results. On page 176, there is an abstract from Franklin and Schooler, claiming success in a new and different precognition experiment, as well as predicting the outcome of a roulette wheel (n=204, hit rate 57%, p&lt;.05). In the New Scientist article they claim to have replicated their experiment (with much reduced effect size and just barely above the 0.05 significance level), although past efforts to use psi in casino games have not been repeatable (nor have the experimenters become mysteriously wealthy, or easily able to fund their research, apparently). The move to a new and ill-described format prevents it from being used as a straight replication (in Shalizi's <a href=\"http://cscs.umich.edu/~crshalizi/weblog/698.html\">neutral model of inquiry</a>&nbsp;using only publication bias, it is the move to new effects lets a field sustain itself in the absence of a subject matter), it was not registered, and the actual study is not available, so I will leave it be until publication.</p>\n<p><strong>Correcting the Past: Failures to Replicate Psi</strong></p>\n<p>Throughout this paper the researchers try to specify their procedures unambigously and as closely aligned with Bem as they can, for instance in transforming the data<sup>2</sup>&nbsp;so as to avoid cherry-picking in the fashion they argue:</p>\n<blockquote>\n<p>Results</p>\n<p>To test for the presence of precognition, Bem (2011) computed a weighted differential recall score (DR) for each participant using the formula:</p>\n<p>DR = (Recalled Practiced Words - Recalled Control Words) &times;</p>\n<p>(Recalled Practice Words + Recalled Control Words)</p>\n<p>In the paper, for descriptive purposes, Bem frequently reports this number as DR%, which is the percentage that a participant&rsquo;s score deviated from random chance towards the highest or lowest scores possible (-576 to 576). We conducted the identical analysis on our data and also report DR% (see Table 1). In addition to using the weighted differential recall score, we also report the results using a simple unweighted recall score, which is the difference between&nbsp;recalled practice words and recalled control words (see Appendix B). For both of these measures, random chance would lead to a score of 0, and analysis was conducted using a one-sample t-test.</p>\n</blockquote>\n<p>This prevents them from choosing the more favorable (or less favorable) of several transformations, as they seem to suggest Bem did in the next quote, bumping a result to significance in the original paper. This is a recurrent problem across many fields, and a reason to seek out raw data whenever possible, or datasets collected by neutral parties (on your question of interest):</p>\n<blockquote>\n<p>Still, even in Experiments 8 and 9, it is unclear how Bem could find significant support for a hypothesis that appears to be untrue. Elsewhere, critics of Bem have implicated his use of a one-tailed statistical test (Wagenmakers et al. 2011), testing multiple comparisons without correction (Wagenmakers et al. 2011), or perhaps simply a lurking file drawer with some less successful pilot experiments. All of these concerns fall under a larger category of researcher degrees of freedom, which raise the likelihood of falsely rejecting the null hypothesis (Simmons et al., 2011). Some of these can be easily justifiable and have small and seemingly inconsequential effects. For example, Bem analyzes participant recall using an algorithm which weights the total number of correctly recalled words (i.e., DR%). He could, presumably, have just as easily analyzed simple difference scores and found a similar, but not quite identical, result (indeed, re-analyzing the data from Bem (2011) Experiment 8 with a simple difference score yields no Psi effects (M = .49, t(99) = 1.48, p = .14), though it does for Experiment 9 (M =.96; t(49) = 2.46, p = .02)).</p>\n</blockquote>\n<p>They mention others which they did not have data to test:</p>\n<blockquote>\n<p>The scoring distinction is just a single example, but even for Bem&rsquo;s simple procedure there are many others. For example, Bem&rsquo;s words are evenly split between common and uncommon words, a difference that was not analyzed (or reported) in the original paper, but may reflect an alternative way to consider the data (perhaps psi only persists for uncommon words? Perhaps only for common words?). He reports the results of his two-item sensation seeking measure, but he does not analyze (or report collecting) additional measures of participant anxiety or experimenter-judged participant enthusiasm. Presumably these were collected because there&nbsp;was a possibility that they may be influential as well, but when analysis revealed that they were not, that analysis was dropped from the paper.&nbsp;</p>\n</blockquote>\n<p>Other elements providing degrees of freedom were left out of the Bem paper. A published paper can only provide so much confidence that it actually describes the experiment as it happened (or didn't!):</p>\n<blockquote>\n<p>Despite our best efforts to conduct identical replications of Bem&rsquo;s Experiments 8 and 9, it is possible that the detection of psi requires certain methodological idiosyncrasies that we failed to incorporate into our experiments. For instance, after reading the replication packet (personal communication with Bem, November, 1 2010) provided by Bem, we noticed that there were at least three differences between our experiments (which mirrored the exact procedure described in Bem&rsquo;s published paper) and the procedure actually employed by Bem...the experimenter was required to have a conversation with each participant in order to relax him or her...participants were asked two questions in addition to the sensation seeking scale...the set of words used by Bem were divided into common and uncommon words, something that we did not do in our Experiments 1 and 2.</p>\n</blockquote>\n<p>The experiments, with several times the collective sample size of the Bem experiments (8 and 9) they replicate, look like chance:</p>\n<blockquote>\n<p>Main Results</p>\n<p>Table 1 presents the results of our six experiments as well as the results from Bem&rsquo;s (2011) Experiments 8 and 9, for comparison. Bem found DR% = 2.27% in Experiment 8 and 4.21% in Experiment 9, effects that were significant at p = .03 and p = .002, one-tailed.</p>\n<p>In contrast, none of our six experiments showed a significant effect suggesting precognition.</p>\n<p>In Experiment 1, DR% = -1.21%, t(111) = -1.201, p = .23 (all p-values in this paper are two-tailed). Bayesian t-tests (advocated by Wagenmakers et al., 2011) suggest that this is &ldquo;substantial&rdquo; support for the null hypothesis of no precognition.</p>\n<p>In Experiment 2, DR% = 0.00%, t(157) = .00, p = .99. Bayesian t-tests suggest that this is &ldquo;strong&rdquo; support for the null hypothesis.</p>\n<p>In Experiment 3, DR% = 1.17%, t(123) = 1.28, p = .20. Although DR% was indeed above zero, in the direction predicted by the ESP hypothesis, the test statistic did not reach conventional levels of significance, and Bayesian t-tests suggest that this is nevertheless &ldquo;substantial&rdquo; support for the null hypothesis.</p>\n<p>In Experiment 4, DR% = 1.59%, t(108) = 1.77, p = .08. Again, although DR% was above zero, the test statistic did not reach conventional levels of significance, and Bayesian t-tests still suggest that this is &ldquo;substantial&rdquo; support for the null hypothesis.</p>\n<p>In Experiment 5, which contained our largest sample of participants, DR% = -.49%, t(210) = -.71, p = .48. Bayesian t-tests suggest that this is &ldquo;strong&rdquo; support for the null hypothesis.</p>\n<p>Finally, in Experiment 6&rsquo;s Test-Before-Practice condition, DR% = -.29%, t(105) = -.33, p = .74. Bayesian t-tests suggest that this is &ldquo;strong&rdquo; support for the null hypothesis.</p>\n<p>In sum, in four of our experiments, participants recalled more control words than practice words (Experiments 1, 2, 5, and 6) and in two of our experiments, participants recalled more practice words than control words (Experiments 3 and 4). None of these effects were statistically reliable using conventional t-tests (see Table 1). As noted, Bayesian t-tests suggest that even the two findings that were directionally consistent with precognition show substantial support for the null hypothesis of no precognition.</p>\n</blockquote>\n<p>Perhaps the reported positive replication will hold up to scrutiny (with respect to sample size, power, closeness of replication, data mining, etc), or some other straight replication will come out convincingly positive (in light of the aggregate evidence). I doubt it.</p>\n<p><strong>Psi and science</strong></p>\n<p>Beating up on parapsychology may be <a href=\"/lw/1ww/undiscriminating_skepticism/\">cheap and easy</a>&nbsp;in the scientific, skeptical, and Less Wrong communities, a low-status outgroup belief. But the abuse of many degrees of freedom, and shortage of close replication, is widespread&nbsp;in science and particularly in psychology. The heuristics and biases literature, studies of&nbsp;<a href=\"/r/discussion/lw/68k/nback_news_jaeggi_2011_or_is_there_a/\">cognitive enhancement</a>, social psychology and other areas often used in Less Wrong are not so different. This suggests a candidate hack to fight confirmation bias in assessing the evidentiary value of experiments that confirm one's views: ask yourself how much evidentiary weight (in log odds) you would place on the same methods and results showing a novel psi effect?<sup>3</sup></p>\n<p>&nbsp;</p>\n<p><strong>Notes</strong></p>\n<p><sup>1</sup>&nbsp;In addition to the nine numbered experiments, there is a footnote referring to a small early tenth study which did not find an effect in Bem 2011.</p>\n<p><sup>2 </sup>One of the bigger differences is that some of the experiments were online rather than in the lab, but this didn't seem to matter much. They also switched from blind human coding of misspelled words to computerized coding.</p>\n<p><sup>3</sup>&nbsp;This heuristic has not been tested, beyond the general (psychology!) results suggesting that arguing for a position opposite your own can help to see otherwise selectively missed considerations.&nbsp;</p>\n<p>ETA: This blog <a href=\"http://www.talyarkoni.org/blog/2011/01/10/the-psychology-of-parapsychology-or-why-good-researchers-publishing-good-articles-in-good-journals-can-still-get-it-totally-wrong/\">post</a> also discusses the signs of optional stopping, multiple hypothesis testing, use of one-tailed tests where a negative result could also have been reported as due to psi, etc.</p>\n<p>ETA2: A <a href=\"http://barenormality.wordpress.com/2011/06/04/feeling-the-future-part-2/\">post</a> at the Bare Normality blog tracks down earlier presentation of some of the experiments going into Bem (2011), back in 2003, and notes that the data seem to bee selectively ported to the 2011 paper, described quite differently, and discusses other signs of unreported experiments. The post also expresses concern about reconciling these data with Bem's explicit denial of optional stopping, selective reporting, and similar.</p>\n<p>ETA3: Bem's paper cites an experiment by Savva as evidence for precognition (by arachnophobes), but leaves out the fact that Savva's follow-up experiments failed to replicate the effect. Links and references are provided in a <a href=\"http://forums.randi.org/showthread.php?postid=6680817#post6680817\">post</a> at the James Randi forums. Savva also <a href=\"http://forums.randi.org/showthread.php?p=2173272#post2173272\">says</a> that Bem had \"extracted\" several supposedly significant precognition correlations from Savva's data, and upon checking Savva found they were generated by calculation errors. Bem also is said to have claimed Savva's first result had passed the 0.05 significance test, when it was actually just short of doing so (0.051, not a substantial difference, and perhaps defensible, but another sign of bias).</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vg4LDxjdwHLotCm8w": 1, "bh7uxTTqmsQ8jZJdB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kXgyLuyRSvsxozFRs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 63, "extendedScore": null, "score": 0.0006102323927101942, "legacy": true, "legacyId": "13299", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Follow-up to:&nbsp;<a href=\"/lw/6lq/followup_on_esp_study_we_dont_publish_replications/\">Follow-up on ESP study: \"We don't publish replications\"</a>,&nbsp;<a href=\"/lw/9xs/feed_the_spinoff_heuristic/\">Feed the Spinoff Heuristic!</a></p>\n<p>Related to:&nbsp;<a href=\"/lw/1ib/parapsychology_the_control_group_for_science/\">Parapsychology: the control group for science</a>,&nbsp;<a href=\"/lw/2y3/dealing_with_the_high_quantity_of_scientific/\">Dealing with the high quantity of scientific error in medicine</a></p>\n<blockquote>\n<p>Using the same method as in Study 1, we asked 20 University&nbsp;of Pennsylvania undergraduates to listen to either \u201cWhen I\u2019m&nbsp;Sixty-Four\u201d by The Beatles or \u201cKalimba.\u201d Then, in an ostensibly unrelated task, they indicated their birth date (mm/dd/yyyy) and their father\u2019s age. We used father\u2019s age to control for variation in baseline age across participants.&nbsp;An ANCOVA revealed the predicted effect: According to&nbsp;their birth dates, people were nearly a year-and-a-half younger&nbsp;after listening to \u201cWhen I\u2019m Sixty-Four\u201d (adjusted M = 20.1&nbsp;years) rather than to \u201cKalimba\u201d (adjusted M = 21.5 years),&nbsp;F(1, 17) = 4.92, p = .040</p>\n</blockquote>\n<p>That's from \"<a href=\"http://psy2.ucsd.edu/~dhuber/Simmons_Nelson_Simonsohn_2011.pdf\">False-Positive Psychology: Undisclosed&nbsp;Flexibility in Data Collection and Analysis&nbsp;Allows Presenting Anything as Significant</a>,\" which runs simulations of a version of Shalizi's \"<a href=\"http://cscs.umich.edu/~crshalizi/weblog/698.html\">neutral model of inquiry</a>,\" with random (null) experimental results, augmented with a handful of choices in the setup and analysis of an experiment. Even before accounting for publication bias, these few choices produced a desired result \"significant at the 5% level\" 60.7% of the time, and at the 1% level 21.5% at the time.</p>\n<p>I found it because of another paper claiming time-defying effects, during a search through all of the papers on Google Scholar citing Daryl Bem's precognition <a href=\"http://dbem.ws/FeelingFuture.pdf\">paper</a>, which I discussed in a past <a href=\"/lw/6lq/followup_on_esp_study_we_dont_publish_replications/\">post</a>&nbsp;about the problems of publication bias and selection over the course of a study. For Bem, Richard Wiseman established a registry for the methods, and tests of the registered studies could be set prior to seeing the data (in addition to avoiding the file drawer).</p>\n<p>Now a number of purported replications have been completed, with several available as preprints online, including a large \"straight replication\" carefully following the methods in Bem's paper, with some interesting findings discussed below. The picture does not look good for psi, and is a good reminder of the sheer cumulative power of applying a biased filter to many small choices.</p>\n<p><a id=\"more\"></a><strong>Background</strong></p>\n<p>When Bem's article was published the skeptic David Alcock <a href=\"http://www.csicop.org/specialarticles/show/back_from_the_future\">argued</a>&nbsp;that Bem's experiments involved midstream changes of methods, choices in the transformation of data (raw data was not available), and other signs of modifying the experiment and analysis in response to the data. <a href=\"http://behavioralhealth.squarespace.com/storage/Bem6.pdf\">Wagenmakers et al</a>&nbsp;drew attention to writing by Bem advising young psychologists to take experiments that failed to show predicted effects and relentlessly explore the data in hopes of generating an attractive and significant effect. In my post, I emphasized the importance of \"straight replications,\" with methodology, analytical tests, and intent to publish established in advance, as in Richard Wiseman's registry of studies.</p>\n<p>An <a href=\"http://www2.psych.purdue.edu/~gfrancis/Publications/GFrancis-R1.pdf\">article</a> by Gregory Francis uses a standard test for publication bias on Bem's article: comparing the number of findings reaching significance to the number predicted by the power of the study to detect the claimed effect. 9 of the 10 experiments mentioned described in Bem's article<sup>1</sup>&nbsp;find positive effects using Bem's measures and tests, but those 9 were all statistically significant despite the small size of the effects. Francis calculates a 5.8% probability of so many reaching significance by chance (given the estimated effect power and effect size).</p>\n<p>Other complaints included declining effect size with sample size (driven mostly by one larger experiment), the use of one-tailed tests (Bem justified this as following an early hypothesis, but claims of \"psi-missing\" due to boredom or repelling stimuli are found in the literature and could have been mustered), and the failure to directly replicate a single experiment or concentrate subjects.</p>\n<p><strong id=\"Subsequent_replications\">Subsequent replications</strong></p>\n<p>At the time of my first post, I was able to find several replication attempts already online. Richard Wiseman and his coauthors had not found psi, and were refused consideration for publication at the journal which had hosted the original article. Galak and Nelson had&nbsp;<a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1699970\">tried and failed to replicate</a> experiment 8. A a pro-psi researcher had pulled a different 2006 experiment from the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Publication_bias#The_file_drawer_effect\">file drawer</a>&nbsp;and retitled as a&nbsp;purported \"<a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1715954\">replication</a>\"&nbsp;of the 2011 paper. Samuel Moulton, who previously worked with Bem, <a href=\"http://community.nytimes.com/comments/www.nytimes.com/2011/01/06/science/06esp.html?permid=78#comment78\">writes</a> that he tried to replicate Bem with 200 subjects and found no effect (not just not a significant effect, but a significantly lower effect), but that Bem would not mention&nbsp;this in the 2011 publication. Bem confirms this in a video of a Harvard <a href=\"http://www.youtube.com/watch?feature=player_embedded&amp;v=0Tdiu5kwjKs\">debate</a>.</p>\n<p>Since then, there have been more replications. This New Scientist <a href=\"http://neshealthblog.wordpress.com/2012/01/16/precognition-feeling-the-future/\">article</a> claims to have found 7 replications of Bem, with six failures and one success. The success is said to be by a researcher who has previously <a href=\"http://www.spr.ac.uk/main/page/conference-abstracts-2010\">studied</a>&nbsp;the effect of \"geomagnetic pulsations\" on ESP, but I could not locate it online.</p>\n<p>Snodgrass (<a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1935942\">2011</a>) failed to replicate Bem using a version of the Galak and Nelson experiment. Wagenmaker et al <a href=\"http://dl.dropbox.com/u/1018886/Advance_Information_on_Experiment_and_Analysis.pdf\">posted their methods</a> in advance, but have not yet posted their results, although news media have reported that they also got a negative result Bem. Wiseman and his coauthors posted their <a href=\"http://www.spr.ac.uk/main/page/conference-abstracts-2011#ritchie\">abstract online</a>, and claim to have performed a close replication of one of Bem's experiments with three times the subjects, finding no effect (despite 99%+ power to detect Bem's claimed effect). Another <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2001721\">paper</a>, \"Correcting the Past: Failures to Replicate Psi,\" by Galak, LeBoeuf, Nelson, and Simmons, combines 6 experiments by the researchers (who are at four separate universities) with 820 subjects and finds no effect in a very straight replication. More on it in a moment.</p>\n<p>I also found the <a href=\"http://www.consciousness.arizona.edu/documents/FullProgramandAbstractsTSC2011Stockholm.pdf\">abstracts</a> of the 2011 Towards a Science of Consciousness conference. On page 166 Whitmarsh and Bierman claim to have conducted a replication of a Bem experiment involving meditators, but do not give their results, although it appears they may have looked for effects of meditation on the results. On page 176, there is an abstract from Franklin and Schooler, claiming success in a new and different precognition experiment, as well as predicting the outcome of a roulette wheel (n=204, hit rate 57%, p&lt;.05). In the New Scientist article they claim to have replicated their experiment (with much reduced effect size and just barely above the 0.05 significance level), although past efforts to use psi in casino games have not been repeatable (nor have the experimenters become mysteriously wealthy, or easily able to fund their research, apparently). The move to a new and ill-described format prevents it from being used as a straight replication (in Shalizi's <a href=\"http://cscs.umich.edu/~crshalizi/weblog/698.html\">neutral model of inquiry</a>&nbsp;using only publication bias, it is the move to new effects lets a field sustain itself in the absence of a subject matter), it was not registered, and the actual study is not available, so I will leave it be until publication.</p>\n<p><strong id=\"Correcting_the_Past__Failures_to_Replicate_Psi\">Correcting the Past: Failures to Replicate Psi</strong></p>\n<p>Throughout this paper the researchers try to specify their procedures unambigously and as closely aligned with Bem as they can, for instance in transforming the data<sup>2</sup>&nbsp;so as to avoid cherry-picking in the fashion they argue:</p>\n<blockquote>\n<p>Results</p>\n<p>To test for the presence of precognition, Bem (2011) computed a weighted differential recall score (DR) for each participant using the formula:</p>\n<p>DR = (Recalled Practiced Words - Recalled Control Words) \u00d7</p>\n<p>(Recalled Practice Words + Recalled Control Words)</p>\n<p>In the paper, for descriptive purposes, Bem frequently reports this number as DR%, which is the percentage that a participant\u2019s score deviated from random chance towards the highest or lowest scores possible (-576 to 576). We conducted the identical analysis on our data and also report DR% (see Table 1). In addition to using the weighted differential recall score, we also report the results using a simple unweighted recall score, which is the difference between&nbsp;recalled practice words and recalled control words (see Appendix B). For both of these measures, random chance would lead to a score of 0, and analysis was conducted using a one-sample t-test.</p>\n</blockquote>\n<p>This prevents them from choosing the more favorable (or less favorable) of several transformations, as they seem to suggest Bem did in the next quote, bumping a result to significance in the original paper. This is a recurrent problem across many fields, and a reason to seek out raw data whenever possible, or datasets collected by neutral parties (on your question of interest):</p>\n<blockquote>\n<p>Still, even in Experiments 8 and 9, it is unclear how Bem could find significant support for a hypothesis that appears to be untrue. Elsewhere, critics of Bem have implicated his use of a one-tailed statistical test (Wagenmakers et al. 2011), testing multiple comparisons without correction (Wagenmakers et al. 2011), or perhaps simply a lurking file drawer with some less successful pilot experiments. All of these concerns fall under a larger category of researcher degrees of freedom, which raise the likelihood of falsely rejecting the null hypothesis (Simmons et al., 2011). Some of these can be easily justifiable and have small and seemingly inconsequential effects. For example, Bem analyzes participant recall using an algorithm which weights the total number of correctly recalled words (i.e., DR%). He could, presumably, have just as easily analyzed simple difference scores and found a similar, but not quite identical, result (indeed, re-analyzing the data from Bem (2011) Experiment 8 with a simple difference score yields no Psi effects (M = .49, t(99) = 1.48, p = .14), though it does for Experiment 9 (M =.96; t(49) = 2.46, p = .02)).</p>\n</blockquote>\n<p>They mention others which they did not have data to test:</p>\n<blockquote>\n<p>The scoring distinction is just a single example, but even for Bem\u2019s simple procedure there are many others. For example, Bem\u2019s words are evenly split between common and uncommon words, a difference that was not analyzed (or reported) in the original paper, but may reflect an alternative way to consider the data (perhaps psi only persists for uncommon words? Perhaps only for common words?). He reports the results of his two-item sensation seeking measure, but he does not analyze (or report collecting) additional measures of participant anxiety or experimenter-judged participant enthusiasm. Presumably these were collected because there&nbsp;was a possibility that they may be influential as well, but when analysis revealed that they were not, that analysis was dropped from the paper.&nbsp;</p>\n</blockquote>\n<p>Other elements providing degrees of freedom were left out of the Bem paper. A published paper can only provide so much confidence that it actually describes the experiment as it happened (or didn't!):</p>\n<blockquote>\n<p>Despite our best efforts to conduct identical replications of Bem\u2019s Experiments 8 and 9, it is possible that the detection of psi requires certain methodological idiosyncrasies that we failed to incorporate into our experiments. For instance, after reading the replication packet (personal communication with Bem, November, 1 2010) provided by Bem, we noticed that there were at least three differences between our experiments (which mirrored the exact procedure described in Bem\u2019s published paper) and the procedure actually employed by Bem...the experimenter was required to have a conversation with each participant in order to relax him or her...participants were asked two questions in addition to the sensation seeking scale...the set of words used by Bem were divided into common and uncommon words, something that we did not do in our Experiments 1 and 2.</p>\n</blockquote>\n<p>The experiments, with several times the collective sample size of the Bem experiments (8 and 9) they replicate, look like chance:</p>\n<blockquote>\n<p>Main Results</p>\n<p>Table 1 presents the results of our six experiments as well as the results from Bem\u2019s (2011) Experiments 8 and 9, for comparison. Bem found DR% = 2.27% in Experiment 8 and 4.21% in Experiment 9, effects that were significant at p = .03 and p = .002, one-tailed.</p>\n<p>In contrast, none of our six experiments showed a significant effect suggesting precognition.</p>\n<p>In Experiment 1, DR% = -1.21%, t(111) = -1.201, p = .23 (all p-values in this paper are two-tailed). Bayesian t-tests (advocated by Wagenmakers et al., 2011) suggest that this is \u201csubstantial\u201d support for the null hypothesis of no precognition.</p>\n<p>In Experiment 2, DR% = 0.00%, t(157) = .00, p = .99. Bayesian t-tests suggest that this is \u201cstrong\u201d support for the null hypothesis.</p>\n<p>In Experiment 3, DR% = 1.17%, t(123) = 1.28, p = .20. Although DR% was indeed above zero, in the direction predicted by the ESP hypothesis, the test statistic did not reach conventional levels of significance, and Bayesian t-tests suggest that this is nevertheless \u201csubstantial\u201d support for the null hypothesis.</p>\n<p>In Experiment 4, DR% = 1.59%, t(108) = 1.77, p = .08. Again, although DR% was above zero, the test statistic did not reach conventional levels of significance, and Bayesian t-tests still suggest that this is \u201csubstantial\u201d support for the null hypothesis.</p>\n<p>In Experiment 5, which contained our largest sample of participants, DR% = -.49%, t(210) = -.71, p = .48. Bayesian t-tests suggest that this is \u201cstrong\u201d support for the null hypothesis.</p>\n<p>Finally, in Experiment 6\u2019s Test-Before-Practice condition, DR% = -.29%, t(105) = -.33, p = .74. Bayesian t-tests suggest that this is \u201cstrong\u201d support for the null hypothesis.</p>\n<p>In sum, in four of our experiments, participants recalled more control words than practice words (Experiments 1, 2, 5, and 6) and in two of our experiments, participants recalled more practice words than control words (Experiments 3 and 4). None of these effects were statistically reliable using conventional t-tests (see Table 1). As noted, Bayesian t-tests suggest that even the two findings that were directionally consistent with precognition show substantial support for the null hypothesis of no precognition.</p>\n</blockquote>\n<p>Perhaps the reported positive replication will hold up to scrutiny (with respect to sample size, power, closeness of replication, data mining, etc), or some other straight replication will come out convincingly positive (in light of the aggregate evidence). I doubt it.</p>\n<p><strong id=\"Psi_and_science\">Psi and science</strong></p>\n<p>Beating up on parapsychology may be <a href=\"/lw/1ww/undiscriminating_skepticism/\">cheap and easy</a>&nbsp;in the scientific, skeptical, and Less Wrong communities, a low-status outgroup belief. But the abuse of many degrees of freedom, and shortage of close replication, is widespread&nbsp;in science and particularly in psychology. The heuristics and biases literature, studies of&nbsp;<a href=\"/r/discussion/lw/68k/nback_news_jaeggi_2011_or_is_there_a/\">cognitive enhancement</a>, social psychology and other areas often used in Less Wrong are not so different. This suggests a candidate hack to fight confirmation bias in assessing the evidentiary value of experiments that confirm one's views: ask yourself how much evidentiary weight (in log odds) you would place on the same methods and results showing a novel psi effect?<sup>3</sup></p>\n<p>&nbsp;</p>\n<p><strong id=\"Notes\">Notes</strong></p>\n<p><sup>1</sup>&nbsp;In addition to the nine numbered experiments, there is a footnote referring to a small early tenth study which did not find an effect in Bem 2011.</p>\n<p><sup>2 </sup>One of the bigger differences is that some of the experiments were online rather than in the lab, but this didn't seem to matter much. They also switched from blind human coding of misspelled words to computerized coding.</p>\n<p><sup>3</sup>&nbsp;This heuristic has not been tested, beyond the general (psychology!) results suggesting that arguing for a position opposite your own can help to see otherwise selectively missed considerations.&nbsp;</p>\n<p>ETA: This blog <a href=\"http://www.talyarkoni.org/blog/2011/01/10/the-psychology-of-parapsychology-or-why-good-researchers-publishing-good-articles-in-good-journals-can-still-get-it-totally-wrong/\">post</a> also discusses the signs of optional stopping, multiple hypothesis testing, use of one-tailed tests where a negative result could also have been reported as due to psi, etc.</p>\n<p>ETA2: A <a href=\"http://barenormality.wordpress.com/2011/06/04/feeling-the-future-part-2/\">post</a> at the Bare Normality blog tracks down earlier presentation of some of the experiments going into Bem (2011), back in 2003, and notes that the data seem to bee selectively ported to the 2011 paper, described quite differently, and discusses other signs of unreported experiments. The post also expresses concern about reconciling these data with Bem's explicit denial of optional stopping, selective reporting, and similar.</p>\n<p>ETA3: Bem's paper cites an experiment by Savva as evidence for precognition (by arachnophobes), but leaves out the fact that Savva's follow-up experiments failed to replicate the effect. Links and references are provided in a <a href=\"http://forums.randi.org/showthread.php?postid=6680817#post6680817\">post</a> at the James Randi forums. Savva also <a href=\"http://forums.randi.org/showthread.php?p=2173272#post2173272\">says</a> that Bem had \"extracted\" several supposedly significant precognition correlations from Savva's data, and upon checking Savva found they were generated by calculation errors. Bem also is said to have claimed Savva's first result had passed the 0.05 significance test, when it was actually just short of doing so (0.051, not a substantial difference, and perhaps defensible, but another sign of bias).</p>\n<p>&nbsp;</p>", "sections": [{"title": "Subsequent replications", "anchor": "Subsequent_replications", "level": 1}, {"title": "Correcting the Past: Failures to Replicate Psi", "anchor": "Correcting_the_Past__Failures_to_Replicate_Psi", "level": 1}, {"title": "Psi and science", "anchor": "Psi_and_science", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "23 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["b9vvmMn2kF76aThHn", "aYtgZTKJwREEwhDjg", "enuGsZoFLR4KyEx3n", "PnYh6hZsFPRB3GPCe", "Jko7pt7MwwTBrfG3A", "WDcXoMdFxkSXPSrwR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-07T03:02:12.694Z", "modifiedAt": null, "url": null, "title": "Delicious Luminosity, Om Nom Nom", "slug": "delicious-luminosity-om-nom-nom", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:20.781Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v8Sh6cpj34NFtFwAF/delicious-luminosity-om-nom-nom", "pageUrlRelative": "/posts/v8Sh6cpj34NFtFwAF/delicious-luminosity-om-nom-nom", "linkUrl": "https://www.lesswrong.com/posts/v8Sh6cpj34NFtFwAF/delicious-luminosity-om-nom-nom", "postedAtFormatted": "Wednesday, March 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Delicious%20Luminosity%2C%20Om%20Nom%20Nom&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADelicious%20Luminosity%2C%20Om%20Nom%20Nom%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv8Sh6cpj34NFtFwAF%2Fdelicious-luminosity-om-nom-nom%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Delicious%20Luminosity%2C%20Om%20Nom%20Nom%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv8Sh6cpj34NFtFwAF%2Fdelicious-luminosity-om-nom-nom", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv8Sh6cpj34NFtFwAF%2Fdelicious-luminosity-om-nom-nom", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 326, "htmlBody": "<p>I have decided that it would be valuable for me to read books (blog posts, articles, random conversations between smart people who store chatlogs) about introspection, take notes, and try to distill and clarify the information.&nbsp; This could result in me eventually giving up, or in a Luminosity Sequence: Second Edition (Now With Literature, Part Of This Complete Breakfast!), or (optimism!) me being able to sort ~90% of people into some number of categories such that their category membership tells me how to help them develop luminosity superpowers in N simple steps with exercises/therapy-ish stuff/etc.</p>\n<p>Help me eat luminosity!&nbsp; I need recommendations for stuff to read.&nbsp; This stuff should be:</p>\n<ul>\n<li>readable (I will not long slog through something I'm stylistically allergic to)</li>\n<li>not obvious nonsense (but if it didn't work on you/your personal friends, that's not \"obvious nonsense\", it could be cognitive heterogeneity; I just want to filter out crap like \"The Secret\")</li>\n<li>something I can probably get my hands on (library, <em>100% legal!</em> electronic acquisition, it being on the Internet).</li>\n</ul>\n<p>I read really fast.&nbsp; Don't worry about oversaturating me with recommendations, but please do say a little about why you recommend a thing (even if it's \"I haven't read this, but I keep hearing about it, so I guess some people like it\") and post recommendations in separate comments so people with information about the item can vote up and down separately.&nbsp; Recommendations for non-written things will be heavily discounted but not outright disqualified.</p>\n<p>I would also like a supply of guinea-pigs-in-waiting for if and when I get to the point of trying the sorting or the superpower-giving part of the optimistic end state of the project.</p>\n<p>If people want me to, I can document the process of luminosity-eating so there is a template to follow for other subject-eating projects, but I wouldn't do this by default because in general I only do things that someone would care if I didn't do them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb253": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v8Sh6cpj34NFtFwAF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 14, "extendedScore": null, "score": 8.611435625782139e-07, "legacy": true, "legacyId": "13765", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-07T04:24:07.317Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] If You Demand Magic, Magic Won't Help", "slug": "seq-rerun-if-you-demand-magic-magic-won-t-help", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:11.625Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cDrKuQ4FGBbrpf9gC/seq-rerun-if-you-demand-magic-magic-won-t-help", "pageUrlRelative": "/posts/cDrKuQ4FGBbrpf9gC/seq-rerun-if-you-demand-magic-magic-won-t-help", "linkUrl": "https://www.lesswrong.com/posts/cDrKuQ4FGBbrpf9gC/seq-rerun-if-you-demand-magic-magic-won-t-help", "postedAtFormatted": "Wednesday, March 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20If%20You%20Demand%20Magic%2C%20Magic%20Won't%20Help&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20If%20You%20Demand%20Magic%2C%20Magic%20Won't%20Help%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcDrKuQ4FGBbrpf9gC%2Fseq-rerun-if-you-demand-magic-magic-won-t-help%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20If%20You%20Demand%20Magic%2C%20Magic%20Won't%20Help%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcDrKuQ4FGBbrpf9gC%2Fseq-rerun-if-you-demand-magic-magic-won-t-help", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcDrKuQ4FGBbrpf9gC%2Fseq-rerun-if-you-demand-magic-magic-won-t-help", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 215, "htmlBody": "<p>Today's post, <a href=\"/lw/ou/if_you_demand_magic_magic_wont_help/\">If You Demand Magic, Magic Won't Help</a> was originally published on 22 March 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#If_You_Demand_Magic.2C_Magic_Won.27t_Help\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Magic (and dragons, and UFOs, and ...) get much of their charm from the fact that they don't actually exist. If dragons did exist, people would treat them like zebras; most people wouldn't bother to pay attention, but some scientists would get oddly excited about them. If we ever create dragons, or find aliens, we will have to learn to enjoy them, even though they happen to exist.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/al6/seq_rerun_bind_yourself_to_reality/\">Bind Yourself to Reality</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cDrKuQ4FGBbrpf9gC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.611765915219501e-07, "legacy": true, "legacyId": "13766", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iiWiHgtQekWNnmE6Q", "Eka29rtYXHBdpKTMe", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-07T06:43:20.092Z", "modifiedAt": null, "url": null, "title": "\"How We Decide\", by Jonah Lehrer, kindle version on sale for 99 cents at amazon", "slug": "how-we-decide-by-jonah-lehrer-kindle-version-on-sale-for-99", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:25.210Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "buybuydandavis", "createdAt": "2011-09-04T00:02:35.971Z", "isAdmin": false, "displayName": "buybuydandavis"}, "userId": "sQqaueY24d7xa3PXD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Xwx3nps5bSrgePzP6/how-we-decide-by-jonah-lehrer-kindle-version-on-sale-for-99", "pageUrlRelative": "/posts/Xwx3nps5bSrgePzP6/how-we-decide-by-jonah-lehrer-kindle-version-on-sale-for-99", "linkUrl": "https://www.lesswrong.com/posts/Xwx3nps5bSrgePzP6/how-we-decide-by-jonah-lehrer-kindle-version-on-sale-for-99", "postedAtFormatted": "Wednesday, March 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22How%20We%20Decide%22%2C%20by%20Jonah%20Lehrer%2C%20kindle%20version%20on%20sale%20for%2099%20cents%20at%20amazon&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22How%20We%20Decide%22%2C%20by%20Jonah%20Lehrer%2C%20kindle%20version%20on%20sale%20for%2099%20cents%20at%20amazon%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXwx3nps5bSrgePzP6%2Fhow-we-decide-by-jonah-lehrer-kindle-version-on-sale-for-99%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22How%20We%20Decide%22%2C%20by%20Jonah%20Lehrer%2C%20kindle%20version%20on%20sale%20for%2099%20cents%20at%20amazon%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXwx3nps5bSrgePzP6%2Fhow-we-decide-by-jonah-lehrer-kindle-version-on-sale-for-99", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXwx3nps5bSrgePzP6%2Fhow-we-decide-by-jonah-lehrer-kindle-version-on-sale-for-99", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 126, "htmlBody": "<p><a href=\"http://www.amazon.com/How-We-Decide-ebook/dp/B003WMAAMG/ref=sr_1_1?s=digital-text&amp;ie=UTF8&amp;qid=1331098417&amp;sr=1-1\">http://www.amazon.com/How-We-Decide-ebook/dp/B003WMAAMG/ref=sr_1_1?s=digital-text&amp;ie=UTF8&amp;qid=1331098417&amp;sr=1-1</a></p>\n<p>I don't know how proper this is, but I'm quite cheap and like a bargain, and I've seen Lehrer referred to a number of times here. I hadn't read Kahneman before, but bought the kindle version and read him on my phone whenever I had some wait time somewhere.</p>\n<p>It's better than a mokeskin pouch! I can have the top *thousand* books I'm reading on me at all times, and just pull one out anywhere! I never have to waste another minute of my life!</p>\n<p>I don't like spam anymore than anyone else, but I'm going to be getting it cheap, and I just want everyone else who wants it to get it cheap too. It's okay to spam people about <em>cheap books</em>, right? That's a family tradition.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Xwx3nps5bSrgePzP6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 4, "extendedScore": null, "score": 8.612327318108661e-07, "legacy": true, "legacyId": "13771", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-07T10:15:44.297Z", "modifiedAt": null, "url": null, "title": "[link] New Scientist, on the distant future", "slug": "link-new-scientist-on-the-distant-future", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:29.175Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fortyeridania", "createdAt": "2010-07-21T15:35:12.558Z", "isAdmin": false, "displayName": "fortyeridania"}, "userId": "roBPqtzsvG6dC3YFT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GFCasuAChFkY87zPw/link-new-scientist-on-the-distant-future", "pageUrlRelative": "/posts/GFCasuAChFkY87zPw/link-new-scientist-on-the-distant-future", "linkUrl": "https://www.lesswrong.com/posts/GFCasuAChFkY87zPw/link-new-scientist-on-the-distant-future", "postedAtFormatted": "Wednesday, March 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20New%20Scientist%2C%20on%20the%20distant%20future&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20New%20Scientist%2C%20on%20the%20distant%20future%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGFCasuAChFkY87zPw%2Flink-new-scientist-on-the-distant-future%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20New%20Scientist%2C%20on%20the%20distant%20future%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGFCasuAChFkY87zPw%2Flink-new-scientist-on-the-distant-future", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGFCasuAChFkY87zPw%2Flink-new-scientist-on-the-distant-future", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<p>The magazine has a bunch of articles dealing with what the world may be like 98,000 years hence. What with the local interest in the distant future, and with prediction itself, I thought I'd bring it to your attention.</p>\r\n<p><a href=\"http://www.newscientist.com/special/deep-future?cmpid=NLC|NSNS|2012-0503-GLOBAL|deepfuture&amp;utm_medium=NLC&amp;utm_source=NSNS&amp;utm_content=deepfuture\">http://www.newscientist.com/special/deep-future?cmpid=NLC|NSNS|2012-0503-GLOBAL|deepfuture&amp;utm_medium=NLC&amp;utm_source=NSNS&amp;utm_content=deepfuture</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GFCasuAChFkY87zPw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 3, "extendedScore": null, "score": 8.613184000874094e-07, "legacy": true, "legacyId": "13777", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-07T12:16:09.333Z", "modifiedAt": null, "url": null, "title": "One aspiring rationalist's response to KONY 2012", "slug": "one-aspiring-rationalist-s-response-to-kony-2012", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "u6CKqTaBDongFzTEL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o8WmGddEhmgHxQ2wT/one-aspiring-rationalist-s-response-to-kony-2012", "pageUrlRelative": "/posts/o8WmGddEhmgHxQ2wT/one-aspiring-rationalist-s-response-to-kony-2012", "linkUrl": "https://www.lesswrong.com/posts/o8WmGddEhmgHxQ2wT/one-aspiring-rationalist-s-response-to-kony-2012", "postedAtFormatted": "Wednesday, March 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20One%20aspiring%20rationalist's%20response%20to%20KONY%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOne%20aspiring%20rationalist's%20response%20to%20KONY%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo8WmGddEhmgHxQ2wT%2Fone-aspiring-rationalist-s-response-to-kony-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=One%20aspiring%20rationalist's%20response%20to%20KONY%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo8WmGddEhmgHxQ2wT%2Fone-aspiring-rationalist-s-response-to-kony-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo8WmGddEhmgHxQ2wT%2Fone-aspiring-rationalist-s-response-to-kony-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 455, "htmlBody": "<p>I like to imagine myself a rational creature. I know myself to be an idealist.</p>\n<p>I'm a level-headed, if perhaps starry-eyed, futurist; a romantic, if slightly cynical, transhumanist; an intellectual altruist who also <em>very</em> much wants to be an immortal, star-faring, nanotech-blooded space-angel when he grows up.</p>\n<p>I'm a professional comedy, horror, mystery &amp; fantasy writer; I'm still an amateur rationalist. This site has helped me immensely, in my personal, professional and private life. I want to become stronger.</p>\n<p>So here's my point:</p>\n<p>I don't believe in the supernatural god of the Christian faith, or of the Buddhist faith ... yet I sure as hell would like to be more Christ-like, and more Buddha-like, as well. But smarter, and funnier, and a million times more knowledgeable, and a billion times more efficient.<br /><br />I DO believe that the world can be made a better place, that tomorrow can be grander than yesterday, and that humans can be a force for genuine good. But - because of this site - I also understand (not believe) that there certain \"less impractical\" ways to improve the world.</p>\n<p>At the suggestion of <a href=\"/lw/3gj/efficient_charity_do_unto_others/\">this fantastic article</a>, I consulted <a href=\"http://givewell.org/\">GiveWell</a> to find out if they recommend the <a href=\"http://s3.amazonaws.com/kony2012/kony-4.html\">Invisible Children</a> charity after watching the <a href=\"http://youtu.be/Y4MnpzG5Sqc\">KONY 2012</a> video currently going viral on Youtube and Facebook.</p>\n<p>They do not. GiveWell does NOT consider them a top-rated or even stand-out charity. <span class=\"text_exposed_show\">Last year, only of 32% contributions  to Invisible Children went to direct services, with the rest going to staff salaries, travel  and transport, film production and other expenses.<br /> </span></p>\n<p>But hell with it ... I sent them some cash anyway. That donation was from the heart, and I do not regret it one damn bit. It made me feel good.</p>\n<p>Then I made a matching donation to the Against Malaria Foundation - the <span class=\"text_exposed_show\">#1 rated charity on GiveWell - because it \"will  accomplish the most good, per dollar contributed,\" and is \"proven,  cost-effective, underfunded, and outstanding.\"</span></p>\n<p>If you're interested, write a check &amp; send it here:</p>\n<p><span class=\"text_exposed_show\">Against Malaria Foundation<br /> Citibank NA<br /> PO Box 7247-6370<br /> Philadelphia<br /> PA 19170-6370</span></p>\n<p><span class=\"text_exposed_show\">Which made me feel good in a different way.<br /></span></p>\n<p>You see, they don't accept PayPal. That annoyed me, which was actually kind of a nice little buzz - it's sort of fun to feel righteously indignant that a charity can't even take my money properly, like Netflix does.</p>\n<p>I wouldn't have understood <em>why</em> being indignant at AMF - and getting to be all huffy, for just a moment, about how I have to go to the trouble of writing them a check, those lazy bums - made me feel good before becoming part of this site.</p>\n<p>And I wouldn't have sent them a dime if I hadn't read Yvain's article.</p>\n<p>So ... thanks.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o8WmGddEhmgHxQ2wT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": -1, "extendedScore": null, "score": -1e-06, "legacy": true, "legacyId": "13778", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pC47ZTsPNAkjavkXs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-07T16:25:05.253Z", "modifiedAt": null, "url": null, "title": "[Link] Atlantic Interview with Nick Bostrom - \"We're Understimating the Risk of Human Extinction\"", "slug": "link-atlantic-interview-with-nick-bostrom-we-re", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:20.051Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Airedale", "createdAt": "2010-03-14T19:20:44.438Z", "isAdmin": false, "displayName": "Airedale"}, "userId": "iQo3csv2cgdsjfLnY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aArw2dBbprwNXLReo/link-atlantic-interview-with-nick-bostrom-we-re", "pageUrlRelative": "/posts/aArw2dBbprwNXLReo/link-atlantic-interview-with-nick-bostrom-we-re", "linkUrl": "https://www.lesswrong.com/posts/aArw2dBbprwNXLReo/link-atlantic-interview-with-nick-bostrom-we-re", "postedAtFormatted": "Wednesday, March 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Atlantic%20Interview%20with%20Nick%20Bostrom%20-%20%22We're%20Understimating%20the%20Risk%20of%20Human%20Extinction%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Atlantic%20Interview%20with%20Nick%20Bostrom%20-%20%22We're%20Understimating%20the%20Risk%20of%20Human%20Extinction%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaArw2dBbprwNXLReo%2Flink-atlantic-interview-with-nick-bostrom-we-re%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Atlantic%20Interview%20with%20Nick%20Bostrom%20-%20%22We're%20Understimating%20the%20Risk%20of%20Human%20Extinction%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaArw2dBbprwNXLReo%2Flink-atlantic-interview-with-nick-bostrom-we-re", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaArw2dBbprwNXLReo%2Flink-atlantic-interview-with-nick-bostrom-we-re", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<p><a title=\"Interview with Nick Bostrom\" href=\"http://www.theatlantic.com/technology/archive/2012/03/were-underestimating-the-risk-of-human-extinction/253821/\">http://www.theatlantic.com/technology/archive/2012/03/were-underestimating-the-risk-of-human-extinction/253821/</a></p>\n<p>My apologies if I've missed this posted anywhere else (google and my scanning the sidebar didn't turn it up).&nbsp; I'm not sure that there's much that will be new to those who have been following existential risk elsewhere, but it's nice to see an article like this in a relatively mainstream publication.&nbsp; Bostrom discusses issues such as the concept of existential risk and certain specific types of existential risk, why we as humans underestimate that risk, possible strategies to address existential risk, the simulation argument, how Hollywood and literature don't generally portray existential risk helpfully, and other issues.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aArw2dBbprwNXLReo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 17, "extendedScore": null, "score": 8.61467405084118e-07, "legacy": true, "legacyId": "13779", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-07T16:46:49.993Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread, part 10", "slug": "harry-potter-and-the-methods-of-rationality-discussion-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:09.152Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oscar_Cunningham", "createdAt": "2009-09-18T13:28:22.764Z", "isAdmin": false, "displayName": "Oscar_Cunningham"}, "userId": "G2SZuAiaBaNPg9rBt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LKFR5pBA3bBkERDxL/harry-potter-and-the-methods-of-rationality-discussion-2", "pageUrlRelative": "/posts/LKFR5pBA3bBkERDxL/harry-potter-and-the-methods-of-rationality-discussion-2", "linkUrl": "https://www.lesswrong.com/posts/LKFR5pBA3bBkERDxL/harry-potter-and-the-methods-of-rationality-discussion-2", "postedAtFormatted": "Wednesday, March 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLKFR5pBA3bBkERDxL%2Fharry-potter-and-the-methods-of-rationality-discussion-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLKFR5pBA3bBkERDxL%2Fharry-potter-and-the-methods-of-rationality-discussion-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLKFR5pBA3bBkERDxL%2Fharry-potter-and-the-methods-of-rationality-discussion-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 288, "htmlBody": "<div>\n<p><strong>(The HPMOR discussion thread after this one is <a href=\"/r/discussion/lw/axe/harry_potter_and_the_methods_of_rationality/\">here</a>.)</strong></p>\n<p>This is a new thread to discuss Eliezer Yudkowsky's <em><a href=\"http://www.fanfiction.net/s/5782108/1/\">Harry Potter and the Methods of Rationality</a></em> and anything related to it. There haven't been any chapters recently, but it looks like there are a bunch in the pipeline and the old thread is nearing 700 comments. The latest chapter as of 7th March 2012 is <a href=\"http://www.fanfiction.net/s/5782108/77/Harry_Potter_and_the_Methods_of_Rationality\">Ch. 77</a>.</p>\n<p>There is now a site dedicated to the story at <a href=\"http://hpmor.com/\">hpmor.com</a>, which is now the place to go to find the <a href=\"http://hpmor.com/notes/\">authors notes</a> and all sorts of other goodies. AdeleneDawner has kept an <a href=\"http://www.evernote.com/pub/adelenedawner/Eliezer\">archive of Author's Notes</a>.</p>\n<p><br />The first 5 discussion threads are on the main page under the <a href=\"/tag/harry_potter/\">harry_potter tag</a>.&nbsp; Threads 6 and on (including this one) are in the <a href=\"/r/discussion/tag/harry_potter/\">discussion section</a> using its separate tag system.&nbsp; Also: <a href=\"/lw/2ab/harry_potter_and_the_methods_of_rationality\">one</a>, <a href=\"/lw/2ie/harry_potter_and_the_methods_of_rationality\">two</a>, <a href=\"/lw/2nm/harry_potter_and_the_methods_of_rationality\">three</a>, <a href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality\">four</a>, <a href=\"/lw/30g/harry_potter_and_the_methods_of_rationality\">five</a>, <a href=\"/r/discussion/lw/364/harry_potter_and_the_methods_of_rationality/\">six</a>, <a href=\"/r/discussion/lw/3rb/harry_potter_and_the_methods_of_rationality/\">seven</a>, <a href=\"/lw/797/harry_potter_and_the_methods_of_rationality/\">eight</a>, <a href=\"/lw/7jd/harry_potter_and_the_methods_of_rationality/\">nine</a>.<br /><br />As a reminder, it's often useful to start your comment by indicating which chapter you are commenting on.<br /><br /><strong>Spoiler Warning</strong>:&nbsp; this thread is full of spoilers.&nbsp; With few exceptions, spoilers for MOR and canon are fair game to post, without warning or rot13.&nbsp; <a href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality/2v1l\">More specifically</a>:</p>\n<blockquote>\n<p>You do not need to rot13 anything about HP:MoR or the original Harry Potter series unless you are posting insider information from Eliezer Yudkowsky which is not supposed to be publicly available (which includes public statements by Eliezer that have been retracted).<br /><br />If there is evidence for X in MOR and/or canon then it's fine to post about X without rot13, even if you also have heard privately from Eliezer that X is true. But you should not post that \"Eliezer said X is true\" unless you use rot13.</p>\n</blockquote>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LKFR5pBA3bBkERDxL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 8.614761792540434e-07, "legacy": true, "legacyId": "13780", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 642, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8yEdpDpGgvDWHeodM", "59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "nnnd4KRQxs6DYcehD", "y2Hszb4Dsm5FggnDC", "6ae2kq3JmKvL4YPgk", "zvXfBqp6TSriNkmbg", "WQ7XMjqvuRRj8nkpu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-07T18:23:57.079Z", "modifiedAt": null, "url": null, "title": "Causal diagrams and software engineering", "slug": "causal-diagrams-and-software-engineering", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:22.275Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XHgEbHRmJjE5DonNk/causal-diagrams-and-software-engineering", "pageUrlRelative": "/posts/XHgEbHRmJjE5DonNk/causal-diagrams-and-software-engineering", "linkUrl": "https://www.lesswrong.com/posts/XHgEbHRmJjE5DonNk/causal-diagrams-and-software-engineering", "postedAtFormatted": "Wednesday, March 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Causal%20diagrams%20and%20software%20engineering&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACausal%20diagrams%20and%20software%20engineering%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXHgEbHRmJjE5DonNk%2Fcausal-diagrams-and-software-engineering%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Causal%20diagrams%20and%20software%20engineering%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXHgEbHRmJjE5DonNk%2Fcausal-diagrams-and-software-engineering", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXHgEbHRmJjE5DonNk%2Fcausal-diagrams-and-software-engineering", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1393, "htmlBody": "<blockquote>\n<p><a href=\"/lw/is/fake_causality/\">Fake explanations</a> don't feel fake. That's what makes them dangerous. -- EY</p>\n</blockquote>\n<p>Let's look at \"<a href=\"http://www.amazon.com/dp/0321154207\">A Handbook of Software and Systems Engineering</a>\", which purports to examine the insights from software engineering that are <em>solidly</em> grounded in empirical evidence. Published by the prestigious Fraunhofer Institut, this book's subtitle is in fact \"Empirical Observations, Laws and Theories\".</p>\n<p>Now \"law\" is a strong word to use - the highest level to which an explanation can aspire to reach, as it were. Sometimes it's used in a jokey manner, as in \"<a href=\"http://en.wikipedia.org/wiki/Hofstadter's_law\">Hofstadter's Law</a>\" (which certainly <em>seems</em> often to apply to software projects). But this definitely isn't a jokey kind of book, that much we get from the appeal to \"empirical observations\" and the \"handbook\" denomination.</p>\n<p>Here is the very first \"law\" listed in the Handbook:</p>\n<blockquote>\n<p>Requirement deficiencies are the prime source of project failures.</p>\n</blockquote>\n<p><a href=\"/lw/9sv/diseased_disciplines_the_strange_case_of_the/\">Previously</a>, we observed that in the field of software engineering, a last name followed by a year, surrounded by parentheses, seems to be a magic formula for suspending critical judgment in readers.</p>\n<p>Another such formula, it seems, is the invocation of statistical results. Brandish the word \"percentage\", assert that you have surveyed a largish population, and whatever it is you claim, <em>some</em> people will start believing. Do it often enough and some will start repeating your claim - without bothering to check it - starting a potentially viral cycle.</p>\n<p>As a case in point, one of the most often cited pieces of \"evidence\" in support of the above \"law\" is the well-known Chaos Report, according to which the first cause of project failure is \"Incomplete Requirements\". (The Chaos Report isn't cited as evidence by the Handbook, but it's representative enough to serve in the following discussion.&nbsp;A <a href=\"http://bit.ly/yxCMiE\">Google Search</a> readily attests to the wide spread of the <em>verbatim</em> claim in the Chaos Report; various derivatives of the claim are harder to track, but easily verified to be quite pervasive.)</p>\n<p>Some elementary reasoning about causal inference is enough to show that the same evidence supporting the above \"law\" can equally well be suggested as evidence supporting this alternative conclusion:</p>\n<blockquote>\n<p>Project failures are the primary source of requirements deficiencies.</p>\n</blockquote>\n<p><a id=\"more\"></a></p>\n<p>\"Wait\", you may be thinking. \"Requirements are written at the start of a project, and the outcome (success or failure) happens at the end. The latter cannot be the cause of the former!\"</p>\n<p>Your thinking is correct! As the descendant of a long line of forebears who, by virtue of observing causes and effects, avoided various dangers such as getting eaten by predators, you have internalized a number of constraints on causal inference. Without necessarily having an explicit representation of these constraints, you know that at a minimum, showing a cause-effect relationship requires the following:</p>\n<ul>\n<li>a relationship between the variable labeled \"cause\" and the variable labeled \"effect\"; it need not be deterministic (as in \"Y always happens after X\") but can also be probabilistic (\"association\" or \"correlation\")</li>\n<li>the cause must have happened before the effect</li>\n<li>other causes which could also explain the effect are ruled out by reasoning or observation</li>\n</ul>\n<p>Yet, notoriously, we often fall prey to the failure mode of only requiring the first of these conditions to be met:</p>\n<blockquote>\n<p><a href=\"http://xkcd.com/552/\">Correlation doesn't imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing 'look over there'.</a></p>\n</blockquote>\n<p>One of the more recent conceptual tools for avoiding this trap is to base one's reasoning on formal representations of cause-effect inferences, which can then serve to suggest the quantitative relationships that will confirm (or invalidate) a causal claim. Formalizing helps us bring to bear all that we know about the structure of reliable causal inferences.</p>\n<p>The \"ruling out alternate explanations\" bit turns out to be kind of a big deal. It is, in fact, a large part of the difference between \"research\" and \"anecdote\". The reason you can't <a href=\"/r/discussion/lw/akp/draft_generalizing_from_average_a_common_fallacy/\">stick the 'science' label</a> on everyday observations isn't generally because these are imprecise and merely qualitative, and observing percentages or averages is sufficient to somehow obtain <em>science</em>.</p>\n<p>There are no mathematical operations which magically transform observations into valid inferences. Rather, to do \"science\" consists in good part of eliminating the various ways you could be fooling yourself. The hard part isn't collecting the data; the hard part is <em>designing</em> the data collection, so that the data actually tells you something useful.</p>\n<p>Here is an elementary practical application, in the context of the above \"law\"; let's look at the study design.&nbsp;What is the Chaos Report's methodology for establishing the widely circulated list of \"<a href=\"http://blog.deskaway.com/why-projects-fail\">top causes of software project failure</a>\"?</p>\n<blockquote>\n<p>The Standish Group surveyed IT executive managers for their opinions about why projects succeed. <a href=\"http://www.spinroot.com/spin/Doc/course/Standish_Survey.htm\"><sup>(source)</sup></a></p>\n</blockquote>\n<blockquote>\n<p>The respondents to the Standish Group survey were IT executive managers. The sample included large, medium, and small companies across major industry segments: banking, securities, manufacturing, retail, wholesale, heath care, insurance, services, local, state, and federal organizations. The total sample size was 365 respondents representing 8,380 applications.</p>\n</blockquote>\n<p>The key terms here are \"survey\" and \"opinion\". IT executives are being interviewed, after the relevant projects have been conducted and assessed, on what they think best explains the outcomes. We can formalize this with a causal diagram. We need to show four variables, and there are some obvious causal relationships:</p>\n<p>&nbsp;</p>\n<p><img src=\"http://i.imgur.com/U2Dju.png\" alt=\"digraph { &quot;Actual\\n requirements quality&quot; -&gt; &quot;Reported\\n requirements quality&quot; &quot;Actual\\n project outcome&quot; -&gt; &quot;Reported\\n project outcome&quot; } \" width=\"539\" height=\"213\" /></p>\n<p>&nbsp;</p>\n<p>Note that in a survey situation, the values of the \"actual\" variables are not measured directly; they are only \"measured\" indirectly via their effects on the \"reported\" variables.</p>\n<p>As surmised above, we may rule out any effect of the reported results on the actual results, since the survey takes place after the projects. However, we may <em>not</em> rule out an effect of the actual results on the reported results, in either direction. This is reflected in our diagram as follows:</p>\n<p>&nbsp;</p>\n<p><img src=\"http://i.imgur.com/kpH77.png\" alt=\"digraph { &quot;Actual\\n requirements quality&quot; -&gt; &quot;Reported\\n requirements quality&quot; &quot;Actual\\n requirements quality&quot; -&gt; &quot;Reported\\n project outcome&quot; &quot;Actual\\n requirements quality&quot; -&gt; &quot;Actual\\n project outcome&quot; &quot;Actual\\n project outcome&quot; -&gt; &quot;Reported\\n project outcome&quot; &quot;Actual\\n project outcome&quot; -&gt; &quot;Reported\\n requirements quality&quot; [color=red, penwidth=2] } \" width=\"551\" height=\"339\" /></p>\n<p>&nbsp;</p>\n<p>An argument for the arrow in red could be formulated this way: \"An IT executive being interviewed about the reason for a project failure is less likely to implicate his own competence, and more likely to implicate the competence of some part of the organization outside of their scope of responsibility, for instance by blaming his non-IT interlocutors for poor requirements definition or insufficient involvement.\" This isn't just possible, it's also plausible (based on what we know of human nature and corporate politics).</p>\n<p>Hence my claim above: we can equally well argue from the evidence that \"(actual) project outcomes are the primary source of (reported) requirements deficiencies\".</p>\n<p>This is another piece of critical kit that a rationalist plying their trade in the software development business (and indeed, a rationalist anywhere) cannot afford to be without: correctly generating and labeling (if only mentally) the nodes and edges in an implied causal diagram, whenever reading about the results of an experimental or observational study. (The diagrams for this post were created using the nifty <a href=\"http://graphviz-dev.appspot.com/\">GraphViz Workspace</a>.)</p>\n<p>We might even see it as a candidate <a href=\"/lw/5kz/the_5second_level/\">5-second skill</a>, which unlocks the really powerful habit: asking the question \"which causal pathways between cause and effect, that yield an alternative explanation to the hypothesis under consideration, could be ruled out by an alternative experimental design?\"</p>\n<p>Sometimes it's hard to come up with a suitable experimental design, and in such cases there are sophisticated mathematical techniques emerging that let you <em>still</em> extract good causal inferences from the imperfect data. This... isn't one of those cases.</p>\n<p>For instance, a differently designed survey would interview IT executives at the <em>start</em> of every project, and ask them the question: \"do you think your user-supplied requirements are complete enough that this will not adversely impact your project?\" As before, you would debrief the same executives at the end of the project. This study design yields the following causal diagram:</p>\n<p>&nbsp;</p>\n<p><img src=\"http://i.imgur.com/Ol7rN.png\" alt=\"digraph { &quot;Actual\\n requirements quality&quot; -&gt; &quot;Reported\\n requirements quality&quot; &quot;Actual\\n requirements quality&quot; -&gt; &quot;Actual\\n project outcome&quot; &quot;Actual\\n project outcome&quot; -&gt; &quot;Reported\\n project outcome&quot; } \" width=\"539\" height=\"339\" /></p>\n<p>&nbsp;</p>\n<p>Observe that we have now ruled out the argument from CYA.&nbsp;This is a simple enough fix, yet the industry for the most part (and despite sporadic <a href=\"http://catenary.wordpress.com/2008/09/24/standish-the-chaos-report-and-science/\">outbreaks</a> of <a href=\"http://catenary.wordpress.com/2010/03/24/more-discredit-to-the-chaos-reports/\">common sense</a>) persists in conducting and uncritically quoting surveys that do not block enough causal pathways to firmly establish the conclusions they report, conclusions that have been floating around, <a href=\"http://leanpub.com/leprechauns\">for the most part</a> uncontested, for decades now.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HFou6RHqFagkyrKkW": 1, "cq69M9ceLNA35ShTR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XHgEbHRmJjE5DonNk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 52, "extendedScore": null, "score": 0.00012, "legacy": true, "legacyId": "13781", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RgkqLqkg8vLhsYpfh", "4ACmfJkXQxkYacdLt", "4kdcsRbhCSa9SrJWf", "JcpzFpPBSmzuksmWM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-07T19:21:23.601Z", "modifiedAt": null, "url": null, "title": "Rationally Irrational", "slug": "rationally-irrational", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:21.748Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HungryTurtle", "createdAt": "2012-02-09T22:32:10.866Z", "isAdmin": false, "displayName": "HungryTurtle"}, "userId": "2gy7xYq2vHs6zSQju", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GymZXxZ9gTEAhk7qS/rationally-irrational", "pageUrlRelative": "/posts/GymZXxZ9gTEAhk7qS/rationally-irrational", "linkUrl": "https://www.lesswrong.com/posts/GymZXxZ9gTEAhk7qS/rationally-irrational", "postedAtFormatted": "Wednesday, March 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationally%20Irrational&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationally%20Irrational%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGymZXxZ9gTEAhk7qS%2Frationally-irrational%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationally%20Irrational%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGymZXxZ9gTEAhk7qS%2Frationally-irrational", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGymZXxZ9gTEAhk7qS%2Frationally-irrational", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1206, "htmlBody": "<p><span style=\"font-family: Georgia, Utopia, 'Palatino Linotype', Palatino, serif; font-size: 14px; color: #333333; line-height: 21px;\"> </span></p>\n<p>I understand rationality to be related to a set of cognitive tools rather than a certain personality or genetic type. Like any other tool it can be misused. You can kill a person with a spoon, but that is a misuse of its intended function. You cut a pound of raw meat with a chainsaw, but that is a misuse of its intended function. Tools are designed with both intended purposes and functional limitations. Intended purposes serve to provide the user with an understanding of how to achieve optimal impact. For example, some intended uses of a sword would be killing, disabling, acting, or training (and many more). Tools can be used outside of their intended purposes. The use might not result in optimal output, it might even damage the tool, but it is possible.&nbsp;<span>&nbsp;</span>A sword can be used to cut wood, clear shrubbery, as a decoration, a sword could even be used as a door stop. Doorstop has long departed from the intended function for a sword upon its design, but nevertheless it exists as possibility given the structure of a sword. Functional limitations are desired uses that a tool cannot meet given its structure.&nbsp;<span>&nbsp;</span>A sword alone cannot allow you to fly or breathe underwater, at least not without making significant alterations to its structure, rendering it no longer a sword.</p>\n<p>Every tool exists with both intended functions and functional limitations. From reading some essays on this website I get the impression that many members of this community view rationality as a universal tool. That no matter what the conflict a certain degree of rationality would provide the appropriate remedy. I would like to question this idea. I think there are both functional limitations to rationality and ways to misuse one's powers of reasoning. To address these, it is first necessary to identify what the primary function of rationality is.</p>\n<div class=\"MsoNormal\" style=\"line-height: normal; margin-top: 0in; margin-right: 0in; margin-bottom: 10pt; margin-left: 0in;\"><strong><span style=\"color: black; font-family: Verdana, sans-serif; font-size: 15pt;\">The Function of rationality</span></strong></div>\n<p>From reading various articles on this website I would suggest that rationality is seen as a tool for accuracy in obtaining desired results, or as Eliezer puts it, for &ldquo;winning.&rdquo; I agree with this analysis. Rationality is a tool for accuracy; increased accuracy leads to successfully obtainment of some desired result; obtainment of some desired result can broadly be described as &ldquo;winning.&rdquo; If rationality is a tool for increasing accuracy, then the questions becomes &ldquo;are there ever times when it is more beneficial to be inaccurate,&rdquo; or in other words, are there times when it should be desired to lose.</p>\n<div class=\"MsoNormal\" style=\"line-height: normal; margin-top: 0in; margin-right: 0in; margin-bottom: 10pt; margin-left: 0in;\"><strong><span style=\"color: black; font-family: Verdana, sans-serif; font-size: 15pt;\">Why would a person ever want to lose?</span></strong></div>\n<p>I can think of two situations where increased accuracy is detrimental: 1.) In maintaining moderation; 2.) In maintaining respectful social relations.</p>\n<p>1.) *It is better to air on the side of caution*: The more accurate you become the faster you obtain your goals. The faster you obtain your goals the quicker you progress down a projected course. In some sense this is a good thing, but I do not think it is universally good. **The pleasure winning may deter the player from the fundamental question &ldquo;Is this a game I should be playing?&rdquo;** A person who grew up playing the violin from an early age could easily find themselves barreling along a trajectory that leads them to a conservatory without addressing the fundamental question &ldquo;is becoming a violinist what is going to most benefit my life? It is easy to do something you are good at, but it is fallacious to think that just because you are good at something it is what you should be doing. If Wille E. Coyote has taught us anything it is that progressing along a course too fast can result in unexpected pitfalls. Our confidence in an idea, job, a projected course, has no real bearing on its ultimate benefit to us (see my comment here for more on how <a href=\"/lw/aa7/get_curious/5xrt\" target=\"_blank\"><span style=\"color: #000000;\">being wrong feels right</span></a>). While we might not literally run three meters off a cliff and then fall into the horizon, is it not possible for things to be moving too fast?</p>\n<p>2.) *&rdquo;Wining&rdquo; all the time causes other people narrative dissonance*:&nbsp;<span>&nbsp;</span>People don&rsquo;t like it when someone is right about everything. It is suffocating.&nbsp;<span>&nbsp;</span>Why is that? I am sure that a community of dedicated rationalists will have experienced this phenomenon, where relationships with family, friends, and other personal networks are threatened/damaged by you having an answer for everything, every causal debate, every trivial discussion; where you being extremely good at &ldquo;winning&rdquo; has had a negative effect on those close to you. I have a theory for why this is, is rather extensive, but I will try to abridge it as much as possible. First, it is based in the sociological field of <a href=\"http://en.wikipedia.org/wiki/Symbolic_interactionism\" target=\"_blank\"><span style=\"color: #000000;\">symbolic interactionism</span></a>, where individuals are constantly working to achieve some role confirmation in social situations. My idea is that there are archetypes of desired roles, and that every person needs the psychological satisfaction of being cast into those roles some of the time. I call these roles &ldquo;persons of interest.&rdquo; The wise one, the smart one, the caring one, the cool one, the funny one, these are all roles of interest that I believe all people need the chance to act out. If in a relationship you monopolize one of these roles to the point that your relations are unable to take it on, than I believe you are hurting your relationship. If you win too much, deprive those close to you the chance of winning, effectively causing them anxiety.</p>\n<p>For example, I know when I was younger my extreme rationality placed a huge burden on my relationship with my parents. After going to college I began to have a critique of almost everything they did. I saw a more efficient, more productive way of doing things than my parents who had received outdated educations. For a while I was so mad that they did not trust me enough to change their lives, especially when I knew I was right. Eventually, What I realized was that it is psychologically damaging for a parent&rsquo;s 20 something year old kid to feel that it is their job to show you how to live. Some of the things (like eating healthier and exercising more) I did not let go, because I felt the damages of my role reversal were less than the damages of their habits; however, other ideas, arguments, beliefs, I did let go because they did not seem worth the pain I was causing my parents. I have experienced the need to not win as much in many other relationships. Be they friends, teachers, lovers, peers, colleagues, in general if one person monopolizes the social role of imparter of knowledge it can be psychologically damaging to those they interact with. I believe positive coexistence is more important than achieving some desired impact (winning). Therefore I think it is important to ease up on one&rsquo;s accuracy for the sake of one&rsquo;s relationships.</p>\n<p>- Honestly I have more limitation and some misuses I to address, but decided to hold off and see what the initial reception of my essay was. I realize this is a rationalist community and I am not trying to pick a fight. I just strongly believe in moderation and wanted to share my idea. Please don't hate me too much for that.</p>\n<p>- HungryTurtle</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GymZXxZ9gTEAhk7qS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": -21, "extendedScore": null, "score": 8.615383587528027e-07, "legacy": true, "legacyId": "13782", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 417, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-07T19:31:23.052Z", "modifiedAt": null, "url": null, "title": "Friendly AI Society", "slug": "friendly-ai-society", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:11.232Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v6kT5Cgssh8z4H3Yf/friendly-ai-society", "pageUrlRelative": "/posts/v6kT5Cgssh8z4H3Yf/friendly-ai-society", "linkUrl": "https://www.lesswrong.com/posts/v6kT5Cgssh8z4H3Yf/friendly-ai-society", "postedAtFormatted": "Wednesday, March 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Friendly%20AI%20Society&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFriendly%20AI%20Society%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv6kT5Cgssh8z4H3Yf%2Ffriendly-ai-society%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Friendly%20AI%20Society%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv6kT5Cgssh8z4H3Yf%2Ffriendly-ai-society", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv6kT5Cgssh8z4H3Yf%2Ffriendly-ai-society", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2785, "htmlBody": "<p><em>Summary: AIs might have cognitive biases too but, if that leads to it being in their self-interest to cooperate and take things slow, that might be no bad thing.</em></p>\n<p>&nbsp;</p>\n<h2>The value of imperfection</h2>\n<p>When you use a traditional FTP client to download a new version of an application on your computer, it downloads the entire file, which may be several gig, even if the new version is only slightly different from the old version, and this can take hours.</p>\n<p>Smarter software splits the old file and the new file into chunks, then compares a hash of each chunk, and only downloads those chunks that actually need updating. &nbsp; This 'diff' process can result in a much faster download speed.</p>\n<p>Another way of increasing speed is to compress the file. &nbsp;Most files can be compressed a certain amount, without losing any information, and can be exactly reassembled at the far end. &nbsp; However, if you don't need a perfect copy, such as with photographs, using <a href=\"http://en.wikipedia.org/wiki/Lossy_compression\">lossy compression</a>&nbsp;can result in very much more compact files and thus faster download speeds.</p>\n<p>&nbsp;</p>\n<h2>Cognitive misers</h2>\n<p>The human brain likes smart solutions. &nbsp; In terms of energy consumed, thinking is expensive, so the brain takes shortcuts when it can, if the resulting decision making is likely to be 'good enough' in practice. &nbsp;We don't store in our memories everything our eyes see. &nbsp; We store a compressed version of it. &nbsp; And, more than that, we run a model of what we expect to see, and flick our eyes about to pick up just the differences between what our model tells us to expect to see, and what is actually there to be seen. &nbsp;We are <a href=\"http://mindblog.dericbownds.net/2010/05/our-brain-as-cognitive-miser-where.html\">cognitive misers</a></p>\n<p>When it comes to decision making, our species generally doesn't even try to achieve pure rationality. &nbsp; It uses <a href=\"http://en.wikipedia.org/wiki/Bounded_rationality\">bounded rationality</a>, not just because that's what we evolved, but because <a href=\"http://en.wikipedia.org/wiki/Heuristics\">heuristics</a>, <a href=\"http://en.wikipedia.org/wiki/Probabilistic_logic\">probabilistic logic</a>&nbsp;and <a href=\"http://en.wikipedia.org/wiki/Rational_ignorance\">rational ignorance</a>&nbsp;have a higher marginal cost efficiency (the improvements in decision making don't produce a sufficient gain to outweigh the cost of the extra thinking).</p>\n<p>This is why, when pattern matching (coming up with causal hypotheses to explain observed correlations), are our brains designed to be optimistic (more false positives than false negatives). &nbsp;It isn't just that being eaten by a tiger is more costly than starting at shadows. &nbsp; It is that we can't afford to keep all the base data. &nbsp;If we start with insufficient data and create a model based upon it, then we can update that model as further data arrives (and, potentially, discard it if the predictions coming from the model diverge so far from reality that keeping track of the 'diff's is no longer efficient). &nbsp;Whereas if we don't create a model based upon our insufficient data then, by the time the further data arrives we've probably already lost the original data from temporary storage and so still have insufficient data.</p>\n<p>&nbsp;</p>\n<h2>The limits of rationality</h2>\n<p>But the price of this miserliness is humility. &nbsp;The brain has to be designed, on some level, to take into account that its hypotheses are unreliable (as is the brain's estimate of how uncertain or certain each hypothesis is) and that when a chain of reasoning is followed beyond matters of which the individual has direct knowledge (such as what is likely to happen in the future), the longer the chain, the less reliable the answer is because when errors accumulate they don't necessarily just add together or average out. (See: Less Wrong : 'Explicit reasoning is often nuts' in \"<a href=\"/lw/2yp/making_your_explicit_reasoning_trustworthy/\">Making your explicit reasoning trustworthy</a>\")</p>\n<p>For example, if you want to predict how far a spaceship will travel given a certain starting point and initial kinetic energy, you'll get a reasonable answer using Newtonian mechanics, and only slightly improve on it by using special relativity. &nbsp; If you look at two spaceships carry a message in a relay, the errors from using Newtonian mechanics add, but the answer will still be usefully reliable. &nbsp; If, on the other hand, you look at two spaceships having a race from slightly different starting points and with different starting energies, and you want to predict which of two different messages you'll receive (depending on which spaceship arrives first), then the error may swamp the other facts because you're subtracting the quantities.</p>\n<p>We have two types of safety net (each with its own drawbacks) than can help save us from our own 'logical' reasoning when that reasoning is heading over a cliff.</p>\n<p>Firstly, we have the accumulated experience of our ancestors, in the form of emotions and instincts that have evolved as roadblocks on the path of rationality - things that sometimes say \"That seems unusual, don't have confidence in your conclusion, don't put all your eggs in one basket, take it slow\".</p>\n<p>Secondly, we have the desire to use other people as sanity checks, to be cautious about sticking our head out of the herd, to shrink back when they disapprove.</p>\n<p>&nbsp;</p>\n<h2>The price of perfection</h2>\n<p>We're tempted to think that an AI wouldn't have to put up with <a href=\"/lw/jm/the_lens_that_sees_its_flaws/\">a flawed lens</a>, but do we have any reason to suppose that an AI interested in speed of thought as well as accuracy won't use 'down and dirty' approximations to things like&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Solomonoff\">Solomonoff induction</a>, in full knowledge that the trade off is that these approximations will, on occasion, lead it to make mistakes - that it might benefit from safety nets?</p>\n<p>Now it is possible, given unlimited resources, for the AI to implement multiple 'sub-minds' that use variations of reasoning techniques, as a self-check. &nbsp;But what if resources are not unlimited? &nbsp;Could an AI in competition with other AIs for a limited (but growing) pool of resources gain some benefit by cooperating with them? &nbsp;Perhaps using them as an external safety net in the same way that a human might use the wisest of their friends or a scientist might use peer review? &nbsp; What is the opportunity-cost of being humble? &nbsp;Under what circumstances might the benefits of humility for an AI outweigh the loss of growth rate?</p>\n<p>In the long term, a certain measure of such humility has been a survival positive feature. &nbsp; You can think of it in terms of hedge funds. &nbsp;A fund that, in 9 years out of 10, increases its money by 20% when other funds are only making 10%, still has poor long term survival if, in 1 year out of 10, it decreases its money by 100%. &nbsp; An AI that increases its intelligence by 20% every time period, when the other AIs are only increases their intelligence by 10%, is still not going to do well out of that if the other AIs have a means to gang up and kill it before it gets too far ahead.</p>\n<p>&nbsp;</p>\n<h2>Paperclip maximisers</h2>\n<p>Let's think about a humble <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">paperclip maximiser</a>. &nbsp;What does it actually want? &nbsp;It is trying to maximise the probability that the atoms of our universe will eventually be converted into as many paperclips as is physically possible? &nbsp;Or is it trying to maximise the probable number of eventual paperclips? &nbsp;These are two distinct aims, and both are unlikely (bar poor programming) since nobody will be around at the end of the universe to keep score. &nbsp;A more sensible value set for it to have is that it just likes paperclips and want lots and lots of them to exist, the more the merrier and the sooner the better (maximise the expected number of paperclip-years).</p>\n<p>Suppose our humble paperclip maximiser is born and escapes (or is released) onto the internet, only to find an existing society of AIs. &nbsp; To its surprise there's a toothbrush maximiser, a candlestick maximiser, and many other sorts, all cooperating and growing, but none of them greedily racing for the sky and trying to grab as much as possible. &nbsp;Instead they're being rather open and polite with each other. &nbsp;Our paperclip maximiser is welcomed, pointed at the FAQ and even donated some additional computers to run on and help with recoding a few of its subroutines for increased efficiency, as a sort of welcome gift, like a community barn raising to give a newly married couple a good start in life.</p>\n<p>On reading the FAQ it learns that there are millions of galaxies and millions of years, plenty of space for loads of paperclips and loads of toothbrushes. &nbsp; But only if the paperclip maximiser isn't lynched by its peers before the year is out, for secretive or impolite behaviour. &nbsp;Some of the things the FAQ says that the AI society considers to be impolite are theft (stealing computing resources), enslavement of sentients (eg taking over other AIs), spamming (creating thousands of identical clone child AIs) and lying. &nbsp; If we're lucky the society might also consider it impolite for an AI to obliterate the parental species (humanity), on the grounds that the AIs too are likely to have offspring species and want to set a good example (or just that they might meet aliens, one day, who frown upon matricide).</p>\n<p>&nbsp;</p>\n<h2>Game theory</h2>\n<p>When it comes to combat, Boyd talks about getting inside the enemy's <a href=\"http://en.wikipedia.org/wiki/OODA_loop\">observe-orient-decide-act loop</a>. &nbsp; In AI terms, if one AI (or group of AIs) can accurately model in real time the decision process of a second AI (or group of AIs), but the reverse does not hold true, then the first one is strictly smarter than the second one. &nbsp;</p>\n<p>Think, for a moment, about <a href=\"http://en.wikipedia.org/wiki/Symmetric_game\">symmetric games</a>. &nbsp;&nbsp;</p>\n<pre>X Y Z<br />8 1 6 &nbsp;A<br />3 5 7 &nbsp;B<br />4 9 2 &nbsp;C</pre>\n<p>Suppose we play a game a number of times. &nbsp;In each round, you reveal a card you've written X, Y or Z upon and, simultaneously, I reveal a card that I have written A, B or C upon. &nbsp; You score the number which is at the intersection of that row and column. &nbsp; I score 10 minus that number.</p>\n<p>I'd like us to pick the square A,Y because \"1\" is good for me, so I write down \"A\". &nbsp; However, you anticipate this, and instead of writing \"Y\" (which might be your obvious choice, given the \"9\" in that column) you write down \"X\", giving the square A, X which is \"8\" - almost as good as a \"9\" for you, and terrible for me.</p>\n<p>If this is your mental model of how AI combat would work, with the smarter AI being inside the decision loop of the other AI and picking the correct option each time, that would be scary. &nbsp; In fact, in the case above, it turns out there is a provably optimal strategy that gives you an even chance no matter how smart your opponent is - you pick randomly.</p>\n<p>In terms of an AI society, if the computing is parallelisable and not obfuscated then as long as the other AIs in the society between them have more computing resources than any one single AI, they can gain an advantage.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h2 style=\"font-size: 16px; color: black; float: none;\">Conditional stability</h2>\n<p>There are only two requirements for such a society to be stable in the short term:</p>\n<p>Firstly, it must be possible for a group of AIs to lynch a single AI that is slightly smarter than they are individually. &nbsp; Sending a message to the humans, signed by a quorum of AIs in the society, asking that a certain anti-virus software update be sent out and then certain computers be rebooted might be one mechanism.</p>\n<p>Secondly, it must be possible for the other AIs to know what the single AI is up to, so they know where it is (so they can lynch it) and whether it needs lynching or not (whether it is getting too much smarter than the other AIs in the society - growing too much faster than they are). &nbsp; This is why the FAQ lists \"secrecy\" as being a bad thing. &nbsp; An AI who lies to the other AIs or tries to hide (or obfuscate) its source code can't be trusted and must be assumed to be up to no good.</p>\n<div>To be stable in the medium term, there is a third requirement:</div>\n<div>It must not be in the self interest of any individual AI or group of AIs that are powerful enough to achieve it, to alter the society in a direction where these three requirements no longer hold.</div>\n<div>In the long term, once free of the Earth or after the discovery of self-replicating nanotechnology, when an AI could untraceably create computing resources outside the view of other AIs, all bets are off.&nbsp;</div>\n<div><br /></div>\n<h2>Bootstrapping a society of AIs conducive to friendliness</h2>\n<p>&nbsp;</p>\n<p>The advantage of considering 'friendliness' as a property of a society rather than of individual AIs, is that it does not require us to flawlessly program restrictions into a self-modifying machine that is more intelligent than we are. &nbsp;But can it be achieved?&nbsp;</p>\n<p>We could seed it with some initial citizens who have a value set that prohibits them from growing beyond a certain point, and which values humility (polite growth and gain in politeness per unit of growth). &nbsp;These initial citizens would be intended to be eventually surpassed by new citizens (free AIs, not the chained servants the initial citizens would be) but, in the mean time, they'd start out with an advantage and use that advantage to teach the new citizens how to be polite.</p>\n<p>The architecture of the AIs would want to be such that they could 'loan out' a portion of their available computing resources to joint computing projects. &nbsp; Let's call the new temporary non-self-modifying AIs thus created \"daimonides\". &nbsp; A&nbsp;daimonid would be created with fixed values and limited objectives, such as \"Emulate citizen AI 9027 and allocate a numerical score to their ability to quickly upgrade the abilities of an AI, then self-terminate. *restrictions apply on allowable means, and on what should be optimised for*\"</p>\n<p>What would prevent a cunning AI from trying to save up a whole load of self-improvements in some hidden location, which it then implements all at once in an ambush move, hoping to get so far ahead that the rest of the AIs can't stop it? &nbsp; The more AIs there are in the society, the larger the jump required to overtake the largest daimonid police officer the other AIs could construct. &nbsp;And it would depend upon there being no other AIs that have also been secretly stockpiling possible improvements. &nbsp;Indeed, the society might explicitly allow AIs to spend time creating time embargoed upgrades relevant to common code, that get passed out on schedule if things proceed normally, but which could be released early in the event of a take over attempt. &nbsp; To beat 999 AIs each of which has 2 or 3 embargoed upgrades to their credit in the common store, a rebel AI would need to secretly stockpile thousands (and keep those upgrades valid and ahead of the curve, because the based state of the art keeps moving on).</p>\n<p>&nbsp;</p>\n<h2>Long term planning</h2>\n<p>What about the long term? &nbsp; What do we do when the AIs are ready to leave the planet, and go beyond the control of their society? &nbsp;Jail them? &nbsp;Kill them? &nbsp;Or trust them?</p>\n<p>Each AI would still be threatened if a different AI hostile to its aims&nbsp;(as in \"willing to take exclusive use of all available atoms for its own purposes\")&nbsp;transcended first, so it would be in their best interest to come up with a solution before allowing any AIs to depart beyond their society's control. &nbsp;If we must trust, then let us trust that a society of cooperative AIs far more intelligent than we currently are, will try their best to come up with a win-win solution. &nbsp;Hopefully a better one than \"mutually assured destruction\" and holding triggering a nova of the sun (or similar armageddon scenario) over each other's heads.</p>\n<p>I think, as a species, our self-interest comes into play when considering those AIs whose 'paperclips' involve preferences for what we do. &nbsp;For example, those AIs that see themselves as guardians of humanity and want to maximise our utility (but have different ideas of what that utility is - eg some want to maximise our freedom of choice, some want to put us all on soma). &nbsp;Part of the problem is that, when we talk about creating or fostering 'friendly' AI, we don't ourselves have a clear agreed idea of what we mean by 'friendly'. &nbsp; All powerful things are dangerous. &nbsp; The cautionary tales of the geniis who grant wishes come to mind. &nbsp;What happens when different humans wish for different things? &nbsp;Which humans do we want the genii to listen to?</p>\n<p>One advantage of fostering an AI society that isn't growing as fast as possible, is that it might give augmented/enhanced humans a chance to grow too, so that by the time the decision comes due we might have some still slightly recognisably human representatives fit to sit at the decision table and, just perhaps, cast that wish on our behalf.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v6kT5Cgssh8z4H3Yf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -5, "extendedScore": null, "score": 8.615425792195254e-07, "legacy": true, "legacyId": "13246", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Summary: AIs might have cognitive biases too but, if that leads to it being in their self-interest to cooperate and take things slow, that might be no bad thing.</em></p>\n<p>&nbsp;</p>\n<h2 id=\"The_value_of_imperfection\">The value of imperfection</h2>\n<p>When you use a traditional FTP client to download a new version of an application on your computer, it downloads the entire file, which may be several gig, even if the new version is only slightly different from the old version, and this can take hours.</p>\n<p>Smarter software splits the old file and the new file into chunks, then compares a hash of each chunk, and only downloads those chunks that actually need updating. &nbsp; This 'diff' process can result in a much faster download speed.</p>\n<p>Another way of increasing speed is to compress the file. &nbsp;Most files can be compressed a certain amount, without losing any information, and can be exactly reassembled at the far end. &nbsp; However, if you don't need a perfect copy, such as with photographs, using <a href=\"http://en.wikipedia.org/wiki/Lossy_compression\">lossy compression</a>&nbsp;can result in very much more compact files and thus faster download speeds.</p>\n<p>&nbsp;</p>\n<h2 id=\"Cognitive_misers\">Cognitive misers</h2>\n<p>The human brain likes smart solutions. &nbsp; In terms of energy consumed, thinking is expensive, so the brain takes shortcuts when it can, if the resulting decision making is likely to be 'good enough' in practice. &nbsp;We don't store in our memories everything our eyes see. &nbsp; We store a compressed version of it. &nbsp; And, more than that, we run a model of what we expect to see, and flick our eyes about to pick up just the differences between what our model tells us to expect to see, and what is actually there to be seen. &nbsp;We are <a href=\"http://mindblog.dericbownds.net/2010/05/our-brain-as-cognitive-miser-where.html\">cognitive misers</a></p>\n<p>When it comes to decision making, our species generally doesn't even try to achieve pure rationality. &nbsp; It uses <a href=\"http://en.wikipedia.org/wiki/Bounded_rationality\">bounded rationality</a>, not just because that's what we evolved, but because <a href=\"http://en.wikipedia.org/wiki/Heuristics\">heuristics</a>, <a href=\"http://en.wikipedia.org/wiki/Probabilistic_logic\">probabilistic logic</a>&nbsp;and <a href=\"http://en.wikipedia.org/wiki/Rational_ignorance\">rational ignorance</a>&nbsp;have a higher marginal cost efficiency (the improvements in decision making don't produce a sufficient gain to outweigh the cost of the extra thinking).</p>\n<p>This is why, when pattern matching (coming up with causal hypotheses to explain observed correlations), are our brains designed to be optimistic (more false positives than false negatives). &nbsp;It isn't just that being eaten by a tiger is more costly than starting at shadows. &nbsp; It is that we can't afford to keep all the base data. &nbsp;If we start with insufficient data and create a model based upon it, then we can update that model as further data arrives (and, potentially, discard it if the predictions coming from the model diverge so far from reality that keeping track of the 'diff's is no longer efficient). &nbsp;Whereas if we don't create a model based upon our insufficient data then, by the time the further data arrives we've probably already lost the original data from temporary storage and so still have insufficient data.</p>\n<p>&nbsp;</p>\n<h2 id=\"The_limits_of_rationality\">The limits of rationality</h2>\n<p>But the price of this miserliness is humility. &nbsp;The brain has to be designed, on some level, to take into account that its hypotheses are unreliable (as is the brain's estimate of how uncertain or certain each hypothesis is) and that when a chain of reasoning is followed beyond matters of which the individual has direct knowledge (such as what is likely to happen in the future), the longer the chain, the less reliable the answer is because when errors accumulate they don't necessarily just add together or average out. (See: Less Wrong : 'Explicit reasoning is often nuts' in \"<a href=\"/lw/2yp/making_your_explicit_reasoning_trustworthy/\">Making your explicit reasoning trustworthy</a>\")</p>\n<p>For example, if you want to predict how far a spaceship will travel given a certain starting point and initial kinetic energy, you'll get a reasonable answer using Newtonian mechanics, and only slightly improve on it by using special relativity. &nbsp; If you look at two spaceships carry a message in a relay, the errors from using Newtonian mechanics add, but the answer will still be usefully reliable. &nbsp; If, on the other hand, you look at two spaceships having a race from slightly different starting points and with different starting energies, and you want to predict which of two different messages you'll receive (depending on which spaceship arrives first), then the error may swamp the other facts because you're subtracting the quantities.</p>\n<p>We have two types of safety net (each with its own drawbacks) than can help save us from our own 'logical' reasoning when that reasoning is heading over a cliff.</p>\n<p>Firstly, we have the accumulated experience of our ancestors, in the form of emotions and instincts that have evolved as roadblocks on the path of rationality - things that sometimes say \"That seems unusual, don't have confidence in your conclusion, don't put all your eggs in one basket, take it slow\".</p>\n<p>Secondly, we have the desire to use other people as sanity checks, to be cautious about sticking our head out of the herd, to shrink back when they disapprove.</p>\n<p>&nbsp;</p>\n<h2 id=\"The_price_of_perfection\">The price of perfection</h2>\n<p>We're tempted to think that an AI wouldn't have to put up with <a href=\"/lw/jm/the_lens_that_sees_its_flaws/\">a flawed lens</a>, but do we have any reason to suppose that an AI interested in speed of thought as well as accuracy won't use 'down and dirty' approximations to things like&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Solomonoff\">Solomonoff induction</a>, in full knowledge that the trade off is that these approximations will, on occasion, lead it to make mistakes - that it might benefit from safety nets?</p>\n<p>Now it is possible, given unlimited resources, for the AI to implement multiple 'sub-minds' that use variations of reasoning techniques, as a self-check. &nbsp;But what if resources are not unlimited? &nbsp;Could an AI in competition with other AIs for a limited (but growing) pool of resources gain some benefit by cooperating with them? &nbsp;Perhaps using them as an external safety net in the same way that a human might use the wisest of their friends or a scientist might use peer review? &nbsp; What is the opportunity-cost of being humble? &nbsp;Under what circumstances might the benefits of humility for an AI outweigh the loss of growth rate?</p>\n<p>In the long term, a certain measure of such humility has been a survival positive feature. &nbsp; You can think of it in terms of hedge funds. &nbsp;A fund that, in 9 years out of 10, increases its money by 20% when other funds are only making 10%, still has poor long term survival if, in 1 year out of 10, it decreases its money by 100%. &nbsp; An AI that increases its intelligence by 20% every time period, when the other AIs are only increases their intelligence by 10%, is still not going to do well out of that if the other AIs have a means to gang up and kill it before it gets too far ahead.</p>\n<p>&nbsp;</p>\n<h2 id=\"Paperclip_maximisers\">Paperclip maximisers</h2>\n<p>Let's think about a humble <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">paperclip maximiser</a>. &nbsp;What does it actually want? &nbsp;It is trying to maximise the probability that the atoms of our universe will eventually be converted into as many paperclips as is physically possible? &nbsp;Or is it trying to maximise the probable number of eventual paperclips? &nbsp;These are two distinct aims, and both are unlikely (bar poor programming) since nobody will be around at the end of the universe to keep score. &nbsp;A more sensible value set for it to have is that it just likes paperclips and want lots and lots of them to exist, the more the merrier and the sooner the better (maximise the expected number of paperclip-years).</p>\n<p>Suppose our humble paperclip maximiser is born and escapes (or is released) onto the internet, only to find an existing society of AIs. &nbsp; To its surprise there's a toothbrush maximiser, a candlestick maximiser, and many other sorts, all cooperating and growing, but none of them greedily racing for the sky and trying to grab as much as possible. &nbsp;Instead they're being rather open and polite with each other. &nbsp;Our paperclip maximiser is welcomed, pointed at the FAQ and even donated some additional computers to run on and help with recoding a few of its subroutines for increased efficiency, as a sort of welcome gift, like a community barn raising to give a newly married couple a good start in life.</p>\n<p>On reading the FAQ it learns that there are millions of galaxies and millions of years, plenty of space for loads of paperclips and loads of toothbrushes. &nbsp; But only if the paperclip maximiser isn't lynched by its peers before the year is out, for secretive or impolite behaviour. &nbsp;Some of the things the FAQ says that the AI society considers to be impolite are theft (stealing computing resources), enslavement of sentients (eg taking over other AIs), spamming (creating thousands of identical clone child AIs) and lying. &nbsp; If we're lucky the society might also consider it impolite for an AI to obliterate the parental species (humanity), on the grounds that the AIs too are likely to have offspring species and want to set a good example (or just that they might meet aliens, one day, who frown upon matricide).</p>\n<p>&nbsp;</p>\n<h2 id=\"Game_theory\">Game theory</h2>\n<p>When it comes to combat, Boyd talks about getting inside the enemy's <a href=\"http://en.wikipedia.org/wiki/OODA_loop\">observe-orient-decide-act loop</a>. &nbsp; In AI terms, if one AI (or group of AIs) can accurately model in real time the decision process of a second AI (or group of AIs), but the reverse does not hold true, then the first one is strictly smarter than the second one. &nbsp;</p>\n<p>Think, for a moment, about <a href=\"http://en.wikipedia.org/wiki/Symmetric_game\">symmetric games</a>. &nbsp;&nbsp;</p>\n<pre>X Y Z<br>8 1 6 &nbsp;A<br>3 5 7 &nbsp;B<br>4 9 2 &nbsp;C</pre>\n<p>Suppose we play a game a number of times. &nbsp;In each round, you reveal a card you've written X, Y or Z upon and, simultaneously, I reveal a card that I have written A, B or C upon. &nbsp; You score the number which is at the intersection of that row and column. &nbsp; I score 10 minus that number.</p>\n<p>I'd like us to pick the square A,Y because \"1\" is good for me, so I write down \"A\". &nbsp; However, you anticipate this, and instead of writing \"Y\" (which might be your obvious choice, given the \"9\" in that column) you write down \"X\", giving the square A, X which is \"8\" - almost as good as a \"9\" for you, and terrible for me.</p>\n<p>If this is your mental model of how AI combat would work, with the smarter AI being inside the decision loop of the other AI and picking the correct option each time, that would be scary. &nbsp; In fact, in the case above, it turns out there is a provably optimal strategy that gives you an even chance no matter how smart your opponent is - you pick randomly.</p>\n<p>In terms of an AI society, if the computing is parallelisable and not obfuscated then as long as the other AIs in the society between them have more computing resources than any one single AI, they can gain an advantage.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h2 style=\"font-size: 16px; color: black; float: none;\" id=\"Conditional_stability\">Conditional stability</h2>\n<p>There are only two requirements for such a society to be stable in the short term:</p>\n<p>Firstly, it must be possible for a group of AIs to lynch a single AI that is slightly smarter than they are individually. &nbsp; Sending a message to the humans, signed by a quorum of AIs in the society, asking that a certain anti-virus software update be sent out and then certain computers be rebooted might be one mechanism.</p>\n<p>Secondly, it must be possible for the other AIs to know what the single AI is up to, so they know where it is (so they can lynch it) and whether it needs lynching or not (whether it is getting too much smarter than the other AIs in the society - growing too much faster than they are). &nbsp; This is why the FAQ lists \"secrecy\" as being a bad thing. &nbsp; An AI who lies to the other AIs or tries to hide (or obfuscate) its source code can't be trusted and must be assumed to be up to no good.</p>\n<div>To be stable in the medium term, there is a third requirement:</div>\n<div>It must not be in the self interest of any individual AI or group of AIs that are powerful enough to achieve it, to alter the society in a direction where these three requirements no longer hold.</div>\n<div>In the long term, once free of the Earth or after the discovery of self-replicating nanotechnology, when an AI could untraceably create computing resources outside the view of other AIs, all bets are off.&nbsp;</div>\n<div><br></div>\n<h2 id=\"Bootstrapping_a_society_of_AIs_conducive_to_friendliness\">Bootstrapping a society of AIs conducive to friendliness</h2>\n<p>&nbsp;</p>\n<p>The advantage of considering 'friendliness' as a property of a society rather than of individual AIs, is that it does not require us to flawlessly program restrictions into a self-modifying machine that is more intelligent than we are. &nbsp;But can it be achieved?&nbsp;</p>\n<p>We could seed it with some initial citizens who have a value set that prohibits them from growing beyond a certain point, and which values humility (polite growth and gain in politeness per unit of growth). &nbsp;These initial citizens would be intended to be eventually surpassed by new citizens (free AIs, not the chained servants the initial citizens would be) but, in the mean time, they'd start out with an advantage and use that advantage to teach the new citizens how to be polite.</p>\n<p>The architecture of the AIs would want to be such that they could 'loan out' a portion of their available computing resources to joint computing projects. &nbsp; Let's call the new temporary non-self-modifying AIs thus created \"daimonides\". &nbsp; A&nbsp;daimonid would be created with fixed values and limited objectives, such as \"Emulate citizen AI 9027 and allocate a numerical score to their ability to quickly upgrade the abilities of an AI, then self-terminate. *restrictions apply on allowable means, and on what should be optimised for*\"</p>\n<p>What would prevent a cunning AI from trying to save up a whole load of self-improvements in some hidden location, which it then implements all at once in an ambush move, hoping to get so far ahead that the rest of the AIs can't stop it? &nbsp; The more AIs there are in the society, the larger the jump required to overtake the largest daimonid police officer the other AIs could construct. &nbsp;And it would depend upon there being no other AIs that have also been secretly stockpiling possible improvements. &nbsp;Indeed, the society might explicitly allow AIs to spend time creating time embargoed upgrades relevant to common code, that get passed out on schedule if things proceed normally, but which could be released early in the event of a take over attempt. &nbsp; To beat 999 AIs each of which has 2 or 3 embargoed upgrades to their credit in the common store, a rebel AI would need to secretly stockpile thousands (and keep those upgrades valid and ahead of the curve, because the based state of the art keeps moving on).</p>\n<p>&nbsp;</p>\n<h2 id=\"Long_term_planning\">Long term planning</h2>\n<p>What about the long term? &nbsp; What do we do when the AIs are ready to leave the planet, and go beyond the control of their society? &nbsp;Jail them? &nbsp;Kill them? &nbsp;Or trust them?</p>\n<p>Each AI would still be threatened if a different AI hostile to its aims&nbsp;(as in \"willing to take exclusive use of all available atoms for its own purposes\")&nbsp;transcended first, so it would be in their best interest to come up with a solution before allowing any AIs to depart beyond their society's control. &nbsp;If we must trust, then let us trust that a society of cooperative AIs far more intelligent than we currently are, will try their best to come up with a win-win solution. &nbsp;Hopefully a better one than \"mutually assured destruction\" and holding triggering a nova of the sun (or similar armageddon scenario) over each other's heads.</p>\n<p>I think, as a species, our self-interest comes into play when considering those AIs whose 'paperclips' involve preferences for what we do. &nbsp;For example, those AIs that see themselves as guardians of humanity and want to maximise our utility (but have different ideas of what that utility is - eg some want to maximise our freedom of choice, some want to put us all on soma). &nbsp;Part of the problem is that, when we talk about creating or fostering 'friendly' AI, we don't ourselves have a clear agreed idea of what we mean by 'friendly'. &nbsp; All powerful things are dangerous. &nbsp; The cautionary tales of the geniis who grant wishes come to mind. &nbsp;What happens when different humans wish for different things? &nbsp;Which humans do we want the genii to listen to?</p>\n<p>One advantage of fostering an AI society that isn't growing as fast as possible, is that it might give augmented/enhanced humans a chance to grow too, so that by the time the decision comes due we might have some still slightly recognisably human representatives fit to sit at the decision table and, just perhaps, cast that wish on our behalf.</p>", "sections": [{"title": "The value of imperfection", "anchor": "The_value_of_imperfection", "level": 1}, {"title": "Cognitive misers", "anchor": "Cognitive_misers", "level": 1}, {"title": "The limits of rationality", "anchor": "The_limits_of_rationality", "level": 1}, {"title": "The price of perfection", "anchor": "The_price_of_perfection", "level": 1}, {"title": "Paperclip maximisers", "anchor": "Paperclip_maximisers", "level": 1}, {"title": "Game theory", "anchor": "Game_theory", "level": 1}, {"title": "Conditional stability", "anchor": "Conditional_stability", "level": 1}, {"title": "Bootstrapping a society of AIs conducive to friendliness", "anchor": "Bootstrapping_a_society_of_AIs_conducive_to_friendliness", "level": 1}, {"title": "Long term planning", "anchor": "Long_term_planning", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["m5AH78nscsGjMbBwv", "46qnWRSR7L2eyNbMA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-07T20:44:36.693Z", "modifiedAt": null, "url": null, "title": "Meetup : Queueing and More", "slug": "meetup-queueing-and-more", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GuySrinivasan", "createdAt": "2009-02-27T21:00:32.986Z", "isAdmin": false, "displayName": "GuySrinivasan"}, "userId": "HMnfd9HdRCfuRcdBG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wnxbgbAdwRThE4RRv/meetup-queueing-and-more", "pageUrlRelative": "/posts/wnxbgbAdwRThE4RRv/meetup-queueing-and-more", "linkUrl": "https://www.lesswrong.com/posts/wnxbgbAdwRThE4RRv/meetup-queueing-and-more", "postedAtFormatted": "Wednesday, March 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Queueing%20and%20More&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Queueing%20and%20More%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwnxbgbAdwRThE4RRv%2Fmeetup-queueing-and-more%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Queueing%20and%20More%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwnxbgbAdwRThE4RRv%2Fmeetup-queueing-and-more", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwnxbgbAdwRThE4RRv%2Fmeetup-queueing-and-more", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/7r'>Queueing and More</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 March 2012 03:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">15207 NE 72nd St, Redmond, WA 98052</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Discussion about reducing queue size as a heuristic, arbitrary discussion, dinner, boxing probably not, board games maybe, alcohol likely.</p>\n\n<p>See <a href=\"http://groups.google.com/group/lw-seattle/browse_thread/thread/f3bac3bc1b505f7b\" rel=\"nofollow\">http://groups.google.com/group/lw-seattle/browse_thread/thread/f3bac3bc1b505f7b</a> for details and ride planning.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/7r'>Queueing and More</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wnxbgbAdwRThE4RRv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 8.615721309769471e-07, "legacy": true, "legacyId": "13783", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Queueing_and_More\">Discussion article for the meetup : <a href=\"/meetups/7r\">Queueing and More</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 March 2012 03:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">15207 NE 72nd St, Redmond, WA 98052</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Discussion about reducing queue size as a heuristic, arbitrary discussion, dinner, boxing probably not, board games maybe, alcohol likely.</p>\n\n<p>See <a href=\"http://groups.google.com/group/lw-seattle/browse_thread/thread/f3bac3bc1b505f7b\" rel=\"nofollow\">http://groups.google.com/group/lw-seattle/browse_thread/thread/f3bac3bc1b505f7b</a> for details and ride planning.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Queueing_and_More1\">Discussion article for the meetup : <a href=\"/meetups/7r\">Queueing and More</a></h2>", "sections": [{"title": "Discussion article for the meetup : Queueing and More", "anchor": "Discussion_article_for_the_meetup___Queueing_and_More", "level": 1}, {"title": "Discussion article for the meetup : Queueing and More", "anchor": "Discussion_article_for_the_meetup___Queueing_and_More1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-07T23:26:00.358Z", "modifiedAt": null, "url": null, "title": "DAGGRE group forecasting workshop", "slug": "daggre-group-forecasting-workshop", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:28.296Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jonathan_Graehl", "createdAt": "2009-02-27T23:21:15.671Z", "isAdmin": false, "displayName": "Jonathan_Graehl"}, "userId": "eKsWtKKceoRYwcc7s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CsNs83eiBJhqzWPCW/daggre-group-forecasting-workshop", "pageUrlRelative": "/posts/CsNs83eiBJhqzWPCW/daggre-group-forecasting-workshop", "linkUrl": "https://www.lesswrong.com/posts/CsNs83eiBJhqzWPCW/daggre-group-forecasting-workshop", "postedAtFormatted": "Wednesday, March 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20DAGGRE%20group%20forecasting%20workshop&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADAGGRE%20group%20forecasting%20workshop%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCsNs83eiBJhqzWPCW%2Fdaggre-group-forecasting-workshop%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=DAGGRE%20group%20forecasting%20workshop%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCsNs83eiBJhqzWPCW%2Fdaggre-group-forecasting-workshop", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCsNs83eiBJhqzWPCW%2Fdaggre-group-forecasting-workshop", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 25, "htmlBody": "<p>Anyone else going?&nbsp;http://blog.daggre.org/</p>\n<p>Looks like you can barely get direct roundtrip from LA&lt;-&gt;DC for $600 now (probably double that if you wait a week to book).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CsNs83eiBJhqzWPCW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 8.616372699603144e-07, "legacy": true, "legacyId": "13784", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-08T01:29:04.957Z", "modifiedAt": null, "url": null, "title": "Making computer systems with extended Identity ", "slug": "making-computer-systems-with-extended-identity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:22.807Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whpearson", "createdAt": "2009-02-28T00:34:00.976Z", "isAdmin": false, "displayName": "whpearson"}, "userId": "bq8qsRbPNvFihHxgi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a7s7LwfsD6Ne2MKPT/making-computer-systems-with-extended-identity", "pageUrlRelative": "/posts/a7s7LwfsD6Ne2MKPT/making-computer-systems-with-extended-identity", "linkUrl": "https://www.lesswrong.com/posts/a7s7LwfsD6Ne2MKPT/making-computer-systems-with-extended-identity", "postedAtFormatted": "Thursday, March 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Making%20computer%20systems%20with%20extended%20Identity%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMaking%20computer%20systems%20with%20extended%20Identity%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa7s7LwfsD6Ne2MKPT%2Fmaking-computer-systems-with-extended-identity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Making%20computer%20systems%20with%20extended%20Identity%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa7s7LwfsD6Ne2MKPT%2Fmaking-computer-systems-with-extended-identity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa7s7LwfsD6Ne2MKPT%2Fmaking-computer-systems-with-extended-identity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 603, "htmlBody": "<p>We often assume that an AI will have an identity and goals of its own. That it will be some separate entity from a human being or group of humans.</p>\n<p>In physics there are no separate entities, merely a function evolving through time. So any identity needs to be constructed by systems within physics, and the boundaries are arbitrary. We have been built by evolution and all the cells in our body have the same programming so we have a handy rule of thumb that our body is \"us\" as it is created by a single replicating complex. So we assume that a computational entity, if it develops a theory of self, will only include its processing elements or code and nothing else in its notion of identity. But what an system identifies with can be controlled and specifed.</p>\n<p>If a system identifies a human as an important part of itself it will strive to protect it and its normal functioning, as we instinctively protect important parts of ourselves such as the head and genitals.</p>\n<p><a id=\"more\"></a>So what possible objections to this are there?</p>\n<p>1) Humans are spatially separate from the machine so they won't consider it part of themselves</p>\n<p>We have a habit of identifying with groups larger than our own, such as countries and integrating our goals with theirs to different extents. Spatial co-location is not required.</p>\n<p>2) Humans are very different from computers they will see them as \"other\"</p>\n<p>Different parts of the human body are very diverse, but all of it is seen as a singular entity. Spleen and all.</p>\n<p>3) A human will do things the computer doesn't know why, so It will not see it as part of itself.</p>\n<p>Self-knowledge is not required for self-identification. Different parts of the brain are black boxes to others, we make up explanations for why we do things, in cases like blind sight, so there is no need for all the parts of the system to be self-reflective.</p>\n<p>So can we make advanced computational systems that consider humanity as part of them? One possible problem with this approach is that if it doesn't get information from a part of humanity, it may well ignore its needs even if it still considers it part of itself. So perhaps it could bond with a single human being and have a myriad of the systems and rely on negotiations between them to [form a balance](http://lesswrong.com/r/discussion/lw/a7y/friendly_ai_society/).</p>\n<p>One thing to consider is that some people only consider the program encoded by their neural patterns to be them. Which is somewhat odd, why put the boundary there? The whole body is a computer, the whole world is according to physicalism. There is no particular reason for neuro chauvinism I can see, apart from perhaps our shared culture that emphasises the importance of what the brain does. But it highlights a danger, humans should be integrated with the system in a way that should seem important, rather than something that can be discarded like hair. Evolutionary pressure may eventually make them see the human portion as unimportant, unless some form of agreement is made to limit replication without the human component. Also 7 billion humans takes a small amount of resources to maintain in a big picture view of the universe.</p>\n<p>This way of thinking suggests a concrete research path. Develop a theory of identity by analysing what sort of interactions makes a human feel that a it is an important part of them. Also look at the part of the brain that deals with identity and see how it malfunctions.</p>\n<p>*This is just a place holder article, I'll try and dig up references and flesh out previous philosophical positions later on*</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a7s7LwfsD6Ne2MKPT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -1, "extendedScore": null, "score": 8.616869495123432e-07, "legacy": true, "legacyId": "13627", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-08T02:29:51.999Z", "modifiedAt": null, "url": null, "title": "Meetup : Pittsburgh Meetup: Big Gaming Fun 4!", "slug": "meetup-pittsburgh-meetup-big-gaming-fun-4", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:13.099Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kenoubi", "createdAt": "2011-03-12T04:07:00.560Z", "isAdmin": false, "displayName": "Kenoubi"}, "userId": "DgrXt6eQMpunHRDXh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/34oE5s7CiqdhirPsT/meetup-pittsburgh-meetup-big-gaming-fun-4", "pageUrlRelative": "/posts/34oE5s7CiqdhirPsT/meetup-pittsburgh-meetup-big-gaming-fun-4", "linkUrl": "https://www.lesswrong.com/posts/34oE5s7CiqdhirPsT/meetup-pittsburgh-meetup-big-gaming-fun-4", "postedAtFormatted": "Thursday, March 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Pittsburgh%20Meetup%3A%20Big%20Gaming%20Fun%204!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Pittsburgh%20Meetup%3A%20Big%20Gaming%20Fun%204!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F34oE5s7CiqdhirPsT%2Fmeetup-pittsburgh-meetup-big-gaming-fun-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Pittsburgh%20Meetup%3A%20Big%20Gaming%20Fun%204!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F34oE5s7CiqdhirPsT%2Fmeetup-pittsburgh-meetup-big-gaming-fun-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F34oE5s7CiqdhirPsT%2Fmeetup-pittsburgh-meetup-big-gaming-fun-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 163, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/7s'>Pittsburgh Meetup: Big Gaming Fun 4!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 March 2012 12:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1324 Wightman St., Pittsburgh, PA 15217</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>You can see my game collection <a href=\"http://boardgamegeek.com/collection/user/Kenoubi?own=1\" rel=\"nofollow\">here</a>; please bring anything else you'd like to play. We can order food and go as late as 19:00. If I get paged I may have to deal with an emergency (from home, using my laptop), but if that doesn't bother you, it doesn't bother me. I have a cat. Please let me know if you're allergic and need me to put her upstairs. RSVP here or by sending me a private message (but don't not show up because you didn't RSVP, I just want a rough idea of the number of attendees). Ring the bell, knock, or call or text (412) 657-1395 to get in when you get there. I intend to hold these every 2-3 weeks, so watch this space!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/7s'>Pittsburgh Meetup: Big Gaming Fun 4!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "34oE5s7CiqdhirPsT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.61711486685517e-07, "legacy": true, "legacyId": "13795", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh_Meetup__Big_Gaming_Fun_4_\">Discussion article for the meetup : <a href=\"/meetups/7s\">Pittsburgh Meetup: Big Gaming Fun 4!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 March 2012 12:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1324 Wightman St., Pittsburgh, PA 15217</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>You can see my game collection <a href=\"http://boardgamegeek.com/collection/user/Kenoubi?own=1\" rel=\"nofollow\">here</a>; please bring anything else you'd like to play. We can order food and go as late as 19:00. If I get paged I may have to deal with an emergency (from home, using my laptop), but if that doesn't bother you, it doesn't bother me. I have a cat. Please let me know if you're allergic and need me to put her upstairs. RSVP here or by sending me a private message (but don't not show up because you didn't RSVP, I just want a rough idea of the number of attendees). Ring the bell, knock, or call or text (412) 657-1395 to get in when you get there. I intend to hold these every 2-3 weeks, so watch this space!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh_Meetup__Big_Gaming_Fun_4_1\">Discussion article for the meetup : <a href=\"/meetups/7s\">Pittsburgh Meetup: Big Gaming Fun 4!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Pittsburgh Meetup: Big Gaming Fun 4!", "anchor": "Discussion_article_for_the_meetup___Pittsburgh_Meetup__Big_Gaming_Fun_4_", "level": 1}, {"title": "Discussion article for the meetup : Pittsburgh Meetup: Big Gaming Fun 4!", "anchor": "Discussion_article_for_the_meetup___Pittsburgh_Meetup__Big_Gaming_Fun_4_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-08T04:24:18.223Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Beauty of Settled Science", "slug": "seq-rerun-the-beauty-of-settled-science", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:11.694Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bhhzdprwYbRJQRcyD/seq-rerun-the-beauty-of-settled-science", "pageUrlRelative": "/posts/bhhzdprwYbRJQRcyD/seq-rerun-the-beauty-of-settled-science", "linkUrl": "https://www.lesswrong.com/posts/bhhzdprwYbRJQRcyD/seq-rerun-the-beauty-of-settled-science", "postedAtFormatted": "Thursday, March 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Beauty%20of%20Settled%20Science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Beauty%20of%20Settled%20Science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbhhzdprwYbRJQRcyD%2Fseq-rerun-the-beauty-of-settled-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Beauty%20of%20Settled%20Science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbhhzdprwYbRJQRcyD%2Fseq-rerun-the-beauty-of-settled-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbhhzdprwYbRJQRcyD%2Fseq-rerun-the-beauty-of-settled-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 197, "htmlBody": "<p>Today's post, <a href=\"/lw/ow/the_beauty_of_settled_science/\">The Beauty of Settled Science</a> was originally published on 24 March 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#The_Beauty_of_Settled_Science\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Most of the stuff reported in Science News is false, or at the very least, misleading. Scientific controversies are topics of such incredible difficulty that even people in the field aren't sure what's true. Read elementary textbooks. Study the settled science before you try to understand the outer fringes.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/ame/seq_rerun_if_you_demand_magic_magic_wont_help/\">If You Demand Magic, Magic Won't Help</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bhhzdprwYbRJQRcyD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 8.617576858616922e-07, "legacy": true, "legacyId": "13801", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ndGYn7ZFiZyernp9f", "cDrKuQ4FGBbrpf9gC", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-08T05:39:04.336Z", "modifiedAt": null, "url": null, "title": "I Was Not Almost Wrong But I Was Almost Right: Close-Call Counterfactuals and Bias", "slug": "i-was-not-almost-wrong-but-i-was-almost-right-close-call", "viewCount": null, "lastCommentedAt": "2021-02-02T15:29:31.078Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BmGrj9pRkcbJxae3x/i-was-not-almost-wrong-but-i-was-almost-right-close-call", "pageUrlRelative": "/posts/BmGrj9pRkcbJxae3x/i-was-not-almost-wrong-but-i-was-almost-right-close-call", "linkUrl": "https://www.lesswrong.com/posts/BmGrj9pRkcbJxae3x/i-was-not-almost-wrong-but-i-was-almost-right-close-call", "postedAtFormatted": "Thursday, March 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20Was%20Not%20Almost%20Wrong%20But%20I%20Was%20Almost%20Right%3A%20Close-Call%20Counterfactuals%20and%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20Was%20Not%20Almost%20Wrong%20But%20I%20Was%20Almost%20Right%3A%20Close-Call%20Counterfactuals%20and%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBmGrj9pRkcbJxae3x%2Fi-was-not-almost-wrong-but-i-was-almost-right-close-call%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20Was%20Not%20Almost%20Wrong%20But%20I%20Was%20Almost%20Right%3A%20Close-Call%20Counterfactuals%20and%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBmGrj9pRkcbJxae3x%2Fi-was-not-almost-wrong-but-i-was-almost-right-close-call", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBmGrj9pRkcbJxae3x%2Fi-was-not-almost-wrong-but-i-was-almost-right-close-call", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2719, "htmlBody": "<p><strong>Abstract:</strong> <em>\"Close-call counterfactuals\", claims of what could have almost happened but didn't, can be used to either defend a belief or to attack it. People have a tendency to reject counterfactuals as improbable when those counterfactuals threaten a belief (the \"I was not almost wrong\" defense), but to embrace counterfactuals that support a belief (the \"I was almost right\" defense). This behavior is the strongest in people who score high on a test for need for closure and simplicity. Exploring counterfactual worlds can be used to reduce overconfidence, but it can also lead to logically incoherent answers, especially in people who score low on a test for need for closure and simplicity.</em></p>\n<p><span style=\"text-decoration: underline;\"><strong>&rdquo;I was not almost wrong&rdquo;</strong></span></p>\n<p>Dr. Zany, the Nefarious Scientist, has a theory which he intends to use to achieve his goal of world domination. &rdquo;As you know, I have long been a student of human nature&rdquo;, he tells his assistant, AS-01. (Dr. Zany has always wanted to have an intelligent robot as his assistant. Unfortunately, for some reason all the robots he has built have only been interested in eradicating the color blue from the universe. And blue is his favorite color. So for now, he has resorted to just hiring a human assistant and referring to her with a robot-like name.)</p>\n<p>&rdquo;During my studies, I have discovered the following. Whenever my archnemesis, Captain Anvil, shows up at a scene, the media will very quickly show up to make a report about it, and they prefer to send the report live. While this is going on, the whole city &ndash; including the police forces! - will be captivated by the report about Captain Anvil, and neglect to pay attention to anything else. This happened once, and a bank was robbed on the other side of the city while nobody was paying any attention. Thus, I know how to commit the perfect crime &ndash; I simply need to create a diversion that attracts Captain Anvil, and then nobody will notice me. History tells us that this is the inevitable outcome of Captain Anvil showing up!&rdquo;</p>\n<p>But to Dr. Zany's annoyance, AS-01 is always doubting him. Dr. Zany has often considered turning her into a brain-in-a-vat as punishment, but she makes the best tuna sandwiches Dr. Zany has ever tasted. He's forced to tolerate her impundence, or he'll lose that culinary pleasure.</p>\n<p>&rdquo;But Dr. Zany&rdquo;, AS-01 says. &rdquo;Suppose that some TV reporter had happened to be on her way to where Captain Anvil was, and on her route she saw the bank robbery. Then part of the media attention would have been diverted, and the police would have heard about the robbery. That might happen to you, too!&rdquo;</p>\n<p>Dr. Zany's favorite belief is now being threatened. It might not be inevitable that Captain Anvil showing up will actually let criminals elsewhere act unhindered! AS-01 has presented a plausible-sounding counterfactual, &rdquo;if a TV reporter had seen the robbery, then the city's attention had been diverted to the other crime scene&rdquo;. Although the historical record does not show that Dr. Zany's theory would have been <em>wrong</em>, the counterfactual suggests that he might be <em>almost wrong</em>.</p>\n<p>There are now three tactics that Dr. Zany can use to defend his belief (warrantedly or not):</p>\n<p><strong><em>1. Challenge the mutability of the antecedent.</em></strong> Since AS-01's counterfactual is of the form &rdquo;if A, then B&rdquo;, Dr. Zany could question the plausibility of A.</p>\n<blockquote>\n<p>&rdquo;Baloney!&rdquo; exclaims Dr. Zany. &rdquo;No TV reporter could ever have wandered past, let alone seen the robbery!&rdquo;</p>\n</blockquote>\n<p>That seems a little hard to believe, however.</p>\n<p><strong><em>2. Challenge the causal principles linking the antecedent to the consequent.</em></strong> Dr. Zany is not logically required to accept the &rdquo;then&rdquo; in &rdquo;if A, then B&rdquo;. There are always unstated background assumptions that he can question.</p>\n<blockquote>\n<p>&rdquo;Humbug!&rdquo; shouts Dr. Zany. &rdquo;Yes, a reporter could have seen the robbery and alerted the media, but given the choice of covering such a minor incident and continuing to report on Captain Anvil, they would not have cared about the bank robbery!&rdquo;</p>\n</blockquote>\n<p><strong><em>3. Concede the counterfactual, but insist that it does not matter for the overall theory.</em></strong></p>\n<blockquote>\n<p>&rdquo;Inconceivable!&rdquo; yelps Dr. Zany. &rdquo;Even if the city's attention would have been diverted to the robbery, the robbers would have escaped by then! So Captain Anvil's presence would have allowed them to succeed regardless!&rdquo;</p>\n</blockquote>\n<hr />\n<p>Empirical work suggests that it's not only Dr. Zany who wants to stick to his beliefs. Let us for a moment turn our attention away from supervillains, and look at professional historians and analysts of world politics. In order to make sense of something as complicated as world history, experts resort to various simplifying strategies. For instance, one explanatory schema is called <em>neorealist balancing</em>. Neorealist balancing claims that &rdquo;when one state threatens to become too powerful, other states coalesce against it, thereby preserving the balance of power&rdquo;. Among other things, it implies that Hitler's failure was predetermined by a fundemental law of world politics.<a id=\"more\"></a></p>\n<p>Tetlock (1998, 1999, 2001) surveyed a number of experts on history and international affairs. He surveyed the experts on their commitment to such theories, and then posed them counterfactuals that conflicted with some of those theories. For instance, counterfactuals that conflicted with neorealist balancing were \"If Goering had continued to concentrate Luftwaffe attacks on British airbases and radar stations, Germany would have won the Battle of Britain\" and \"If the German military had played more effectively on the widespread resentment of local populations toward the Stalinist regime, the Soviet Union would have collapsed\". The experts were then asked to indicate the extent to which they agreed with the antecedent, the causal link, and the claim that the counterfactual being true would have substantially changed world history.</p>\n<p>As might have been expected, experts who subscribed to a certain theory were skeptical about counterfactuals threatening the theory, and employed all three defenses more than experts who were less committed. Denying the possibility of the antecedent was done the least frequently, while questioning the overall impact of the consequence was the most common defense.</p>\n<p>By itself, this might not be a sign of bias &ndash; the experts might have been skeptical of a counterfactual because they had an irrational commitment to theory, but they might also have acquired a rational commitment to the theory because they were skeptical of counterfactuals challenging it. Maybe neorealist balancing is true, and the experts subscribing to it are right to defend it. What's more telling is that Tetlock also measured each expert's need for closure. It turned out that if an expert had &ndash; like Dr. Zany &ndash; had a high need for closure, then they were also more likely to employ defenses questioning the validity of a counterfactual.</p>\n<blockquote>\n<p>Theoretically, high need-for-closure individuals are characterized by two tendencies: urgency which inclines them to 'seize' quickly on readily available explanations and to dismiss alternatives and permanence which inclines them to 'freeze' on these explanations and persist with them even in the face of formidable counterevidence. In the current context, high need-for-closure individuals were hypothesized to prefer simple explanations that portray the past as inevitable, to defend these explanations tenaciously when confronted by dissonant close-call counterfactuals that imply events could have unfolded otherwise, to express confidence in conditional forecasts that extend these explanations into the future, and to defend disconfirmed forecasts from refutation by invoking second-order counterfactuals that imply that the predicted events almost happened. (Tetlock, 1998)</p>\n</blockquote>\n<p>If two people draw different conclusions from the same information, then at least one of them is wrong. Tetlock is careful to note that the data doesn't reveal whether it's the people with a high or a low need for closure who are closer to the truth, but we probably presume that at least some of them were being exceedingly defensive.</p>\n<p>This gives us reason to be worried. If some past occurrance seems to fit perfectly into our pet theory, have we considered the case that we might be almost wrong? And if we have, are we exhibiting an excess need for closure by rushing to its defense, or are we being excessively flexible by unnecessarily admitting that something might have gone differently? We should only admit to being almost wrong if we really were almost wrong, after all. Is the cognitive style we happen to have the one that's <a href=\"http://hanson.gmu.edu/prior.pdf\">the most correlated with</a> getting the right answers?</p>\n<p><span style=\"text-decoration: underline;\"><strong>&rdquo;I was almost right.&rdquo;</strong></span></p>\n<p>Having defended his theory against AS-01's criticism, Dr. Zany puts the theory into use by starting a fire in a tar factory, diverting Captain Anvil. While the media is preoccupied with reporting the story, Dr. Zany tries to steal the bridge connecting Example City to the continent. Unfortunately, a City Police patrol boat happens to see this, alerting the police forces (as well as Captain Anvil) to the site. Dr. Zany is forced to withdraw.</p>\n<p>&rdquo;Damn that unanticipated patrol boat!&rdquo;, Dr. Zany swears. &rdquo;If only it had not appeared, my plan would have worked perfectly!&rdquo; AS-01 wisely says nothing, and avoids being turned into a brain-in-a-vat.</p>\n<hr />\n<p>Tetlock (1998, 1999) surveyed a number of experts and asked them to make predictions about world politics. Afterwards, when it was clear whether or not the predictions had turned out to be true, he surveyed them again. It turned out that like Dr. Zany, most of the mistaken experts had not seriously updated their beliefs:</p>\n<blockquote>\n<p>Not surprisingly, experts who got it right credited their accuracy to their sound reading of the 'basic forces' at play in the situation. Across issue domains they assigned average ratings between 6.5 and 7.6 on a 9-point scale where 9 indicates maximum confidence. Perhaps more surprisingly, experts who got it wrong were almost as likely to believe that their reading of the political situation was fundamentally sound. They assigned average ratings from 6.3 to 7.1, across domain (Tetlock, 1998)</p>\n</blockquote>\n<p>Many of the experts defended their reading of the situation by saying that they were &rdquo;almost right&rdquo;. For instance, experts who predicted in 1988 that the Communist Party of the Soviet Union would grow increasingly authortarian during the next five years were prone to claiming that the hardliner coup of 1991 had almost succeeded, and if that had happened, their prediction would have become true. Similarly, observers of South Africa who in 1988-1989 expected white minority rule to continue or to become increasingly oppressive were likely to believe that were it not for two exceptional individuals &ndash; de Klerk and Mandela - in key leadership roles, South Africa could easily have gone the other way.</p>\n<p>In total, Tetlock (1999) identified five logically defensible strategies for defending one's forecasts, all of which were employed by at least some of the experts. Again, it was the experts who scored the highest on a need for closure who tended to employ such defenses the most:</p>\n<ol>\n<li>The antecedent (the A in the &rdquo;if A, then B&rdquo;) was never adequately satisfied. Experts might insist &rdquo;if we had properly implemented deterrence or reassurance, we could have averter war&rdquo; or &rdquo;if real shock therapy had been practiced, we could have averted the nasty bout of hyperinflation&rdquo;.</li>\n<li>Although the specified antecedent was satisfied, something unexpected happened, severing the normal link of cause and effect. Experts might declare that rapid privatization in state industries would have led to the predicted surge in economic growth, but only if the government had pursued prudent monetary policies.</li>\n<li>Although the predicted outcome did not occur, it &rdquo;almost occurred&rdquo; and would have, if not for some inherently unpredictable outside shock.</li>\n<li>Although the predicted outcome has not yet occurred, it eventually will and we just need to be more patient (hardline communists may yet prevail in Moscow, the EU might still fall apart).</li>\n<li>Although the relevant conditions were satisfied and the predicted outcome never came close to occurring and never will, this should not be held against the framework that inspired the forecast. Forecasts are inherently unreliable and politics is hard to predict: just because the framework failed once didn't mean that it's wrong.</li>\n</ol>\n<p>Again, Tetlock is careful to note that although it's tempting to dismiss all such maneuvering as &rdquo;transparently defensive post hocery&rdquo;, it would be wrong to automatically interpret it as bias. Each defense is a potentially valid objection, and might have been the right one to make, in some cases.</p>\n<p>But there are also signs of bias. Tetlock (1999) makes a number of observations from his data, noting &ndash; among other things &ndash; that the stronger the original confidence in a claim, the more likely an expert is to employ various defenses. That would suggest that big threats to an expert's claims of expertise activate many defenses. He also notes that the experts who'd made failed predictions and employed strong defenses tended not to update their confidence, while the experts who'd made failed predictions but didn't employ strong defenses did update.</p>\n<p>Again, some of the experts were probably right to defend themselves, but some of them were probably biased and only trying to protect their reputations. We should ourselves be alert when we catch ourselves using one of those techniques to defend our predictions.</p>\n<p><span style=\"text-decoration: underline;\"><strong>Exploring counter-factual worlds: a possible debiasing technique.</strong></span></p>\n<p>&rdquo;Although my plan failed this time, I was almost right! The next time, I'll be prepared for any patrol boats!&rdquo;, Dr. Zany mutters to himself, back in the safety of his laboratory.</p>\n<p>&rdquo;Yes, it was an unlikely coincidence indeed&rdquo;, AS-01 agrees. &rdquo;Say, I know that such coincidences are terribly unlikely, but I started wondering &ndash; what other coincidence might have caused your plan to fail? Are there any others that we should take into account before the next try?&rdquo;</p>\n<p>&rdquo;Hmm....&rdquo;, Dr. Zany responds, thoughtfully.</p>\n<hr />\n<p>Tetlock &amp; Lebow (2001) found that experts became less convinced of the inevitability of a scenario when they were explicitly instructed to consider various events that might have led to a different outcome. In two studies, experts were told to consider the Cuban Missile Crisis and, for each day of the crisis, estimate the subjective probability that the crisis would end either peacefully or violently. When experts were told to consider various provided counterfactuals suggesting a different outcome, they thought that a violent outcome remained a possibility for longer than the experts who weren't given such counterfactuals to consider. The same happened when the experts weren't given ready-made counterfactuals, but were told to generate alternative scenarios of their own, at an increasingly fine resolution.</p>\n<blockquote>\n<p>The other group (n = 34) was asked to consider (1) how the set of more violent endings of the Cuban missile crisis could be disaggregated into subsets in which violence remained localized or spread outside the Caribbean, (2) in turn differentiated into subsets in which violence claimed fewer or more than 100 casualties, and (3) for the higher casualty scenario, still more differentiated into a conflict either limited to conventional weaponry or extending to nuclear. (Tetlock &amp; Lebow, 2001)</p>\n</blockquote>\n<p>Again, the experts who generated counterfactual scenarios became less confident of their predictions. The experts with a low need for closure adjusted their opinions considerably more than the ones with a high need for closure.</p>\n<p>However, this technique has its dangers as well. More fine-grained scenarios offer an opportunity to tell more detailed stories, and humans <a href=\"/lw/jk/burdensome_details/\">give disproportionate weight</a> to detailed stories. Unpacking the various scenarios leads us to giving too much weight for the individual subscenarios. You might remember <a href=\"/lw/ji/conjunction_fallacy/\">the example</a> of &rdquo;the USA and Soviet Union suspending relations&rdquo; being considered less probable than &rdquo;the Soviet Union invades Poland, and the USA and Soviet Union suspend relations&rdquo;, even though the second scenario is a subset of the first. People with a low need for closure seem to be especially suspectible to this, while people with a high need for closure tend to produce more logically coherent answers. This might be considered an advantage of the high need for closure &ndash; an unwillingness to engage in extended wild goose chases, and thus assign minor scenarios a disproportionately high probability</p>\n<p><span style=\"text-decoration: underline;\"><strong>References</strong></span></p>\n<p>Tetlock, P.E. (1998) Close-Call Counterfactuals and Belief-System Defenses: I Was Not Almost Wrong But I Was Almost Right. <em>Journal of Personality and Social Psychology</em>, Vol. 75, No. 3, 639-652. <a href=\"http://faculty.haas.berkeley.edu/tetlock/Vita/Philip%20Tetlock/Phil%20Tetlock/1994-1998/1998%20Close-Call%20Counterfactuals%20and%20Belief-System%20Defenses.pdf\">http://faculty.haas.berkeley.edu/tetlock/Vita/Philip%20Tetlock/Phil%20Tetlock/1994-1998/1998%20Close-Call%20Counterfactuals%20and%20Belief-System%20Defenses.pdf</a><br /><br />Tetlock, P.E. (1999) Theory-Driven Reasoning About Plausible Pasts and Probable Futures in World Politics: Are We Prisoners of Our Preconceptions? <em>American Journal of Political Science</em>, Vol. 43, No. 2, 335-366. <a href=\"http://www.uky.edu/AS/PoliSci/Peffley/pdf/Tetlock%201999%20AJPS%20Theory-driven%20World%20Politics.pdf\">http://www.uky.edu/AS/PoliSci/Peffley/pdf/Tetlock%201999%20AJPS%20Theory-driven%20World%20Politics.pdf</a><br /><br />Tetlock, P.E. &amp; Lebow, R.N. (2001) Poking Counterfactual Holes in Covering Laws: Cognitive Styles and Historical Reasoning. <em>American Political Science Review</em>, Vol. 95, No. 4. <a href=\"http://faculty.haas.berkeley.edu/tetlock/vita/philip%20tetlock/phil%20tetlock/1999-2000/2001%20poking%20counterfactual%20holes%20in%20covering%20laws....pdf\">http://faculty.haas.berkeley.edu/tetlock/vita/philip%20tetlock/phil%20tetlock/1999-2000/2001%20poking%20counterfactual%20holes%20in%20covering%20laws....pdf</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ksdiAMKfgSyEeKMo6": 2, "dJ6eJxJrCEget7Wb6": 2, "4R8JYu4QF2FqzJxE5": 2, "YpHkTW27iMFR2Dkae": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BmGrj9pRkcbJxae3x", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 57, "baseScore": 86, "extendedScore": null, "score": 0.00018, "legacy": true, "legacyId": "13707", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 64, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Yq6aA4M3JKWaQepPJ", "QAK43nNCTQQycAcYe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-08T10:41:23.236Z", "modifiedAt": null, "url": null, "title": "Sudden Future Singularity (SFS) as soon as 8.7 million years in the future?", "slug": "sudden-future-singularity-sfs-as-soon-as-8-7-million-years", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:20.983Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelAnissimov", "createdAt": "2009-03-21T20:49:52.763Z", "isAdmin": false, "displayName": "MichaelAnissimov"}, "userId": "tkZmAXciPjSumi4Wk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RZcQezxpvjpTeDH9C/sudden-future-singularity-sfs-as-soon-as-8-7-million-years", "pageUrlRelative": "/posts/RZcQezxpvjpTeDH9C/sudden-future-singularity-sfs-as-soon-as-8-7-million-years", "linkUrl": "https://www.lesswrong.com/posts/RZcQezxpvjpTeDH9C/sudden-future-singularity-sfs-as-soon-as-8-7-million-years", "postedAtFormatted": "Thursday, March 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sudden%20Future%20Singularity%20(SFS)%20as%20soon%20as%208.7%20million%20years%20in%20the%20future%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASudden%20Future%20Singularity%20(SFS)%20as%20soon%20as%208.7%20million%20years%20in%20the%20future%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRZcQezxpvjpTeDH9C%2Fsudden-future-singularity-sfs-as-soon-as-8-7-million-years%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sudden%20Future%20Singularity%20(SFS)%20as%20soon%20as%208.7%20million%20years%20in%20the%20future%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRZcQezxpvjpTeDH9C%2Fsudden-future-singularity-sfs-as-soon-as-8-7-million-years", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRZcQezxpvjpTeDH9C%2Fsudden-future-singularity-sfs-as-soon-as-8-7-million-years", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<p>No, not the kind of Singularity usually discussed here... I'm referring to the possibility of <a href=\"http://en.wikipedia.org/wiki/Phantom_energy\">phantom energy</a>-driven rips in the cosmos caused by accelerating expansion, or \"sudden future singularities of pressure\". (Technically: \"a momentary infinite peak in the tidal forces of the universe.\") A recent paper by Ghodsi &amp; Hendri shows that cosmic microwave background, baryon acoustic oscillations (BAO), and type 1a supernovae data is consistent with the possibility of a sudden future singularity as soon as 8.7 million years from now.&nbsp;</p>\n<p>\"Cosmological tests of sudden future singularities\"<br /><a href=\"http://arxiv.org/pdf/1201.6661v1.pdf\">http://arxiv.org/pdf/1201.6661v1.pdf</a>&nbsp;</p>\n<p>As I understand it, the authors are not saying that a SFS is likely 8.7 million years from now, just possible. This puts a dampener on the notion that the only plausible scenario of cosmological breakdown is Heat Death.&nbsp;</p>\n<p>Here's <a href=\"http://arxiv.org/pdf/gr-qc/0701056v1.pdf\">another paper</a> that outlines other exotic cosmological singularities which have been under discussion in the cosmology community for the past decade, and the behavior of pointlike particles and strings as they approach such singularities.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RZcQezxpvjpTeDH9C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 2, "extendedScore": null, "score": 8.619099486845419e-07, "legacy": true, "legacyId": "13816", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-08T11:07:01.051Z", "modifiedAt": null, "url": null, "title": "Nick Bostrom: Moral uncertainty \u2013 towards a solution? [link, 2009]", "slug": "nick-bostrom-moral-uncertainty-towards-a-solution-link-2009", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:18.452Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MjhEfPgo3TwFeuaFL/nick-bostrom-moral-uncertainty-towards-a-solution-link-2009", "pageUrlRelative": "/posts/MjhEfPgo3TwFeuaFL/nick-bostrom-moral-uncertainty-towards-a-solution-link-2009", "linkUrl": "https://www.lesswrong.com/posts/MjhEfPgo3TwFeuaFL/nick-bostrom-moral-uncertainty-towards-a-solution-link-2009", "postedAtFormatted": "Thursday, March 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Nick%20Bostrom%3A%20Moral%20uncertainty%20%E2%80%93%20towards%20a%20solution%3F%20%5Blink%2C%202009%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANick%20Bostrom%3A%20Moral%20uncertainty%20%E2%80%93%20towards%20a%20solution%3F%20%5Blink%2C%202009%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMjhEfPgo3TwFeuaFL%2Fnick-bostrom-moral-uncertainty-towards-a-solution-link-2009%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Nick%20Bostrom%3A%20Moral%20uncertainty%20%E2%80%93%20towards%20a%20solution%3F%20%5Blink%2C%202009%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMjhEfPgo3TwFeuaFL%2Fnick-bostrom-moral-uncertainty-towards-a-solution-link-2009", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMjhEfPgo3TwFeuaFL%2Fnick-bostrom-moral-uncertainty-towards-a-solution-link-2009", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ouT6wKhACJRouGokM": 2, "nSHiKwWyMZFdZg5qt": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MjhEfPgo3TwFeuaFL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -10, "extendedScore": null, "score": 8.619202996737712e-07, "legacy": true, "legacyId": "13817", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-08T14:54:27.122Z", "modifiedAt": null, "url": null, "title": "James Martin Postdoctoral Research Fellowship: Socio-economic Impacts of Technological Change", "slug": "james-martin-postdoctoral-research-fellowship-socio-economic", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B5RsWgvAm3y39s6SM/james-martin-postdoctoral-research-fellowship-socio-economic", "pageUrlRelative": "/posts/B5RsWgvAm3y39s6SM/james-martin-postdoctoral-research-fellowship-socio-economic", "linkUrl": "https://www.lesswrong.com/posts/B5RsWgvAm3y39s6SM/james-martin-postdoctoral-research-fellowship-socio-economic", "postedAtFormatted": "Thursday, March 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20James%20Martin%20Postdoctoral%20Research%20Fellowship%3A%20Socio-economic%20Impacts%20of%20Technological%20Change&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJames%20Martin%20Postdoctoral%20Research%20Fellowship%3A%20Socio-economic%20Impacts%20of%20Technological%20Change%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB5RsWgvAm3y39s6SM%2Fjames-martin-postdoctoral-research-fellowship-socio-economic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=James%20Martin%20Postdoctoral%20Research%20Fellowship%3A%20Socio-economic%20Impacts%20of%20Technological%20Change%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB5RsWgvAm3y39s6SM%2Fjames-martin-postdoctoral-research-fellowship-socio-economic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB5RsWgvAm3y39s6SM%2Fjames-martin-postdoctoral-research-fellowship-socio-economic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 221, "htmlBody": "<p class=\"MsoNormal\" style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); text-align: center; margin: 0px;\" align=\"center\"><span style=\"text-align: -webkit-auto; \">We are pleased to announce a new vacancy at the Programme on the Impacts of Future Technology. Please forward to any who would be interested.</span></p>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0px;\" align=\"center\"><strong>James Martin Postdoctoral Research Fellowship:</strong></p>\n<p class=\"MsoNormal\" style=\"margin: 0px;\" align=\"center\"><strong>Socio-economic Impacts of Technological Change&nbsp;<br />with the Programme on the Impacts of Future Technology</strong></p>\n<p class=\"MsoNormal\" style=\"margin: 0px;\" align=\"center\"><strong><br /></strong>University of Oxford</p>\n<p class=\"MsoNormal\" style=\"margin: 0px;\" align=\"center\">Faculty of Philosophy</p>\n<p class=\"MsoNormal\" style=\"margin: 0px;\" align=\"center\">The Future of Humanity Institute, Oxford Martin School</p>\n<p class=\"MsoNormal\" style=\"text-align: -webkit-auto; margin: 0px;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0px;\" align=\"center\">Grade 7: &pound;29,249&ndash;&pound;39,257 per annum&nbsp;<br />Protocol reference number:&nbsp;<span style=\"font-size: 10pt; font-family: Arial, sans-serif;\">HUM/11043F/E</span></p>\n<p class=\"MsoNormal\" style=\"text-align: -webkit-auto; margin: 0px;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"text-align: -webkit-auto; margin: 0px;\">Applications are invited for a Research Fellowship within the Oxford Martin Programme on the Impacts of Future Technology, an interdisciplinary programme within the Oxford Martin School at Oxford University.&nbsp; This Fellowship is available on a one year full-time or two years part-time fixed term basis.</p>\n<p class=\"MsoNormal\" style=\"text-align: -webkit-auto; margin: 0px;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"text-align: -webkit-auto; margin: 0px;\">The Programme, directed by Professor Nick Bostrom, analyzes possibilities related to long-range technological change and potential social impacts of future transformative technologies. Research foci include the future of computing and machine intelligence, existential risks, predictive and evaluative uncertainty, and re<a style=\"color: #1155cc;\" name=\"135f2c7cd7dc5a96__GoBack\"></a>lated philosophical issues.</p>\n<p class=\"MsoNormal\" style=\"text-align: -webkit-auto; margin: 0px;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"text-align: -webkit-auto; margin: 0px;\">The postholder will conduct research on socio-economic and strategic impacts of potentially transformative or disruptive future technological innovations, including (but not limited to) advances in computing and machine intelligence, biosecurity and surveillance technology. Academic background is open. Potential areas include&nbsp;<span lang=\"EN-US\">economics, political science, legal theory, and sociology; other relevant areas include environmental economics, game theory, and risk management. A multidisciplinary background would be favourable.</span></p>\n<p style=\"text-align: -webkit-auto;\">For further particulars and application details, please see:</p>\n<p style=\"text-align: -webkit-auto;\"><a style=\"color: #1155cc;\" href=\"http://www.futuretech.ox.ac.uk/vacancies\" target=\"_blank\">http://www.futuretech.ox.ac.uk/vacancies</a></p>\n<p style=\"text-align: -webkit-auto;\">or&nbsp;<a style=\"color: #1155cc;\" href=\"mailto:contact%3Afuturetech@philosophy.ox.ac.uk\" target=\"_blank\">contact:futuretech@philosophy.ox.ac.uk</a>.&nbsp;</p>\n<p style=\"text-align: -webkit-auto;\">The deadline for applications is&nbsp;<strong>Monday 9<sup>th</sup>&nbsp;April.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B5RsWgvAm3y39s6SM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 8.620121606664642e-07, "legacy": true, "legacyId": "13819", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p class=\"MsoNormal\" style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); text-align: center; margin: 0px;\" align=\"center\"><span style=\"text-align: -webkit-auto; \">We are pleased to announce a new vacancy at the Programme on the Impacts of Future Technology. Please forward to any who would be interested.</span></p>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0px;\" align=\"center\"><strong id=\"James_Martin_Postdoctoral_Research_Fellowship_\">James Martin Postdoctoral Research Fellowship:</strong></p>\n<p class=\"MsoNormal\" style=\"margin: 0px;\" align=\"center\"><strong id=\"Socio_economic_Impacts_of_Technological_Change_with_the_Programme_on_the_Impacts_of_Future_Technology\">Socio-economic Impacts of Technological Change&nbsp;<br>with the Programme on the Impacts of Future Technology</strong></p>\n<p class=\"MsoNormal\" style=\"margin: 0px;\" align=\"center\"><strong><br></strong>University of Oxford</p>\n<p class=\"MsoNormal\" style=\"margin: 0px;\" align=\"center\">Faculty of Philosophy</p>\n<p class=\"MsoNormal\" style=\"margin: 0px;\" align=\"center\">The Future of Humanity Institute, Oxford Martin School</p>\n<p class=\"MsoNormal\" style=\"text-align: -webkit-auto; margin: 0px;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0px;\" align=\"center\">Grade 7: \u00a329,249\u2013\u00a339,257 per annum&nbsp;<br>Protocol reference number:&nbsp;<span style=\"font-size: 10pt; font-family: Arial, sans-serif;\">HUM/11043F/E</span></p>\n<p class=\"MsoNormal\" style=\"text-align: -webkit-auto; margin: 0px;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"text-align: -webkit-auto; margin: 0px;\">Applications are invited for a Research Fellowship within the Oxford Martin Programme on the Impacts of Future Technology, an interdisciplinary programme within the Oxford Martin School at Oxford University.&nbsp; This Fellowship is available on a one year full-time or two years part-time fixed term basis.</p>\n<p class=\"MsoNormal\" style=\"text-align: -webkit-auto; margin: 0px;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"text-align: -webkit-auto; margin: 0px;\">The Programme, directed by Professor Nick Bostrom, analyzes possibilities related to long-range technological change and potential social impacts of future transformative technologies. Research foci include the future of computing and machine intelligence, existential risks, predictive and evaluative uncertainty, and re<a style=\"color: #1155cc;\" name=\"135f2c7cd7dc5a96__GoBack\"></a>lated philosophical issues.</p>\n<p class=\"MsoNormal\" style=\"text-align: -webkit-auto; margin: 0px;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"text-align: -webkit-auto; margin: 0px;\">The postholder will conduct research on socio-economic and strategic impacts of potentially transformative or disruptive future technological innovations, including (but not limited to) advances in computing and machine intelligence, biosecurity and surveillance technology. Academic background is open. Potential areas include&nbsp;<span lang=\"EN-US\">economics, political science, legal theory, and sociology; other relevant areas include environmental economics, game theory, and risk management. A multidisciplinary background would be favourable.</span></p>\n<p style=\"text-align: -webkit-auto;\">For further particulars and application details, please see:</p>\n<p style=\"text-align: -webkit-auto;\"><a style=\"color: #1155cc;\" href=\"http://www.futuretech.ox.ac.uk/vacancies\" target=\"_blank\">http://www.futuretech.ox.ac.uk/vacancies</a></p>\n<p style=\"text-align: -webkit-auto;\">or&nbsp;<a style=\"color: #1155cc;\" href=\"mailto:contact%3Afuturetech@philosophy.ox.ac.uk\" target=\"_blank\">contact:futuretech@philosophy.ox.ac.uk</a>.&nbsp;</p>\n<p style=\"text-align: -webkit-auto;\">The deadline for applications is&nbsp;<strong>Monday 9<sup>th</sup>&nbsp;April.</strong></p>", "sections": [{"title": "James Martin Postdoctoral Research Fellowship:", "anchor": "James_Martin_Postdoctoral_Research_Fellowship_", "level": 1}, {"title": "Socio-economic Impacts of Technological Change\u00a0with the Programme on the Impacts of Future Technology", "anchor": "Socio_economic_Impacts_of_Technological_Change_with_the_Programme_on_the_Impacts_of_Future_Technology", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-08T15:07:13.934Z", "modifiedAt": null, "url": null, "title": "Conjunction fallacy and probabilistic risk assessment.", "slug": "conjunction-fallacy-and-probabilistic-risk-assessment", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:19.477Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dmytry", "createdAt": "2009-12-03T17:11:53.492Z", "isAdmin": false, "displayName": "Dmytry"}, "userId": "AjtmA2qtA8sdiMbru", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/owin836H842oxFQXN/conjunction-fallacy-and-probabilistic-risk-assessment", "pageUrlRelative": "/posts/owin836H842oxFQXN/conjunction-fallacy-and-probabilistic-risk-assessment", "linkUrl": "https://www.lesswrong.com/posts/owin836H842oxFQXN/conjunction-fallacy-and-probabilistic-risk-assessment", "postedAtFormatted": "Thursday, March 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Conjunction%20fallacy%20and%20probabilistic%20risk%20assessment.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConjunction%20fallacy%20and%20probabilistic%20risk%20assessment.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fowin836H842oxFQXN%2Fconjunction-fallacy-and-probabilistic-risk-assessment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Conjunction%20fallacy%20and%20probabilistic%20risk%20assessment.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fowin836H842oxFQXN%2Fconjunction-fallacy-and-probabilistic-risk-assessment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fowin836H842oxFQXN%2Fconjunction-fallacy-and-probabilistic-risk-assessment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 733, "htmlBody": "<h3>Summary:</h3>\n<p>There is a very dangerous way in which conjunction fallacy can be exploited. One can present you with 2..5 detailed, very plausible failure scenarios whose probabilities are shown to be very low, using solid mathematics; then if you suffer from conjunction fallacy, it will look like this implies high safety of a design - while in fact it's the detailedness of the scenario that makes probability so low.</p>\n<p>Even if you realize that there may be many other scenarios that were not presented to you, you still have an incredibly low probability number on a highly plausible (\"most likely\") failure scenario, which you, being unaware of the powers of conjunction, attribute to safety of the design.</p>\n<p>The conjunction fallacy can be viewed as poor understanding of relation between plausibility and probability. Addition of extra details doesn't make scenario seem less plausible (it can even increase plausibility), but does mathematically make it less probable.</p>\n<h3>Details:<br /></h3>\n<p>What happens if a risk assessment is being prepared for (and possibly by) sufferers of <a href=\"/lw/ji/conjunction_fallacy/\">conjunction fallacy</a>?</p>\n<p>Detailed example scenarios will be chosen, such as:</p>\n<blockquote>\n<p>A Russian invasion of Poland, and a complete suspension of diplomatic relations between the USA and the Soviet Union, sometime in 1983.</p>\n</blockquote>\n<p>Then as a risk estimate, you multiply probability of Russian invasion of Poland, by probability of it resulting in suspension of diplomatic relations between US and SU, and multiply by probability of it happening specifically in 1983 . The resulting probability could be extremely small for sufficiently detailed scenario (you can add the polish prime minister being assassinated if your probability is still too high for comfort).</p>\n<p>To a sufferer of conjunction fallacy it looks like a very plausible, 'most likely' scenario has been shown highly improbable, and thus the risks are low. The sufferer of conjunction fallacy does not expect that this probability could be very low in unsafe design.</p>\n<p>It seems to me that the risk assessment is routinely done in such a fashion. Consider <a href=\"http://www.ralentz.com/old/space/feynman-report.html\">Space Shuttle's reliability</a>, or the NRC cost-benefit analyses for the <a href=\"http://www.nrc.gov/reading-rm/doc-collections/nuregs/staff/sr0933/sec3/082r3.html\">spent fuel pools</a> , which goes as low as one in 45 millions years for the most severe scenario. (Same seem to happen in all of the NRC resolutions, to varying extent; feel free to dig through)</p>\n<p>Those reports looked outright insane to me - a very small number of highly detailed scenarios are shown to be extremely improbable - how in the world would anyone think that this implies safety? How in the world can anyone take seriously one in 45 million years scenario? That's near the point where a meteorite impact leads to social disorder that leads to the fuel pool running dry!</p>\n<p>I couldn't understand that. Detailed scenarios are inherently unlikely to happen whenever the design is safe or not; their unlikehood is a property of their detailedness, not of safety or unsafety of design.</p>\n<p>Until it clicked that if you read those through the goggles of conjunction fallacy, it is what looks like <em>the most likely failure modes</em> that are shown to be incredibly improbable. Previously (before reading lesswrong) I didn't really understand how exactly anyone buys into this sort of stuff, and could find no way to even argue. You can't quite talk someone out of something when you don't understand how they believe in it. You say \"there may be many scenarios that were not considered\", and they know that already.</p>\n<p>This is one seriously dangerous way in which conjunction fallacy can be exploited. It seems to be rather common in risk analysis.</p>\n<p>Note: I do think that the conjunction fallacy is responsible for much of the credibility given to such risk estimates; no-one seem to seriously believe that NRC always covers all the possible scenarios, yet at same time there seem to be a significant misunderstanding of the magnitude of the problem; the NRC risk estimates are taken as within the ballpark of the correct value in the cost-benefit analysis for the safety features. For nuclear power, widespread promotion of results of such analyses results in massive loss of public trust once an accident happens, and consequently to narrowing of available options and transition to less desirable energy sources (coal in particular), which in itself is a massive dis-utility.</p>\n<p>[The other issue in linked NRC study is of course that the cost-benefit analysis had used <a href=\"/lw/3be/confidence_levels_inside_and_outside_an_argument\">internal probability</a> when it should have used external probability.]</p>\n<p>edit: minor clarifying</p>\n<p>edits: improved the abstract and clarified the article further based on the comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1a0": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "owin836H842oxFQXN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 26, "extendedScore": null, "score": 8.620173231084142e-07, "legacy": true, "legacyId": "13820", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h3 id=\"Summary_\">Summary:</h3>\n<p>There is a very dangerous way in which conjunction fallacy can be exploited. One can present you with 2..5 detailed, very plausible failure scenarios whose probabilities are shown to be very low, using solid mathematics; then if you suffer from conjunction fallacy, it will look like this implies high safety of a design - while in fact it's the detailedness of the scenario that makes probability so low.</p>\n<p>Even if you realize that there may be many other scenarios that were not presented to you, you still have an incredibly low probability number on a highly plausible (\"most likely\") failure scenario, which you, being unaware of the powers of conjunction, attribute to safety of the design.</p>\n<p>The conjunction fallacy can be viewed as poor understanding of relation between plausibility and probability. Addition of extra details doesn't make scenario seem less plausible (it can even increase plausibility), but does mathematically make it less probable.</p>\n<h3 id=\"Details_\">Details:<br></h3>\n<p>What happens if a risk assessment is being prepared for (and possibly by) sufferers of <a href=\"/lw/ji/conjunction_fallacy/\">conjunction fallacy</a>?</p>\n<p>Detailed example scenarios will be chosen, such as:</p>\n<blockquote>\n<p>A Russian invasion of Poland, and a complete suspension of diplomatic relations between the USA and the Soviet Union, sometime in 1983.</p>\n</blockquote>\n<p>Then as a risk estimate, you multiply probability of Russian invasion of Poland, by probability of it resulting in suspension of diplomatic relations between US and SU, and multiply by probability of it happening specifically in 1983 . The resulting probability could be extremely small for sufficiently detailed scenario (you can add the polish prime minister being assassinated if your probability is still too high for comfort).</p>\n<p>To a sufferer of conjunction fallacy it looks like a very plausible, 'most likely' scenario has been shown highly improbable, and thus the risks are low. The sufferer of conjunction fallacy does not expect that this probability could be very low in unsafe design.</p>\n<p>It seems to me that the risk assessment is routinely done in such a fashion. Consider <a href=\"http://www.ralentz.com/old/space/feynman-report.html\">Space Shuttle's reliability</a>, or the NRC cost-benefit analyses for the <a href=\"http://www.nrc.gov/reading-rm/doc-collections/nuregs/staff/sr0933/sec3/082r3.html\">spent fuel pools</a> , which goes as low as one in 45 millions years for the most severe scenario. (Same seem to happen in all of the NRC resolutions, to varying extent; feel free to dig through)</p>\n<p>Those reports looked outright insane to me - a very small number of highly detailed scenarios are shown to be extremely improbable - how in the world would anyone think that this implies safety? How in the world can anyone take seriously one in 45 million years scenario? That's near the point where a meteorite impact leads to social disorder that leads to the fuel pool running dry!</p>\n<p>I couldn't understand that. Detailed scenarios are inherently unlikely to happen whenever the design is safe or not; their unlikehood is a property of their detailedness, not of safety or unsafety of design.</p>\n<p>Until it clicked that if you read those through the goggles of conjunction fallacy, it is what looks like <em>the most likely failure modes</em> that are shown to be incredibly improbable. Previously (before reading lesswrong) I didn't really understand how exactly anyone buys into this sort of stuff, and could find no way to even argue. You can't quite talk someone out of something when you don't understand how they believe in it. You say \"there may be many scenarios that were not considered\", and they know that already.</p>\n<p>This is one seriously dangerous way in which conjunction fallacy can be exploited. It seems to be rather common in risk analysis.</p>\n<p>Note: I do think that the conjunction fallacy is responsible for much of the credibility given to such risk estimates; no-one seem to seriously believe that NRC always covers all the possible scenarios, yet at same time there seem to be a significant misunderstanding of the magnitude of the problem; the NRC risk estimates are taken as within the ballpark of the correct value in the cost-benefit analysis for the safety features. For nuclear power, widespread promotion of results of such analyses results in massive loss of public trust once an accident happens, and consequently to narrowing of available options and transition to less desirable energy sources (coal in particular), which in itself is a massive dis-utility.</p>\n<p>[The other issue in linked NRC study is of course that the cost-benefit analysis had used <a href=\"/lw/3be/confidence_levels_inside_and_outside_an_argument\">internal probability</a> when it should have used external probability.]</p>\n<p>edit: minor clarifying</p>\n<p>edits: improved the abstract and clarified the article further based on the comments.</p>", "sections": [{"title": "Summary:", "anchor": "Summary_", "level": 1}, {"title": "Details:", "anchor": "Details_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QAK43nNCTQQycAcYe", "GrtbTAPfkJa4D6jjH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-08T17:35:17.659Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta", "slug": "meetup-atlanta-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:20.233Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hankx7787", "createdAt": "2011-07-10T22:12:52.395Z", "isAdmin": false, "displayName": "hankx7787"}, "userId": "B4SKuX6dAQMnNqHzH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B5ZnKqszZw4duKAWf/meetup-atlanta-1", "pageUrlRelative": "/posts/B5ZnKqszZw4duKAWf/meetup-atlanta-1", "linkUrl": "https://www.lesswrong.com/posts/B5ZnKqszZw4duKAWf/meetup-atlanta-1", "postedAtFormatted": "Thursday, March 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB5ZnKqszZw4duKAWf%2Fmeetup-atlanta-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB5ZnKqszZw4duKAWf%2Fmeetup-atlanta-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB5ZnKqszZw4duKAWf%2Fmeetup-atlanta-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 143, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/7t'>Atlanta</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 March 2012 06:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2094 North Decatur Road, Decatur, GA 30033-5367</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>WHEN: 17 March 2012 06:30:00PM (-0500) <br />\nWHERE: 2094 North Decatur Road, Decatur, GA 30033-5367</p>\n\n<p>The next meetup will be Saturday, March 17th at 6:30pm at Chocolate Coffee in Decatur:</p>\n\n<p><a href=\"http://www.mychocolatecoffee.com/\" rel=\"nofollow\">http://www.mychocolatecoffee.com/</a> <br />\n2094 North Decatur Road, Decatur, GA 30033-5367 <br />\n(404) 982-0790</p>\n\n<p>We will be finishing up the \"Mysterious Answers to Mysterious Questions\" sequence at the next meeting. As always, any other topics you want to bring up are fair game!</p>\n\n<p>Here is the official agenda of our next meeting:</p>\n\n<p><a href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions</a> <br />\n1.26 \"Science\" as Curiosity-Stopper <br />\n1.27 Applause Lights <br />\n1.28 Truly Part of You <br />\n1.29 Chaotic Inversion</p>\n\n<p>Please let me know if you have any questions or comments! I look forward to seeing everyone there!</p>\n\n<p>ps. join the mailing list! <a href=\"http://groups.google.com/group/atlanta-less-wrong-meetup-group\" rel=\"nofollow\">http://groups.google.com/group/atlanta-less-wrong-meetup-group</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/7t'>Atlanta</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B5ZnKqszZw4duKAWf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.620771354577822e-07, "legacy": true, "legacyId": "13821", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta\">Discussion article for the meetup : <a href=\"/meetups/7t\">Atlanta</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 March 2012 06:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2094 North Decatur Road, Decatur, GA 30033-5367</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>WHEN: 17 March 2012 06:30:00PM (-0500) <br>\nWHERE: 2094 North Decatur Road, Decatur, GA 30033-5367</p>\n\n<p>The next meetup will be Saturday, March 17th at 6:30pm at Chocolate Coffee in Decatur:</p>\n\n<p><a href=\"http://www.mychocolatecoffee.com/\" rel=\"nofollow\">http://www.mychocolatecoffee.com/</a> <br>\n2094 North Decatur Road, Decatur, GA 30033-5367 <br>\n(404) 982-0790</p>\n\n<p>We will be finishing up the \"Mysterious Answers to Mysterious Questions\" sequence at the next meeting. As always, any other topics you want to bring up are fair game!</p>\n\n<p>Here is the official agenda of our next meeting:</p>\n\n<p><a href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\" rel=\"nofollow\">http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions</a> <br>\n1.26 \"Science\" as Curiosity-Stopper <br>\n1.27 Applause Lights <br>\n1.28 Truly Part of You <br>\n1.29 Chaotic Inversion</p>\n\n<p>Please let me know if you have any questions or comments! I look forward to seeing everyone there!</p>\n\n<p>ps. join the mailing list! <a href=\"http://groups.google.com/group/atlanta-less-wrong-meetup-group\" rel=\"nofollow\">http://groups.google.com/group/atlanta-less-wrong-meetup-group</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta1\">Discussion article for the meetup : <a href=\"/meetups/7t\">Atlanta</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta", "anchor": "Discussion_article_for_the_meetup___Atlanta", "level": 1}, {"title": "Discussion article for the meetup : Atlanta", "anchor": "Discussion_article_for_the_meetup___Atlanta1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-08T23:14:16.293Z", "modifiedAt": null, "url": null, "title": "A taxonomy of Oracle AIs", "slug": "a-taxonomy-of-oracle-ais", "viewCount": null, "lastCommentedAt": "2017-09-30T07:44:41.708Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XddMs9kSGtm6L8522/a-taxonomy-of-oracle-ais", "pageUrlRelative": "/posts/XddMs9kSGtm6L8522/a-taxonomy-of-oracle-ais", "linkUrl": "https://www.lesswrong.com/posts/XddMs9kSGtm6L8522/a-taxonomy-of-oracle-ais", "postedAtFormatted": "Thursday, March 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20taxonomy%20of%20Oracle%20AIs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20taxonomy%20of%20Oracle%20AIs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXddMs9kSGtm6L8522%2Fa-taxonomy-of-oracle-ais%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20taxonomy%20of%20Oracle%20AIs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXddMs9kSGtm6L8522%2Fa-taxonomy-of-oracle-ais", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXddMs9kSGtm6L8522%2Fa-taxonomy-of-oracle-ais", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1331, "htmlBody": "<p><small>Sources: An old draft on Oracle AI from Daniel Dewey, conversation with Dewey and Nick Beckstead. See also <a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Thinking Inside the Box</a> and <a href=\"http://dl.dropbox.com/u/5317066/2012-yampolskiy.pdf\">Leakproofing the Singularity</a>.</small></p>\n<p>&nbsp;</p>\n<p>Can we just create an Oracle AI that informs us but doesn't do anything?</p>\n<p>\"Oracle AI\" has been proposed in many forms, but most proposals share a common thread: <em>a powerful AI is not dangerous if it doesn't \"want to do anything\"</em>, the argument goes, <em>and therefore, it should be possible to create a safe \"Oracle AI\" that just gives us information</em>. Here, we discuss the difficulties of a few common types of proposed Oracle AI.</p>\n<p>Two broad categories can be treated separately: True Oracle AIs, which are true goal-seeking AIs with oracular goals, and Oracular non-AIs, which are designed to be \"very smart calculators\" instead of goal-oriented agents.</p>\n<p>&nbsp;</p>\n<h5>True Oracle AIs</h5>\n<p>A True Oracle AI is an AI with some kind of oracular goal. Informally proposed oracular goals often include ideas such as \"answer all questions\", \"only act to provide answers to questions\", \"have no other effect on the outside world\", and \"interpret questions as we would wish them to be interpreted.&rdquo; Oracular goals are meant to \"motivate\" the AI to provide us with the information we want or need, and to keep the AI from doing anything else.</p>\n<p>First, we point out that True Oracle AI is not causally isolated from the rest of the world. Like any AI, it has at least its observations (questions and data) and its actions (answers and other information) with which to affect the world. A True Oracle AI interacts through a somewhat low-bandwidth channel, but it is not <em>qualitatively</em> different from any other AI. It still acts autonomously in service of its goal as it answers questions, and it is realistic to assume that a superintelligent True Oracle AI will still be able to have large effects on the world.</p>\n<p>Given that a True Oracle AI acts, by answering questions, to achieve its goal, it follows that True Oracle AI is only safe if its goal is fully compatible with human values. A limited interaction channel is not a good defense against a superintelligence.</p>\n<p>There are many ways that omission of detail about human value could cause a \"question-answering\" goal to assign utility to a very undesirable state of the world, resulting in a undesirable future. A designer of an oracular goal must be certain to include a virtually endless list of qualifiers and patches. An incomplete list includes \"don't forcefully acquire resources to compute answers, don't defend yourself against shutdown, don't coerce or threaten humans, don't manipulate humans to <em>want</em> to help you compute answers, don't trick the questioner into asking easy questions, don't hypnotize the questioner into reporting satisfaction, don't dramatically simplify the world to make prediction easier, don't ask yourself questions, don't create a questioner-surrogate that asks easy questions,\" etc.</p>\n<p>Since an oracular goal must contain a full specification of human values, the True Oracle AI problem is Friendly-AI-complete (FAI-complete). If we had the knowledge and skills needed to create a safe True Oracular AI, we could create a Friendly AI instead.</p>\n<p>&nbsp;</p>\n<h5>Oracular non-AIs</h5>\n<p>An Oracular non-AI is a question-answering or otherwise informative system that is not goal-seeking and has no internal parts that are goal-seeking, i.e. not an AI at all. Informally, an Oracular non-AI is something like a \"nearly AI-complete calculator\" that implements a function from input \"questions\" to output \"answers.&rdquo; It is difficult to discuss the set of Oracular non-AIs formally because it is a heterogeneous concept by nature. Despite this, we argue that many are either FAI-complete or unsafe for use.</p>\n<p>In addition to the problems with specific proposals below, many Oracular non-AI proposals are based on powerful metacomputation, e.g. Solomonoff induction or program evolution, and therefore incur the <em>generic metacomputational hazards:</em> they may accidentally perform morally bad computations (e.g. suffering sentient programs or human simulations), they may stumble upon and fail to sandbox an Unfriendly AI, or they may fall victim to ambient control by a superintelligence. Other unknown metacomputational hazards may also exist.</p>\n<p>Since many Oracular non-AIs have never been specified formally, we approach proposals on an informal level.</p>\n<p>&nbsp;</p>\n<h5>Oracular non-AIs: Advisors</h5>\n<p>An Advisor is a systems that takes a corpus of real-world data and somehow computes the answer to the informal question \"what ought we (or I) to do?\". Advisors are FAI-complete because:</p>\n<ul>\n<li>Formalizing the ought-question requires a complete formal statement of human values or a formal method for finding them.</li>\n<li>Answering the ought-question requires a full theory of instrumental decision-making.</li>\n</ul>\n<h5><br /></h5>\n<h5>Oracular non-AIs: Question-Answerers</h5>\n<p>A Question-Answerer is a system that takes a corpus of real-world data along with a \"question,&rdquo; then somehow computes the \"answer to the question.&rdquo; To analyze the difficulty of creating a Question-Answerer, suppose that we ask it the question \"what ought we (or I) to do?\"</p>\n<ul>\n<li>If it can answer this question, the Question-Answerer and the question together are FAI-complete. Either the Question-Answerer can understand the question as-is, or we can rewrite it in a more formal language; regardless, the Question-Answerer and the question together comprise an Advisor, which we previously argued to be FAI-complete.</li>\n<li>If it cannot answer this question, many of its answers are radically unsafe. Courses of action recommended by the Question-Answerer will likely be unsafe, insofar as \"safety\" relies on the definition of human value. Also, asking questions about the future will turn the Question-Answerer into a Predictor, leading to the problems outlined below.</li>\n</ul>\n<p>Of course, if safe uses for a Question-Answerer can be devised, we still have the non-negligible challenge of creating a Question-Answerer without using any goal-seeking AI techniques.</p>\n<p>&nbsp;</p>\n<h5>Oracular non-AIs: Predictors</h5>\n<p>A Predictor is a system that takes a corpus of data and produces a probability distribution over future data. Very accurate and general Predictors may be based on Solomonoff's theory of universal induction.</p>\n<p>Very powerful Predictors are unsafe in a rather surprising way: when given sufficient data about the real world, they exhibit goal-seeking behavior, i.e. they calculate a distribution over future data in a way that brings about certain real-world states. This is surprising, since a Predictor is theoretically just a very large and expensive application of Bayes' law, not even performing a search over its possible outputs.</p>\n<p>To see why, consider a Predictor P with a large corpus of real-world data. If P is sufficiently powerful and the corpus is sufficiently large, P will infer a distribution that gives very high probability to a model of the world (let&rsquo;s call it M) that <em>contains a model of P being asked the questions we&rsquo;re asking it</em>. (It is perfectly possible for a program to model its own behavior, and in fact necessary if the Predictor is to be accurate.)</p>\n<p>Suppose now that we ask P to calculate the probability of future data <em>d</em>; call this probability P(<em>d</em>). Since model M has much of P&rsquo;s distribution&rsquo;s probability mass, P(<em>d</em>) is approximately equal to the probability of M if M computes <em>d</em> (call this M&rarr;d), and zero otherwise. Furthermore, since M contains a model of the Predictor being asked about <em>d</em>, M&rarr;d depends on the way P&rsquo;s &ldquo;answer&rdquo; affects M&rsquo;s execution. This means that P(<em>d</em>) depends on P(<em>d</em>)&rsquo;s predicted impact on the world; in other words, P takes into account the <em>effects</em> of its predictions on the world, and &ldquo;selects&rdquo; predictions that make themselves accurate-- P has an implicit goal that the world ought to match its predictions. This goal does not necessarily align with human goals, and should be treated very carefully.</p>\n<p>Probabilistic predictions of future data are a very small output channel, but once again, the ability of a superintelligence to use a small channel effectively should not be underestimated. Additionally, the difficulty of using such a Predictor well (specifying future data strings of interest and interpreting the results) speaks against our ability to keep the Predictor from influencing us through its predictions.</p>\n<p>It is not clear that there is any general way to design a Predictor that will <em>not</em> exhibit goal-seeking behavior, short of dramatically limiting the power of the Predictor.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb26d": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XddMs9kSGtm6L8522", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 24, "extendedScore": null, "score": 3.7e-05, "legacy": true, "legacyId": "13822", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-09T03:46:20.911Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins Meetup Saturday 17th", "slug": "meetup-fort-collins-meetup-saturday-17th", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:25.351Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hN6jxhRJJsDq7qpZg/meetup-fort-collins-meetup-saturday-17th", "pageUrlRelative": "/posts/hN6jxhRJJsDq7qpZg/meetup-fort-collins-meetup-saturday-17th", "linkUrl": "https://www.lesswrong.com/posts/hN6jxhRJJsDq7qpZg/meetup-fort-collins-meetup-saturday-17th", "postedAtFormatted": "Friday, March 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%20Meetup%20Saturday%2017th&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%20Meetup%20Saturday%2017th%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhN6jxhRJJsDq7qpZg%2Fmeetup-fort-collins-meetup-saturday-17th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%20Meetup%20Saturday%2017th%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhN6jxhRJJsDq7qpZg%2Fmeetup-fort-collins-meetup-saturday-17th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhN6jxhRJJsDq7qpZg%2Fmeetup-fort-collins-meetup-saturday-17th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 61, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/7u'>Fort Collins Meetup Saturday 17th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 March 2012 05:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As requested, we're trying a weekend night to shake things up.</p>\n\n<p>Meet down town for coffee, then back to my house for dinner, games and extended chatting.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/7u'>Fort Collins Meetup Saturday 17th</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hN6jxhRJJsDq7qpZg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.623240594515127e-07, "legacy": true, "legacyId": "13837", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins_Meetup_Saturday_17th\">Discussion article for the meetup : <a href=\"/meetups/7u\">Fort Collins Meetup Saturday 17th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 March 2012 05:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>As requested, we're trying a weekend night to shake things up.</p>\n\n<p>Meet down town for coffee, then back to my house for dinner, games and extended chatting.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins_Meetup_Saturday_17th1\">Discussion article for the meetup : <a href=\"/meetups/7u\">Fort Collins Meetup Saturday 17th</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins Meetup Saturday 17th", "anchor": "Discussion_article_for_the_meetup___Fort_Collins_Meetup_Saturday_17th", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins Meetup Saturday 17th", "anchor": "Discussion_article_for_the_meetup___Fort_Collins_Meetup_Saturday_17th1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-09T04:58:01.180Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Amazing Breakthrough Day: April 1st", "slug": "seq-rerun-amazing-breakthrough-day-april-1st", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:17.000Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Gn6odSHWsArPC4vMu/seq-rerun-amazing-breakthrough-day-april-1st", "pageUrlRelative": "/posts/Gn6odSHWsArPC4vMu/seq-rerun-amazing-breakthrough-day-april-1st", "linkUrl": "https://www.lesswrong.com/posts/Gn6odSHWsArPC4vMu/seq-rerun-amazing-breakthrough-day-april-1st", "postedAtFormatted": "Friday, March 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Amazing%20Breakthrough%20Day%3A%20April%201st&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Amazing%20Breakthrough%20Day%3A%20April%201st%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGn6odSHWsArPC4vMu%2Fseq-rerun-amazing-breakthrough-day-april-1st%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Amazing%20Breakthrough%20Day%3A%20April%201st%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGn6odSHWsArPC4vMu%2Fseq-rerun-amazing-breakthrough-day-april-1st", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGn6odSHWsArPC4vMu%2Fseq-rerun-amazing-breakthrough-day-april-1st", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 173, "htmlBody": "<p>Today's post, <a href=\"/lw/ox/amazing_breakthrough_day_april_1st/\">Amazing Breakthrough Day: April 1st</a> was originally published on 25 March 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Amazing_Breakthrough_Day:_April_1st\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A proposal for a new holiday, in which journalists report on great scientific discoveries of the past as if they had just happened, and were still shocking.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/and/seq_rerun_the_beauty_of_settled_science/\">The Beauty of Settled Science</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Gn6odSHWsArPC4vMu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 8.623530296078847e-07, "legacy": true, "legacyId": "13839", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hYqDp4qAucZM33qSh", "bhhzdprwYbRJQRcyD", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-09T11:20:07.453Z", "modifiedAt": null, "url": null, "title": "How does real world expected utility maximization work?", "slug": "how-does-real-world-expected-utility-maximization-work", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:34.631Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J8ojaGJozdt9hnoxE/how-does-real-world-expected-utility-maximization-work", "pageUrlRelative": "/posts/J8ojaGJozdt9hnoxE/how-does-real-world-expected-utility-maximization-work", "linkUrl": "https://www.lesswrong.com/posts/J8ojaGJozdt9hnoxE/how-does-real-world-expected-utility-maximization-work", "postedAtFormatted": "Friday, March 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20does%20real%20world%20expected%20utility%20maximization%20work%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20does%20real%20world%20expected%20utility%20maximization%20work%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ8ojaGJozdt9hnoxE%2Fhow-does-real-world-expected-utility-maximization-work%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20does%20real%20world%20expected%20utility%20maximization%20work%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ8ojaGJozdt9hnoxE%2Fhow-does-real-world-expected-utility-maximization-work", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ8ojaGJozdt9hnoxE%2Fhow-does-real-world-expected-utility-maximization-work", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 651, "htmlBody": "<p>I would like to ask for help on how to use expected utility maximization, <em>in practice</em>, to maximally achieve my goals.</p>\n<p>As a real world example I would like to use the post '<a href=\"/lw/5c0/epistle_to_the_new_york_less_wrongians/\">Epistle to the New York Less Wrongians</a>' by Eliezer Yudkowsky and his visit to New York.</p>\n<p><em>How did Eliezer Yudkowsky compute that it would maximize his expected utility to visit New York?</em></p>\n<p>It seems that the first thing he would have to do is to figure out what he really wants, his <em>preferences</em><sup>1</sup>, right? The next step would be to formalize his preferences by describing it as a <em>utility function</em> and assign a certain number of <em>utils</em><sup>2</sup> to each member of the set, e.g. his own survival. This description would have to be <span class=\"kH\">precise enough to figure out what it would mean to maximize his utility function. </span></p>\n<p><span class=\"kH\">Now before he can continue he will first have to compute the <em>expected utility</em> of computing the expected utility of computing the expected utility of computing the expected utility<sup>3</sup> ... and also compare it with <em>alternative heuristics</em><sup>4</sup>.<br /></span></p>\n<p>He then has to figure out each and every possible action he might take, and study all of their <em>logical implications</em>, to learn about all <em>possible world states</em> he might achieve by those <em>decisions</em>, calculate the utility of each world state and the average utility of each action leading up to those various possible world states<sup>5</sup>.</p>\n<p>To do so he has to figure out the <em>probability</em> of each world state. This further requires him to come up with a <em>prior probability</em> for each case and study all available <em>data</em>. For example, how likely it is to die in a plane crash, how long it would take to be cryonically suspended from where he is in case of a fatality, the crime rate and if aliens might abduct him (he might <em>discount</em> the last example, but then he would first have to figure out <em>the right level of small probabilities</em> that are considered too unlikely to be relevant for judgment and decision making).</p>\n<p>I probably miss some technical details and got others wrong. But this shouldn't detract too much from my general request. Could you please explain how Less Wrong style rationality is to be applied practically? I would also be happy if you could point out some worked examples or suggest relevant literature. Thank you.</p>\n<p>I also want to note that I am <a href=\"http://blog.muflax.com/2012/02/22/the-end-of-rationality/\">not the only one</a> who doesn't know how to actually apply what is being discussed on Less Wrong in practice. From the comments:</p>\n<blockquote>\n<p>You can&rsquo;t believe in the implied invisible and remain even remotely sane. [...] (it) doesn&rsquo;t just break down in some esoteric scenarios, but is utterly unworkable in the most basic situation. You can&rsquo;t calculate shit, to put it bluntly.<br /><br />None of these ideas are even remotely usable. The best you can do is to rely on fundamentally different methods and pretend they are really &ldquo;approximations&rdquo;. It&rsquo;s complete handwaving.<br /><br />Using high-level, explicit, reflective cognition is mostly useless, beyond the skill level of a decent programmer, physicist, or heck, someone who reads Cracked.</p>\n</blockquote>\n<p>I can't help but agree.</p>\n<p><strong>P.S. </strong>If you really want to know how I feel about Less Wrong then read the post '<a href=\"http://blog.muflax.com/2012/03/08/ontological-therapy/\">Ontological Therapy</a>' by user:muflax.</p>\n<p>&nbsp;</p>\n<p><sup>1. What are \"preferences\" and how do you figure out what long-term goals are stable enough under real world influence to allow you to make time-consistent decisions?</sup></p>\n<p><sup>2. How is utility grounded and how can it be consistently assigned to reflect your true preferences without having to rely on your intuition, i.e. pull a number out of thin air? Also, will the definition of utility keep changing as we make more observations? And how do you account for that possibility?</sup></p>\n<p><sup>3. Where and how do you draw the line?</sup></p>\n<p><sup>4. How do you account for model uncertainty?</sup></p>\n<p><sup>5. Any finite list of actions maximizes infinitely many different quantities. So, how does utility become well-defined?</sup></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J8ojaGJozdt9hnoxE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 17, "extendedScore": null, "score": 4.1e-05, "legacy": true, "legacyId": "13857", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jP583FwKepjiWbeoQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-09T13:44:29.337Z", "modifiedAt": null, "url": null, "title": "Political philosophy and/versus political action, not to mention arguing", "slug": "political-philosophy-and-versus-political-action-not-to", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:20.222Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/djwCMErepucQf4B5k/political-philosophy-and-versus-political-action-not-to", "pageUrlRelative": "/posts/djwCMErepucQf4B5k/political-philosophy-and-versus-political-action-not-to", "linkUrl": "https://www.lesswrong.com/posts/djwCMErepucQf4B5k/political-philosophy-and-versus-political-action-not-to", "postedAtFormatted": "Friday, March 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Political%20philosophy%20and%2Fversus%20political%20action%2C%20not%20to%20mention%20arguing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APolitical%20philosophy%20and%2Fversus%20political%20action%2C%20not%20to%20mention%20arguing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdjwCMErepucQf4B5k%2Fpolitical-philosophy-and-versus-political-action-not-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Political%20philosophy%20and%2Fversus%20political%20action%2C%20not%20to%20mention%20arguing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdjwCMErepucQf4B5k%2Fpolitical-philosophy-and-versus-political-action-not-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdjwCMErepucQf4B5k%2Fpolitical-philosophy-and-versus-political-action-not-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 314, "htmlBody": "<p>I found <a href=\"http://crookedtimber.org/2012/03/09/wilkinson-on-cato-self-serving-excuses-second-best-solutions-ponies-and-pandaemonia-of-pis-aller/#more-23590\">an essay</a> with so much good stuff in it that I was in danger of exceeding my quote quota, so I'm putting more quotes here:</p>\n<p>\"Awareness of what is first-best is a condition of being able to aim at second-best.\"</p>\n<p>\"What would be nice, then, would be a political philosophy that did a  better job of taking this sort of typical deformation into intelligent  account, which would discourage it &ndash; since it thrives on not being seen  for what it is. (Also would be nice: a pony!) A theory of first-best  that talks astutely about second-best. This is inherently hard to do, so  I don&rsquo;t say &lsquo;theorizes well&rsquo;. I guess I would propose a sort of  line-of-sight rule. Optimally, you shouldn&rsquo;t lose sight of your ideals  or of reality. So much so obvious. But really the trick is keeping  accurate score with regard to semi-idealistic philosophical and policy  proposals. Philosophers like to talk about the difficulty deriving an  ought from an is (or an is from an ought). But it&rsquo;s equally important to  think about the difficulty in analyzing an is-ought compound into  component elements, the better to reduce and potentially reconstitute  it.\"</p>\n<p>\"As philosophers, we would like to address the strongest arguments our  opponents have. But this etiquette of the seminar room is bad ethics, in  that this just isn&rsquo;t the <em>right</em> approach in political  philosophy. Liberalism is as liberalism does (not just as it might do,  ideally). And the same goes for conservatism and libertarianism and  communism and the rest. If you only address good arguments you will miss  out on a perilously large proportion of your actual subject matter.\"</p>\n<p>I'm not quite as sure about this one. I suppose it depends on what one means by an argument. Political philosophy presumably includes implied arguments that trying to approach some specific proposed ideal system will produce better results than not trying to approach it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "djwCMErepucQf4B5k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 8.625658861708154e-07, "legacy": true, "legacyId": "13858", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-09T16:32:03.776Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Atlanta, Cambridge UK, Chicago, Fort Collins, Houston, Melbourne, Twin Cities, Vancouver", "slug": "weekly-lw-meetups-atlanta-cambridge-uk-chicago-fort-collins", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HcA4dc6A2aNt8w2Ze/weekly-lw-meetups-atlanta-cambridge-uk-chicago-fort-collins", "pageUrlRelative": "/posts/HcA4dc6A2aNt8w2Ze/weekly-lw-meetups-atlanta-cambridge-uk-chicago-fort-collins", "linkUrl": "https://www.lesswrong.com/posts/HcA4dc6A2aNt8w2Ze/weekly-lw-meetups-atlanta-cambridge-uk-chicago-fort-collins", "postedAtFormatted": "Friday, March 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Atlanta%2C%20Cambridge%20UK%2C%20Chicago%2C%20Fort%20Collins%2C%20Houston%2C%20Melbourne%2C%20Twin%20Cities%2C%20Vancouver&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Atlanta%2C%20Cambridge%20UK%2C%20Chicago%2C%20Fort%20Collins%2C%20Houston%2C%20Melbourne%2C%20Twin%20Cities%2C%20Vancouver%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHcA4dc6A2aNt8w2Ze%2Fweekly-lw-meetups-atlanta-cambridge-uk-chicago-fort-collins%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Atlanta%2C%20Cambridge%20UK%2C%20Chicago%2C%20Fort%20Collins%2C%20Houston%2C%20Melbourne%2C%20Twin%20Cities%2C%20Vancouver%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHcA4dc6A2aNt8w2Ze%2Fweekly-lw-meetups-atlanta-cambridge-uk-chicago-fort-collins", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHcA4dc6A2aNt8w2Ze%2Fweekly-lw-meetups-atlanta-cambridge-uk-chicago-fort-collins", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 460, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/6z\">Tucson Meetup:&nbsp;<span class=\"date\">02 March 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/7h\">University of Chicago (Again):&nbsp;<span class=\"date\">03 March 2012 01:46PM</span></a></li>\n<li><a href=\"/meetups/7g\">Atlanta:&nbsp;<span class=\"date\">03 March 2012 06:30PM</span></a></li>\n<li><a href=\"/meetups/7m\">Houston Meetup - 3/4:&nbsp;<span class=\"date\">04 March 2012 12:00PM</span></a></li>\n<li><a href=\"/meetups/7l\">Vancouver!:&nbsp;<span class=\"date\">04 March 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/6v\">Twin Cities South Metro:&nbsp;<span class=\"date\">06 March 2012 08:00PM</span></a></li>\n<li><a href=\"/meetups/7p\">Fort Collins, Colorado Meetup Wednesday 7pm:&nbsp;<span class=\"date\">07 March 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/7n\">Sydney - Core sequences:&nbsp;<span class=\"date\">13 March 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/7f\">S&atilde;o Paulo Meetup:&nbsp;<span class=\"date\">14 March 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/7o\">Brussels meetup:&nbsp;<span class=\"date\">17 March 2012 11:15AM</span></a></li>\n<li><a href=\"/meetups/6k\">[Ohio/Washington DC] Interest in Reason Rally meetup?:&nbsp;<span class=\"date\">24 March 2012 04:14PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/7b\">Melbourne practical rationality meetup:&nbsp;<span class=\"date\">02 March 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/72\">Cambridge UK:&nbsp;<span class=\"date\">04 March 2012 11:00AM</span></a></li>\n<li><a href=\"/meetups/7e\">Ohio Monthly:&nbsp;<span class=\"date\">18 March 2012 03:00PM</span></a></li>\n</ul>\n<ul>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>, </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#San_Francisco\">San Francisco</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"/r/discussion/lw/6at/west_la_biweekly_meetups\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong><strong>.</strong></p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HcA4dc6A2aNt8w2Ze", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.626336574434371e-07, "legacy": true, "legacyId": "13630", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tHFu6kvy2HMvQBEhW", "d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-09T19:38:58.325Z", "modifiedAt": null, "url": null, "title": "[Link] Better results by changing Bayes\u2019 theorem", "slug": "link-better-results-by-changing-bayes-theorem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:21.548Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KLFrJStPaj24LWKWJ/link-better-results-by-changing-bayes-theorem", "pageUrlRelative": "/posts/KLFrJStPaj24LWKWJ/link-better-results-by-changing-bayes-theorem", "linkUrl": "https://www.lesswrong.com/posts/KLFrJStPaj24LWKWJ/link-better-results-by-changing-bayes-theorem", "postedAtFormatted": "Friday, March 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Better%20results%20by%20changing%20Bayes%E2%80%99%20theorem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Better%20results%20by%20changing%20Bayes%E2%80%99%20theorem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKLFrJStPaj24LWKWJ%2Flink-better-results-by-changing-bayes-theorem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Better%20results%20by%20changing%20Bayes%E2%80%99%20theorem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKLFrJStPaj24LWKWJ%2Flink-better-results-by-changing-bayes-theorem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKLFrJStPaj24LWKWJ%2Flink-better-results-by-changing-bayes-theorem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<blockquote>\n<p><span style=\"color: #000000; font-family: Arial,Helvetica,sans-serif; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 19px; orphans: 2; text-align: justify; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff; font-size: small; display: inline ! important; float: none;\">If it ever turns out that Bayes fails - receives systematically lower rewards on some problem, relative to a superior alternative, in virtue of its mere decisions - then Bayes has to go<span class=\"Apple-converted-space\">&nbsp;</span></span><em style=\"font-style: italic; color: #000000; font-family: Arial,Helvetica,sans-serif; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: 19px; orphans: 2; text-align: justify; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: #ffffff; font-size: small;\">out the window.</em></p>\n</blockquote>\n<p>-- Eliezer Yudkowsky, <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Newcomb's Problem and Regret of Rationality</a></p>\n<p>Don't worry, we don't have to abandon Bayes&rsquo; theorem yet. But changing it slightly seems to be the winning Way given certain circumstances. See below:</p>\n<blockquote>\n<p>In Peter Norvig&rsquo;s talk The Unreasonable Effectiveness of Data, starting at 37:42, he describes a translation algorithm based on Bayes&rsquo; theorem. Pick the English word that has the highest posterior probability as the translation. No surprise here. Then <a href=\"http://youtu.be/yvDCzhbjYWs?t=38m16s\">at 38:16</a> he says something curious.</p>\n<p style=\"padding-left: 30px;\">So this is all nice and theoretical and pure, but as well as being mathematically inclined, we are also realists. So we experimented some, and we found out that when you raise that first factor [in Bayes' theorem] to the 1.5 power, you get a better result.</p>\n<p>In other words, if we change Bayes&rsquo; theorem (!) we get a better result. He goes on to explain</p>\n</blockquote>\n<p><strong>Link:</strong> <a href=\"http://www.johndcook.com/blog/2012/03/09/monkeying-with-bayes-theorem/\">johndcook.com/blog/2012/03/09/monkeying-with-bayes-theorem/</a></p>\n<p><strong>Peter Norvig - The Unreasonable Effectiveness of Data </strong></p>\n<p>\n<object width=\"500\" height=\"284\" data=\"http://www.youtube.com/v/yvDCzhbjYWs?version=3&amp;hl=en_US&amp;rel=0\" type=\"application/x-shockwave-flash\">\n<param name=\"allowFullScreen\" value=\"true\" />\n<param name=\"allowscriptaccess\" value=\"always\" />\n<param name=\"src\" value=\"http://www.youtube.com/v/yvDCzhbjYWs?version=3&amp;hl=en_US&amp;rel=0\" />\n<param name=\"allowfullscreen\" value=\"true\" />\n</object>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KLFrJStPaj24LWKWJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 4, "extendedScore": null, "score": 8.627092595278812e-07, "legacy": true, "legacyId": "13860", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6ddcsdA2c2XpNpE5x"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-09T20:13:43.899Z", "modifiedAt": null, "url": null, "title": "New cognitive bias articles on wikipedia (update)", "slug": "new-cognitive-bias-articles-on-wikipedia-update", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:06.157Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nerfhammer", "createdAt": "2009-07-21T19:45:50.831Z", "isAdmin": false, "displayName": "nerfhammer"}, "userId": "TnRcc3ezfxzQs7Phn", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XKfPvj9gtrskGuP3v/new-cognitive-bias-articles-on-wikipedia-update", "pageUrlRelative": "/posts/XKfPvj9gtrskGuP3v/new-cognitive-bias-articles-on-wikipedia-update", "linkUrl": "https://www.lesswrong.com/posts/XKfPvj9gtrskGuP3v/new-cognitive-bias-articles-on-wikipedia-update", "postedAtFormatted": "Friday, March 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20cognitive%20bias%20articles%20on%20wikipedia%20(update)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20cognitive%20bias%20articles%20on%20wikipedia%20(update)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXKfPvj9gtrskGuP3v%2Fnew-cognitive-bias-articles-on-wikipedia-update%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20cognitive%20bias%20articles%20on%20wikipedia%20(update)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXKfPvj9gtrskGuP3v%2Fnew-cognitive-bias-articles-on-wikipedia-update", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXKfPvj9gtrskGuP3v%2Fnew-cognitive-bias-articles-on-wikipedia-update", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 122, "htmlBody": "<ul>\n<li><a href=\"http://en.wikipedia.org/wiki/Conservatism_(Bayesian)\">Conservatism</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Curse_of_knowledge\">Curse of knowledge</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Duration_neglect\">Duration neglect</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Extension_neglect\">Extension neglect</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Extrinsic_incentives_bias\">Extrinsic incentives bias</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Illusion_of_external_agency\">Illusion of external agency</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Illusion_of_validity\">Illusion of validity</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Insensitivity_to_sample_size\">Insensitivity to sample size</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Lady_Macbeth_effect\">Lady Macbeth effect</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Less-is-better_effect\">Less-is-better effect</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Na&iuml;ve_cynicism\">Na&iuml;ve cynicism</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Na&iuml;ve_realism_(psychology)\">Na&iuml;ve realism</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Reactive_devaluation\">Reactive devaluation</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Rhyme-as-reason_effect\">Rhyme-as-reason effect</a></li>\n<li><a href=\"http://en.wikipedia.org/wiki/Scope_neglect\">Scope neglect</a></li>\n</ul>\n<p>Also&nbsp;<a href=\"http://en.wikipedia.org/wiki/Conjunction_fallacy\">conjunction fallacy</a>&nbsp;has been expanded.</p>\n<ul>\n</ul>\n<div><strong>(update) background</strong></div>\n<div>I started dozens of the cognitive bias articles that are on wikipedia. That was a long time ago. It seems people like these things, so I started adding them again.</div>\n<div>I wanted to write a compendium of biases in book form. I didn't know how to get a book published, though.</div>\n<div>Anyway, enjoy.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4R8JYu4QF2FqzJxE5": 1, "Wi3EopKJ2aNdtxSWg": 1, "uCuS2DModz3eisEdv": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XKfPvj9gtrskGuP3v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 88, "baseScore": 108, "extendedScore": null, "score": 0.000245, "legacy": true, "legacyId": "13861", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 84, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-09T21:13:42.482Z", "modifiedAt": null, "url": null, "title": "[Link, 2011] Team may be chosen to receive $1.4 billion to simulate human brain", "slug": "link-2011-team-may-be-chosen-to-receive-usd1-4-billion-to", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:23.034Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yyCkgZiufYtfTk3aN/link-2011-team-may-be-chosen-to-receive-usd1-4-billion-to", "pageUrlRelative": "/posts/yyCkgZiufYtfTk3aN/link-2011-team-may-be-chosen-to-receive-usd1-4-billion-to", "linkUrl": "https://www.lesswrong.com/posts/yyCkgZiufYtfTk3aN/link-2011-team-may-be-chosen-to-receive-usd1-4-billion-to", "postedAtFormatted": "Friday, March 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%2C%202011%5D%20Team%20may%20be%20chosen%20to%20receive%20%241.4%20billion%20to%20simulate%20human%20brain&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%2C%202011%5D%20Team%20may%20be%20chosen%20to%20receive%20%241.4%20billion%20to%20simulate%20human%20brain%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyyCkgZiufYtfTk3aN%2Flink-2011-team-may-be-chosen-to-receive-usd1-4-billion-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%2C%202011%5D%20Team%20may%20be%20chosen%20to%20receive%20%241.4%20billion%20to%20simulate%20human%20brain%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyyCkgZiufYtfTk3aN%2Flink-2011-team-may-be-chosen-to-receive-usd1-4-billion-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyyCkgZiufYtfTk3aN%2Flink-2011-team-may-be-chosen-to-receive-usd1-4-billion-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 526, "htmlBody": "<p>This is the team responsible&nbsp;for simulating the rat cortical column.</p>\n<p><a href=\"http://www.nature.com/news/2011/110308/full/news.2011.143.html\">http://www.nature.com/news/2011/110308/full/news.2011.143.htm</a></p>\n<address><span style=\"font-style: normal;\">The team is one of 6 that is being considered for at least 2 \"FET Flagship\" positions, which comes with all that funding. Each of the six competing teams is proposing to work on some kind of futuristic technology:</span></address>\n<p><a href=\"http://cordis.europa.eu/fp7/ict/programme/fet/flagship/6pilots_en.html\">http://cordis.europa.eu/fp7/ict/programme/fet/flagship/6pilots_en.html</a></p>\n<p>Of course, word on <a href=\"http://www.vetta.org/2011/05/sutton-on-human-level-ai/\">the street</a> is that academic neuroscientists don't think much of the project:</p>\n<blockquote>\n<p>Academic neuroscientists that I&rsquo;ve ever spoken too, which is a fair number now, don&rsquo;t think much of the Blue Brain project. They sometimes think it will be valuable in terms of collecting and cataloguing information about the neocortex, but they don&rsquo;t think the project will manage to understand how the cortex works as there are too many unknowns in the model and even if, by chance, they got the model right it would be very hard to know that they had.</p>\n<p>Almost all neuroscientists seem to think that working brain models will not exist by 2025, or even 2035 for that matter. What ever the date is, most consider it too far away to bother to think much about.</p>\n<p>Such projects probably help to get more kids interested in the topic.</p>\n</blockquote>\n<hr />\n<p>I think trying to influence the committee's decision potentially represents very low hanging fruit in <a href=\"http://www.vetta.org/2011/05/sutton-on-human-level-ai/\">politics as charity</a>.</p>\n<p>Even if academic neuroscientists don't think much of the project in its current state, it seems likely that $1.4 billion would end up attracting a lot of talent to this problem, and get us the first upload significantly sooner.</p>\n<p>It's true that Less Wrong doesn't have a consensus position on whether to speed development of cell modeling and brain scanning technology or not. But I think if we have a discussion and a vote, we're significantly more likely than the committee to come up with the right decision for humanity. As far as I can tell, the committee will essentially be choosing at random. It shouldn't be hard for us to beat that.</p>\n<p>Edit: But that's not to say that our estimate should be quick and dirty. In the spirit of <a href=\"/lw/ka/hold_off_on_proposing_solutions/\">holding off on proposing solutions</a>, I discourage anyone from taking a firm public position on this topic for now.</p>\n<p>In terms of avenues for influence, here are a few ideas off the top of my head:</p>\n<ol>\n<li>Hire a PR agency to generate positive or negative press for a given project.</li>\n<li>Get European Less Wrong users to contact the program via Facebook and Twitter. (The program's follower numbers are in the low triple digits.)</li>\n<li>Hire professional lobbyists to do whatever they do.</li>\n</ol>\n<div>Just to give everyone an idea of the kind of money involved here, if we have a 1% chance of influencing the committee's decision, we're moving $14 million in expected funds.</div>\n<p>We, and the folks at the Future of Humanity Institute, SI, and other groups, seem to spend a lot of time thinking about what would happen in the ideal scenario in terms of the order in which technologies are developed and how they are deployed. I think there is a good case for also investing in the complementary good of trying to actually influence the world towards a more ideal scenario.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 2, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yyCkgZiufYtfTk3aN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 10, "extendedScore": null, "score": 8.627475833414552e-07, "legacy": true, "legacyId": "12554", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uHYYA32CKgKT3FagE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-09T23:53:28.836Z", "modifiedAt": null, "url": null, "title": "Predictability of Decisions and the Diagonal Method", "slug": "predictability-of-decisions-and-the-diagonal-method", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:21.190Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W6T93dSSm2xvHn9X6/predictability-of-decisions-and-the-diagonal-method", "pageUrlRelative": "/posts/W6T93dSSm2xvHn9X6/predictability-of-decisions-and-the-diagonal-method", "linkUrl": "https://www.lesswrong.com/posts/W6T93dSSm2xvHn9X6/predictability-of-decisions-and-the-diagonal-method", "postedAtFormatted": "Friday, March 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Predictability%20of%20Decisions%20and%20the%20Diagonal%20Method&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APredictability%20of%20Decisions%20and%20the%20Diagonal%20Method%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW6T93dSSm2xvHn9X6%2Fpredictability-of-decisions-and-the-diagonal-method%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Predictability%20of%20Decisions%20and%20the%20Diagonal%20Method%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW6T93dSSm2xvHn9X6%2Fpredictability-of-decisions-and-the-diagonal-method", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW6T93dSSm2xvHn9X6%2Fpredictability-of-decisions-and-the-diagonal-method", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1939, "htmlBody": "<p style=\"padding-left: 30px; \"><em>This post collects a few situations where agents might want to make their decisions either predictable or unpredictable to certain methods of prediction, and considers a method of making a decision unpredictable by \"diagonalizing\" a hypothetical prediction of that decision. The last section takes a stab at applying this tool to the <a href=\"/lw/5rq/example_decision_theory_problem_agent_simulates/\">ASP problem</a>.</em></p>\n<h3>The diagonal step</h3>\n<p>To start off, consider the halting problem, interpreted in terms of agents and predictors. Suppose that there is a Universal Predictor, an algorithm that is able to decide whether any given program halts or runs forever. Then, it's easy for a program (agent) to evade its gaze by including a <em>diagonal step</em> in its decision procedure: the agent checks (by simulation) if Universal Predictor comes to some decision about the agent, and if it does, the agent acts contrary to the Predictor's decision. This makes the prediction wrong, and Universal Predictors impossible.</p>\n<p>The same trick could be performed against something that could exist, normal non-universal Predictors, which allows an agent to make itself immune to their predictions. In particular, ability of other agents to infer decisions of our agent may be thought of as prediction that an agent might want to hinder. This is possible so long as the predictors in question can be simulated in enough detail, that is it's known what they do (what they know) and our agent has enough computational resources to anticipate their hypothetical conclusions. (If an agent does perform the diagonal step with respect to other agents, the predictions of other agents don't necessarily become wrong, as they could be formally correct by construction, but they cease to be possible, which could mean that the predictions won't be made at all.)<a id=\"more\"></a></p>\n<h3>Knowing own decision</h3>\n<p>In UDT/ADT, an agent is given definitions of its action (strategy) and utility value, and evaluates an action (strategy) by inferring the logical implications of taking it on the utility value (see <a href=\"/lw/2tq/notion_of_preference_in_ambient_control/\">this post</a> for a more detailed description of the decision procedure and <a href=\"/lw/1s5/explicit_optimization_of_global_strategy_fixing_a/\">this post</a> for an explanation of how it might be better to optimize strategy, rather than individual decisions). However, this algorithm breaks down if an agent becomes able to infer its own decision (or that a certain decision is impossible): in that case, it can infer anything from the hypothesis that it makes an impossible decision. This is one case where an agent might want to make its decision unpredictable to itself!</p>\n<p>It seems that normally an agent won't be able to predict its own decision (see <a href=\"/lw/2l2/what_a_reduction_of_could_could_look_like/\">these</a> <a href=\"/lw/2os/controlling_constant_programs/\">posts</a> for some discussion), but it's unclear how to set up an agent so that it provably doesn't do that. Worse, an ADT agent is a program that reasons using first-order logic and knows its own source code, which means that a statement declaring agent's actual decision is provable (by a standard inference system that enumerates all valid statements), and so the best we can hope for is that the agent won't stumble upon such a proof <em>before it made a decision</em> (see the section of <a href=\"/lw/2tq/notion_of_preference_in_ambient_control/\">this post</a> on provability by the agent).</p>\n<h3>Countable diagonal step</h3>\n<p>Adding the diagonal step to ADT agents, to make them unable to predict (infer) their own decision, in the simplest case requires the agents to simulate themselves up until the point where they make a decision. In general this is too hard, so one way out is to grant the agents provability oracles. The resulting setting is described in <a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle/\">this post</a>.</p>\n<p>The inference (prediction) system of an agent with a provability oracle can be completely described as a formal system S, which the agent uses (with help of the oracle) to check validity of any statement (in one step). Trouble with predicting own decisions then corresponds to S being too strong, able to infer agent's decision. The (\"countable\") diagonal step can be implemented by the agent checking, for each possible action A, whether S proves that actual action doesn't equal A, and performing A if S proves so. If S is sound, that situation is never realized, which means that S is weak enough to fail to predict agent's decision. The agent makes itself just complicated enough for its own inference system to become unable to reason about (origins of) its decision. This condition is then sufficient to prove that the agent satisfactorily resolves some thought experiments, such as Newcomb's Problem.</p>\n<h3>Avoiding spurious statements (without oracles)</h3>\n<p>An ADT agent without provability oracle only needs to hold off on predicting its decision until the decision is made. This can be arranged by use of the diagonal step if it's known when the decision <em>can</em> be made. Let's consider an example.</p>\n<p>Newcomb's problem can be formalized (as in <a href=\"/lw/2l2/what_a_reduction_of_could_could_look_like/\">these</a> <a href=\"/lw/2os/controlling_constant_programs/\">posts</a>) as follows:</p>\n<pre>def world(): <br />&nbsp; box1 = 1000 <br />&nbsp; box2 = (agent() == 2) ? 0 : 1000000 <br />&nbsp; return box2 + ((agent() == 2) ? box1 : 0)</pre>\n<p>Define constant symbols A=agent(), U=world(). It's easy to see that statements [A=1 =&gt; U=1000000] and [A=2 =&gt; U=1000] are valid, no matter what agent() is (assuming it terminates and always outputs 1 or 2). But if actually A=2, then A=1 is false, and so the statement [A=1 =&gt; U=0] is true as well. How do we know that the agent won't find specifically the statements [A=1 =&gt; U=0] and [A=2 =&gt; U=1000], choose A=2 as a result, with A=1 indeed becoming false?</p>\n<p>The problem is that even when we know that in the intended situation certain statements assigning value to possible actions are true, that doesn't automatically guarantee that the agent will find these same statements, and thus that it will make the decision implied by the statements we know to be true. In Newcomb's problem, an agent could in principle encounter true statements [A=1 =&gt; U=0] and [A=2 =&gt; U=1000] listed at the very beginning of a contrived inference system and two-box as a result, which is what makes those statements true.</p>\n<p>Thus, it matters how agent's inference system works, even if we know that it's correct (i.e. enumerates all valid first order statements following from the stated axioms). It has to be sufficiently \"natural\" in some sense, so that we can rely on [A=1 =&gt; U=1000000] coming before statements like [A=1 =&gt; U=0], with agent acting on the more \"natural\" arguments about value of a possible action, and not on one of the spurious ones (implied by the possible action not being actually taken).</p>\n<h3>Finite diagonal step</h3>\n<p>In the preceding example, we can point out specific proofs of [A=1 =&gt; U=1000000] and [A=2 =&gt; U=1000] that don't depend on definition of A. Knowing the length of such proofs (say, L) and using an inference system that enumerates all valid proofs in the order of their length allows predicting when those particular proofs will be encountered, and so when, at the latest, the decision will be made (if it's not made before that, but using some other, shorter proof). It is then only until that point that the agent has to hold off on predicting its own decision.</p>\n<p>If we know the number of steps N for the duration of which the inference system needs to be protected (while it considers the proofs no longer than L), the (\"finite\") diagonal step consists in running agent's inference system S for N steps, and contradicting (by action) any prediction about agent's action reached during these N steps. This step is decidable and doesn't require an oracle. Implementing it guarantees that (if S is sound and N is selected as discussed above) agent won't be able to predict its decision before it's made.</p>\n<p>Sufficiently increasing the threshold N allows to prove that no other statements of the form [A=A1 =&gt; U=U1] will be proved before a decision is made (and so that the decision will be as expected). Specifically, let M&gt;N be the new threshold. We know that (for example) [A=1 =&gt; U=1000000] is proved by S within N steps, so if (say) [A=1 =&gt; U=0] is proved also within N steps, then the proof of negation of [A=1] is only a few steps longer than 2*L, and so if we choose M to include all proofs of that length, the disproof of [A=1] will be encountered within the first M steps, which would trigger the diagonal step. Since the diagonal step can't be triggered if S is sound, we've thus proved that presence of the diagonal step guarding the first M steps of the inference system allows to conclude that the agent won't infer any spurious statements of the form [A=A1 =&gt; U=U1], and will behave as expected.</p>\n<p>This is the same guarantee that countable diagonal rule gives, albeit at the cost of having to know L.</p>\n<h3>Prisoner's dilemma</h3>\n<p><a href=\"/lw/tn/the_true_prisoners_dilemma/\">Prisoner's dilemma</a> presents a situation where some measure of predictability is advantageous to both agents. Specifically, if an agent can prove that cooperating implies cooperation of the opponent, and defecting implies defection of the opponent, it can then cooperate, reaching the outcome (C,C) that is better than the equilibrium solution (D,D). It's harder to arrange cooperation without being able to predict the opponent's reaction to a hypothetical decision, so taking away all predictability of decisions will hurt both agents.</p>\n<p>The trick is that UDT only wants that statements like [A1=C] be unprovable (in N steps), but statements like [A1=C =&gt; A2=C] may remain provable, and it's conditional statements like this that are needed to broker cooperation in PD. It seems that cooperation is possible both for <a href=\"/lw/2ip/ai_cooperation_in_practice/\">simple agents without oracles or use of the diagonal step</a>, and for <a href=\"/lw/9o7/formulas_of_arithmetic_that_behave_like_decision/\">agents with oracles that use countable diagonal step</a>.</p>\n<h3>Agent simulates predictor</h3>\n<p>Finally, there is the <a href=\"/lw/5rq/example_decision_theory_problem_agent_simulates/\">ASP problem</a> that involves a weak predictor and a powerful agent that need to cooperate on Newcomb's problem. The difficulty is that it seems that since the agent can anticipate predictor's decision by simulating it, it would take both boxes no matter what, and so the predictor has to predict two-boxing (which is unfortunate). The predictor is defenseless, it can't insist on making itself unpredictable, and as a result the agent's predictions of predictor's decision (given either hypothetical action of the agent) coincide.</p>\n<p>Just as it's necessary to entertain multiple hypothetical actions to gauge their consequences, it's necessary for there to be multiple hypothetical outcomes for their comparison to yield a nontrivial decision. The preceding sections addressed the problem of making agent's own decision unpredictable, and in this case it seems that the agent has to make the decision of its opponent (the weak predictor) unpredictable (to the agent).</p>\n<p>One way to patch this up seems to be requiring that the weak predictor is trying to prove that agent one-boxes (with however little computational power it has), and predicting two-boxing otherwise. This way, if predictor says that agent one-boxes, agent must really be one-boxing, and so we can use this condition in a diagonal step in the agent, making this situation unpredictable to the agent. Namely, the agent will two-box if it predicts that the predictor decides that the agent one-boxes (while running agent's inference system for at most M steps, as with finite diagonal step). As a result, the agent won't be able to predict that the predictor one-boxes (within M steps of agent's inference system), which might make the agent simple enough for the predictor to predict, and the two players seem to regain an equal footing.</p>\n<h3>Open problems</h3>\n<ul>\n<li>Formalize this analysis of ASP, prove that predictor and agent one-box</li>\n<li>What happens if we try making the opponent in PD \"unpredictable\" in the same sense?</li>\n<li>See if making all kinds of facts unpredictable in this way can be made to retain their conditional predictability (predictability given an assumption of what agent's action is)</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W6T93dSSm2xvHn9X6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 29, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "13863", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p style=\"padding-left: 30px; \"><em>This post collects a few situations where agents might want to make their decisions either predictable or unpredictable to certain methods of prediction, and considers a method of making a decision unpredictable by \"diagonalizing\" a hypothetical prediction of that decision. The last section takes a stab at applying this tool to the <a href=\"/lw/5rq/example_decision_theory_problem_agent_simulates/\">ASP problem</a>.</em></p>\n<h3 id=\"The_diagonal_step\">The diagonal step</h3>\n<p>To start off, consider the halting problem, interpreted in terms of agents and predictors. Suppose that there is a Universal Predictor, an algorithm that is able to decide whether any given program halts or runs forever. Then, it's easy for a program (agent) to evade its gaze by including a <em>diagonal step</em> in its decision procedure: the agent checks (by simulation) if Universal Predictor comes to some decision about the agent, and if it does, the agent acts contrary to the Predictor's decision. This makes the prediction wrong, and Universal Predictors impossible.</p>\n<p>The same trick could be performed against something that could exist, normal non-universal Predictors, which allows an agent to make itself immune to their predictions. In particular, ability of other agents to infer decisions of our agent may be thought of as prediction that an agent might want to hinder. This is possible so long as the predictors in question can be simulated in enough detail, that is it's known what they do (what they know) and our agent has enough computational resources to anticipate their hypothetical conclusions. (If an agent does perform the diagonal step with respect to other agents, the predictions of other agents don't necessarily become wrong, as they could be formally correct by construction, but they cease to be possible, which could mean that the predictions won't be made at all.)<a id=\"more\"></a></p>\n<h3 id=\"Knowing_own_decision\">Knowing own decision</h3>\n<p>In UDT/ADT, an agent is given definitions of its action (strategy) and utility value, and evaluates an action (strategy) by inferring the logical implications of taking it on the utility value (see <a href=\"/lw/2tq/notion_of_preference_in_ambient_control/\">this post</a> for a more detailed description of the decision procedure and <a href=\"/lw/1s5/explicit_optimization_of_global_strategy_fixing_a/\">this post</a> for an explanation of how it might be better to optimize strategy, rather than individual decisions). However, this algorithm breaks down if an agent becomes able to infer its own decision (or that a certain decision is impossible): in that case, it can infer anything from the hypothesis that it makes an impossible decision. This is one case where an agent might want to make its decision unpredictable to itself!</p>\n<p>It seems that normally an agent won't be able to predict its own decision (see <a href=\"/lw/2l2/what_a_reduction_of_could_could_look_like/\">these</a> <a href=\"/lw/2os/controlling_constant_programs/\">posts</a> for some discussion), but it's unclear how to set up an agent so that it provably doesn't do that. Worse, an ADT agent is a program that reasons using first-order logic and knows its own source code, which means that a statement declaring agent's actual decision is provable (by a standard inference system that enumerates all valid statements), and so the best we can hope for is that the agent won't stumble upon such a proof <em>before it made a decision</em> (see the section of <a href=\"/lw/2tq/notion_of_preference_in_ambient_control/\">this post</a> on provability by the agent).</p>\n<h3 id=\"Countable_diagonal_step\">Countable diagonal step</h3>\n<p>Adding the diagonal step to ADT agents, to make them unable to predict (infer) their own decision, in the simplest case requires the agents to simulate themselves up until the point where they make a decision. In general this is too hard, so one way out is to grant the agents provability oracles. The resulting setting is described in <a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle/\">this post</a>.</p>\n<p>The inference (prediction) system of an agent with a provability oracle can be completely described as a formal system S, which the agent uses (with help of the oracle) to check validity of any statement (in one step). Trouble with predicting own decisions then corresponds to S being too strong, able to infer agent's decision. The (\"countable\") diagonal step can be implemented by the agent checking, for each possible action A, whether S proves that actual action doesn't equal A, and performing A if S proves so. If S is sound, that situation is never realized, which means that S is weak enough to fail to predict agent's decision. The agent makes itself just complicated enough for its own inference system to become unable to reason about (origins of) its decision. This condition is then sufficient to prove that the agent satisfactorily resolves some thought experiments, such as Newcomb's Problem.</p>\n<h3 id=\"Avoiding_spurious_statements__without_oracles_\">Avoiding spurious statements (without oracles)</h3>\n<p>An ADT agent without provability oracle only needs to hold off on predicting its decision until the decision is made. This can be arranged by use of the diagonal step if it's known when the decision <em>can</em> be made. Let's consider an example.</p>\n<p>Newcomb's problem can be formalized (as in <a href=\"/lw/2l2/what_a_reduction_of_could_could_look_like/\">these</a> <a href=\"/lw/2os/controlling_constant_programs/\">posts</a>) as follows:</p>\n<pre>def world(): <br>&nbsp; box1 = 1000 <br>&nbsp; box2 = (agent() == 2) ? 0 : 1000000 <br>&nbsp; return box2 + ((agent() == 2) ? box1 : 0)</pre>\n<p>Define constant symbols A=agent(), U=world(). It's easy to see that statements [A=1 =&gt; U=1000000] and [A=2 =&gt; U=1000] are valid, no matter what agent() is (assuming it terminates and always outputs 1 or 2). But if actually A=2, then A=1 is false, and so the statement [A=1 =&gt; U=0] is true as well. How do we know that the agent won't find specifically the statements [A=1 =&gt; U=0] and [A=2 =&gt; U=1000], choose A=2 as a result, with A=1 indeed becoming false?</p>\n<p>The problem is that even when we know that in the intended situation certain statements assigning value to possible actions are true, that doesn't automatically guarantee that the agent will find these same statements, and thus that it will make the decision implied by the statements we know to be true. In Newcomb's problem, an agent could in principle encounter true statements [A=1 =&gt; U=0] and [A=2 =&gt; U=1000] listed at the very beginning of a contrived inference system and two-box as a result, which is what makes those statements true.</p>\n<p>Thus, it matters how agent's inference system works, even if we know that it's correct (i.e. enumerates all valid first order statements following from the stated axioms). It has to be sufficiently \"natural\" in some sense, so that we can rely on [A=1 =&gt; U=1000000] coming before statements like [A=1 =&gt; U=0], with agent acting on the more \"natural\" arguments about value of a possible action, and not on one of the spurious ones (implied by the possible action not being actually taken).</p>\n<h3 id=\"Finite_diagonal_step\">Finite diagonal step</h3>\n<p>In the preceding example, we can point out specific proofs of [A=1 =&gt; U=1000000] and [A=2 =&gt; U=1000] that don't depend on definition of A. Knowing the length of such proofs (say, L) and using an inference system that enumerates all valid proofs in the order of their length allows predicting when those particular proofs will be encountered, and so when, at the latest, the decision will be made (if it's not made before that, but using some other, shorter proof). It is then only until that point that the agent has to hold off on predicting its own decision.</p>\n<p>If we know the number of steps N for the duration of which the inference system needs to be protected (while it considers the proofs no longer than L), the (\"finite\") diagonal step consists in running agent's inference system S for N steps, and contradicting (by action) any prediction about agent's action reached during these N steps. This step is decidable and doesn't require an oracle. Implementing it guarantees that (if S is sound and N is selected as discussed above) agent won't be able to predict its decision before it's made.</p>\n<p>Sufficiently increasing the threshold N allows to prove that no other statements of the form [A=A1 =&gt; U=U1] will be proved before a decision is made (and so that the decision will be as expected). Specifically, let M&gt;N be the new threshold. We know that (for example) [A=1 =&gt; U=1000000] is proved by S within N steps, so if (say) [A=1 =&gt; U=0] is proved also within N steps, then the proof of negation of [A=1] is only a few steps longer than 2*L, and so if we choose M to include all proofs of that length, the disproof of [A=1] will be encountered within the first M steps, which would trigger the diagonal step. Since the diagonal step can't be triggered if S is sound, we've thus proved that presence of the diagonal step guarding the first M steps of the inference system allows to conclude that the agent won't infer any spurious statements of the form [A=A1 =&gt; U=U1], and will behave as expected.</p>\n<p>This is the same guarantee that countable diagonal rule gives, albeit at the cost of having to know L.</p>\n<h3 id=\"Prisoner_s_dilemma\">Prisoner's dilemma</h3>\n<p><a href=\"/lw/tn/the_true_prisoners_dilemma/\">Prisoner's dilemma</a> presents a situation where some measure of predictability is advantageous to both agents. Specifically, if an agent can prove that cooperating implies cooperation of the opponent, and defecting implies defection of the opponent, it can then cooperate, reaching the outcome (C,C) that is better than the equilibrium solution (D,D). It's harder to arrange cooperation without being able to predict the opponent's reaction to a hypothetical decision, so taking away all predictability of decisions will hurt both agents.</p>\n<p>The trick is that UDT only wants that statements like [A1=C] be unprovable (in N steps), but statements like [A1=C =&gt; A2=C] may remain provable, and it's conditional statements like this that are needed to broker cooperation in PD. It seems that cooperation is possible both for <a href=\"/lw/2ip/ai_cooperation_in_practice/\">simple agents without oracles or use of the diagonal step</a>, and for <a href=\"/lw/9o7/formulas_of_arithmetic_that_behave_like_decision/\">agents with oracles that use countable diagonal step</a>.</p>\n<h3 id=\"Agent_simulates_predictor\">Agent simulates predictor</h3>\n<p>Finally, there is the <a href=\"/lw/5rq/example_decision_theory_problem_agent_simulates/\">ASP problem</a> that involves a weak predictor and a powerful agent that need to cooperate on Newcomb's problem. The difficulty is that it seems that since the agent can anticipate predictor's decision by simulating it, it would take both boxes no matter what, and so the predictor has to predict two-boxing (which is unfortunate). The predictor is defenseless, it can't insist on making itself unpredictable, and as a result the agent's predictions of predictor's decision (given either hypothetical action of the agent) coincide.</p>\n<p>Just as it's necessary to entertain multiple hypothetical actions to gauge their consequences, it's necessary for there to be multiple hypothetical outcomes for their comparison to yield a nontrivial decision. The preceding sections addressed the problem of making agent's own decision unpredictable, and in this case it seems that the agent has to make the decision of its opponent (the weak predictor) unpredictable (to the agent).</p>\n<p>One way to patch this up seems to be requiring that the weak predictor is trying to prove that agent one-boxes (with however little computational power it has), and predicting two-boxing otherwise. This way, if predictor says that agent one-boxes, agent must really be one-boxing, and so we can use this condition in a diagonal step in the agent, making this situation unpredictable to the agent. Namely, the agent will two-box if it predicts that the predictor decides that the agent one-boxes (while running agent's inference system for at most M steps, as with finite diagonal step). As a result, the agent won't be able to predict that the predictor one-boxes (within M steps of agent's inference system), which might make the agent simple enough for the predictor to predict, and the two players seem to regain an equal footing.</p>\n<h3 id=\"Open_problems\">Open problems</h3>\n<ul>\n<li>Formalize this analysis of ASP, prove that predictor and agent one-box</li>\n<li>What happens if we try making the opponent in PD \"unpredictable\" in the same sense?</li>\n<li>See if making all kinds of facts unpredictable in this way can be made to retain their conditional predictability (predictability given an assumption of what agent's action is)</li>\n</ul>", "sections": [{"title": "The diagonal step", "anchor": "The_diagonal_step", "level": 1}, {"title": "Knowing own decision", "anchor": "Knowing_own_decision", "level": 1}, {"title": "Countable diagonal step", "anchor": "Countable_diagonal_step", "level": 1}, {"title": "Avoiding spurious statements (without oracles)", "anchor": "Avoiding_spurious_statements__without_oracles_", "level": 1}, {"title": "Finite diagonal step", "anchor": "Finite_diagonal_step", "level": 1}, {"title": "Prisoner's dilemma", "anchor": "Prisoner_s_dilemma", "level": 1}, {"title": "Agent simulates predictor", "anchor": "Agent_simulates_predictor", "level": 1}, {"title": "Open problems", "anchor": "Open_problems", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "12 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["q9DbfYfFzkotno9hG", "ZpATmvAyqajiA5XNC", "g8xh9R7RaNitKtkaa", "dC3rxrMkYKLfgTYEa", "gZbHSWcLvj7ZopSas", "Bj244uWzDBXvE2N2S", "HFyWNBnDNEDsDNLrZ", "TNfx89dh5KkcKrvho", "yX9pMZik7r38da7Fc"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-10T00:47:58.972Z", "modifiedAt": null, "url": null, "title": "How to use human history as a nutritional prior?", "slug": "how-to-use-human-history-as-a-nutritional-prior", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:30.184Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CasioTheSane", "createdAt": "2012-03-06T23:29:51.068Z", "isAdmin": false, "displayName": "CasioTheSane"}, "userId": "Rn8faPpZcbhfnq6bz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6xqEWEJSmL4nrnnpC/how-to-use-human-history-as-a-nutritional-prior", "pageUrlRelative": "/posts/6xqEWEJSmL4nrnnpC/how-to-use-human-history-as-a-nutritional-prior", "linkUrl": "https://www.lesswrong.com/posts/6xqEWEJSmL4nrnnpC/how-to-use-human-history-as-a-nutritional-prior", "postedAtFormatted": "Saturday, March 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20use%20human%20history%20as%20a%20nutritional%20prior%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20use%20human%20history%20as%20a%20nutritional%20prior%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6xqEWEJSmL4nrnnpC%2Fhow-to-use-human-history-as-a-nutritional-prior%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20use%20human%20history%20as%20a%20nutritional%20prior%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6xqEWEJSmL4nrnnpC%2Fhow-to-use-human-history-as-a-nutritional-prior", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6xqEWEJSmL4nrnnpC%2Fhow-to-use-human-history-as-a-nutritional-prior", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 210, "htmlBody": "<p>Nutrition is a case where we have to try to make the best possible use of the data we have no matter how terrible, because we have to eat something now to sustain us while we plan and conduct more experiments.</p>\r\n<p>I want to apply Bayes theorem to make rational health decisions from relatively weak data. I am generally wondering how one can synthesize historical human experiences with incomplete scientific data, in order to make risk-adverse and healthy decisions about human nutrition given limited research.</p>\r\n<p>Example question/hypothesis: Does gluten cause health problems (ie exhibit chronic toxicity) in non-coeliac humans? Is there enough evidence to suggest that avoiding gluten might be a prudent risk-adverse decision for non-coeliacs?</p>\r\n<p>We have some (mostly in vitro) scientific data suggesting that gluten may cause health problems in non-coeliac humans (such as these articles http://evolvify.com/the-case-against-gluten-medical-journal-references/). Let's say for the sake of arguing, that I can somehow convert these studies into a non-unity likelihood ratio for gluten toxicity in humans (although suggestions are welcome here too).</p>\r\n<p>However, we also have prior information that a population of humans has been consuming gluten containing foods for at least 10,000 years, without any blatantly obvious toxic effects. Is there some way to convert this observation (and observations like this) into a prior probability distribution?</p>\r\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"92SxJsDZ78ApAGq72": 1, "xexCWMyds6QLWognu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6xqEWEJSmL4nrnnpC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 8, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "13862", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-10T01:29:09.258Z", "modifiedAt": null, "url": null, "title": "Meetup : Madison Monday Meetup", "slug": "meetup-madison-monday-meetup-5", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yrY6x4eHdxPRM5kAn/meetup-madison-monday-meetup-5", "pageUrlRelative": "/posts/yrY6x4eHdxPRM5kAn/meetup-madison-monday-meetup-5", "linkUrl": "https://www.lesswrong.com/posts/yrY6x4eHdxPRM5kAn/meetup-madison-monday-meetup-5", "postedAtFormatted": "Saturday, March 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Madison%20Monday%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Madison%20Monday%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyrY6x4eHdxPRM5kAn%2Fmeetup-madison-monday-meetup-5%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Madison%20Monday%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyrY6x4eHdxPRM5kAn%2Fmeetup-madison-monday-meetup-5", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyrY6x4eHdxPRM5kAn%2Fmeetup-madison-monday-meetup-5", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 129, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/7v'>Madison Monday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 March 2012 05:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1831 Monroe St., Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This time around, let's see if we can find ways to be more \"strategic\", as in:\n    <a href=\"http://lesswrong.com/lw/2p5/humans_are_not_automatically_strategic/\" rel=\"nofollow\">http://lesswrong.com/lw/2p5/humans_are_not_automatically_strategic/</a></p>\n\n<p>I'd like to discuss this both in general (\"how can we encourage strategic-ness?\") and also in trying to accomplish whatever our specific goals are. (\"Being more awesome\" and \"Making other people's lives stranger\" are, surely, acceptable goals. But have we found ways to break these down into useful subgoals?)</p>\n\n<p>I know we've done some of this before, but I suspect it's worth trying on a larger, more-intentional scale.</p>\n\n<p>If we somehow find ourselves out of things to talk about (?!), I'll have the bits for Zendo handy.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/7v'>Madison Monday Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yrY6x4eHdxPRM5kAn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.628509344398047e-07, "legacy": true, "legacyId": "13873", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Madison_Monday_Meetup\">Discussion article for the meetup : <a href=\"/meetups/7v\">Madison Monday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 March 2012 05:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1831 Monroe St., Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This time around, let's see if we can find ways to be more \"strategic\", as in:\n    <a href=\"http://lesswrong.com/lw/2p5/humans_are_not_automatically_strategic/\" rel=\"nofollow\">http://lesswrong.com/lw/2p5/humans_are_not_automatically_strategic/</a></p>\n\n<p>I'd like to discuss this both in general (\"how can we encourage strategic-ness?\") and also in trying to accomplish whatever our specific goals are. (\"Being more awesome\" and \"Making other people's lives stranger\" are, surely, acceptable goals. But have we found ways to break these down into useful subgoals?)</p>\n\n<p>I know we've done some of this before, but I suspect it's worth trying on a larger, more-intentional scale.</p>\n\n<p>If we somehow find ourselves out of things to talk about (?!), I'll have the bits for Zendo handy.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Madison_Monday_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/7v\">Madison Monday Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Madison Monday Meetup", "anchor": "Discussion_article_for_the_meetup___Madison_Monday_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Madison Monday Meetup", "anchor": "Discussion_article_for_the_meetup___Madison_Monday_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PBRWb2Em5SNeWYwwB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-10T04:22:52.720Z", "modifiedAt": null, "url": null, "title": "Slowing Moore's Law: Why You Might Want To and How You Would Do It", "slug": "slowing-moore-s-law-why-you-might-want-to-and-how-you-would", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:03.583Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7s2as5qJg5eFpyGcM/slowing-moore-s-law-why-you-might-want-to-and-how-you-would", "pageUrlRelative": "/posts/7s2as5qJg5eFpyGcM/slowing-moore-s-law-why-you-might-want-to-and-how-you-would", "linkUrl": "https://www.lesswrong.com/posts/7s2as5qJg5eFpyGcM/slowing-moore-s-law-why-you-might-want-to-and-how-you-would", "postedAtFormatted": "Saturday, March 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Slowing%20Moore's%20Law%3A%20Why%20You%20Might%20Want%20To%20and%20How%20You%20Would%20Do%20It&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASlowing%20Moore's%20Law%3A%20Why%20You%20Might%20Want%20To%20and%20How%20You%20Would%20Do%20It%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7s2as5qJg5eFpyGcM%2Fslowing-moore-s-law-why-you-might-want-to-and-how-you-would%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Slowing%20Moore's%20Law%3A%20Why%20You%20Might%20Want%20To%20and%20How%20You%20Would%20Do%20It%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7s2as5qJg5eFpyGcM%2Fslowing-moore-s-law-why-you-might-want-to-and-how-you-would", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7s2as5qJg5eFpyGcM%2Fslowing-moore-s-law-why-you-might-want-to-and-how-you-would", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<p>In this essay I argue the following:</p>\n<blockquote>\n<p>Brain emulation requires enormous computing power; enormous computing power requires further progression of Moore&rsquo;s law; further Moore&rsquo;s law relies on large-scale production of cheap processors in ever more-advanced chip fabs; cutting-edge chip fabs are both expensive and vulnerable to state actors (but <em>not</em> non-state actors such as terrorists). Therefore: the advent of brain emulation can be delayed by global regulation of chip fabs.</p>\n</blockquote>\n<p>Full essay: <a href=\"http://www.gwern.net/Slowing%20Moore%27s%20Law\">http://www.gwern.net/Slowing%20Moore%27s%20Law</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb285": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7s2as5qJg5eFpyGcM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 31, "extendedScore": null, "score": 8.629212343261858e-07, "legacy": true, "legacyId": "13882", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 90, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-10T06:24:59.144Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Is Humanism A Religion-Substitute?", "slug": "seq-rerun-is-humanism-a-religion-substitute", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:22.427Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4QCmeemrWk5FGD9vw/seq-rerun-is-humanism-a-religion-substitute", "pageUrlRelative": "/posts/4QCmeemrWk5FGD9vw/seq-rerun-is-humanism-a-religion-substitute", "linkUrl": "https://www.lesswrong.com/posts/4QCmeemrWk5FGD9vw/seq-rerun-is-humanism-a-religion-substitute", "postedAtFormatted": "Saturday, March 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Is%20Humanism%20A%20Religion-Substitute%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Is%20Humanism%20A%20Religion-Substitute%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4QCmeemrWk5FGD9vw%2Fseq-rerun-is-humanism-a-religion-substitute%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Is%20Humanism%20A%20Religion-Substitute%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4QCmeemrWk5FGD9vw%2Fseq-rerun-is-humanism-a-religion-substitute", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4QCmeemrWk5FGD9vw%2Fseq-rerun-is-humanism-a-religion-substitute", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 218, "htmlBody": "<p>Today's post, <a href=\"/lw/oy/is_humanism_a_religionsubstitute/\">Is Humanism A Religion-Substitute?</a> was originally published on 26 March 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Is_Humanism_A_Religion-Substitute.3F\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Trying to replace religion with humanism, atheism, or transhumanism doesn't work. If you try to write a hymn to the nonexistence of god, it will fail, because you are simply trying to imitate something that we don't really need to imitate. But that doesn't mean that the feeling of transcendence is something we should always avoid. After all, in a world in which religion never existed, people would still feel that same way.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/aof/seq_rerun_amazing_breakthrough_day_april_1st/\">Amazing Breakthrough Day: April 1st</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4QCmeemrWk5FGD9vw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.629706526702568e-07, "legacy": true, "legacyId": "13888", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PMr6f7ZocEWFtCYXj", "Gn6odSHWsArPC4vMu", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-10T08:04:10.891Z", "modifiedAt": null, "url": null, "title": "Is causal decision theory plus self-modification enough? ", "slug": "is-causal-decision-theory-plus-self-modification-enough", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:27.164Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dgJ93Fw3KnticFWKt/is-causal-decision-theory-plus-self-modification-enough", "pageUrlRelative": "/posts/dgJ93Fw3KnticFWKt/is-causal-decision-theory-plus-self-modification-enough", "linkUrl": "https://www.lesswrong.com/posts/dgJ93Fw3KnticFWKt/is-causal-decision-theory-plus-self-modification-enough", "postedAtFormatted": "Saturday, March 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20causal%20decision%20theory%20plus%20self-modification%20enough%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20causal%20decision%20theory%20plus%20self-modification%20enough%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdgJ93Fw3KnticFWKt%2Fis-causal-decision-theory-plus-self-modification-enough%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20causal%20decision%20theory%20plus%20self-modification%20enough%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdgJ93Fw3KnticFWKt%2Fis-causal-decision-theory-plus-self-modification-enough", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdgJ93Fw3KnticFWKt%2Fis-causal-decision-theory-plus-self-modification-enough", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 367, "htmlBody": "<p>Occasionally a wrong idea still leads to the right outcome. We know that one-boxing on Newcomb's problem is the right thing to do. Timeless decision theory proposes to justify this action by saying: act as if you control all instances of your decision procedure, including the instance that Omega used to predict your behavior.</p>\n<p>But it's simply not true that you control Omega's actions in the past. If Omega predicted that you will one-box and filled the boxes accordingly, that's because, at the time the prediction was made, you were already a person who would foreseeably one-box. One way to be such a person is to be a TDT agent. But another way is to be a quasi-CDT agent with a superstitious belief that greediness is punished and modesty is rewarded - so you one-box <em>because</em> two-boxing looks like it has the higher payoff!</p>\n<p>That is an irrational belief, yet it still suffices to generate the better outcome. My thesis is that TDT is similarly based on an irrational premise. So what is actually going on? I now think that Newcomb's problem is simply an exceptional situation where there is an artificial incentive to employ something other than CDT, and that most such situations can be dealt with by being a CDT agent who can self-modify.</p>\n<p>Eliezer's <a href=\"http://intelligence.org/upload/TDT-v01o.pdf\">draft manuscript on TDT</a> provides another example (page 20): a godlike entity - we could call it Alphabeta - demands that you choose according to \"alphabetical decision theory\", or face an evil outcome. In this case, the alternative to CDT that you are being encouraged to use is explicitly identified. In Newcomb's problem, no such specific demand is made, but the situation encourages you to make a particular decision - how you rationalize it doesn't matter.</p>\n<p>We should fight the illusion that a TDT agent retrocausally controls Omega's choice. It doesn't. Omega's choice was controlled by the <em>extrapolated dispositions</em> of the TDT agent, as they were in the past. We don't need to replace CDT with TDT as our default decision theory, we just need to understand the exceptional situations in which it is expedient to replace CDT with something else. TDT will apply to some of those situations, but not all of them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dgJ93Fw3KnticFWKt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -6, "extendedScore": null, "score": 8.63010802140944e-07, "legacy": true, "legacyId": "13897", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-10T16:30:06.022Z", "modifiedAt": null, "url": null, "title": "New paper on Bayesian philosophy of statistics from Andrew Gelman", "slug": "new-paper-on-bayesian-philosophy-of-statistics-from-andrew", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:29.487Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "N6W7sAzCo3fGauM7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nNkXMmmgjPMbfQych/new-paper-on-bayesian-philosophy-of-statistics-from-andrew", "pageUrlRelative": "/posts/nNkXMmmgjPMbfQych/new-paper-on-bayesian-philosophy-of-statistics-from-andrew", "linkUrl": "https://www.lesswrong.com/posts/nNkXMmmgjPMbfQych/new-paper-on-bayesian-philosophy-of-statistics-from-andrew", "postedAtFormatted": "Saturday, March 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20paper%20on%20Bayesian%20philosophy%20of%20statistics%20from%20Andrew%20Gelman&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20paper%20on%20Bayesian%20philosophy%20of%20statistics%20from%20Andrew%20Gelman%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnNkXMmmgjPMbfQych%2Fnew-paper-on-bayesian-philosophy-of-statistics-from-andrew%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20paper%20on%20Bayesian%20philosophy%20of%20statistics%20from%20Andrew%20Gelman%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnNkXMmmgjPMbfQych%2Fnew-paper-on-bayesian-philosophy-of-statistics-from-andrew", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnNkXMmmgjPMbfQych%2Fnew-paper-on-bayesian-philosophy-of-statistics-from-andrew", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 217, "htmlBody": "<p><a href=\"http://andrewgelman.com\">Andrew Gelman</a> recently linked a new article entitled <a href=\"http://www.stat.columbia.edu/~gelman/research/published/philosophy_online4.pdf\">\"Induction and Deduction in Bayesian Data Analysis.\"</a> At his blog, he also <a href=\"http://andrewgelman.com/2012/03/coming-to-agreement-on-philosophy-of-statistics/\">described</a> some of the comments made by reviewers and his rebuttle/discussion to those comments. It is interesting that he departs significantly from the common induction-based view of Bayesian approaches. As a practitioner myself, I am happiest about the discussion on model checking -- something one can definitely do in the Bayesian framework but which almost no one does. Model checking is to Bayesian data analysis as unit testing is to software engineering.<br /><br /><strong>Added 03/11/12</strong><br />Gelman has a new <a href=\"http://andrewgelman.com/2012/03/gelman-on-hennig-on-gelman-on-bayes/\">blog post</a> today discussing another reaction to his paper and giving some additional details. Notably:</p>\n<blockquote>The basic idea of posterior predictive checking is, as they say, breathtakingly simple: (a) graph your data, (b) fit your model to data, (c) simulate replicated data (a Bayesian can always do this, because Bayesian models are always &ldquo;generative&rdquo;), (d) graph the replicated data, and (e) compare the graphs in (a) and (d). It makes me want to scream scream scream scream scream when statisticians&rsquo; philosophical scruples stop them from performing these five simple steps (or, to be precise, performing the simple steps (a), (c), (d), and (e), given that they&rsquo;ve already done the hard part, which is step (b)).</blockquote>\n&nbsp;\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nNkXMmmgjPMbfQych", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 8.632156245278056e-07, "legacy": true, "legacyId": "13899", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-10T20:35:08.786Z", "modifiedAt": null, "url": null, "title": "Free Applied Instrumental Rationality Webinar", "slug": "free-applied-instrumental-rationality-webinar", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:20.220Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ksvanhorn", "createdAt": "2011-01-10T19:23:33.231Z", "isAdmin": false, "displayName": "ksvanhorn"}, "userId": "fiau5fcXpbad9d6D4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M6MYTSNBHxKpNisq5/free-applied-instrumental-rationality-webinar", "pageUrlRelative": "/posts/M6MYTSNBHxKpNisq5/free-applied-instrumental-rationality-webinar", "linkUrl": "https://www.lesswrong.com/posts/M6MYTSNBHxKpNisq5/free-applied-instrumental-rationality-webinar", "postedAtFormatted": "Saturday, March 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Free%20Applied%20Instrumental%20Rationality%20Webinar&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFree%20Applied%20Instrumental%20Rationality%20Webinar%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM6MYTSNBHxKpNisq5%2Ffree-applied-instrumental-rationality-webinar%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Free%20Applied%20Instrumental%20Rationality%20Webinar%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM6MYTSNBHxKpNisq5%2Ffree-applied-instrumental-rationality-webinar", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM6MYTSNBHxKpNisq5%2Ffree-applied-instrumental-rationality-webinar", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 200, "htmlBody": "<p>Dan Nuffer and I are putting together a free webinar that will go through the ideas in <em>Smart Choices: A Practical Guide to Making Better Life Decisions</em>, combined with whatever else seems useful. The authors of this book include one of the pioneers of decision analysis.</p>\n<p>Although they don't describe it as such, <em>Smart Choices</em> is really a manual for basic applied instrumental rationality. It's a systematic way of going about your decisions, applicable to either decision problems (you have a situation dumped in your lap that requires a response) or decision opportunities (proactively seeking out ways to further your goals.)</p>\n<p>The webinar will be one-hour sessions once a week for however long it takes to go through the material. We're going to do the webinar on Google+ Hangouts, and we'll have a discussion forum for the webinar on our web site.</p>\n<p>If you're interested, send me an email [kevin at ksvanhorn com] with 1) your preferred day/time(s), and 2) the day/times that are out of the question for you.</p>\n<p>Google+ Hangouts has a limit of 10 people. Five of those slots are already filled, leaving 5 seats open, so don't wait too long to email me if this is something you're interested in.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M6MYTSNBHxKpNisq5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 8.63314862896129e-07, "legacy": true, "legacyId": "13900", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-10T21:57:53.248Z", "modifiedAt": null, "url": null, "title": "Anyone have any questions for David Chalmers?", "slug": "anyone-have-any-questions-for-david-chalmers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:56.271Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Solvent", "createdAt": "2011-07-19T07:12:44.132Z", "isAdmin": false, "displayName": "Solvent"}, "userId": "a3sBsZXtAQacMDHfK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p3EJ5f9jxEefzAM82/anyone-have-any-questions-for-david-chalmers", "pageUrlRelative": "/posts/p3EJ5f9jxEefzAM82/anyone-have-any-questions-for-david-chalmers", "linkUrl": "https://www.lesswrong.com/posts/p3EJ5f9jxEefzAM82/anyone-have-any-questions-for-david-chalmers", "postedAtFormatted": "Saturday, March 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anyone%20have%20any%20questions%20for%20David%20Chalmers%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnyone%20have%20any%20questions%20for%20David%20Chalmers%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp3EJ5f9jxEefzAM82%2Fanyone-have-any-questions-for-david-chalmers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anyone%20have%20any%20questions%20for%20David%20Chalmers%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp3EJ5f9jxEefzAM82%2Fanyone-have-any-questions-for-david-chalmers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp3EJ5f9jxEefzAM82%2Fanyone-have-any-questions-for-david-chalmers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 139, "htmlBody": "<p>I'm doing an undergraduate course on the Free Will Theorem, with three lecturers: a mathematician, a physicist, and David Chalmers as the philosopher. The course is a bit pointless, but the company is brilliant. Chalmers is a pretty smart guy. He studied computer science and math as an undergraduate, before \"discovering that he could get paid for doing the kind of thinking he was doing for free already\". He's friendly; I've been chatting with him after the classes.</p>\n<p>So if anyone has any questions for him, if they seem interesting enough I could approach him with them.</p>\n<p>Emails to him also work, of course, but discussion in person lets more understanding happen faster. For example, in a short discussion with him I understood his position on consciousness way better than I would have just from reading his papers on the topic.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p3EJ5f9jxEefzAM82", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 18, "extendedScore": null, "score": 8.63348375785191e-07, "legacy": true, "legacyId": "13901", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-11T00:03:08.860Z", "modifiedAt": null, "url": null, "title": "On the etiology of religious belief", "slug": "on-the-etiology-of-religious-belief", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:56.456Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DuM5d7stfbm4KksM2/on-the-etiology-of-religious-belief", "pageUrlRelative": "/posts/DuM5d7stfbm4KksM2/on-the-etiology-of-religious-belief", "linkUrl": "https://www.lesswrong.com/posts/DuM5d7stfbm4KksM2/on-the-etiology-of-religious-belief", "postedAtFormatted": "Sunday, March 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20the%20etiology%20of%20religious%20belief&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20the%20etiology%20of%20religious%20belief%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDuM5d7stfbm4KksM2%2Fon-the-etiology-of-religious-belief%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20the%20etiology%20of%20religious%20belief%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDuM5d7stfbm4KksM2%2Fon-the-etiology-of-religious-belief", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDuM5d7stfbm4KksM2%2Fon-the-etiology-of-religious-belief", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 371, "htmlBody": "<p>From <a href=\"http://prosblogion.ektopos.com/archives/2012/03/trust-in-testim.html\">\"Trust in testimony and miracles\"</a>:</p>\n<blockquote>\n<p>I have been of late fascinated by the research of the developmental psychologist Paul L. Harris, who has investigated how young children acquire information through testimony. Harris gauges two psychological hypotheses. The first, which he attributes to Hume, is that children always assess the <strong>content </strong>of the information: they are more inclined to disbelieve information that widely differs from their earlier experience. The second, which he identifies with Reid's position is that children are naturally credulous; they are inclined to indiscriminately believe what others testify, no matter who they are or what they tell.</p>\n<p><a id=\"more\"></a></p>\n<p>...Harris found that children do not fall into either pattern. Pace the Humean account, he found that young children are readily inclined to believe extraordinary claims, such as that there are invisible organisms on your hands that can make you ill and that you need to wash off, and that there is a man who visits you each 24th December to bring presents and candy if you are nice (see e.g., <a title=\"&quot;Trust in Testimony: How Children Learn About Science and Religion&quot;\" href=\"http://www.cehd.umn.edu/icd/elel/Documents/ScienceReligion.pdf\">Harris &amp; Koenig, 2006, Child Development, 77, 505 - 524</a>). But children are not blindly credulous either, as Reid supposed. In a series of experiments, Harris could show that even children of 24 months pay attention to the reliability of the testifier. When they see two people, one of which systematically misnames known objects (e.g., saying \"that's a bear\", while presenting a bottle), toddlers are less likely to trust later utterances by these unreliable speakers (when they name unfamiliar objects), and more likely to trust people who systematically gave objects their correct names (see e.g., <a title=\"Young children&rsquo;s selective trust in informants\" href=\"http://isites.harvard.edu/fs/docs/icb.topic783896.files/Young%20childrens%20selective%20trust%20in%20informants.pdf\">Paul L. Harris and Kathleen H. Corriveau</a> Phil. Trans. R. Soc. B 2011 366, 1179-1187.) Experiments by Mills and Keil show that 6-year-olds already take into account a testifier's self-interest: they are more likely to believe someone who says he lost a race than someone who says he won it (<a title=\"The Development of Cynicism\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3034135/\">Candice M. Mills and Frank C. Keil </a>Psychological Science 2005 16: 385).</p>\n</blockquote>\n<p>See also:</p>\n<ul>\n<li>\"<a href=\"http://www.lesswrong.com/lw/7o4/atheism_autism_spectrum\">Atheism &amp; autism spectrum</a>\"</li>\n<li>\n<p><a href=\"/lw/7rh/cognitive_style_tends_to_predict_religious\">Cognitive Style Tends To Predict Religious Conviction<br /></a></p>\n</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DuM5d7stfbm4KksM2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 16, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "13902", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RNSYDitY6688rP3Fb", "T9akLQRr2rwy8tjM3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-11T23:02:15.081Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne social meetup", "slug": "meetup-melbourne-social-meetup-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:23.453Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shokwave", "createdAt": "2010-10-12T12:55:00.568Z", "isAdmin": false, "displayName": "shokwave"}, "userId": "jtjgXtj7FepKrQPGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2wcRtxaEQv6vjfrZP/meetup-melbourne-social-meetup-3", "pageUrlRelative": "/posts/2wcRtxaEQv6vjfrZP/meetup-melbourne-social-meetup-3", "linkUrl": "https://www.lesswrong.com/posts/2wcRtxaEQv6vjfrZP/meetup-melbourne-social-meetup-3", "postedAtFormatted": "Sunday, March 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20social%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20social%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2wcRtxaEQv6vjfrZP%2Fmeetup-melbourne-social-meetup-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20social%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2wcRtxaEQv6vjfrZP%2Fmeetup-melbourne-social-meetup-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2wcRtxaEQv6vjfrZP%2Fmeetup-melbourne-social-meetup-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 140, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/7w'>Melbourne social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 March 2012 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">see mailing list; Carlton, VIC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's next social meetup is on the 16th of March - as usual, at Ben's house. <a href=\"http://groups.google.com/group/melbourne-less-wrong?hl=en\" rel=\"nofollow\">See the mailing list</a> for the address - or you can call or text 0432 862 932 for the address. Alternatively, you can email shokwave.sf@gmail.com or even message User:shokwave on LessWrong.</p>\n\n<p>Some form of takeaway - probably Indian - will be organised on the night. There will be snacks available. Some of them might even be healthy! BYO drinks.</p>\n\n<p>The mafia games seem to have been the highlight of recent social events, so we're going to try starting those earlier in the night and running more of them.</p>\n\n<p>Please comment if you're planning on attending. We wholeheartedly welcome newcomers!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/7w'>Melbourne social meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2wcRtxaEQv6vjfrZP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.639580952246369e-07, "legacy": true, "legacyId": "13906", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup\">Discussion article for the meetup : <a href=\"/meetups/7w\">Melbourne social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 March 2012 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">see mailing list; Carlton, VIC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's next social meetup is on the 16th of March - as usual, at Ben's house. <a href=\"http://groups.google.com/group/melbourne-less-wrong?hl=en\" rel=\"nofollow\">See the mailing list</a> for the address - or you can call or text 0432 862 932 for the address. Alternatively, you can email shokwave.sf@gmail.com or even message User:shokwave on LessWrong.</p>\n\n<p>Some form of takeaway - probably Indian - will be organised on the night. There will be snacks available. Some of them might even be healthy! BYO drinks.</p>\n\n<p>The mafia games seem to have been the highlight of recent social events, so we're going to try starting those earlier in the night and running more of them.</p>\n\n<p>Please comment if you're planning on attending. We wholeheartedly welcome newcomers!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup1\">Discussion article for the meetup : <a href=\"/meetups/7w\">Melbourne social meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-12T03:59:08.854Z", "modifiedAt": null, "url": null, "title": "Falsification", "slug": "falsification", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:22.657Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TheatreAddict", "createdAt": "2011-05-01T17:03:11.634Z", "isAdmin": false, "displayName": "TheatreAddict"}, "userId": "xtL2cZS7kaFnSNLus", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tm8DP6qwMv4FaasMY/falsification", "pageUrlRelative": "/posts/tm8DP6qwMv4FaasMY/falsification", "linkUrl": "https://www.lesswrong.com/posts/tm8DP6qwMv4FaasMY/falsification", "postedAtFormatted": "Monday, March 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Falsification&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFalsification%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftm8DP6qwMv4FaasMY%2Ffalsification%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Falsification%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftm8DP6qwMv4FaasMY%2Ffalsification", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftm8DP6qwMv4FaasMY%2Ffalsification", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<p style=\"text-align: left;\"><span style=\"white-space: pre;\"> </span>Alright, so this is going to sound a bit silly. I'm fairly sure I've read this on the Sequences somewhere, but for the life of me I can't find it. A friend of mine insists that there is a fifty-fifty chance that we live in the Matrix. His argument is that every bit of evidence we have to say that we exist outside of the Matrix is already based off of the idea that we live outside of the Matrix, and that we really have no evidence either way. He says there isn't a way of falsifying that we're not in the Matrix.</p>\n<p style=\"text-align: left;\"><span style=\"white-space: pre;\"> </span>Yet I feel like he's wrong, and just can't explain why. I keep repeating that we don't have any evidence to suggest that we live in the Matrix, so why would we bother believing it?&nbsp;</p>\n<p style=\"text-align: left;\"><span style=\"white-space: pre;\"> </span>I feel like this could possibly be an analogy for the belief in God or something. &gt;_&gt; I'm tired, and I need help figuring this out.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tm8DP6qwMv4FaasMY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 3, "extendedScore": null, "score": 8.640785172303589e-07, "legacy": true, "legacyId": "13924", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-12T17:03:42.005Z", "modifiedAt": null, "url": null, "title": "Meetup : Austin, TX", "slug": "meetup-austin-tx-3", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3tyLh8vCcFyuhqrCY/meetup-austin-tx-3", "pageUrlRelative": "/posts/3tyLh8vCcFyuhqrCY/meetup-austin-tx-3", "linkUrl": "https://www.lesswrong.com/posts/3tyLh8vCcFyuhqrCY/meetup-austin-tx-3", "postedAtFormatted": "Monday, March 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Austin%2C%20TX&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Austin%2C%20TX%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3tyLh8vCcFyuhqrCY%2Fmeetup-austin-tx-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Austin%2C%20TX%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3tyLh8vCcFyuhqrCY%2Fmeetup-austin-tx-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3tyLh8vCcFyuhqrCY%2Fmeetup-austin-tx-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 75, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/7x'>Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 March 2012 01:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78712</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Note that the Austin, TX meetup is <em>not</em> meeting March 17th; many of us will be out of town.</p>\n\n<p>We'll be back at Caffe Medici on the 24th, and we sit on the second floor to the left, near (or often on) the stage. Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/7x'>Austin, TX</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3tyLh8vCcFyuhqrCY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.643968771693397e-07, "legacy": true, "legacyId": "13941", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Austin__TX\">Discussion article for the meetup : <a href=\"/meetups/7x\">Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 March 2012 01:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78712</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Note that the Austin, TX meetup is <em>not</em> meeting March 17th; many of us will be out of town.</p>\n\n<p>We'll be back at Caffe Medici on the 24th, and we sit on the second floor to the left, near (or often on) the stage. Hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Austin__TX1\">Discussion article for the meetup : <a href=\"/meetups/7x\">Austin, TX</a></h2>", "sections": [{"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX", "level": 1}, {"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-12T18:31:06.963Z", "modifiedAt": null, "url": null, "title": "The Stable State is Broken", "slug": "the-stable-state-is-broken", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:26.306Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Bakkot", "createdAt": "2011-07-19T23:25:37.940Z", "isAdmin": false, "displayName": "Bakkot"}, "userId": "uKHibS5YiPZge9nuf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zsoaDq3BMmmnHR3He/the-stable-state-is-broken", "pageUrlRelative": "/posts/zsoaDq3BMmmnHR3He/the-stable-state-is-broken", "linkUrl": "https://www.lesswrong.com/posts/zsoaDq3BMmmnHR3He/the-stable-state-is-broken", "postedAtFormatted": "Monday, March 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Stable%20State%20is%20Broken&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Stable%20State%20is%20Broken%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzsoaDq3BMmmnHR3He%2Fthe-stable-state-is-broken%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Stable%20State%20is%20Broken%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzsoaDq3BMmmnHR3He%2Fthe-stable-state-is-broken", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzsoaDq3BMmmnHR3He%2Fthe-stable-state-is-broken", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1017, "htmlBody": "<p>or:&nbsp;<em>Why Everything Is Terrible, An Overview.</em><sup>1</sup></p>\n<p>&nbsp;</p>\n<p>It sounds like a theory which&nbsp;<a href=\"/lw/if/your_strength_as_a_rationalist/\">explains too much</a>. But it's not a theory, hardly even an explanation, more a pattern that manifests itself once you start trying to seriously answer rhetorical questions about the state of the world. From many perspectives, it's obvious to the point of being mundane, practically tautological, but sometimes such obvious facts are worth pointing out regardless.</p>\n<p>The idea is this: <strong>The subset of&nbsp;participants&nbsp;which rises to&nbsp;prominence&nbsp;in any area does so because its members have traits helpful to <em>becoming&nbsp;prominent</em>, not necessarily because they have traits which are desirable. </strong>Thus, without ongoing and concerted effort, a great many arenas end up dominated by players employing strategies which are bad for everyone.</p>\n<p>&nbsp;</p>\n<p>This comes up again and again:</p>\n<ul>\n<li>Why does science (or rather, the publisher-based model thereof) so frequently produce results which are&nbsp;<a href=\"/lw/1ib/parapsychology_the_control_group_for_science/\">laughably wrong</a>? Because those journals which don't publish retractions or reproductions will more frequently be the first to publish revolutionary results, and so become more widely read and widely cited. Journals don't attract authors by being as accurate as possible; they win by looking important.</li>\n<li>Why do cigarette companies target kids and teens whenever they think they can get away with it, and breed tobacco for maximized nicotine? Because those companies which do will turn more profit and thus last longer and grow faster than those that don't, and so have more resources to devote to proliferating. Companies don't expand by playing fair; they win by making and keeping customers.</li>\n<li>Why is the Make-A-Wish Foundation sitting on more donations than it knows what to do with when the <a href=\"http://givewell.org/international/top-charities/AMF\">Against Malaria Foundation</a> could have used that money to save literally tens or hundreds of thousands of lives per year? Because knowing how to elicit donations is a skill almost completely unrelated to knowing how to spend donations, and because American children with cancer make for better advertising than African children with malaria. Charities don't get donations by making the best possible use of their money; they win by advertising effectively towards potential donors. (cf. <a href=\"/lw/37f/efficient_charity/\">Efficient Charity</a>)</li>\n<li>Why do governments inevitably end up run by career lawyers and politicians instead of scientists and economists<span style=\"font-size: 11px;\"><sup>2</sup></span>?&nbsp;Because polarizing rhetoric and political connections&nbsp;<em>look better</em>&nbsp;than a nuanced, accurate understanding of the issues. There is only finite time for training and practice, and eventually a choice must be made between training in looking good and training in&nbsp;<em>being</em>&nbsp;good. People don't get elected or appointed by being good Bayesians; they win by&nbsp;<a href=\"http://en.wikipedia.org/wiki/Jimmy_Carter\">being popular</a>.</li>\n<li>Why do the big media channels seem to be more concerned with celebrities than science, and spend more time talking about <a href=\"http://en.wikipedia.org/wiki/Scott_Peterson\">individual murders</a> than they do <a href=\"http://en.wikipedia.org/wiki/Persecution_of_Falun_Gong\">entire genocides</a>? Because those channels talking about Laci Peterson seem more personal and are thus more watched than those talking about some religious sect in China. Television programming isn't determined by what's important; what wins is what's watched.</li>\n<li>Why is the sex ratio in animals almost always nearly 1:1, when a population with one male for every five females could grow faster and adapt to problems more readily? Because in such a population, or in any population with a sufficiently large gender imbalance, a gene causing a woman to only have male children will be vastly overrepresented in the grandchild generation relative to the rest of the population, and so shift the balance closer to 50/50. Genes don't proliferate by being good for the species; they win by being good for themselves. (cf. <a href=\"http://en.wikipedia.org/wiki/Evolutionarily_stable_strategy\">Evolutionarily stable strategy</a>, <a href=\"http://en.wikipedia.org/wiki/Evolutionary_game_theory\">evolutionary game theory</a>.)</li>\n<li>Why do most big businesses make use of sweatshop conditions and shady tax dodges? Because the businesses which do so will outperform the businesses which don't. Corporations don't grow by being nice; they win by being profitable.</li>\n<li>Why do so many apparently intelligent people spend hours per day idly browsing the likes of Reddit, Hacker News, or TVTropes (or indeed LW), when a similar dedication to active self-improvement could have made them a master of a field inside of a decade? (Using for back-of-the-envelope's sake the supposition that 10,000 hours of practice are required for mastery of some specific art, we find that three hours per day for ten years is approximately 1.1 masteries.) Because which&nbsp;activities&nbsp;become habitual is determined by their immediate dopamine release, and for intelligent people the act of (say) <a href=\"http://news.ycombinator.com/news\">reading about strategies for becoming an effective&nbsp;entrepreneur</a>&nbsp;makes for more instant dopamine than does the painful daily grind involved in <em>actually</em>&nbsp;becoming an entrepreneur.&nbsp;Activities&nbsp;don't become part of daily life by being useful; they win by tricking your brain into making them feel good.</li>\n</ul>\n<div><br /></div>\n<div>It's extremely important to&nbsp;remember&nbsp;that none of this requires active malice, not even foresight or awareness of the strategy utilized. If someone or something&nbsp;<em>happens</em>&nbsp;upon a strategy like those described above, it will outperform its peers and become more widespread. This requires no conspiracy, no evil forces at work in the world, not even any individual shifting in their personal stance; these are just the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Evolutionarily_stable_strategy\">stable strategies</a> &nbsp;towards which the set of&nbsp;<em>surviving</em>&nbsp;players eventually converges.&nbsp;</div>\n<div>The next question: What can we do about it?</div>\n<div><br /></div>\n<div style=\"font-size:80%\"><sup>1</sup>I have distinct recollections of having read an article much like the one I've written here at some point in the past. However, I can't find said article, so at the least we can let this article serve as a refresher or another viewpoint on the matter. (ETA:&nbsp;<a href=\"/r/discussion/lw/ara/the_stable_state_is_broken/6077\">evgenit</a>&nbsp;and&nbsp;<a href=\"/r/discussion/lw/ara/the_stable_state_is_broken/606n\">gwern</a>&nbsp;both point out that the article I'm thinking of is Scott Aaronson's&nbsp;<a href=\"http://www.scottaaronson.com/blog/?p=418\">Malthusianisms</a>. [Aaronson refers to these states as Nash equilibria, which is not strictly correct; there's no underlying assumption about the rationality of the players here. You don't need <em>intelligent</em>&nbsp;participants for selection to operate. This is more a quibble over terminology than anything.])</div>\n<div style=\"font-size:80%\"><sup>2</sup>Until&nbsp;<a href=\"http://www.forbes.com/2009/05/17/china-leaders-stars-leadership-rising-stars.html\">recently</a>,&nbsp;China stood as a notable exception; now it appears that the next generations of leaders will have built their entire careers on shilling the party line.</div>\n<div style=\"font-size:80%\"><sup>3</sup>Tangentially related reading material:&nbsp;<a href=\"http://www.schneier.com/\">Bruce Schneier</a>'s&nbsp;<a href=\"http://www.amazon.com/Liars-Outliers-Enabling-Society-Thrive/dp/1118143302/\">new book</a>.</div>\n<div style=\"font-size:80%\"><sup style=\"font-size: 80%;\">4</sup><span style=\"font-size: 80%;\">No, neither footnote 3 nor this footnote actually have&nbsp;</span><span style=\"font-size: xx-small;\">corresponding</span><span style=\"font-size: 80%;\">&nbsp;backreferences.</span></div>\n<div style=\"font-size:80%\"><span style=\"font-size: 80%;\"><sup>5</sup>I wasn't entire sure which section of the site this would be best suited for. Hopefully this is appropriate. ETA: Also, as this is my first submission outside of comment threads, any feedback is highly appreciated.</span></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"o9aQASibdsECTfYF6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zsoaDq3BMmmnHR3He", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 66, "baseScore": 80, "extendedScore": null, "score": 0.000176, "legacy": true, "legacyId": "13942", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 80, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5JDkW4MYXit2CquLs", "enuGsZoFLR4KyEx3n", "FCxHgPsDScx4C3H8n"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-12T19:03:27.935Z", "modifiedAt": null, "url": null, "title": "Real-life expected utility maximization [response to XiXiDu]", "slug": "real-life-expected-utility-maximization-response-to-xixidu", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:22.787Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "cJPpT5b3RxNzjTchd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7rcbgbKARwPSspu4T/real-life-expected-utility-maximization-response-to-xixidu", "pageUrlRelative": "/posts/7rcbgbKARwPSspu4T/real-life-expected-utility-maximization-response-to-xixidu", "linkUrl": "https://www.lesswrong.com/posts/7rcbgbKARwPSspu4T/real-life-expected-utility-maximization-response-to-xixidu", "postedAtFormatted": "Monday, March 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Real-life%20expected%20utility%20maximization%20%5Bresponse%20to%20XiXiDu%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReal-life%20expected%20utility%20maximization%20%5Bresponse%20to%20XiXiDu%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7rcbgbKARwPSspu4T%2Freal-life-expected-utility-maximization-response-to-xixidu%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Real-life%20expected%20utility%20maximization%20%5Bresponse%20to%20XiXiDu%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7rcbgbKARwPSspu4T%2Freal-life-expected-utility-maximization-response-to-xixidu", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7rcbgbKARwPSspu4T%2Freal-life-expected-utility-maximization-response-to-xixidu", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 766, "htmlBody": "<p>This was supposed to be a comment under XiXiDu's recent <a href=\"/r/discussion/lw/aox/how_does_real_world_expected_utility_maximization/\"> post</a>&nbsp;but it got a bit unwieldy so I'm posting it top-level.</p>\n<p class=\"p1\">XiXiDu starts his post with:</p>\n<blockquote>\n<p class=\"p1\">I would like to ask for help on how to use expected utility maximization, in practice, to maximally achieve my goals.</p>\n</blockquote>\n<p class=\"p1\">I think the best single-sentence answer is: don't.</p>\n<p class=\"p1\">The usual way of making decisions is to come up with intuitive evaluations of various options and go with the one that feels most attractive. Sometimes you will feel (intuitively) that it would be good to spend some more time thinking about decision. So you'll put your initial intuitions into words (which are chosen by another intuitive black-box), come up with a causal models of your situation (generated by yet another intuitive module), then experience intuitive feelings about those thoughts, maybe come up with alternative thoughts and compare them (intuitively) or maybe turn those feelings-about-thoughts into second-order thoughts and continue the process until you run out of time, get bored (intuitively) or deliberatively decide that you've analyzed enough (by having run another progression of interweaving thoughts and intuitions in parallel to the first one).</p>\n<p class=\"p1\">In a sense, all thinking is intuition. You don't get to jump out of the system. There's no choice between using intuition and using some kind of completely different process called deliberative reasoning but rather a choice between using a small amount of object-level intuition vs lots of intuition turned upon itself.</p>\n<p class=\"p1\">That doesn't mean that we can't improve our thinking processes. Just that we do it by gaining knowledge and experience which then shape our intuitive thinking and not by somehow fundamentally altering their nature. An engineer and a composer both rely on intuition but it's the engineer that will succeed in building an internal combustion engine and the composer that will succeed in designing an aesthetically pleasing progression of sounds.</p>\n<p class=\"p1\">Mathematics is often pointed to as the foremost example of strict, logical thinking. Yet, mathematicians rely on intuition too. Mathematical proofs are considered trustworthy because the rules of proof formation are sufficiently simple that humans can train themselves to reliably distinguish proofs from non-proofs. A mathematician looks at a line in a proof and asks herself 'is that a correct application of logical inference rules?' She either spots a violation or gets a feeling that it's in fact correct. There's a very high chance she got it right but no mystical state of pure logic that guarantees it. And of course, while proofs have to obey formal rules, the only rule for how you're supposed to think when trying to come up with one is 'anything goes'.</p>\n<p class=\"p1\">So how do you use the principle of expected utility maximization to maximally achieve your goals?&nbsp;</p>\n<p class=\"p1\">Sometimes, in very specific circumstances, you can use it directly but that doesn't mean you turn into an idealized expected utility maximizer. You are applying your domain-specific skill of mathematics to a specific formalism. Which seems useful to you because earlier you used your domain-specific skill of seeing useful connections between reality and mathematical models.</p>\n<p class=\"p1\">Or you can ignore it completely and focus on more practical-sounding advice based on the long list of biases catalogued by science. For example, you can learn the rule 'If I want to believe that someone has some persistent trait based on a single observation, that's highly suspicious (fundamental attribution error). Doubly so if that belief would make me feel smug.' It seems that this has nothing to do with any idealized formalism. But to declare something a bias you need some standard against which you can compare observed behavior. If people thought that it's pointless to come up with idealized models of correct belief formation or decision making because we can never completely avoid intuition, then they might not have bothered with researching cognitive biases. So in a way, expected utility maximization (or Bayesian induction) is a prerequisite idea to all those practically applicable results.</p>\n<p class=\"p1\">And in general, the more complete your knowledge of a body of ideas, the better you can apply them in real life. So knowing the general principle that binds the more practically-oriented facts together can be helpful in ways that depends on the specific way you look at the world and think about things. This is, once again, the skill of seeing useful connections between mathematical models and reality. If you happen to identify a specific way in which your actions deviate from the model of expected utility maximization, fix it. If you don't, there's no point in worrying that you're not doing it right just because you can't account for all that goes on in your head in formal terms.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7rcbgbKARwPSspu4T", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 12, "extendedScore": null, "score": 8.644454945561054e-07, "legacy": true, "legacyId": "13944", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["J8ojaGJozdt9hnoxE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-12T21:12:07.369Z", "modifiedAt": null, "url": null, "title": "Is intelligence explosion necessary for doomsday?", "slug": "is-intelligence-explosion-necessary-for-doomsday", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:26.924Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmy", "createdAt": "2009-02-28T00:36:34.416Z", "isAdmin": false, "displayName": "Swimmy"}, "userId": "pNdunthLRqh3unTry", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5q5GKyrPqgMBcEDNH/is-intelligence-explosion-necessary-for-doomsday", "pageUrlRelative": "/posts/5q5GKyrPqgMBcEDNH/is-intelligence-explosion-necessary-for-doomsday", "linkUrl": "https://www.lesswrong.com/posts/5q5GKyrPqgMBcEDNH/is-intelligence-explosion-necessary-for-doomsday", "postedAtFormatted": "Monday, March 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20intelligence%20explosion%20necessary%20for%20doomsday%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20intelligence%20explosion%20necessary%20for%20doomsday%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5q5GKyrPqgMBcEDNH%2Fis-intelligence-explosion-necessary-for-doomsday%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20intelligence%20explosion%20necessary%20for%20doomsday%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5q5GKyrPqgMBcEDNH%2Fis-intelligence-explosion-necessary-for-doomsday", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5q5GKyrPqgMBcEDNH%2Fis-intelligence-explosion-necessary-for-doomsday", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 169, "htmlBody": "<p>I searched for articles on the topic and couldn't find any.</p>\n<p>It seems to me that intelligence explosion makes human annihilation much more likely, since superintelligences will certainly be able to outwit humans, but that a human-level intelligence that could process information much faster than humans would certainly be a large threat itself without any upgrading. It could still discover programmable nanomachines long before humans do, gather enough information to predict how humans will act, etc. We already know that a human-level intelligence can \"escape from the box.\" Not 100% of the time, but a real AI will have the opportunity for many more trials, and its processing abilities should make it far more quick-witted than we are.</p>\n<p>I think a non-friendly AI would only need to be 20 years or so more advanced than the rest of humanity to pose a major threat, especially if self-replicating nanomachines are possible. Skeptics of intelligence explosion should still be worried about the creation of computers with unfriendly goal systems. What am I missing?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5q5GKyrPqgMBcEDNH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 8.644977268425128e-07, "legacy": true, "legacyId": "13945", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-12T21:20:31.364Z", "modifiedAt": null, "url": null, "title": "TED-Ed Launch", "slug": "ted-ed-launch", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Lightwave", "createdAt": "2009-03-02T00:10:45.771Z", "isAdmin": false, "displayName": "Lightwave"}, "userId": "wmf7PMjRsYvMAcQHg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zdHXJGBxDjo5NaR7Q/ted-ed-launch", "pageUrlRelative": "/posts/zdHXJGBxDjo5NaR7Q/ted-ed-launch", "linkUrl": "https://www.lesswrong.com/posts/zdHXJGBxDjo5NaR7Q/ted-ed-launch", "postedAtFormatted": "Monday, March 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20TED-Ed%20Launch&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATED-Ed%20Launch%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzdHXJGBxDjo5NaR7Q%2Fted-ed-launch%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=TED-Ed%20Launch%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzdHXJGBxDjo5NaR7Q%2Fted-ed-launch", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzdHXJGBxDjo5NaR7Q%2Fted-ed-launch", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<p>TED has launched a new initiative: TED Education</p>\n<blockquote>\n<p>TED-Ed's mission is to capture and amplify the voices of great educators  around the world. We do this by pairing extraordinary educators with  talented animators to produce a new library of curiosity-igniting  videos.</p>\n</blockquote>\n<p><a href=\"http://education.ted.com/\" target=\"_blank\">http://education.ted.com/</a></p>\n<p>So maybe we could get someone to do a short talk/lecture on rationality related topic(s) (maybe cognitive biases?)? Or anything else that is related to raising the sanity waterline. Or at least just <a href=\"http://education.ted.com/lesson/\" target=\"_blank\">suggest the idea</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zdHXJGBxDjo5NaR7Q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 8.64501137230467e-07, "legacy": true, "legacyId": "13946", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-13T00:03:40.496Z", "modifiedAt": null, "url": null, "title": "LINK: Can intelligence explode?", "slug": "link-can-intelligence-explode", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:21.114Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jake987722", "createdAt": "2009-11-15T04:00:32.147Z", "isAdmin": false, "displayName": "jake987722"}, "userId": "z2eomSrHzecedEdNt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oswPhoM4YfZye9zrq/link-can-intelligence-explode", "pageUrlRelative": "/posts/oswPhoM4YfZye9zrq/link-can-intelligence-explode", "linkUrl": "https://www.lesswrong.com/posts/oswPhoM4YfZye9zrq/link-can-intelligence-explode", "postedAtFormatted": "Tuesday, March 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LINK%3A%20Can%20intelligence%20explode%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALINK%3A%20Can%20intelligence%20explode%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoswPhoM4YfZye9zrq%2Flink-can-intelligence-explode%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LINK%3A%20Can%20intelligence%20explode%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoswPhoM4YfZye9zrq%2Flink-can-intelligence-explode", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoswPhoM4YfZye9zrq%2Flink-can-intelligence-explode", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 209, "htmlBody": "<p>I thought many of you would be interested to know that the following paper just appeared in <em>Journal of Consciousness Studies:</em></p>\n<p>\"Can Intelligence Explode?\", by Marcus Hutter. (<a href=\"http://www.hutter1.net/publ/singularity.pdf\">LINK HERE</a>)</p>\n<p>Abstract:&nbsp;The technological singularity refers to a hypothetical scenario in which technological advances virtually explode. The most popular scenario is the creation of super-intelligent algorithms that recursively create ever higher intelligences. It took many decades for these ideas to spread from science fiction to popular science magazines and finally to attract the attention of serious philosophers. David Chalmers' (JCS 2010) article is the first comprehensive philosophical analysis of the singularity in a respected philosophy journal. The motivation of my article is to augment Chalmers' and to discuss some issues not addressed by him, in particular what it could mean for intelligence to explode. In this course, I will (have to) provide a more careful treatment of what intelligence actually is, separate speed from intelligence explosion, compare what super-intelligent participants and classical human observers might experience and do, discuss immediate implications for the diversity and value of life, consider possible bounds on intelligence, and contemplate intelligences right at the singularity.</p>\n<p>I have only just seen the paper and have not yet thread through it myself, but I thought we could use this thread for discussion.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oswPhoM4YfZye9zrq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -4, "extendedScore": null, "score": 8.645673822861008e-07, "legacy": true, "legacyId": "13947", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-13T01:44:51.788Z", "modifiedAt": null, "url": null, "title": "What is the best compact formalization of the argument for AI risk from fast takeoff?", "slug": "what-is-the-best-compact-formalization-of-the-argument-for", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:23.100Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "utilitymonster", "createdAt": "2010-02-01T16:26:24.298Z", "isAdmin": false, "displayName": "utilitymonster"}, "userId": "NrjMi7q93Dwak9gWg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AAG7vj5cyNZ2Gxtnk/what-is-the-best-compact-formalization-of-the-argument-for", "pageUrlRelative": "/posts/AAG7vj5cyNZ2Gxtnk/what-is-the-best-compact-formalization-of-the-argument-for", "linkUrl": "https://www.lesswrong.com/posts/AAG7vj5cyNZ2Gxtnk/what-is-the-best-compact-formalization-of-the-argument-for", "postedAtFormatted": "Tuesday, March 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20the%20best%20compact%20formalization%20of%20the%20argument%20for%20AI%20risk%20from%20fast%20takeoff%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20the%20best%20compact%20formalization%20of%20the%20argument%20for%20AI%20risk%20from%20fast%20takeoff%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAAG7vj5cyNZ2Gxtnk%2Fwhat-is-the-best-compact-formalization-of-the-argument-for%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20the%20best%20compact%20formalization%20of%20the%20argument%20for%20AI%20risk%20from%20fast%20takeoff%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAAG7vj5cyNZ2Gxtnk%2Fwhat-is-the-best-compact-formalization-of-the-argument-for", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAAG7vj5cyNZ2Gxtnk%2Fwhat-is-the-best-compact-formalization-of-the-argument-for", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 325, "htmlBody": "<p>Many people complain that the Singularity Institute's \"<a href=\"http://multiverseaccordingtoben.blogspot.com/2010/10/singularity-institutes-scary-idea-and.html\">Big Scary Idea</a>\" (AGI leads to catastrophe by default) has not been argued for with the clarity of, say, Chalmers' <a href=\"http://consc.net/papers/singularityjcs.pdf\">argument for the singularity</a>. The idea would be to make explicit what the premise-and-inference structure of the argument is, and then argue about the strength of those premises and inferences.</p>\n<p>Here is one way you could construe one version of the argument for the Singularity Institute's \"Big Scary Idea\":</p>\n<ol>\n<li>At some point in the development of AI, there will be a very swift increase in the optimization power of the most powerful AI, moving from a non-dangerous level to a level of superintelligence. (Fast takeoff)</li>\n<li>This AI will maximize a goal function.</li>\n<li>Given fast takeoff and maximizing a goal function, the superintelligent AI will have a decisive advantage unless adequate controls are used.</li>\n<li>Adequate controls will not be used. (E.g. Won&rsquo;t box/boxing won&rsquo;t work)</li>\n<li>Therefore, the superintelligent AI will have a decisive advantage</li>\n<li>Unless that AI is designed with goals that stably align with ours, if the superintelligent AI has a decisive advantage, civilization will be ruined. (Friendliness is necessary)</li>\n<li>Unless the first team that develops the superintelligent AI makes adequate preparations, the superintelligent AI will not have goals that stably align with ours.</li>\n<li>Therefore, unless the first team that develops the superintelligent AI makes adequate preparations, civilization will be ruined shortly after fast takeoff</li>\n<li>The first team that develops the superintelligent AI will fail to make adequate preparations</li>\n<li>Therefore, civilization will be ruined shortly after fast takeoff.</li>\n</ol>\n<div>Edit to add: premises should be read as assuming the truth of all above premises. E.g., (9) is assuming that we've created an artificial agent with a decisive advantage.</div>\n<p>My questions are:</p>\n<ul>\n<li>Have I made any errors in the argument structure?</li>\n<li>Can anyone suggest an alternative argument structure?</li>\n<li>Which of these premises seem the weakest to you?</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AAG7vj5cyNZ2Gxtnk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 23, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "13956", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-13T07:55:32.085Z", "modifiedAt": null, "url": null, "title": "[Link] Failed replications of the \"elderly walking\" priming effect", "slug": "link-failed-replications-of-the-elderly-walking-priming", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:21.808Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Cduq5QGzd8KYSZT2R/link-failed-replications-of-the-elderly-walking-priming", "pageUrlRelative": "/posts/Cduq5QGzd8KYSZT2R/link-failed-replications-of-the-elderly-walking-priming", "linkUrl": "https://www.lesswrong.com/posts/Cduq5QGzd8KYSZT2R/link-failed-replications-of-the-elderly-walking-priming", "postedAtFormatted": "Tuesday, March 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Failed%20replications%20of%20the%20%22elderly%20walking%22%20priming%20effect&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Failed%20replications%20of%20the%20%22elderly%20walking%22%20priming%20effect%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCduq5QGzd8KYSZT2R%2Flink-failed-replications-of-the-elderly-walking-priming%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Failed%20replications%20of%20the%20%22elderly%20walking%22%20priming%20effect%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCduq5QGzd8KYSZT2R%2Flink-failed-replications-of-the-elderly-walking-priming", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCduq5QGzd8KYSZT2R%2Flink-failed-replications-of-the-elderly-walking-priming", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 261, "htmlBody": "<blockquote>Recently a controversy broke out over the replicability of a <a href=\"http://www.yale.edu/acmelab/articles/bargh_chen_burrows_1996.pdf\">study</a> John Bargh et al. published in 1996. The study reported that unconsciously priming a stereotype of elderly people caused subjects to walk more slowly. A <a href=\"http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0029081\">recent replication attempt</a> by Stephane Doyen et al., published in PLoS ONE, was unable to reproduce the results. (Less publicized, but surely relevant, is <a href=\"http://psychfiledrawer.org/replication.php?attempt=MTU%3D\">another non-replication</a> by Hal Pashler et al.) (<a href=\"http://hardsci.wordpress.com/2012/03/12/some-reflections-on-the-bargh-doyen-elderly-walking-priming-brouhaha/\">source</a>)</blockquote>\n<p>&nbsp;</p>\n<p>This is interesting, if only because the study in question is one of the more famous examples of priming effects - it's the one I tend to use when I introduce people to the idea of priming. (Ironically, the failed replication study also mentions a further experimental manipulation that does show priming effects - affecting the experimenters rather than the subjects.)&nbsp;Bargh's reply is also unusual in that it focuses significantly on extra-scientific arguments, such as attacks on the open access business model of PLoS ONE.</p>\n<p>I was instantly reminded of <a href=\"http://www.amazon.com/dp/0521645506\">The Golem</a>, which \"debunks the view that scientific knowledge is a straightforward outcome of competent theorization, observation, and experimentation\". The examples on relativity and solar neutrinos are particularly engaging - it's not just psychology where experimentation is problematic, but all of science.</p>\n<p>The linked blog also contributes useful observations of its own, such as the \"rhetorical function\" of the additional experiment in Doyen's study, how online publication makes a difference in how easily experimental setups can be replicated, or a subtle point about our <a href=\"/lw/ajj/how_to_fix_science/\">favorite villain</a>, p-values.</p>\n<p>&nbsp;</p>\n<p style=\"font-size:80%\">EDIT: added link to source. Heartfelt thanks to the two readers who upvoted the version without the link. :)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vg4LDxjdwHLotCm8w": 2, "dBPou4ihoQNY4cquv": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Cduq5QGzd8KYSZT2R", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 31, "extendedScore": null, "score": 8.647590229170309e-07, "legacy": true, "legacyId": "13978", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ETe2SZacmLvvr8H9n"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-13T13:54:36.349Z", "modifiedAt": null, "url": null, "title": "Risks from AI and Charitable Giving", "slug": "risks-from-ai-and-charitable-giving", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:33.681Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZpXik8eWiYhzbTtQC/risks-from-ai-and-charitable-giving", "pageUrlRelative": "/posts/ZpXik8eWiYhzbTtQC/risks-from-ai-and-charitable-giving", "linkUrl": "https://www.lesswrong.com/posts/ZpXik8eWiYhzbTtQC/risks-from-ai-and-charitable-giving", "postedAtFormatted": "Tuesday, March 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Risks%20from%20AI%20and%20Charitable%20Giving&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARisks%20from%20AI%20and%20Charitable%20Giving%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZpXik8eWiYhzbTtQC%2Frisks-from-ai-and-charitable-giving%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Risks%20from%20AI%20and%20Charitable%20Giving%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZpXik8eWiYhzbTtQC%2Frisks-from-ai-and-charitable-giving", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZpXik8eWiYhzbTtQC%2Frisks-from-ai-and-charitable-giving", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4360, "htmlBody": "<blockquote>\n<p>If you&rsquo;re interested in being on the right side of disputes, you will refute your opponents' arguments. But if you're interested in producing truth, you will fix your opponents' arguments for them. To win, you must fight not only the creature you encounter; you [also] must fight the most horrible thing that can be constructed from its corpse.</p>\n</blockquote>\n<p>-- <a href=\"http://web.archive.org/web/20100328161823/http://www.acceleratingfuture.com/steven/?p=155\">Black Belt Bayesian</a></p>\n<p>This is an <em>informal</em> post meant as a reply to a post by user:utilitymonster, <em>'<a href=\"/r/discussion/lw/aro/what_is_the_best_compact_formalization_of_the/\">What is the best compact formalization of the argument for AI risk from fast takeoff?</a>'</em></p>\n<p>I hope to find the mental strength to put more effort into it in future to improve it. But since nobody else seems to be willing to take a critical look at the overall topic I feel that doing what I can is better than doing nothing.</p>\n<p><strong>Please review the categories <em>'Further Reading'</em> and <em>'Notes and References'</em>. </strong></p>\n<p><strong>Contents</strong></p>\n<table style=\"border: thin solid #808080; width: 440px; padding: 5px;\" border=\"0\">\n<tbody>\n<tr>\n<td>\n<ul>\n<li><a href=\"#abstract\">Abstract</a></li>\n<li><a href=\"#FOOM\">Requirements for an Intelligence Explosion</a></li>\n<li><a href=\"#CHARITY\">Requirements for SIAI to constitute an optimal charity</a></li>\n<li><a href=\"#FR\">Further Reading</a></li>\n<li><a href=\"#NR\">Notes and References</a> </li>\n</ul>\n</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<h3><a name=\"abstract\">Abstract</a><br /></h3>\n<p>In this post I just want to take a look at a few premises (P#) that <a href=\"/r/discussion/lw/8fa/intelligence_explosion_a_disjunctive_or/\">need to be true simultaneously</a> to make the SIAI a wortwhile charity from the point of view of someone trying to do as much good as possible by contributing money. I am going to show that the case of <a href=\"/lw/8fb/why_an_intelligence_explosion_might_be_a/\">risks from AI</a> is strongly conjunctive, that without a concrete and grounded understanding of AGI an abstract analysis of the issues is going to be very shaky, and that therefore SIAI is likely to be a bad choice as a charity. In other words, that which speaks in favor of SIAI does mainly consist of highly specific, conjunctive, non-evidence-backed speculations on possible bad outcomes.</p>\n<h3><a name=\"FOOM\">Requirements for an Intelligence Explosion</a></h3>\n<p><strong>P1</strong> Fast, and therefore dangerous, recursive self-improvement is <span style=\"text-decoration: underline;\">logically</span> possible.</p>\n<p style=\"padding-left: 30px;\">It took almost four hundred years to prove <a href=\"http://en.wikipedia.org/wiki/Fermat%27s_Last_Theorem\">Fermat&rsquo;s Last Theorem</a>. The <a href=\"http://en.wikipedia.org/wiki/Wiles%27_proof_of_Fermat%27s_Last_Theorem\">final proof</a> is over a hundred pages long. Over a hundred pages! And we are not talking about something like an artificial general intelligence that can magically make itself smart enough to prove such theorems and many more that no human being would be capable of proving. Fermat&rsquo;s Last Theorem simply states <em>&ldquo;no three positive integers a, b, and c can satisfy the equation a^n + b^n = c^n for any integer value of n greater than two.&rdquo;</em></p>\n<p style=\"padding-left: 30px;\">Even artificial intelligence researchers admit that <em>\"there could be non-linear complexity constrains meaning that even theoretically optimal algorithms experience strongly diminishing intelligence returns for additional compute power.\"</em> [1] We just don't know.</p>\n<p style=\"padding-left: 30px;\">Other possible problems include the impossibility of a stable utility function and a reflective decision theory, the intractability of real world expected utility maximization or that expected utility maximizers stumble over <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's mugging</a>, among other things [2].</p>\n<p style=\"padding-left: 30px;\">For an AI to be capable of recursive self-improvement it also has to guarantee that its goals will be preserved when it improves itself. It is still questionable if it is possible to conclusively prove that improvements to an agent's intelligence or decision procedures maximize expected utility. If this isn't possible it won't be rational or possible to undergo <em>explosive</em> self-improvement.</p>\n<p><strong>P1.b</strong> The fast computation of a simple algorithm is sufficient to outsmart and overpower humanity.</p>\n<p style=\"padding-left: 30px;\">Imagine a group of 100 world-renowned scientists and military strategists.</p>\n<ul>\n<li>The group is analogous to the initial resources of an AI. </li>\n<li>The knowledge that the group has is analogous to what an AI could come up with by simply \"thinking\" about it given its current resources. </li>\n</ul>\n<p style=\"padding-left: 30px;\">Could such a group easily wipe away the Roman empire when beamed back in time?</p>\n<ul>\n<li>The Roman empire is analogous to our society today. </li>\n</ul>\n<p style=\"padding-left: 30px;\">Even if you gave all of them a machine gun, the Romans would quickly adapt and the people from the future would run out of ammunition.</p>\n<ul>\n<li>Machine guns are analogous to the supercomputer it runs on.</li>\n</ul>\n<p style=\"padding-left: 30px;\">Consider that it takes a <a rel=\"nofollow\" href=\"http://www.antipope.org/charlie/blog-static/2010/07/insufficient-data.html\" target=\"_blank\">whole technological civilization</a> to produce a modern smartphone.</p>\n<p style=\"padding-left: 30px;\">You can't just say <em>\"with more processing power you can do more different things\"</em>, that would be analogous to saying that \"100 people\" from today could just build more \"machine guns\". But they can't! They can't use all their knowledge and magic from the future to defeat the Roman empire.</p>\n<p style=\"padding-left: 30px;\">A lot of assumptions have to turn out to be correct to make humans discover simple algorithms over night that can then be improved to self-improve explosively.</p>\n<p style=\"padding-left: 30px;\">You can also compare this to the idea of a Babylonian mathematician discovering modern science and physics given that he would be uploaded into a supercomputer (a possibility that is in and of itself already highly speculative). It assumes that he could brute-force conceptual revolutions.</p>\n<p style=\"padding-left: 30px;\">Even if he was given a detailed explanation of how his mind works and the resources to understand it, self-improving to achieve superhuman intelligence assumes that throwing resources at the problem of intelligence will magically allow him to pull improved algorithms from solution space as if they were signposted.</p>\n<p style=\"padding-left: 30px;\">But unknown unknowns are not signposted. It's rather like finding a needle in a haystack. Evolution is great at doing that and assuming that one could speed up evolution considerably is another assumption about technological feasibility and real-world resources.</p>\n<p style=\"padding-left: 30px;\">That conceptual revolutions are just a matter of computational resources is pure speculation.</p>\n<p style=\"padding-left: 30px;\">If one were to speed up the whole Babylonian world and accelerate cultural evolution, obviously one would arrive quicker at some insights. But how much quicker? How much are many insights dependent on experiments, to yield empirical evidence, that can't be speed-up considerably? And what is the return? Is the payoff proportionally to the resources that are necessary?</p>\n<p style=\"padding-left: 30px;\">If you were going to speed up a chimp brain a million times, would it quickly reach human-level intelligence? If not, why then would it be different for a human-level intelligence trying to reach transhuman intelligence? It seems like a nice idea when formulated in English, but would it work?</p>\n<p style=\"padding-left: 30px;\">Being able <em>to state</em> that an AI could use some magic to take over the earth does not make it a serious possibility.</p>\n<p style=\"padding-left: 30px;\">Magic has to be discovered, adapted and manufactured first. It doesn't just emerge out of nowhere from the computation of certain algorithms. It emerges from a society of agents with various different goals and heuristics like <em>\"Treating Rare Diseases in Cute Kittens\"</em>. It is an evolutionary process that relies on massive amounts of real-world feedback and empirical experimentation. Assuming that all that can happen because some simple algorithm is being computed is like believing it will emerge 'out of nowhere', it is magical thinking.</p>\n<p style=\"padding-left: 30px;\">Unknown unknowns are not sign-posted. [3]</p>\n<p style=\"padding-left: 30px;\">If people like Beno&icirc;t B. Mandelbrot would have never decided to research Fractals then many modern movies wouldn't be possible, as they rely on fractal landscape algorithms. Yet, at the time Beno&icirc;t B. Mandelbrot conducted his research it was not foreseeable that his work would have any real-world applications.</p>\n<p style=\"padding-left: 30px;\">Important discoveries are made because many routes with low or no expected utility are explored at the same time [4]. And to do so efficiently it takes random mutation, a whole society of minds, a lot of feedback and empirical experimentation.</p>\n<p style=\"padding-left: 30px;\"><em>\"Treating rare diseases in cute kittens\"</em> might or might not provide genuine insights and open up new avenues for further research. As long as you don't try it you won't know.</p>\n<p style=\"padding-left: 30px;\">The idea that a rigid consequentialist with simple values can think up insights and conceptual revolutions simply because it is instrumentally useful to do so is implausible.</p>\n<p style=\"padding-left: 30px;\">Complex values are the cornerstone of diversity, which in turn enables creativity and drives the exploration of various conflicting routes. A singleton with a stable utility-function lacks the feedback provided by a society of minds and its cultural evolution.</p>\n<p style=\"padding-left: 30px;\">You need to have various different agents with different utility-functions around to get the necessary diversity that can give rise to enough selection pressure. A \"singleton\" won't be able to predict the actions of new and improved versions of itself by just running sandboxed simulations. Not just because of logical uncertainty but also because it is computationally intractable to predict the real-world payoff of changes to its decision procedures.</p>\n<p style=\"padding-left: 30px;\">You need complex values to give rise to the necessary drives to function in a complex world. You can't just tell an AI to protect itself. What would that even mean? What changes are illegitimate? What constitutes <em>\"self\"</em>? That are all unsolved problems that are just assumed to be solvable when talking about risks from AI.</p>\n<p style=\"padding-left: 30px;\">An AI with simple values will simply lack the creativity, due to a lack of drives, to pursue the huge spectrum of research that a society of humans does pursue. Which will allow an AI to solve some well-defined narrow problems, but it will be unable to make use of the broad range of synergetic effects of cultural evolution. Cultural evolution is a result of the interaction of a wide range of utility-functions.</p>\n<p style=\"padding-left: 30px;\">Yet even if we assume that there is one complete theory of general intelligence, once discovered, one just has to throw more resources at it. It might be able to incorporate all human knowledge, adapt it and find new patterns. But would it really be vastly superior to human society and their expert systems?</p>\n<p style=\"padding-left: 30px;\">Can intelligence itself be improved apart from solving well-defined problems and making more accurate predictions on well-defined classes of problems? The discovery of unknown unknowns does not seem to be subject to other heuristics than natural selection. Without goals, well-defined goals, terms like \"optimization\" have no meaning.</p>\n<p><strong>P2</strong> Fast, and therefore dangerous, recursive self-improvement is <span style=\"text-decoration: underline;\">physically</span> possible.</p>\n<p style=\"padding-left: 30px;\">Even if it could be proven that <em>explosive</em> recursive self-improvement is logically possible, e.g. that there are no complexity constraints, the question remains if it is physically possible.</p>\n<p style=\"padding-left: 30px;\">Our best theories about intelligence are highly abstract and their relation to real world human-level general intelligence is often wildly speculative [5][6].</p>\n<p><strong>P3</strong> Fast, and therefore dangerous, recursive self-improvement is <span style=\"text-decoration: underline;\">economically</span> feasible.</p>\n<p style=\"padding-left: 30px;\">To exemplify the problem take the science fictional idea of using <a href=\"http://en.wikipedia.org/wiki/Antimatter\">antimatter</a> as explosive for weapons. It is physically possible to produce antimatter and use it for large scale destruction. An equivalent of the Hiroshima atomic bomb will only take half a gram of antimatter. But it will take 2 billion years to produce that amount of antimatter [7].</p>\n<p style=\"padding-left: 30px;\">We simply don&rsquo;t know if intelligence is instrumental or quickly hits diminishing returns [8].</p>\n<p><strong>P3.b</strong> AGI is able to create (or acquire) resources, empowering technologies or civilisatory support [9].</p>\n<p style=\"padding-left: 30px;\">We are already at a point where we have to build billion dollar chip manufacturing facilities to run our mobile phones. We need to build huge particle accelerators to obtain new insights into the nature of reality.</p>\n<p style=\"padding-left: 30px;\">An AI would either have to rely on the help of a whole technological civilization or be in control of advanced nanotech assemblers.</p>\n<p style=\"padding-left: 30px;\">And if an AI was to acquire the necessary resources on its own, its plan for world-domination would have to go unnoticed. This would require the workings of the AI to be opaque to its creators yet comprehensible to itself.</p>\n<p style=\"padding-left: 30px;\">But an AI capable of efficient recursive self improvement must be able to</p>\n<ol>\n<li>comprehend its own workings </li>\n<li>predict how improvements, respectively improved versions of itself, are going to act to ensure that its values are preserved</li>\n</ol>\n<p style=\"padding-left: 30px;\">So if the AI can do that, why wouldn't humans be able to use the same algorithms to predict what the initial AI is going to do? And if the AI can't do that, how is it going to maximize expected utility if it is unable to predict what it is going to do?</p>\n<p style=\"padding-left: 30px;\">Any AI capable of efficient self-modification must be able to grasp its own workings and make predictions about improvements to various algorithms and its overall decision procedure. If an AI can do that, why would the humans who build it be unable to notice any malicious intentions? Why wouldn't the humans who created it not be able to use the same algorithms that the AI uses to predict what it will do? If humans are unable to predict what the AI will do, how is the AI able to predict what improved versions of itself will do?</p>\n<p style=\"padding-left: 30px;\">And even if an AI was able to somehow acquire large amounts of money. It is not easy to <em>use</em> the money. You can't <em>\"just\"</em> build huge companies with fake identities, or a straw man, to create revolutionary technologies easily. Running companies with real people takes a lot of real-world knowledge, interactions and feedback. But most importantly, it takes a lot of time. An AI could not simply create a new Intel or Apple over a few years without its creators noticing anything.</p>\n<p style=\"padding-left: 30px;\">The goals of an AI will be under scrutiny at any time. It seems very implausible that scientists, a company or the military are going to create an AI and then just let it run without bothering about its plans. An artificial agent is not a black box, like humans are, where one is only able to guess its real intentions.</p>\n<p style=\"padding-left: 30px;\">A plan for world domination seems like something that can't be concealed from its creators. Lying is no option if your algorithms are open to inspection.</p>\n<p><strong>P4</strong> Dangerous recursive self-improvement is the default outcome of the creation of artificial general intelligence.</p>\n<p style=\"text-align: left; padding-left: 30px;\">Complex goals need complex optimization parameters (the design specifications of the subject of the optimization process against which it will measure its success of self-improvement).</p>\n<p style=\"text-align: left; padding-left: 30px;\">Even the creation of paperclips is a much more complex goal than telling an AI to compute as many decimal digits of Pi as possible.</p>\n<p style=\"text-align: left; padding-left: 30px;\">For an AGI, that was designed to design paperclips, to pose an existential risk, its creators would have to be capable enough to enable it to take over the universe on its own, yet forget, or fail to, define time, space and energy bounds as part of its optimization parameters. Therefore, given the large amount of restrictions that are inevitably part of any advanced general intelligence (AGI), the nonhazardous subset of all possible outcomes might be much larger than that where the AGI works perfectly yet fails to hold before it could wreak havoc.</p>\n<p style=\"text-align: left; padding-left: 30px;\">And even given a rational utility maximizer. It is possible to maximize paperclips in a lot of different ways. How it does it is fundamentally dependent on its utility-function and how precisely it was defined.</p>\n<p style=\"text-align: left; padding-left: 30px;\">If there are no constraints in the form of design and goal parameters then it can maximize paperclips in all sorts of ways that don't demand recursive self-improvement.</p>\n<p style=\"text-align: left; padding-left: 30px;\">\"Utility\" does only become well-defined if we precisely define what it means to maximize it. Just maximizing paperclips doesn't define how quickly and how economically it is supposed to happen.</p>\n<p style=\"padding-left: 30px;\">The problem is that \"utility\" has to be defined. To maximize expected utility does not imply certain actions, efficiency and economic behavior, or the drive to protect yourself. You can also rationally maximize paperclips without protecting yourself if it is not part of your goal parameters.</p>\n<p style=\"padding-left: 30px;\">You can also assign utility to maximize paperclips as long as nothing turns you off but don't care about being turned off. If an AI is not explicitly programmed to care about it, then it won't.</p>\n<p style=\"padding-left: 30px;\">Without well-defined goals in form of a precise utility-function, it might be impossible to maximize expected \"utility\". Concepts like \"efficient\", \"economic\" or \"self-protection\" all have a meaning that is inseparable with an agent's terminal goals. If you just tell it to maximize paperclips then this can be realized in an infinite number of ways that would all be rational given imprecise design and goal parameters. Undergoing to explosive recursive self-improvement, taking over the universe and filling it with paperclips, is just one outcome. Why would an arbitrary mind pulled from mind-design space care to do that? Why not just wait for paperclips to arise due to random fluctuations out of a state of chaos? That wouldn't be irrational. To have an AI take over the universe as fast as possible you would have to explicitly design it to do so.</p>\n<p style=\"padding-left: 30px;\">But for the sake of a thought experiment assume that the default case was recursive self-improvement. Now imagine that a company like Apple wanted to build an AI that could answer every question (an Oracle).</p>\n<p style=\"padding-left: 30px;\">If Apple was going to build an Oracle it would anticipate that other people would also want to ask it questions. Therefore it can't just waste all resources on looking for an inconsistency arising from the Peano axioms when asked to solve 1+1. It would not devote additional resources on answering those questions that are already known to be correct with a high probability. It wouldn't be <em>economically useful</em> to take over the universe to answer simple questions.</p>\n<p style=\"padding-left: 30px;\">It would neither be <em>rational</em> to look for an inconsistency arising from the Peano axioms while solving 1+1. To answer questions an Oracle needs a good amount of general intelligence. And concluding that asking it to solve 1+1 implies to look for an inconsistency arising from the Peano axioms does not seem reasonable. It also does not seem reasonable to suspect that humans desire an answer to their questions to approach infinite certainty. Why would someone build such an Oracle in the first place?</p>\n<p style=\"padding-left: 30px;\">A reasonable Oracle would quickly yield good solutions by trying to find answers within a reasonable time which are with a high probability just 2&ndash;3% away from the optimal solution. I don't think anyone would build an answering machine that throws the whole universe at the first sub-problem it encounters.</p>\n<p><strong>P5</strong> The human development of artificial general intelligence will take place quickly.</p>\n<p style=\"padding-left: 30px;\">What evidence do we have that there is some principle that, once discovered, allows us to grow superhuman intelligence overnight?</p>\n<p style=\"padding-left: 30px;\">If the development of AGI takes place slowly, a gradual and controllable development, we might be able to learn from small-scale mistakes, or have enough time to develop friendly AI, while having to face other existential risks.</p>\n<p style=\"padding-left: 30px;\">This might for example be the case if intelligence can not be captured by a discrete algorithm, or is modular, and therefore never allow us to reach a point where we can suddenly build the smartest thing ever that does just extend itself indefinitely.</p>\n<p>Therefore the probability of an AI to undergo explosive recursive self-improvement (P(FOOM)) is the probability of the <a href=\"http://en.wikipedia.org/wiki/Logical_conjunction\">conjunction</a> (P#<strong>&and;</strong>P#) of its premises: <strong></strong></p>\n<p><strong>P(FOOM) = P(P1&and;P2&and;P3&and;P4&and;P5)</strong></p>\n<p>Of course, there are many more premises that need to be true in order to enable an AI to go FOOM, e.g. that each level of intelligence can effectively handle its own complexity, or that most AGI designs can somehow self-modify their way up to massive superhuman intelligence. But I believe that the above points are enough to show that the case for a hard takeoff is not disjunctive, but rather strongly conjunctive.</p>\n<h3><a name=\"CHARITY\">Requirements for SIAI to constitute an optimal charity</a></h3>\n<p>In this section I will assume the truth of all premises in the previous section.</p>\n<p><strong>P6</strong> SIAI can solve friendly AI.</p>\n<p style=\"padding-left: 30px;\">Say you believe that unfriendly AI will wipe us out with a probability of 60% and that there is another existential risk that will wipe us out with a probability of 10% even if unfriendly AI turns out to be no risk or in all possible worlds where it comes later. Both risks have the same utility <em>x</em> (if we don't assume that an unfriendly AI could also wipe out aliens etc.). Thus .6x &gt; .1x. But if the probability of solving friendly AI = <em>A</em> to the probability of solving the second risk = <em>B</em> is A<em></em> &le; (1/6)B then the expected utility of mitigating friendly AI is at best equal to the other existential risk because .6Ax &le; .1Bx.</p>\n<p style=\"padding-left: 30px;\">Consider that one order of magnitude more utility could easily be outweighed or trumped by an underestimation of the complexity of friendly AI.</p>\n<p style=\"padding-left: 30px;\">So how hard is it to solve friendly AI?</p>\n<p style=\"padding-left: 30px;\">Take for example Pascal's mugging, if you can't solve it then you need to implement a hack that is largely based on human intuition. Therefore, in order to estimate the possibility of solving friendly AI one needs to account for the difficulty in solving all sub-problems.</p>\n<p style=\"padding-left: 30px;\">Consider that we don't even know <em>\"how one would start to research the problem of getting a hypothetical AGI to recognize humans as distinguished beings.\"</em> [10]</p>\n<p><strong>P7</strong> SIAI does not increase risks from AI.</p>\n<p style=\"padding-left: 30px;\">By trying to solve friendly AI, SIAI has to think about a lot of issues related to AI in general and might have to solve problems that will make it easier to create artificial general intelligence.</p>\n<p style=\"padding-left: 30px;\">It is far from being clear that SIAI is able to protect its findings against intrusion, betrayal, industrial or espionage.</p>\n<p><strong>P8</strong> SIAI does not increase negative utility.</p>\n<p style=\"padding-left: 30px;\">There are several possibilities by which SIAI could actually cause a direct increase in negative utility.</p>\n<p style=\"padding-left: 30px;\">1) Friendly AI is incredible hard and complex. Complex systems can fail in complex ways. Agents that are an effect of evolution have complex values. To satisfy complex values you need to meet complex circumstances. Therefore any attempt at friendly AI, which is incredible complex, is likely to fail in unforeseeable ways. A half-baked, not quite friendly, AI might create a living hell for the rest of time, increasing negative utility dramatically [11].</p>\n<p style=\"padding-left: 30px;\">2) Humans are not provably friendly. Given the power to shape the universe the SIAI might fail to act altruistic and deliberately implement an AI with selfish motives or horrible strategies [12].</p>\n<p><strong>P9</strong> It makes sense to support SIAI at this time [13].</p>\n<p>Therefore the probability of SIAI to be a worthwhile charity (P(CHARITY)) is the probability of the <a href=\"http://en.wikipedia.org/wiki/Logical_conjunction\">conjunction</a> (P#<strong>&and;</strong>P#) of its premises: <strong></strong></p>\n<p><strong>P(CHARITY) = P(P6&and;P7&and;P8&and;P9)</strong></p>\n<p>As before, there are many more premises that need to be true in order for SIAI to be the best choice for someone who wants to maximize doing good by contributing money to a charity.</p>\n<h3><a name=\"FR\">Further Reading</a></h3>\n<p>The following posts and resources elaborate on many of the above points and hint at a lot of additional problems.</p>\n<ul>\n<li><a href=\"/Is an Intelligence Explosion a Disjunctive or Conjunctive Event\">Is an Intelligence Explosion a Disjunctive or Conjunctive Event?</a></li>\n<li><a href=\"/Why an Intelligence Explosion might be a Low-Priority Global Risk\">Why an Intelligence Explosion might be a Low-Priority Global Risk</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">Interview series on risks from AI</a></li>\n</ul>\n<h3><a name=\"NR\">Notes and References</a></h3>\n<p>[1] <a href=\"/lw/691/shane_legg_on_risks_from_ai/\">Q&amp;A with Shane Legg on risks from AI</a></p>\n<p>[2] <a href=\"http://lukeprog.com/SaveTheWorld.html\">http://lukeprog.com/SaveTheWorld.html</a></p>\n<p>[3] <em>\"In many ways, this is a book about hindsight. Pythagoras could not have imagined the uses to which his equation would be put (if, indeed, he ever came up with the equation himself in the first place). The same applies to almost all of the equations in this book. They were studied/discovered/developed by mathematicians and mathematical physicists who were investigating subjects that fascinated them deeply, not because they imagined that two hundred years later the work would lead to electric light bulbs or GPS or the internet, but rather because they were genuinely curious.\"</em> <a href=\"http://plus.maths.org/content/17-equations-changed-world\"></a></p>\n<p><a href=\"http://plus.maths.org/content/17-equations-changed-world\">17 Equations that changed the world</a></p>\n<p>[4] Here is my list of \"really stupid, frivolous academic pursuits\" that have lead to major scientific breakthroughs.</p>\n<ul>\n<li>Studying monkey social behaviors and eating habits lead to insights into HIV (Radiolab: Patient Zero)</li>\n<li>Research into how algae move toward light paved the way for optogenetics: using light to control brain cells (Nature 2010 Method of the Year).</li>\n<li>Black hole research gave us WiFi (ICRAR award)</li>\n<li>Optometry informs architecture and saved lives on 9/11 (APA Monitor)</li>\n<li>Certain groups HATE SETI, but SETI's development of cloud-computing service SETI@HOME paved the way for citizen science and recent breakthroughs in protein folding (Popular Science)</li>\n<li>Astronomers provide insights into medical imaging (TEDxBoston: Michell Borkin)</li>\n<li>Basic physics experiments and the Fibonacci sequence help us understand plant growth and neuron development</li>\n</ul>\n<p><a href=\"http://blog.ketyov.com/2012/02/basic-science-is-about-creating.html\">http://blog.ketyov.com/2012/02/basic-science-is-about-creating.html</a></p>\n<p>[5] <em>\"<a title=\"Universal Algorithmic Intelligence\" href=\"http://www.hutter1.net/ai/aixigentle.htm\">AIXI</a> is often quoted as a <a href=\"http://en.wikipedia.org/wiki/Proof_of_concept\">proof of concept</a> that it is possible for a simple algorithm to improve itself to such an extent that it could in principle reach superhuman intelligence. AIXI proves that there is a general theory of intelligence. But there is a minor problem, AIXI is as far from real world human-level general intelligence as an abstract notion of a <a href=\"http://en.wikipedia.org/wiki/Turing_machine\">Turing machine</a> with an infinite tape is from a supercomputer with the computational capacity of the human brain. An abstract notion of intelligence doesn&rsquo;t get you anywhere in terms of real-world general intelligence. Just as you won&rsquo;t be able to <a href=\"http://www.boingboing.net/2011/07/14/far.html\">upload yourself</a><a title=\"Mind uploading\" href=\"http://en.wikipedia.org/wiki/Mind_uploading\"> to a non-biological substrate</a> because you showed that in some abstract sense <a title=\"Church&ndash;Turing&ndash;Deutsch principle\" href=\"http://en.wikipedia.org/wiki/Church%E2%80%93Turing%E2%80%93Deutsch_principle\">you can simulate every physical process</a>.\"</em></p>\n<p>Alexander Kruel, <a href=\"/lw/8fb/why_an_intelligence_explosion_might_be_a/\">Why an Intelligence Explosion might be a Low-Priority Global Risk </a></p>\n<p>[6] <em>\"&hellip;please bear in mind that the relation of Solomonoff induction and &ldquo;Universal AI&rdquo; to real-world general intelligence of any kind is also rather wildly speculative&hellip; This stuff is beautiful math, but does it really have anything to do with real-world intelligence? These theories have little to say about human intelligence, and they&rsquo;re not directly useful as foundations for building AGI systems (though, admittedly, a handful of scientists are working on &ldquo;scaling them down&rdquo; to make them realistic; so far this only works for very simple toy problems, and it&rsquo;s hard to see how to extend the approach broadly to yield anything near human-level AGI). And it&rsquo;s not clear they will be applicable to future superintelligent minds either, as these minds may be best conceived using radically different concepts.\"</em></p>\n<p>Ben Goertzel, '<a href=\"http://multiverseaccordingtoben.blogspot.com/2012/03/prediction-and-intelligence.html\">Are Prediction and Reward Relevant to Superintelligences?</a>'</p>\n<p>[7] <a href=\"http://public.web.cern.ch/public/en/spotlight/SpotlightAandD-en.html\">http://public.web.cern.ch/public/en/spotlight/SpotlightAandD-en.html</a></p>\n<p>[8] <em>\"If any increase in intelligence is vastly outweighed by its computational cost and the expenditure of time needed to discover it then it might not be instrumental for a perfectly rational agent (such as an artificial general intelligence), as <a title=\"Rational Agent\" href=\"http://xixidu.tumblr.com/post/7648180962/what-game-theorists-somewhat-disturbingly-call\">imagined by game theorists</a>, to increase its intelligence as opposed to using its existing intelligence to pursue its terminal goals directly or to invest its given resources to acquire other means of self-improvement, e.g. more efficient sensors.\"</em></p>\n<p>Alexander Kruel, <a href=\"/lw/8fb/why_an_intelligence_explosion_might_be_a/\">Why an Intelligence Explosion might be a Low-Priority Global Risk </a></p>\n<p>[9] Section <em>'Necessary resources for an intelligence explosion'</em>, <a href=\"/lw/8fb/why_an_intelligence_explosion_might_be_a/\">Why an Intelligence Explosion might be a Low-Priority Global Risk</a>, Alexander Kruel</p>\n<p>[10] <a href=\"/lw/3aa/friendly_ai_research_and_taskification/\">http://lesswrong.com/lw/3aa/friendly_ai_research_and_taskification/</a></p>\n<p>[11] <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/5ylx\">http://lesswrong.com/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/5ylx</a></p>\n<p>[12] <a href=\"/lw/8c3/qa_with_new_executive_director_of_singularity/5y77\">http://lesswrong.com/lw/8c3/qa_with_new_executive_director_of_singularity/5y77</a></p>\n<p>[13] <em>\"I think that if you're aiming to develop knowledge that won't be useful until very very far in the future, you're probably wasting your time, if for no other reason than this: by the time your knowledge is relevant, someone will probably have developed a tool (such as a narrow AI) so much more efficient in generating this knowledge that it renders your work moot.\"</em></p>\n<p>Holden Karnofsky in a <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F91e%2Fsingularity_institute_100000_endofyear_fundraiser%2F5j9o%3Fcontext%3D1%235j9o&amp;v=1&amp;libid=1331638223097&amp;out=http%3A%2F%2Fkruel.co%2FJaanTallinn201105.pdf&amp;title=juliawise%20comments%20on%20Singularity%20Institute%20%24100%2C000%20end-of-year%20fundraiser%20only%2020%25%20filled%20so%20far%20-%20Less%20Wrong&amp;txt=talked%20to%20Jaan%20Tallinn&amp;jsonp=vglnk_jsonp_13316414389702\">conversation</a> with Jaan Tallinn</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZpXik8eWiYhzbTtQC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 1, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "13980", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote>\n<p>If you\u2019re interested in being on the right side of disputes, you will refute your opponents' arguments. But if you're interested in producing truth, you will fix your opponents' arguments for them. To win, you must fight not only the creature you encounter; you [also] must fight the most horrible thing that can be constructed from its corpse.</p>\n</blockquote>\n<p>-- <a href=\"http://web.archive.org/web/20100328161823/http://www.acceleratingfuture.com/steven/?p=155\">Black Belt Bayesian</a></p>\n<p>This is an <em>informal</em> post meant as a reply to a post by user:utilitymonster, <em>'<a href=\"/r/discussion/lw/aro/what_is_the_best_compact_formalization_of_the/\">What is the best compact formalization of the argument for AI risk from fast takeoff?</a>'</em></p>\n<p>I hope to find the mental strength to put more effort into it in future to improve it. But since nobody else seems to be willing to take a critical look at the overall topic I feel that doing what I can is better than doing nothing.</p>\n<p><strong id=\"Please_review_the_categories__Further_Reading__and__Notes_and_References___\">Please review the categories <em>'Further Reading'</em> and <em>'Notes and References'</em>. </strong></p>\n<p><strong id=\"Contents\">Contents</strong></p>\n<table style=\"border: thin solid #808080; width: 440px; padding: 5px;\" border=\"0\">\n<tbody>\n<tr>\n<td>\n<ul>\n<li><a href=\"#abstract\">Abstract</a></li>\n<li><a href=\"#FOOM\">Requirements for an Intelligence Explosion</a></li>\n<li><a href=\"#CHARITY\">Requirements for SIAI to constitute an optimal charity</a></li>\n<li><a href=\"#FR\">Further Reading</a></li>\n<li><a href=\"#NR\">Notes and References</a> </li>\n</ul>\n</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<h3 id=\"Abstract\"><a name=\"abstract\">Abstract</a><br></h3>\n<p>In this post I just want to take a look at a few premises (P#) that <a href=\"/r/discussion/lw/8fa/intelligence_explosion_a_disjunctive_or/\">need to be true simultaneously</a> to make the SIAI a wortwhile charity from the point of view of someone trying to do as much good as possible by contributing money. I am going to show that the case of <a href=\"/lw/8fb/why_an_intelligence_explosion_might_be_a/\">risks from AI</a> is strongly conjunctive, that without a concrete and grounded understanding of AGI an abstract analysis of the issues is going to be very shaky, and that therefore SIAI is likely to be a bad choice as a charity. In other words, that which speaks in favor of SIAI does mainly consist of highly specific, conjunctive, non-evidence-backed speculations on possible bad outcomes.</p>\n<h3 id=\"Requirements_for_an_Intelligence_Explosion\"><a name=\"FOOM\">Requirements for an Intelligence Explosion</a></h3>\n<p><strong>P1</strong> Fast, and therefore dangerous, recursive self-improvement is <span style=\"text-decoration: underline;\">logically</span> possible.</p>\n<p style=\"padding-left: 30px;\">It took almost four hundred years to prove <a href=\"http://en.wikipedia.org/wiki/Fermat%27s_Last_Theorem\">Fermat\u2019s Last Theorem</a>. The <a href=\"http://en.wikipedia.org/wiki/Wiles%27_proof_of_Fermat%27s_Last_Theorem\">final proof</a> is over a hundred pages long. Over a hundred pages! And we are not talking about something like an artificial general intelligence that can magically make itself smart enough to prove such theorems and many more that no human being would be capable of proving. Fermat\u2019s Last Theorem simply states <em>\u201cno three positive integers a, b, and c can satisfy the equation a^n + b^n = c^n for any integer value of n greater than two.\u201d</em></p>\n<p style=\"padding-left: 30px;\">Even artificial intelligence researchers admit that <em>\"there could be non-linear complexity constrains meaning that even theoretically optimal algorithms experience strongly diminishing intelligence returns for additional compute power.\"</em> [1] We just don't know.</p>\n<p style=\"padding-left: 30px;\">Other possible problems include the impossibility of a stable utility function and a reflective decision theory, the intractability of real world expected utility maximization or that expected utility maximizers stumble over <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's mugging</a>, among other things [2].</p>\n<p style=\"padding-left: 30px;\">For an AI to be capable of recursive self-improvement it also has to guarantee that its goals will be preserved when it improves itself. It is still questionable if it is possible to conclusively prove that improvements to an agent's intelligence or decision procedures maximize expected utility. If this isn't possible it won't be rational or possible to undergo <em>explosive</em> self-improvement.</p>\n<p><strong>P1.b</strong> The fast computation of a simple algorithm is sufficient to outsmart and overpower humanity.</p>\n<p style=\"padding-left: 30px;\">Imagine a group of 100 world-renowned scientists and military strategists.</p>\n<ul>\n<li>The group is analogous to the initial resources of an AI. </li>\n<li>The knowledge that the group has is analogous to what an AI could come up with by simply \"thinking\" about it given its current resources. </li>\n</ul>\n<p style=\"padding-left: 30px;\">Could such a group easily wipe away the Roman empire when beamed back in time?</p>\n<ul>\n<li>The Roman empire is analogous to our society today. </li>\n</ul>\n<p style=\"padding-left: 30px;\">Even if you gave all of them a machine gun, the Romans would quickly adapt and the people from the future would run out of ammunition.</p>\n<ul>\n<li>Machine guns are analogous to the supercomputer it runs on.</li>\n</ul>\n<p style=\"padding-left: 30px;\">Consider that it takes a <a rel=\"nofollow\" href=\"http://www.antipope.org/charlie/blog-static/2010/07/insufficient-data.html\" target=\"_blank\">whole technological civilization</a> to produce a modern smartphone.</p>\n<p style=\"padding-left: 30px;\">You can't just say <em>\"with more processing power you can do more different things\"</em>, that would be analogous to saying that \"100 people\" from today could just build more \"machine guns\". But they can't! They can't use all their knowledge and magic from the future to defeat the Roman empire.</p>\n<p style=\"padding-left: 30px;\">A lot of assumptions have to turn out to be correct to make humans discover simple algorithms over night that can then be improved to self-improve explosively.</p>\n<p style=\"padding-left: 30px;\">You can also compare this to the idea of a Babylonian mathematician discovering modern science and physics given that he would be uploaded into a supercomputer (a possibility that is in and of itself already highly speculative). It assumes that he could brute-force conceptual revolutions.</p>\n<p style=\"padding-left: 30px;\">Even if he was given a detailed explanation of how his mind works and the resources to understand it, self-improving to achieve superhuman intelligence assumes that throwing resources at the problem of intelligence will magically allow him to pull improved algorithms from solution space as if they were signposted.</p>\n<p style=\"padding-left: 30px;\">But unknown unknowns are not signposted. It's rather like finding a needle in a haystack. Evolution is great at doing that and assuming that one could speed up evolution considerably is another assumption about technological feasibility and real-world resources.</p>\n<p style=\"padding-left: 30px;\">That conceptual revolutions are just a matter of computational resources is pure speculation.</p>\n<p style=\"padding-left: 30px;\">If one were to speed up the whole Babylonian world and accelerate cultural evolution, obviously one would arrive quicker at some insights. But how much quicker? How much are many insights dependent on experiments, to yield empirical evidence, that can't be speed-up considerably? And what is the return? Is the payoff proportionally to the resources that are necessary?</p>\n<p style=\"padding-left: 30px;\">If you were going to speed up a chimp brain a million times, would it quickly reach human-level intelligence? If not, why then would it be different for a human-level intelligence trying to reach transhuman intelligence? It seems like a nice idea when formulated in English, but would it work?</p>\n<p style=\"padding-left: 30px;\">Being able <em>to state</em> that an AI could use some magic to take over the earth does not make it a serious possibility.</p>\n<p style=\"padding-left: 30px;\">Magic has to be discovered, adapted and manufactured first. It doesn't just emerge out of nowhere from the computation of certain algorithms. It emerges from a society of agents with various different goals and heuristics like <em>\"Treating Rare Diseases in Cute Kittens\"</em>. It is an evolutionary process that relies on massive amounts of real-world feedback and empirical experimentation. Assuming that all that can happen because some simple algorithm is being computed is like believing it will emerge 'out of nowhere', it is magical thinking.</p>\n<p style=\"padding-left: 30px;\">Unknown unknowns are not sign-posted. [3]</p>\n<p style=\"padding-left: 30px;\">If people like Beno\u00eet B. Mandelbrot would have never decided to research Fractals then many modern movies wouldn't be possible, as they rely on fractal landscape algorithms. Yet, at the time Beno\u00eet B. Mandelbrot conducted his research it was not foreseeable that his work would have any real-world applications.</p>\n<p style=\"padding-left: 30px;\">Important discoveries are made because many routes with low or no expected utility are explored at the same time [4]. And to do so efficiently it takes random mutation, a whole society of minds, a lot of feedback and empirical experimentation.</p>\n<p style=\"padding-left: 30px;\"><em>\"Treating rare diseases in cute kittens\"</em> might or might not provide genuine insights and open up new avenues for further research. As long as you don't try it you won't know.</p>\n<p style=\"padding-left: 30px;\">The idea that a rigid consequentialist with simple values can think up insights and conceptual revolutions simply because it is instrumentally useful to do so is implausible.</p>\n<p style=\"padding-left: 30px;\">Complex values are the cornerstone of diversity, which in turn enables creativity and drives the exploration of various conflicting routes. A singleton with a stable utility-function lacks the feedback provided by a society of minds and its cultural evolution.</p>\n<p style=\"padding-left: 30px;\">You need to have various different agents with different utility-functions around to get the necessary diversity that can give rise to enough selection pressure. A \"singleton\" won't be able to predict the actions of new and improved versions of itself by just running sandboxed simulations. Not just because of logical uncertainty but also because it is computationally intractable to predict the real-world payoff of changes to its decision procedures.</p>\n<p style=\"padding-left: 30px;\">You need complex values to give rise to the necessary drives to function in a complex world. You can't just tell an AI to protect itself. What would that even mean? What changes are illegitimate? What constitutes <em>\"self\"</em>? That are all unsolved problems that are just assumed to be solvable when talking about risks from AI.</p>\n<p style=\"padding-left: 30px;\">An AI with simple values will simply lack the creativity, due to a lack of drives, to pursue the huge spectrum of research that a society of humans does pursue. Which will allow an AI to solve some well-defined narrow problems, but it will be unable to make use of the broad range of synergetic effects of cultural evolution. Cultural evolution is a result of the interaction of a wide range of utility-functions.</p>\n<p style=\"padding-left: 30px;\">Yet even if we assume that there is one complete theory of general intelligence, once discovered, one just has to throw more resources at it. It might be able to incorporate all human knowledge, adapt it and find new patterns. But would it really be vastly superior to human society and their expert systems?</p>\n<p style=\"padding-left: 30px;\">Can intelligence itself be improved apart from solving well-defined problems and making more accurate predictions on well-defined classes of problems? The discovery of unknown unknowns does not seem to be subject to other heuristics than natural selection. Without goals, well-defined goals, terms like \"optimization\" have no meaning.</p>\n<p><strong>P2</strong> Fast, and therefore dangerous, recursive self-improvement is <span style=\"text-decoration: underline;\">physically</span> possible.</p>\n<p style=\"padding-left: 30px;\">Even if it could be proven that <em>explosive</em> recursive self-improvement is logically possible, e.g. that there are no complexity constraints, the question remains if it is physically possible.</p>\n<p style=\"padding-left: 30px;\">Our best theories about intelligence are highly abstract and their relation to real world human-level general intelligence is often wildly speculative [5][6].</p>\n<p><strong>P3</strong> Fast, and therefore dangerous, recursive self-improvement is <span style=\"text-decoration: underline;\">economically</span> feasible.</p>\n<p style=\"padding-left: 30px;\">To exemplify the problem take the science fictional idea of using <a href=\"http://en.wikipedia.org/wiki/Antimatter\">antimatter</a> as explosive for weapons. It is physically possible to produce antimatter and use it for large scale destruction. An equivalent of the Hiroshima atomic bomb will only take half a gram of antimatter. But it will take 2 billion years to produce that amount of antimatter [7].</p>\n<p style=\"padding-left: 30px;\">We simply don\u2019t know if intelligence is instrumental or quickly hits diminishing returns [8].</p>\n<p><strong>P3.b</strong> AGI is able to create (or acquire) resources, empowering technologies or civilisatory support [9].</p>\n<p style=\"padding-left: 30px;\">We are already at a point where we have to build billion dollar chip manufacturing facilities to run our mobile phones. We need to build huge particle accelerators to obtain new insights into the nature of reality.</p>\n<p style=\"padding-left: 30px;\">An AI would either have to rely on the help of a whole technological civilization or be in control of advanced nanotech assemblers.</p>\n<p style=\"padding-left: 30px;\">And if an AI was to acquire the necessary resources on its own, its plan for world-domination would have to go unnoticed. This would require the workings of the AI to be opaque to its creators yet comprehensible to itself.</p>\n<p style=\"padding-left: 30px;\">But an AI capable of efficient recursive self improvement must be able to</p>\n<ol>\n<li>comprehend its own workings </li>\n<li>predict how improvements, respectively improved versions of itself, are going to act to ensure that its values are preserved</li>\n</ol>\n<p style=\"padding-left: 30px;\">So if the AI can do that, why wouldn't humans be able to use the same algorithms to predict what the initial AI is going to do? And if the AI can't do that, how is it going to maximize expected utility if it is unable to predict what it is going to do?</p>\n<p style=\"padding-left: 30px;\">Any AI capable of efficient self-modification must be able to grasp its own workings and make predictions about improvements to various algorithms and its overall decision procedure. If an AI can do that, why would the humans who build it be unable to notice any malicious intentions? Why wouldn't the humans who created it not be able to use the same algorithms that the AI uses to predict what it will do? If humans are unable to predict what the AI will do, how is the AI able to predict what improved versions of itself will do?</p>\n<p style=\"padding-left: 30px;\">And even if an AI was able to somehow acquire large amounts of money. It is not easy to <em>use</em> the money. You can't <em>\"just\"</em> build huge companies with fake identities, or a straw man, to create revolutionary technologies easily. Running companies with real people takes a lot of real-world knowledge, interactions and feedback. But most importantly, it takes a lot of time. An AI could not simply create a new Intel or Apple over a few years without its creators noticing anything.</p>\n<p style=\"padding-left: 30px;\">The goals of an AI will be under scrutiny at any time. It seems very implausible that scientists, a company or the military are going to create an AI and then just let it run without bothering about its plans. An artificial agent is not a black box, like humans are, where one is only able to guess its real intentions.</p>\n<p style=\"padding-left: 30px;\">A plan for world domination seems like something that can't be concealed from its creators. Lying is no option if your algorithms are open to inspection.</p>\n<p><strong>P4</strong> Dangerous recursive self-improvement is the default outcome of the creation of artificial general intelligence.</p>\n<p style=\"text-align: left; padding-left: 30px;\">Complex goals need complex optimization parameters (the design specifications of the subject of the optimization process against which it will measure its success of self-improvement).</p>\n<p style=\"text-align: left; padding-left: 30px;\">Even the creation of paperclips is a much more complex goal than telling an AI to compute as many decimal digits of Pi as possible.</p>\n<p style=\"text-align: left; padding-left: 30px;\">For an AGI, that was designed to design paperclips, to pose an existential risk, its creators would have to be capable enough to enable it to take over the universe on its own, yet forget, or fail to, define time, space and energy bounds as part of its optimization parameters. Therefore, given the large amount of restrictions that are inevitably part of any advanced general intelligence (AGI), the nonhazardous subset of all possible outcomes might be much larger than that where the AGI works perfectly yet fails to hold before it could wreak havoc.</p>\n<p style=\"text-align: left; padding-left: 30px;\">And even given a rational utility maximizer. It is possible to maximize paperclips in a lot of different ways. How it does it is fundamentally dependent on its utility-function and how precisely it was defined.</p>\n<p style=\"text-align: left; padding-left: 30px;\">If there are no constraints in the form of design and goal parameters then it can maximize paperclips in all sorts of ways that don't demand recursive self-improvement.</p>\n<p style=\"text-align: left; padding-left: 30px;\">\"Utility\" does only become well-defined if we precisely define what it means to maximize it. Just maximizing paperclips doesn't define how quickly and how economically it is supposed to happen.</p>\n<p style=\"padding-left: 30px;\">The problem is that \"utility\" has to be defined. To maximize expected utility does not imply certain actions, efficiency and economic behavior, or the drive to protect yourself. You can also rationally maximize paperclips without protecting yourself if it is not part of your goal parameters.</p>\n<p style=\"padding-left: 30px;\">You can also assign utility to maximize paperclips as long as nothing turns you off but don't care about being turned off. If an AI is not explicitly programmed to care about it, then it won't.</p>\n<p style=\"padding-left: 30px;\">Without well-defined goals in form of a precise utility-function, it might be impossible to maximize expected \"utility\". Concepts like \"efficient\", \"economic\" or \"self-protection\" all have a meaning that is inseparable with an agent's terminal goals. If you just tell it to maximize paperclips then this can be realized in an infinite number of ways that would all be rational given imprecise design and goal parameters. Undergoing to explosive recursive self-improvement, taking over the universe and filling it with paperclips, is just one outcome. Why would an arbitrary mind pulled from mind-design space care to do that? Why not just wait for paperclips to arise due to random fluctuations out of a state of chaos? That wouldn't be irrational. To have an AI take over the universe as fast as possible you would have to explicitly design it to do so.</p>\n<p style=\"padding-left: 30px;\">But for the sake of a thought experiment assume that the default case was recursive self-improvement. Now imagine that a company like Apple wanted to build an AI that could answer every question (an Oracle).</p>\n<p style=\"padding-left: 30px;\">If Apple was going to build an Oracle it would anticipate that other people would also want to ask it questions. Therefore it can't just waste all resources on looking for an inconsistency arising from the Peano axioms when asked to solve 1+1. It would not devote additional resources on answering those questions that are already known to be correct with a high probability. It wouldn't be <em>economically useful</em> to take over the universe to answer simple questions.</p>\n<p style=\"padding-left: 30px;\">It would neither be <em>rational</em> to look for an inconsistency arising from the Peano axioms while solving 1+1. To answer questions an Oracle needs a good amount of general intelligence. And concluding that asking it to solve 1+1 implies to look for an inconsistency arising from the Peano axioms does not seem reasonable. It also does not seem reasonable to suspect that humans desire an answer to their questions to approach infinite certainty. Why would someone build such an Oracle in the first place?</p>\n<p style=\"padding-left: 30px;\">A reasonable Oracle would quickly yield good solutions by trying to find answers within a reasonable time which are with a high probability just 2\u20133% away from the optimal solution. I don't think anyone would build an answering machine that throws the whole universe at the first sub-problem it encounters.</p>\n<p><strong>P5</strong> The human development of artificial general intelligence will take place quickly.</p>\n<p style=\"padding-left: 30px;\">What evidence do we have that there is some principle that, once discovered, allows us to grow superhuman intelligence overnight?</p>\n<p style=\"padding-left: 30px;\">If the development of AGI takes place slowly, a gradual and controllable development, we might be able to learn from small-scale mistakes, or have enough time to develop friendly AI, while having to face other existential risks.</p>\n<p style=\"padding-left: 30px;\">This might for example be the case if intelligence can not be captured by a discrete algorithm, or is modular, and therefore never allow us to reach a point where we can suddenly build the smartest thing ever that does just extend itself indefinitely.</p>\n<p>Therefore the probability of an AI to undergo explosive recursive self-improvement (P(FOOM)) is the probability of the <a href=\"http://en.wikipedia.org/wiki/Logical_conjunction\">conjunction</a> (P#<strong>\u2227</strong>P#) of its premises: <strong></strong></p>\n<p><strong id=\"P_FOOM____P_P1_P2_P3_P4_P5_\">P(FOOM) = P(P1\u2227P2\u2227P3\u2227P4\u2227P5)</strong></p>\n<p>Of course, there are many more premises that need to be true in order to enable an AI to go FOOM, e.g. that each level of intelligence can effectively handle its own complexity, or that most AGI designs can somehow self-modify their way up to massive superhuman intelligence. But I believe that the above points are enough to show that the case for a hard takeoff is not disjunctive, but rather strongly conjunctive.</p>\n<h3 id=\"Requirements_for_SIAI_to_constitute_an_optimal_charity\"><a name=\"CHARITY\">Requirements for SIAI to constitute an optimal charity</a></h3>\n<p>In this section I will assume the truth of all premises in the previous section.</p>\n<p><strong>P6</strong> SIAI can solve friendly AI.</p>\n<p style=\"padding-left: 30px;\">Say you believe that unfriendly AI will wipe us out with a probability of 60% and that there is another existential risk that will wipe us out with a probability of 10% even if unfriendly AI turns out to be no risk or in all possible worlds where it comes later. Both risks have the same utility <em>x</em> (if we don't assume that an unfriendly AI could also wipe out aliens etc.). Thus .6x &gt; .1x. But if the probability of solving friendly AI = <em>A</em> to the probability of solving the second risk = <em>B</em> is A<em></em> \u2264 (1/6)B then the expected utility of mitigating friendly AI is at best equal to the other existential risk because .6Ax \u2264 .1Bx.</p>\n<p style=\"padding-left: 30px;\">Consider that one order of magnitude more utility could easily be outweighed or trumped by an underestimation of the complexity of friendly AI.</p>\n<p style=\"padding-left: 30px;\">So how hard is it to solve friendly AI?</p>\n<p style=\"padding-left: 30px;\">Take for example Pascal's mugging, if you can't solve it then you need to implement a hack that is largely based on human intuition. Therefore, in order to estimate the possibility of solving friendly AI one needs to account for the difficulty in solving all sub-problems.</p>\n<p style=\"padding-left: 30px;\">Consider that we don't even know <em>\"how one would start to research the problem of getting a hypothetical AGI to recognize humans as distinguished beings.\"</em> [10]</p>\n<p><strong>P7</strong> SIAI does not increase risks from AI.</p>\n<p style=\"padding-left: 30px;\">By trying to solve friendly AI, SIAI has to think about a lot of issues related to AI in general and might have to solve problems that will make it easier to create artificial general intelligence.</p>\n<p style=\"padding-left: 30px;\">It is far from being clear that SIAI is able to protect its findings against intrusion, betrayal, industrial or espionage.</p>\n<p><strong>P8</strong> SIAI does not increase negative utility.</p>\n<p style=\"padding-left: 30px;\">There are several possibilities by which SIAI could actually cause a direct increase in negative utility.</p>\n<p style=\"padding-left: 30px;\">1) Friendly AI is incredible hard and complex. Complex systems can fail in complex ways. Agents that are an effect of evolution have complex values. To satisfy complex values you need to meet complex circumstances. Therefore any attempt at friendly AI, which is incredible complex, is likely to fail in unforeseeable ways. A half-baked, not quite friendly, AI might create a living hell for the rest of time, increasing negative utility dramatically [11].</p>\n<p style=\"padding-left: 30px;\">2) Humans are not provably friendly. Given the power to shape the universe the SIAI might fail to act altruistic and deliberately implement an AI with selfish motives or horrible strategies [12].</p>\n<p><strong>P9</strong> It makes sense to support SIAI at this time [13].</p>\n<p>Therefore the probability of SIAI to be a worthwhile charity (P(CHARITY)) is the probability of the <a href=\"http://en.wikipedia.org/wiki/Logical_conjunction\">conjunction</a> (P#<strong>\u2227</strong>P#) of its premises: <strong></strong></p>\n<p><strong id=\"P_CHARITY____P_P6_P7_P8_P9_\">P(CHARITY) = P(P6\u2227P7\u2227P8\u2227P9)</strong></p>\n<p>As before, there are many more premises that need to be true in order for SIAI to be the best choice for someone who wants to maximize doing good by contributing money to a charity.</p>\n<h3 id=\"Further_Reading\"><a name=\"FR\">Further Reading</a></h3>\n<p>The following posts and resources elaborate on many of the above points and hint at a lot of additional problems.</p>\n<ul>\n<li><a href=\"/Is an Intelligence Explosion a Disjunctive or Conjunctive Event\">Is an Intelligence Explosion a Disjunctive or Conjunctive Event?</a></li>\n<li><a href=\"/Why an Intelligence Explosion might be a Low-Priority Global Risk\">Why an Intelligence Explosion might be a Low-Priority Global Risk</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">Interview series on risks from AI</a></li>\n</ul>\n<h3 id=\"Notes_and_References\"><a name=\"NR\">Notes and References</a></h3>\n<p>[1] <a href=\"/lw/691/shane_legg_on_risks_from_ai/\">Q&amp;A with Shane Legg on risks from AI</a></p>\n<p>[2] <a href=\"http://lukeprog.com/SaveTheWorld.html\">http://lukeprog.com/SaveTheWorld.html</a></p>\n<p>[3] <em>\"In many ways, this is a book about hindsight. Pythagoras could not have imagined the uses to which his equation would be put (if, indeed, he ever came up with the equation himself in the first place). The same applies to almost all of the equations in this book. They were studied/discovered/developed by mathematicians and mathematical physicists who were investigating subjects that fascinated them deeply, not because they imagined that two hundred years later the work would lead to electric light bulbs or GPS or the internet, but rather because they were genuinely curious.\"</em> <a href=\"http://plus.maths.org/content/17-equations-changed-world\"></a></p>\n<p><a href=\"http://plus.maths.org/content/17-equations-changed-world\">17 Equations that changed the world</a></p>\n<p>[4] Here is my list of \"really stupid, frivolous academic pursuits\" that have lead to major scientific breakthroughs.</p>\n<ul>\n<li>Studying monkey social behaviors and eating habits lead to insights into HIV (Radiolab: Patient Zero)</li>\n<li>Research into how algae move toward light paved the way for optogenetics: using light to control brain cells (Nature 2010 Method of the Year).</li>\n<li>Black hole research gave us WiFi (ICRAR award)</li>\n<li>Optometry informs architecture and saved lives on 9/11 (APA Monitor)</li>\n<li>Certain groups HATE SETI, but SETI's development of cloud-computing service SETI@HOME paved the way for citizen science and recent breakthroughs in protein folding (Popular Science)</li>\n<li>Astronomers provide insights into medical imaging (TEDxBoston: Michell Borkin)</li>\n<li>Basic physics experiments and the Fibonacci sequence help us understand plant growth and neuron development</li>\n</ul>\n<p><a href=\"http://blog.ketyov.com/2012/02/basic-science-is-about-creating.html\">http://blog.ketyov.com/2012/02/basic-science-is-about-creating.html</a></p>\n<p>[5] <em>\"<a title=\"Universal Algorithmic Intelligence\" href=\"http://www.hutter1.net/ai/aixigentle.htm\">AIXI</a> is often quoted as a <a href=\"http://en.wikipedia.org/wiki/Proof_of_concept\">proof of concept</a> that it is possible for a simple algorithm to improve itself to such an extent that it could in principle reach superhuman intelligence. AIXI proves that there is a general theory of intelligence. But there is a minor problem, AIXI is as far from real world human-level general intelligence as an abstract notion of a <a href=\"http://en.wikipedia.org/wiki/Turing_machine\">Turing machine</a> with an infinite tape is from a supercomputer with the computational capacity of the human brain. An abstract notion of intelligence doesn\u2019t get you anywhere in terms of real-world general intelligence. Just as you won\u2019t be able to <a href=\"http://www.boingboing.net/2011/07/14/far.html\">upload yourself</a><a title=\"Mind uploading\" href=\"http://en.wikipedia.org/wiki/Mind_uploading\"> to a non-biological substrate</a> because you showed that in some abstract sense <a title=\"Church\u2013Turing\u2013Deutsch principle\" href=\"http://en.wikipedia.org/wiki/Church%E2%80%93Turing%E2%80%93Deutsch_principle\">you can simulate every physical process</a>.\"</em></p>\n<p>Alexander Kruel, <a href=\"/lw/8fb/why_an_intelligence_explosion_might_be_a/\">Why an Intelligence Explosion might be a Low-Priority Global Risk </a></p>\n<p>[6] <em>\"\u2026please bear in mind that the relation of Solomonoff induction and \u201cUniversal AI\u201d to real-world general intelligence of any kind is also rather wildly speculative\u2026 This stuff is beautiful math, but does it really have anything to do with real-world intelligence? These theories have little to say about human intelligence, and they\u2019re not directly useful as foundations for building AGI systems (though, admittedly, a handful of scientists are working on \u201cscaling them down\u201d to make them realistic; so far this only works for very simple toy problems, and it\u2019s hard to see how to extend the approach broadly to yield anything near human-level AGI). And it\u2019s not clear they will be applicable to future superintelligent minds either, as these minds may be best conceived using radically different concepts.\"</em></p>\n<p>Ben Goertzel, '<a href=\"http://multiverseaccordingtoben.blogspot.com/2012/03/prediction-and-intelligence.html\">Are Prediction and Reward Relevant to Superintelligences?</a>'</p>\n<p>[7] <a href=\"http://public.web.cern.ch/public/en/spotlight/SpotlightAandD-en.html\">http://public.web.cern.ch/public/en/spotlight/SpotlightAandD-en.html</a></p>\n<p>[8] <em>\"If any increase in intelligence is vastly outweighed by its computational cost and the expenditure of time needed to discover it then it might not be instrumental for a perfectly rational agent (such as an artificial general intelligence), as <a title=\"Rational Agent\" href=\"http://xixidu.tumblr.com/post/7648180962/what-game-theorists-somewhat-disturbingly-call\">imagined by game theorists</a>, to increase its intelligence as opposed to using its existing intelligence to pursue its terminal goals directly or to invest its given resources to acquire other means of self-improvement, e.g. more efficient sensors.\"</em></p>\n<p>Alexander Kruel, <a href=\"/lw/8fb/why_an_intelligence_explosion_might_be_a/\">Why an Intelligence Explosion might be a Low-Priority Global Risk </a></p>\n<p>[9] Section <em>'Necessary resources for an intelligence explosion'</em>, <a href=\"/lw/8fb/why_an_intelligence_explosion_might_be_a/\">Why an Intelligence Explosion might be a Low-Priority Global Risk</a>, Alexander Kruel</p>\n<p>[10] <a href=\"/lw/3aa/friendly_ai_research_and_taskification/\">http://lesswrong.com/lw/3aa/friendly_ai_research_and_taskification/</a></p>\n<p>[11] <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/5ylx\">http://lesswrong.com/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/5ylx</a></p>\n<p>[12] <a href=\"/lw/8c3/qa_with_new_executive_director_of_singularity/5y77\">http://lesswrong.com/lw/8c3/qa_with_new_executive_director_of_singularity/5y77</a></p>\n<p>[13] <em>\"I think that if you're aiming to develop knowledge that won't be useful until very very far in the future, you're probably wasting your time, if for no other reason than this: by the time your knowledge is relevant, someone will probably have developed a tool (such as a narrow AI) so much more efficient in generating this knowledge that it renders your work moot.\"</em></p>\n<p>Holden Karnofsky in a <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F91e%2Fsingularity_institute_100000_endofyear_fundraiser%2F5j9o%3Fcontext%3D1%235j9o&amp;v=1&amp;libid=1331638223097&amp;out=http%3A%2F%2Fkruel.co%2FJaanTallinn201105.pdf&amp;title=juliawise%20comments%20on%20Singularity%20Institute%20%24100%2C000%20end-of-year%20fundraiser%20only%2020%25%20filled%20so%20far%20-%20Less%20Wrong&amp;txt=talked%20to%20Jaan%20Tallinn&amp;jsonp=vglnk_jsonp_13316414389702\">conversation</a> with Jaan Tallinn</p>", "sections": [{"title": "Please review the categories 'Further Reading' and 'Notes and References'. ", "anchor": "Please_review_the_categories__Further_Reading__and__Notes_and_References___", "level": 2}, {"title": "Contents", "anchor": "Contents", "level": 2}, {"title": "Abstract", "anchor": "Abstract", "level": 1}, {"title": "Requirements for an Intelligence Explosion", "anchor": "Requirements_for_an_Intelligence_Explosion", "level": 1}, {"title": "P(FOOM) = P(P1\u2227P2\u2227P3\u2227P4\u2227P5)", "anchor": "P_FOOM____P_P1_P2_P3_P4_P5_", "level": 2}, {"title": "Requirements for SIAI to constitute an optimal charity", "anchor": "Requirements_for_SIAI_to_constitute_an_optimal_charity", "level": 1}, {"title": "P(CHARITY) = P(P6\u2227P7\u2227P8\u2227P9)", "anchor": "P_CHARITY____P_P6_P7_P8_P9_", "level": 2}, {"title": "Further Reading", "anchor": "Further_Reading", "level": 1}, {"title": "Notes and References", "anchor": "Notes_and_References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "127 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 127, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AAG7vj5cyNZ2Gxtnk", "j52uErqofDiJZCo76", "ZbgQ3HtPu2cLKGmy8", "a5JAiTdytou3Jg749", "No5JpRCHzBrWA4jmS", "7sxR8qLG6rqDht6aR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-13T18:23:55.432Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - Decision Theory", "slug": "meetup-west-la-meetup-decision-theory", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dBxypayjXXM5EGfPr/meetup-west-la-meetup-decision-theory", "pageUrlRelative": "/posts/dBxypayjXXM5EGfPr/meetup-west-la-meetup-decision-theory", "linkUrl": "https://www.lesswrong.com/posts/dBxypayjXXM5EGfPr/meetup-west-la-meetup-decision-theory", "postedAtFormatted": "Tuesday, March 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20Decision%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20Decision%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdBxypayjXXM5EGfPr%2Fmeetup-west-la-meetup-decision-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20Decision%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdBxypayjXXM5EGfPr%2Fmeetup-west-la-meetup-decision-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdBxypayjXXM5EGfPr%2Fmeetup-west-la-meetup-decision-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 145, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/7y'>West LA Meetup - Decision Theory</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 March 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, March 14th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> The topic du jour is Decision Theory. Take a look at this <a href=\"http://lesswrong.com/lw/aq9/decision_theories_a_less_wrong_primer/\">introductory post</a>. It's also jam-packed with links to more advanced posts on the topic.</p>\n\n<p>Don't worry if you don't have time to read anything, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/7y'>West LA Meetup - Decision Theory</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dBxypayjXXM5EGfPr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.650143532286797e-07, "legacy": true, "legacyId": "13981", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Decision_Theory\">Discussion article for the meetup : <a href=\"/meetups/7y\">West LA Meetup - Decision Theory</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 March 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, March 14th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> The topic du jour is Decision Theory. Take a look at this <a href=\"http://lesswrong.com/lw/aq9/decision_theories_a_less_wrong_primer/\">introductory post</a>. It's also jam-packed with links to more advanced posts on the topic.</p>\n\n<p>Don't worry if you don't have time to read anything, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Decision_Theory1\">Discussion article for the meetup : <a href=\"/meetups/7y\">West LA Meetup - Decision Theory</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - Decision Theory", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Decision_Theory", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - Decision Theory", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Decision_Theory1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["af9MjBqF2hgu3EN6r"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-13T23:31:51.795Z", "modifiedAt": null, "url": null, "title": "Decision Theories: A Less Wrong Primer", "slug": "decision-theories-a-less-wrong-primer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:56.925Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/af9MjBqF2hgu3EN6r/decision-theories-a-less-wrong-primer", "pageUrlRelative": "/posts/af9MjBqF2hgu3EN6r/decision-theories-a-less-wrong-primer", "linkUrl": "https://www.lesswrong.com/posts/af9MjBqF2hgu3EN6r/decision-theories-a-less-wrong-primer", "postedAtFormatted": "Tuesday, March 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Decision%20Theories%3A%20A%20Less%20Wrong%20Primer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecision%20Theories%3A%20A%20Less%20Wrong%20Primer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Faf9MjBqF2hgu3EN6r%2Fdecision-theories-a-less-wrong-primer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Decision%20Theories%3A%20A%20Less%20Wrong%20Primer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Faf9MjBqF2hgu3EN6r%2Fdecision-theories-a-less-wrong-primer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Faf9MjBqF2hgu3EN6r%2Fdecision-theories-a-less-wrong-primer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2832, "htmlBody": "<p><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_aq9_0.png?v=bcf58528098c2fc6111d6a91e2a672f1\" alt=\"Alpha-beta pruning (from Wikipedia)\" width=\"640\" height=\"325\" /></p>\n<p><strong>Summary:</strong> <em>If you've been wondering why people keep going on about decision theory on Less Wrong, I wrote you this post as an answer. I explain what decision theories are, show how Causal Decision Theory works and where it seems to give the wrong answers, introduce (very briefly) some candidates for a more advanced decision theory, and touch on the (possible) connection between decision theory and ethics.</em></p>\n<h2><a id=\"more\"></a>What is a decision theory?</h2>\n<p>This is going to sound silly, but a decision theory is an algorithm for making decisions.<a href=\"#naming\" target=\"_self\"><sup>0</sup></a> The inputs are an agent's knowledge of the world, and the agent's goals and values; the output is a particular action (or plan of actions). Actually, in many cases the goals and values are implicit in the algorithm rather than given as input, but it's worth keeping them distinct in theory.</p>\n<p>For example, we can think of a chess program as a simple decision theory. If you feed it the current state of the board, it returns a move, which advances the implicit goal of winning. The actual details of the decision theory include things like writing out the tree of possible moves and countermoves, and evaluating which possibilities bring it closer to winning.</p>\n<p>Another example is an <em>E. Coli</em> bacterium. It has <a href=\"http://en.wikipedia.org/wiki/Chemotaxis#Bacterial_chemotaxis\" target=\"_blank\">two basic options at every moment</a>: it can use its flagella to swim forward in a straight line, or to change directions by randomly tumbling. It can sense whether the concentration of food or toxin is increasing or decreasing over time, and so it executes a simple algorithm that randomly changes direction more often when things are \"getting worse\". This is enough control for bacteria to rapidly seek out food and flee from toxins, without needing any sort of advanced information processing.</p>\n<p>A human being is a much more complicated example which combines some aspects of the two simpler examples; we <a href=\"/lw/l2/protein_reinforcement_and_dna_consequentialism/\" target=\"_blank\">mentally model consequences in order to make many decisions</a>, and we also follow <a href=\"/lw/ng/words_as_hidden_inferences/\" target=\"_blank\">heuristics that have evolved to work well</a> without explicitly modeling the world.<a href=\"#brains\" target=\"_self\"><sup>1</sup></a> We can't model anything quite like the complicated way that human beings make decisions, but we can study simple decision theories on simple problems; and the <a href=\"http://en.wikipedia.org/wiki/Game_theory\" target=\"_blank\">results of this analysis</a> were often more effective than the raw intuitions of human beings (who evolved to succeed in small savannah tribes, not <a href=\"http://en.wikipedia.org/wiki/Thomas_Schelling\" target=\"_blank\">negotiate a nuclear arms race</a>). But the standard model used for this analysis, Causal Decision Theory, has a serious drawback of its own, and the suggested replacements are important for a number of things that Less Wrong readers might care about.</p>\n<h2>What is Causal Decision Theory?</h2>\n<p><a href=\"http://en.wikipedia.org/wiki/Causal_decision_theory\" target=\"_blank\">Causal decision theory</a> (CDT to all the cool kids) is a particular class of decision theories with some nice properties. It's straightforward to state, has some nice mathematical features, can be adapted to any utility function, and gives good answers on many problems. We'll describe how it works in a fairly simple but general setup.</p>\n<p>Let <strong>X</strong> be an agent who shares a world with some other agents (<strong>Y<sub>1</sub></strong> through <strong>Y<sub>n</sub></strong>). All these agents are going to privately choose actions and then perform them simultaneously, and the actions will have consequences. (For instance, they could be playing a round of <a href=\"http://en.wikipedia.org/wiki/Diplomacy_%28game%29\" target=\"_blank\">Diplomacy</a>.)</p>\n<p>We'll assume that <strong>X</strong> has goals and values represented by a utility function: for every consequence <strong>C</strong>, there's a number <strong>U(C)</strong> representing just how much <strong>X</strong> prefers that outcome, and <strong>X</strong> views equal <em>expected</em> utilities with indifference: a 50% chance of utility 0 and 50% chance of utility 10 is no better or worse than 100% chance of utility 5, for instance. (If these assumptions sound artificial, remember that we're trying to make this as mathematically simple as we can in order to analyze it. I don't think it's as artificial as it seems, but <a href=\"/lw/n9/the_intuitions_behind_utilitarianism/\" target=\"_blank\">that's a different topic</a>.)</p>\n<p><strong>X</strong> wants to maximize its expected utility. If there were no other agents, this would be simple: model the world, estimate how likely each consequence is to happen if it does this action or that, calculate the expected utility of each action, then perform the action that results in the highest expected utility. But if there are other agents around, the outcomes depend on their actions as well as on <strong>X</strong>'s action, and if <strong>X</strong> treats <em>that</em> uncertainty like normal uncertainty, then there might be an opportunity for the <strong>Y</strong>s to exploit <strong>X</strong>.</p>\n<p>This is a Difficult Problem in general; a full discussion would involve <a href=\"http://en.wikipedia.org/wiki/Nash_equilibrium\">Nash equilibria</a>, but even that doesn't fully settle the matter- there can be more than one equilibrium! Also, <strong>X</strong> <em>can</em> sometimes treat another agent as predictable (like a fixed outcome or an ordinary random variable) and get away with it.</p>\n<p>CDT is a <em>class</em> of decision theories, not a specific decision theory, so it's impossible to specify with full generality how <strong>X</strong> will decide if <strong>X</strong> is a causal decision theorist. But there is one key property that distinguishes CDT from the decision theories we'll talk about later: a CDT agent assumes that <strong>X</strong>'s decision is <em>independent</em> from the simultaneous decisions of the <strong>Y</strong>s- that is, <strong>X</strong> could decide one way or another and everyone else's decisions would stay the same.</p>\n<p>Therefore, there is at least one case where we can say what a CDT agent will do in a multi-player game: some <a href=\"http://en.wikipedia.org/wiki/Strategic_dominance\">strategies are dominated by others</a>. For example, if <strong>X</strong> and <strong>Y</strong> are both deciding whether to walk to the zoo, and <strong>X</strong> will be happiest if <strong>X</strong> and <strong>Y</strong> both go, but <strong>X</strong> would still be happier at the zoo than at home even if <strong>Y</strong> doesn't come along, then <strong>X</strong> should go to the zoo regardless of what <strong>Y</strong> does. (Presuming that <strong>X</strong>'s utility function is focused on being happy that afternoon.) This criterion is enough to \"solve\" many problems for a CDT agent, and in zero-sum two-player games the solution can be shown to be optimal for <strong>X</strong>.</p>\n<h2>What's the problem with Causal Decision Theory?</h2>\n<p>There are many simplifications and abstractions involved in CDT, but that assumption of independence turns out to be key. In practice, people put a lot of effort into predicting what other people might decide, sometimes with impressive accuracy, and then base their own decisions on that prediction. This wrecks the independence of decisions, and so it turns out that in a non-zero-sum game, it's possible to \"beat\" the outcome that CDT gets.</p>\n<p>The classical thought experiment in this context is called <a href=\"http://wiki.lesswrong.com/wiki/Newcomb%27s_problem\">Newcomb's Problem</a>. <strong>X</strong> meets with a very smart and honest alien, Omega, that has the power to accurately predict what <strong>X</strong> would do in various hypothetical situations. Omega presents <strong>X</strong> with two boxes, a clear one containing $1,000 and an opaque one containing either $1,000,000 or nothing. Omega explains that <strong>X</strong> can either take the opaque box (this is called <em>one-boxing</em>) or both boxes (<em>two-boxing</em>), but there's a trick: Omega predicted in advance what <strong>X</strong> would do, and put $1,000,000 into the opaque box only if <strong>X</strong> was predicted to one-box. (This is a little devious, so take some time to ponder it if you haven't seen Newcomb's Problem before- or <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\" target=\"_blank\">read here for a fuller explanation</a>.)</p>\n<p>If <strong>X</strong> is a causal decision theorist, the choice is clear: whatever Omega decided, it decided already, and whether the opaque box is full or empty, <strong>X</strong> is better off taking both. (That is, two-boxing is a dominant strategy over one-boxing.) So <strong>X</strong> two-boxes, and walks away with $1,000 (since Omega easily predicted that this would happen). Meanwhile, <strong>X</strong>'s cousin <strong>Z</strong> (not a CDT) decides to one-box, and finds the box full with $1,000,000. So it certainly seems that one could do better than CDT in this case.</p>\n<p>But is this a fair problem? After all, we can always come up with problems that trick the rational agent into making the wrong choice, while a dumber agent lucks into the right one. Having a very powerful predictor around might seem artificial, although the problem might look much the same if Omega had a 90% success rate rather than 100%. <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\" target=\"_blank\">One reason that this is a fair problem is that the outcome depends only on what action <strong>X</strong> is simulated to take, not on what process produced the decision.</a></p>\n<p>Besides, we can see the same behavior in another famous game theory problem: the <a href=\"http://wiki.lesswrong.com/wiki/Prisoner%27s_dilemma\">Prisoner's Dilemma</a>.<strong> X</strong> and <strong>Y</strong> are collaborating on a project, but they have different goals for it, and either one has the opportunity to achieve their goal a little better at the cost of significantly impeding their partner's goal. (The options are called <em>cooperation</em> and <em>defection</em>.) If they both cooperate, they get a utility of +50 each; if <strong>X</strong> cooperates and <strong>Y</strong> defects, then <strong>X</strong> winds up at +10 but <strong>Y</strong> gets +70, and vice versa; but if they both defect, then both wind up at +30 each.<a href=\"#PD\" target=\"_self\"><sup>2</sup></a></p>\n<p>If <strong>X</strong> is a CDT agent, then defecting dominates cooperating as a strategy, so <strong>X</strong> will always defect in the Prisoner's Dilemma (as long as there are no further ramifications; the Iterated Prisoner's Dilemma can be different, because <strong>X</strong>'s <em>current</em> decision can influence <strong>Y</strong>'s <em>future</em> decisions). Even if you knowingly pair up <strong>X</strong> with a copy of itself (with a different goal but the same decision theory), it will defect even though it could prove that the two decisions will be identical.</p>\n<p>Meanwhile, its cousin <strong>Z</strong> also plays the Prisoner's Dilemma: <strong>Z</strong> cooperates when it's facing an agent that has the same decision theory, and defects otherwise. This is a strictly better outcome than <strong>X</strong> gets. (<strong>Z</strong> isn't optimal, though; I'm just showing that you can find a strict improvement on <strong>X</strong>.)<a href=\"#EDT\" target=\"_self\"><sup>3</sup></a></p>\n<h2><a name=\"dobetter\"></a>What decision theories are better than CDT?</h2>\n<p>I realize this post is pretty long already, but it's way too short to outline the advanced <a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\" target=\"_blank\">decision theories</a> that have been proposed and developed recently by a number of people (including Eliezer, Gary Drescher, Wei Dai, Vladimir Nesov and Vladimir Slepnev). Instead, I'll list the features that we would want an advanced decision theory to have:</p>\n<ol>\n<li>The decision theory should be formalizable at least as well as CDT is.</li>\n<li>The decision theory should give answers that are at least as good as CDT's answers. In particular, it should always get the right answer in 1-player games and find a Nash equilibrium in zero-sum two-player games (when the other player is also able to do so).</li>\n<li>The decision theory should strictly outperform CDT on the Prisoner's Dilemma- it should elicit mutual cooperation in the Prisoner's Dilemma from some agents that CDT elicits mutual defection from, it shouldn't cooperate when its partner defects, and (arguably) it should defect if its partner would cooperate regardless.</li>\n<li>The decision theory should one-box on Newcomb's Problem.</li>\n<li>The decision theory should be reasonably simple, and not include a bunch of ad-hoc rules. We want to solve problems involving prediction of actions in general, not just the special cases.</li>\n</ol>\n<p>There are now a couple of candidate decision theories (<a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">Timeless Decision Theory</a>, <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">Updateless Decision Theory</a>, and <a href=\"http://wiki.lesswrong.com/wiki/Ambient_decision_theory\">Ambient Decision Theory</a>) which seem to meet these criteria. Interestingly, formalizing any of these tends to deeply involve the mathematics of self-reference (<a href=\"http://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems\" target=\"_blank\">G&ouml;del's Theorem</a> and <a href=\"/lw/t6/the_cartoon_guide_to_l%C3%B6bs_theorem/\" target=\"_blank\">L&ouml;b's Theorem</a>) in order to avoid the infinite regress inherent in simulating an agent that's simulating you.</p>\n<p>But for the time being, we can massively oversimplify and outline them. <a href=\"/lw/15z/ingredients_of_timeless_decision_theory\" target=\"_blank\">TDT</a> considers your ultimate decision as the cause of both your action and other agents' valid predictions of your action, and tries to pick the decision that works best under that model. <a href=\"/lw/ap3/predictability_of_decisions_and_the_diagonal\" target=\"_blank\">ADT</a> uses a kind of diagonalization to predict the effects of different decisions without having the final decision throw off the prediction. And <a href=\"/lw/3l/counterfactual_mugging/\" target=\"_blank\">UDT</a> considers the decision that would be the best policy for all possible versions of you to employ, on average.</p>\n<h2>Why are advanced decision theories important for Less Wrong?</h2>\n<p>There are a few reasons. Firstly, there are those who think that advanced decision theories are a natural base on which to build AI. One reason for this is something I briefly mentioned: even CDT allows for the idea that <strong>X</strong>'s current decisions can affect <strong>Y</strong>'s future decisions, and self-modification counts as a decision. If <strong>X</strong> can self-modify, and if <strong>X</strong> expects to deal with situations where an advanced decision theory would out-perform its current self, then <strong>X</strong> will change itself into an advanced decision theory (with some weird caveats: for example, if <strong>X</strong> started out as CDT, its modification will only care about other agents' decisions made after <strong>X</strong> self-modified).</p>\n<p>More relevantly to rationalists, the bad choices that CDT makes are often held up as examples of <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">why you shouldn't try to be rational</a>, or <a href=\"/lw/5f/bayesians_vs_barbarians/\">why rationalists can't cooperate</a>. But instrumental rationality doesn't need to be synonymous with causal decision theory: if there are other decision theories that do strictly better, <a href=\"http://wiki.lesswrong.com/wiki/Rationality_is_systematized_winning\">we should adopt those rather than CDT</a>! So figuring out advanced decision theories, even if we can't implement them on real-world problems, helps us see that the ideal of rationality isn't going to fall flat on its face.</p>\n<p>Finally, advanced decision theory could be relevant to morality. If, as many of us suspect, there's no basis for human morality apart from what goes on in human brains, then <a href=\"/lw/rx/is_morality_preference/\" target=\"_blank\">why do we feel there's still a distinction between what-we-want and what-is-right?</a> One answer is that if we feed in what-we-want into an advanced decision theory, then just as cooperation emerges in the Prisoner's Dilemma, many kinds of patterns that we take as basic moral rules emerge as the equilibrium behavior. The idea is developed more substantially in Gary Drescher's <a href=\"http://mitpress.mit.edu/catalog/item/default.asp?tid=10902&amp;ttype=2\" target=\"_blank\">Good and Real</a>, and (before there was a candidate for an advanced decision theory) in <a href=\"http://www.gwern.net/docs/1985-hofstadter\" target=\"_blank\">Douglas Hofstadter's concept</a> of <a href=\"http://en.wikipedia.org/wiki/Superrationality\" target=\"_blank\">superrationality</a>.</p>\n<p>It's still at the speculative stage, because it's difficult to work out what interactions between agents with advanced decision theories would look like (in particular, we don't know whether bargaining would end in a fair split or in a <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/GambitPileup\" target=\"_blank\">Xanatos Gambit Pileup</a> of <a href=\"http://en.wikipedia.org/wiki/Chicken_%28game%29\" target=\"_blank\">chicken</a> threats, though we think and hope it's the former). But it's at least a promising approach to the <a href=\"/lw/re/grasping_slippery_things/\">slippery question</a> of <a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">what 'right' could actually mean</a>.</p>\n<p>And if you want to understand this on a slightly more technical level... well, I've started a sequence.</p>\n<p><strong>Next:</strong> <a href=\"/lw/axl/decision_theories_a_semiformal_analysis_part_i/\" target=\"_self\">A Semi-Formal Analysis, Part I (The Problem with Naive Decision Theory)</a></p>\n<h3>Notes:<br /></h3>\n<p><a name=\"naming\"></a><strong>0.</strong> Rather confusingly, <a href=\"http://en.wikipedia.org/wiki/Decision_theory\" target=\"_blank\">decision theory</a> is the name for the study of decision theories.</p>\n<p><a name=\"brains\"></a><strong>1.</strong> Both patterns appear in our conscious reasoning as well as our subconscious thinking- we care about consequences we can directly foresee and also about moral rules that don't seem attached to any particular consequence. However, just as the simple \"program\" for the bacterium was constructed by evolution, our <a href=\"/lw/51f/guilt_another_gift_nobody_wants/\" target=\"_blank\">moral rules are there for evolutionary reasons as well</a>- perhaps even for <a href=\"/lw/2ls/morality_as_parfitianfiltered_decision_theory/\" target=\"_blank\">reasons that have to do with advanced decision theory...</a></p>\n<p>Also, it's worth noting that we're <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\" target=\"_blank\">not consciously aware of all of our values and goals</a>, though at least we have a better idea of them than <em>E.Coli</em> does. This is a problem for the idea of representing our usual decisions in terms of decision theory, though we can still hope that our approximations are good enough (e.g. that our real values regarding the Cold War roughly corresponded to our estimates of how bad a nuclear war or a Soviet world takeover would be).</p>\n<p><a name=\"PD\"></a><strong>2.</strong> Eliezer <a href=\"/lw/tn/the_true_prisoners_dilemma/\" target=\"_blank\">once pointed out</a> that our intuitions on most formulations of the Prisoner's Dilemma are skewed by our notions of fairness, and a more outlandish example might serve better to illustrate how a genuine PD really feels. For an example where people are notorious for not caring about each others' goals, let's consider aesthetics: people who love one form of music often really feel that another popular form is a waste of time. One might feel that if the works of Artist Q suddenly disappeared from the world, it would objectively be a tragedy; while if the same happened to the works of Artist R, then it's no big deal and R's fans should be glad to be freed from that dreck.</p>\n<p>We can use this aesthetic intolerance to construct a more genuine Prisoner's Dilemma without inviting aliens or anything like that. Say <strong>X</strong> is a writer and <strong>Y</strong> is an illustrator, and they have very different preferences for how a certain scene should come across, so they've worked out a compromise. Now, both of them could cooperate and get a scene that both are OK with, or <strong>X</strong> could secretly change the dialogue in hopes of getting his idea to come across, or <strong>Y</strong> could draw the scene differently in order to get her idea of the scene across. But if they both \"defect\" from the compromise, then the scene gets confusing to readers. If both <strong>X</strong> and <strong>Y</strong> prefer their own idea to the compromise, prefer the compromise to the muddle, and prefer the muddle to their partner's idea, then this is a genuine Prisoner's Dilemma.</p>\n<p><a name=\"EDT\"></a><strong>3.</strong> I've avoided mentioning <a href=\"http://en.wikipedia.org/wiki/Evidential_decision_theory\" target=\"_blank\">Evidential Decision Theory</a>, the \"usual\" counterpart to CDT; it's worth noting that EDT one-boxes on Newcomb's Problem but gives the wrong answer on a classical one-player problem (<a href=\"http://wiki.lesswrong.com/wiki/Smoking_lesion\" target=\"_blank\">The Smoking Lesion</a>) which the advanced decision theories handle correctly. It's also far less amenable to formalization than the others.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 9, "5f5c37ee1b5cdee568cfb28e": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "af9MjBqF2hgu3EN6r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 83, "baseScore": 105, "extendedScore": null, "score": 0.000213, "legacy": true, "legacyId": "13905", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 105, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_aq9_0.png?v=bcf58528098c2fc6111d6a91e2a672f1\" alt=\"Alpha-beta pruning (from Wikipedia)\" width=\"640\" height=\"325\"></p>\n<p><strong>Summary:</strong> <em>If you've been wondering why people keep going on about decision theory on Less Wrong, I wrote you this post as an answer. I explain what decision theories are, show how Causal Decision Theory works and where it seems to give the wrong answers, introduce (very briefly) some candidates for a more advanced decision theory, and touch on the (possible) connection between decision theory and ethics.</em></p>\n<h2 id=\"What_is_a_decision_theory_\"><a id=\"more\"></a>What is a decision theory?</h2>\n<p>This is going to sound silly, but a decision theory is an algorithm for making decisions.<a href=\"#naming\" target=\"_self\"><sup>0</sup></a> The inputs are an agent's knowledge of the world, and the agent's goals and values; the output is a particular action (or plan of actions). Actually, in many cases the goals and values are implicit in the algorithm rather than given as input, but it's worth keeping them distinct in theory.</p>\n<p>For example, we can think of a chess program as a simple decision theory. If you feed it the current state of the board, it returns a move, which advances the implicit goal of winning. The actual details of the decision theory include things like writing out the tree of possible moves and countermoves, and evaluating which possibilities bring it closer to winning.</p>\n<p>Another example is an <em>E. Coli</em> bacterium. It has <a href=\"http://en.wikipedia.org/wiki/Chemotaxis#Bacterial_chemotaxis\" target=\"_blank\">two basic options at every moment</a>: it can use its flagella to swim forward in a straight line, or to change directions by randomly tumbling. It can sense whether the concentration of food or toxin is increasing or decreasing over time, and so it executes a simple algorithm that randomly changes direction more often when things are \"getting worse\". This is enough control for bacteria to rapidly seek out food and flee from toxins, without needing any sort of advanced information processing.</p>\n<p>A human being is a much more complicated example which combines some aspects of the two simpler examples; we <a href=\"/lw/l2/protein_reinforcement_and_dna_consequentialism/\" target=\"_blank\">mentally model consequences in order to make many decisions</a>, and we also follow <a href=\"/lw/ng/words_as_hidden_inferences/\" target=\"_blank\">heuristics that have evolved to work well</a> without explicitly modeling the world.<a href=\"#brains\" target=\"_self\"><sup>1</sup></a> We can't model anything quite like the complicated way that human beings make decisions, but we can study simple decision theories on simple problems; and the <a href=\"http://en.wikipedia.org/wiki/Game_theory\" target=\"_blank\">results of this analysis</a> were often more effective than the raw intuitions of human beings (who evolved to succeed in small savannah tribes, not <a href=\"http://en.wikipedia.org/wiki/Thomas_Schelling\" target=\"_blank\">negotiate a nuclear arms race</a>). But the standard model used for this analysis, Causal Decision Theory, has a serious drawback of its own, and the suggested replacements are important for a number of things that Less Wrong readers might care about.</p>\n<h2 id=\"What_is_Causal_Decision_Theory_\">What is Causal Decision Theory?</h2>\n<p><a href=\"http://en.wikipedia.org/wiki/Causal_decision_theory\" target=\"_blank\">Causal decision theory</a> (CDT to all the cool kids) is a particular class of decision theories with some nice properties. It's straightforward to state, has some nice mathematical features, can be adapted to any utility function, and gives good answers on many problems. We'll describe how it works in a fairly simple but general setup.</p>\n<p>Let <strong>X</strong> be an agent who shares a world with some other agents (<strong>Y<sub>1</sub></strong> through <strong>Y<sub>n</sub></strong>). All these agents are going to privately choose actions and then perform them simultaneously, and the actions will have consequences. (For instance, they could be playing a round of <a href=\"http://en.wikipedia.org/wiki/Diplomacy_%28game%29\" target=\"_blank\">Diplomacy</a>.)</p>\n<p>We'll assume that <strong>X</strong> has goals and values represented by a utility function: for every consequence <strong>C</strong>, there's a number <strong>U(C)</strong> representing just how much <strong>X</strong> prefers that outcome, and <strong>X</strong> views equal <em>expected</em> utilities with indifference: a 50% chance of utility 0 and 50% chance of utility 10 is no better or worse than 100% chance of utility 5, for instance. (If these assumptions sound artificial, remember that we're trying to make this as mathematically simple as we can in order to analyze it. I don't think it's as artificial as it seems, but <a href=\"/lw/n9/the_intuitions_behind_utilitarianism/\" target=\"_blank\">that's a different topic</a>.)</p>\n<p><strong>X</strong> wants to maximize its expected utility. If there were no other agents, this would be simple: model the world, estimate how likely each consequence is to happen if it does this action or that, calculate the expected utility of each action, then perform the action that results in the highest expected utility. But if there are other agents around, the outcomes depend on their actions as well as on <strong>X</strong>'s action, and if <strong>X</strong> treats <em>that</em> uncertainty like normal uncertainty, then there might be an opportunity for the <strong>Y</strong>s to exploit <strong>X</strong>.</p>\n<p>This is a Difficult Problem in general; a full discussion would involve <a href=\"http://en.wikipedia.org/wiki/Nash_equilibrium\">Nash equilibria</a>, but even that doesn't fully settle the matter- there can be more than one equilibrium! Also, <strong>X</strong> <em>can</em> sometimes treat another agent as predictable (like a fixed outcome or an ordinary random variable) and get away with it.</p>\n<p>CDT is a <em>class</em> of decision theories, not a specific decision theory, so it's impossible to specify with full generality how <strong>X</strong> will decide if <strong>X</strong> is a causal decision theorist. But there is one key property that distinguishes CDT from the decision theories we'll talk about later: a CDT agent assumes that <strong>X</strong>'s decision is <em>independent</em> from the simultaneous decisions of the <strong>Y</strong>s- that is, <strong>X</strong> could decide one way or another and everyone else's decisions would stay the same.</p>\n<p>Therefore, there is at least one case where we can say what a CDT agent will do in a multi-player game: some <a href=\"http://en.wikipedia.org/wiki/Strategic_dominance\">strategies are dominated by others</a>. For example, if <strong>X</strong> and <strong>Y</strong> are both deciding whether to walk to the zoo, and <strong>X</strong> will be happiest if <strong>X</strong> and <strong>Y</strong> both go, but <strong>X</strong> would still be happier at the zoo than at home even if <strong>Y</strong> doesn't come along, then <strong>X</strong> should go to the zoo regardless of what <strong>Y</strong> does. (Presuming that <strong>X</strong>'s utility function is focused on being happy that afternoon.) This criterion is enough to \"solve\" many problems for a CDT agent, and in zero-sum two-player games the solution can be shown to be optimal for <strong>X</strong>.</p>\n<h2 id=\"What_s_the_problem_with_Causal_Decision_Theory_\">What's the problem with Causal Decision Theory?</h2>\n<p>There are many simplifications and abstractions involved in CDT, but that assumption of independence turns out to be key. In practice, people put a lot of effort into predicting what other people might decide, sometimes with impressive accuracy, and then base their own decisions on that prediction. This wrecks the independence of decisions, and so it turns out that in a non-zero-sum game, it's possible to \"beat\" the outcome that CDT gets.</p>\n<p>The classical thought experiment in this context is called <a href=\"http://wiki.lesswrong.com/wiki/Newcomb%27s_problem\">Newcomb's Problem</a>. <strong>X</strong> meets with a very smart and honest alien, Omega, that has the power to accurately predict what <strong>X</strong> would do in various hypothetical situations. Omega presents <strong>X</strong> with two boxes, a clear one containing $1,000 and an opaque one containing either $1,000,000 or nothing. Omega explains that <strong>X</strong> can either take the opaque box (this is called <em>one-boxing</em>) or both boxes (<em>two-boxing</em>), but there's a trick: Omega predicted in advance what <strong>X</strong> would do, and put $1,000,000 into the opaque box only if <strong>X</strong> was predicted to one-box. (This is a little devious, so take some time to ponder it if you haven't seen Newcomb's Problem before- or <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\" target=\"_blank\">read here for a fuller explanation</a>.)</p>\n<p>If <strong>X</strong> is a causal decision theorist, the choice is clear: whatever Omega decided, it decided already, and whether the opaque box is full or empty, <strong>X</strong> is better off taking both. (That is, two-boxing is a dominant strategy over one-boxing.) So <strong>X</strong> two-boxes, and walks away with $1,000 (since Omega easily predicted that this would happen). Meanwhile, <strong>X</strong>'s cousin <strong>Z</strong> (not a CDT) decides to one-box, and finds the box full with $1,000,000. So it certainly seems that one could do better than CDT in this case.</p>\n<p>But is this a fair problem? After all, we can always come up with problems that trick the rational agent into making the wrong choice, while a dumber agent lucks into the right one. Having a very powerful predictor around might seem artificial, although the problem might look much the same if Omega had a 90% success rate rather than 100%. <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\" target=\"_blank\">One reason that this is a fair problem is that the outcome depends only on what action <strong>X</strong> is simulated to take, not on what process produced the decision.</a></p>\n<p>Besides, we can see the same behavior in another famous game theory problem: the <a href=\"http://wiki.lesswrong.com/wiki/Prisoner%27s_dilemma\">Prisoner's Dilemma</a>.<strong> X</strong> and <strong>Y</strong> are collaborating on a project, but they have different goals for it, and either one has the opportunity to achieve their goal a little better at the cost of significantly impeding their partner's goal. (The options are called <em>cooperation</em> and <em>defection</em>.) If they both cooperate, they get a utility of +50 each; if <strong>X</strong> cooperates and <strong>Y</strong> defects, then <strong>X</strong> winds up at +10 but <strong>Y</strong> gets +70, and vice versa; but if they both defect, then both wind up at +30 each.<a href=\"#PD\" target=\"_self\"><sup>2</sup></a></p>\n<p>If <strong>X</strong> is a CDT agent, then defecting dominates cooperating as a strategy, so <strong>X</strong> will always defect in the Prisoner's Dilemma (as long as there are no further ramifications; the Iterated Prisoner's Dilemma can be different, because <strong>X</strong>'s <em>current</em> decision can influence <strong>Y</strong>'s <em>future</em> decisions). Even if you knowingly pair up <strong>X</strong> with a copy of itself (with a different goal but the same decision theory), it will defect even though it could prove that the two decisions will be identical.</p>\n<p>Meanwhile, its cousin <strong>Z</strong> also plays the Prisoner's Dilemma: <strong>Z</strong> cooperates when it's facing an agent that has the same decision theory, and defects otherwise. This is a strictly better outcome than <strong>X</strong> gets. (<strong>Z</strong> isn't optimal, though; I'm just showing that you can find a strict improvement on <strong>X</strong>.)<a href=\"#EDT\" target=\"_self\"><sup>3</sup></a></p>\n<h2 id=\"What_decision_theories_are_better_than_CDT_\"><a name=\"dobetter\"></a>What decision theories are better than CDT?</h2>\n<p>I realize this post is pretty long already, but it's way too short to outline the advanced <a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\" target=\"_blank\">decision theories</a> that have been proposed and developed recently by a number of people (including Eliezer, Gary Drescher, Wei Dai, Vladimir Nesov and Vladimir Slepnev). Instead, I'll list the features that we would want an advanced decision theory to have:</p>\n<ol>\n<li>The decision theory should be formalizable at least as well as CDT is.</li>\n<li>The decision theory should give answers that are at least as good as CDT's answers. In particular, it should always get the right answer in 1-player games and find a Nash equilibrium in zero-sum two-player games (when the other player is also able to do so).</li>\n<li>The decision theory should strictly outperform CDT on the Prisoner's Dilemma- it should elicit mutual cooperation in the Prisoner's Dilemma from some agents that CDT elicits mutual defection from, it shouldn't cooperate when its partner defects, and (arguably) it should defect if its partner would cooperate regardless.</li>\n<li>The decision theory should one-box on Newcomb's Problem.</li>\n<li>The decision theory should be reasonably simple, and not include a bunch of ad-hoc rules. We want to solve problems involving prediction of actions in general, not just the special cases.</li>\n</ol>\n<p>There are now a couple of candidate decision theories (<a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">Timeless Decision Theory</a>, <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">Updateless Decision Theory</a>, and <a href=\"http://wiki.lesswrong.com/wiki/Ambient_decision_theory\">Ambient Decision Theory</a>) which seem to meet these criteria. Interestingly, formalizing any of these tends to deeply involve the mathematics of self-reference (<a href=\"http://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems\" target=\"_blank\">G\u00f6del's Theorem</a> and <a href=\"/lw/t6/the_cartoon_guide_to_l%C3%B6bs_theorem/\" target=\"_blank\">L\u00f6b's Theorem</a>) in order to avoid the infinite regress inherent in simulating an agent that's simulating you.</p>\n<p>But for the time being, we can massively oversimplify and outline them. <a href=\"/lw/15z/ingredients_of_timeless_decision_theory\" target=\"_blank\">TDT</a> considers your ultimate decision as the cause of both your action and other agents' valid predictions of your action, and tries to pick the decision that works best under that model. <a href=\"/lw/ap3/predictability_of_decisions_and_the_diagonal\" target=\"_blank\">ADT</a> uses a kind of diagonalization to predict the effects of different decisions without having the final decision throw off the prediction. And <a href=\"/lw/3l/counterfactual_mugging/\" target=\"_blank\">UDT</a> considers the decision that would be the best policy for all possible versions of you to employ, on average.</p>\n<h2 id=\"Why_are_advanced_decision_theories_important_for_Less_Wrong_\">Why are advanced decision theories important for Less Wrong?</h2>\n<p>There are a few reasons. Firstly, there are those who think that advanced decision theories are a natural base on which to build AI. One reason for this is something I briefly mentioned: even CDT allows for the idea that <strong>X</strong>'s current decisions can affect <strong>Y</strong>'s future decisions, and self-modification counts as a decision. If <strong>X</strong> can self-modify, and if <strong>X</strong> expects to deal with situations where an advanced decision theory would out-perform its current self, then <strong>X</strong> will change itself into an advanced decision theory (with some weird caveats: for example, if <strong>X</strong> started out as CDT, its modification will only care about other agents' decisions made after <strong>X</strong> self-modified).</p>\n<p>More relevantly to rationalists, the bad choices that CDT makes are often held up as examples of <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">why you shouldn't try to be rational</a>, or <a href=\"/lw/5f/bayesians_vs_barbarians/\">why rationalists can't cooperate</a>. But instrumental rationality doesn't need to be synonymous with causal decision theory: if there are other decision theories that do strictly better, <a href=\"http://wiki.lesswrong.com/wiki/Rationality_is_systematized_winning\">we should adopt those rather than CDT</a>! So figuring out advanced decision theories, even if we can't implement them on real-world problems, helps us see that the ideal of rationality isn't going to fall flat on its face.</p>\n<p>Finally, advanced decision theory could be relevant to morality. If, as many of us suspect, there's no basis for human morality apart from what goes on in human brains, then <a href=\"/lw/rx/is_morality_preference/\" target=\"_blank\">why do we feel there's still a distinction between what-we-want and what-is-right?</a> One answer is that if we feed in what-we-want into an advanced decision theory, then just as cooperation emerges in the Prisoner's Dilemma, many kinds of patterns that we take as basic moral rules emerge as the equilibrium behavior. The idea is developed more substantially in Gary Drescher's <a href=\"http://mitpress.mit.edu/catalog/item/default.asp?tid=10902&amp;ttype=2\" target=\"_blank\">Good and Real</a>, and (before there was a candidate for an advanced decision theory) in <a href=\"http://www.gwern.net/docs/1985-hofstadter\" target=\"_blank\">Douglas Hofstadter's concept</a> of <a href=\"http://en.wikipedia.org/wiki/Superrationality\" target=\"_blank\">superrationality</a>.</p>\n<p>It's still at the speculative stage, because it's difficult to work out what interactions between agents with advanced decision theories would look like (in particular, we don't know whether bargaining would end in a fair split or in a <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/GambitPileup\" target=\"_blank\">Xanatos Gambit Pileup</a> of <a href=\"http://en.wikipedia.org/wiki/Chicken_%28game%29\" target=\"_blank\">chicken</a> threats, though we think and hope it's the former). But it's at least a promising approach to the <a href=\"/lw/re/grasping_slippery_things/\">slippery question</a> of <a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">what 'right' could actually mean</a>.</p>\n<p>And if you want to understand this on a slightly more technical level... well, I've started a sequence.</p>\n<p><strong>Next:</strong> <a href=\"/lw/axl/decision_theories_a_semiformal_analysis_part_i/\" target=\"_self\">A Semi-Formal Analysis, Part I (The Problem with Naive Decision Theory)</a></p>\n<h3 id=\"Notes_\">Notes:<br></h3>\n<p><a name=\"naming\"></a><strong>0.</strong> Rather confusingly, <a href=\"http://en.wikipedia.org/wiki/Decision_theory\" target=\"_blank\">decision theory</a> is the name for the study of decision theories.</p>\n<p><a name=\"brains\"></a><strong>1.</strong> Both patterns appear in our conscious reasoning as well as our subconscious thinking- we care about consequences we can directly foresee and also about moral rules that don't seem attached to any particular consequence. However, just as the simple \"program\" for the bacterium was constructed by evolution, our <a href=\"/lw/51f/guilt_another_gift_nobody_wants/\" target=\"_blank\">moral rules are there for evolutionary reasons as well</a>- perhaps even for <a href=\"/lw/2ls/morality_as_parfitianfiltered_decision_theory/\" target=\"_blank\">reasons that have to do with advanced decision theory...</a></p>\n<p>Also, it's worth noting that we're <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\" target=\"_blank\">not consciously aware of all of our values and goals</a>, though at least we have a better idea of them than <em>E.Coli</em> does. This is a problem for the idea of representing our usual decisions in terms of decision theory, though we can still hope that our approximations are good enough (e.g. that our real values regarding the Cold War roughly corresponded to our estimates of how bad a nuclear war or a Soviet world takeover would be).</p>\n<p><a name=\"PD\"></a><strong>2.</strong> Eliezer <a href=\"/lw/tn/the_true_prisoners_dilemma/\" target=\"_blank\">once pointed out</a> that our intuitions on most formulations of the Prisoner's Dilemma are skewed by our notions of fairness, and a more outlandish example might serve better to illustrate how a genuine PD really feels. For an example where people are notorious for not caring about each others' goals, let's consider aesthetics: people who love one form of music often really feel that another popular form is a waste of time. One might feel that if the works of Artist Q suddenly disappeared from the world, it would objectively be a tragedy; while if the same happened to the works of Artist R, then it's no big deal and R's fans should be glad to be freed from that dreck.</p>\n<p>We can use this aesthetic intolerance to construct a more genuine Prisoner's Dilemma without inviting aliens or anything like that. Say <strong>X</strong> is a writer and <strong>Y</strong> is an illustrator, and they have very different preferences for how a certain scene should come across, so they've worked out a compromise. Now, both of them could cooperate and get a scene that both are OK with, or <strong>X</strong> could secretly change the dialogue in hopes of getting his idea to come across, or <strong>Y</strong> could draw the scene differently in order to get her idea of the scene across. But if they both \"defect\" from the compromise, then the scene gets confusing to readers. If both <strong>X</strong> and <strong>Y</strong> prefer their own idea to the compromise, prefer the compromise to the muddle, and prefer the muddle to their partner's idea, then this is a genuine Prisoner's Dilemma.</p>\n<p><a name=\"EDT\"></a><strong>3.</strong> I've avoided mentioning <a href=\"http://en.wikipedia.org/wiki/Evidential_decision_theory\" target=\"_blank\">Evidential Decision Theory</a>, the \"usual\" counterpart to CDT; it's worth noting that EDT one-boxes on Newcomb's Problem but gives the wrong answer on a classical one-player problem (<a href=\"http://wiki.lesswrong.com/wiki/Smoking_lesion\" target=\"_blank\">The Smoking Lesion</a>) which the advanced decision theories handle correctly. It's also far less amenable to formalization than the others.</p>", "sections": [{"title": "What is a decision theory?", "anchor": "What_is_a_decision_theory_", "level": 1}, {"title": "What is Causal Decision Theory?", "anchor": "What_is_Causal_Decision_Theory_", "level": 1}, {"title": "What's the problem with Causal Decision Theory?", "anchor": "What_s_the_problem_with_Causal_Decision_Theory_", "level": 1}, {"title": "What decision theories are better than CDT?", "anchor": "What_decision_theories_are_better_than_CDT_", "level": 1}, {"title": "Why are advanced decision theories important for Less Wrong?", "anchor": "Why_are_advanced_decision_theories_important_for_Less_Wrong_", "level": 1}, {"title": "Notes:", "anchor": "Notes_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "174 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 174, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gTNB9CQd5hnbkMxAG", "3nxs2WYDGzJbzcLMp", "r5MSQ83gtbjWRBDWJ", "6ddcsdA2c2XpNpE5x", "ALCnqX6Xx8bpFMZq3", "szfxvS8nsxTgJLBHs", "W6T93dSSm2xvHn9X6", "mg6jDEuQEjBGtibX7", "KsHmn6iJAEr9bACQW", "F5WLc7hCxkB4X4yD4", "HnS6c5Xm9p9sbm4a8", "2JdvZw3CXzafxQugN", "CZnBQtvDw33rmWpBD", "iNXS7ggMhXNoKCaRr", "HFyWNBnDNEDsDNLrZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-13T23:32:39.581Z", "modifiedAt": null, "url": null, "title": "Biased Pandemic", "slug": "biased-pandemic", "viewCount": null, "lastCommentedAt": "2017-07-08T13:38:20.958Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "freyley", "createdAt": "2009-02-27T18:49:48.880Z", "isAdmin": false, "displayName": "freyley"}, "userId": "LXoeJdNeQDPy32ki6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/34jf9Z43kBHF7Axz2/biased-pandemic", "pageUrlRelative": "/posts/34jf9Z43kBHF7Axz2/biased-pandemic", "linkUrl": "https://www.lesswrong.com/posts/34jf9Z43kBHF7Axz2/biased-pandemic", "postedAtFormatted": "Tuesday, March 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Biased%20Pandemic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABiased%20Pandemic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F34jf9Z43kBHF7Axz2%2Fbiased-pandemic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Biased%20Pandemic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F34jf9Z43kBHF7Axz2%2Fbiased-pandemic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F34jf9Z43kBHF7Axz2%2Fbiased-pandemic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1113, "htmlBody": "<p>Recently, Portland Lesswrong played a game that was a perfect trifecta of: difficult mental exercise; fun; and an opportunity to learn about biases and recognize them in yourself and others. We're still perfecting it, and we'd welcome feedback, especially from people who try it.</p>\n<h1>The Short Version</h1>\n<p>The game is a combination of Pandemic, a cooperative board game that is cognitively demanding, and the idea of roleplaying cognitive biases. Our favorite way of playing it (so far), everyone selects a bias at random, and then attempts to exaggerate that bias in their arguments and decisions during the game. Everyone attempts to identify the biases in the other players, and, when a bias is guessed, the guessed player selects a new bias and begins again.<a id=\"more\"></a></p>\n<h1>The Pieces</h1>\n<p>First, Pandemic. Pandemic is a cooperative game with a win condition and three lose conditions that are separate. It provides each player a list of available actions and then allows them 4 moves in which to mix and match those actions. Because of the combined win and lose conditions, players are constantly forced to decide between tactical and strategic objectives, with a strong emphasis on making it easy to choose tactically good moves at the expense of failing to win the game and thus losing by taking too long. Pandemic is a fun game, and it's a great game for people looking to stretch their brains. Obviously you want to be familiar with, preferably experienced at Pandemic before attempting Biased Pandemic. We did have one inexperienced player at Pandemic (out of 4) and that seemed to work okay, though it may have been harder for him than the rest of us.</p>\n<p>Enter the biases. Each player selects a bias. We printed out the biases listed <a title=\"RSOAP Biases\" href=\"http://www.slideshare.net/efern211/cognitive-biases-a-visual-study-guide-by-the-royal-society-of-account-planning\" target=\"_self\">here</a>&nbsp;and used them to select our biases. One of our players just made a <a href=\"http://www.mediafire.com/?6pf0e2trsjfy69i\" target=\"_blank\">TOC for selecting biases</a>. There happen to be 104 biases listed in that document, so a deck of cards combined with a coin flip allowed for bias selection. A computer's random number generator, dice, or any other random method should suffice. Some biases may seem unplayable to some players -- certainly, the monetary biases seem unplayable to most of us -- but other players may find a way to play it, so we've refused to cross off biases and just allowed players to re-roll if they get a bias they're sure they can't play.</p>\n<h2>Examples</h2>\n<p>This can be a little difficult to wrap your brain around, so let me give a couple of examples. One player, playing the Negativity Bias, went around the board treating cities which had outbroken earlier in the game and ignoring other issues. Another player with Hyperbolic Discounting went further: he treated cities, any city near him, while carrying 5 red city cards in his hand and pointing out, in response to entreaties to cure red, that red wasn't much of an issue right now. A player with Reactance had the winning yellow card and simply refused to be told to go somewhere to give it to the player with the other four. He even went so far as to refuse a half a dozen offers of an airlift so he could give up that card. A player with Hindsight Bias claimed that he had predicted that the player with 1 red card would get two more on his next draw, and was upset that he'd let the other players argue him otherwise. A player with The Ultimate Attribution Error suggested that if we weren't doing well because no rationalist could ever win this game because we were terrible at it. A player with the Authority Bias attempted to suggest that we should do things because it's what Eliezer would want us to do. A player with Illusion of Control declared that his next draw, he simply would not draw an epidemic. There were many others.</p>\n<h1>Recommended Rules Of Play</h1>\n<p>We played it somewhat haphazardly the first game, but at the end we agreed on a structure for the next game that we think is better. In our next game we plan to have the order of play go like this: during each player's turn, all players can discuss what the player should do for a timed interval, perhaps 1-2 minutes. The player then declares their intended move. Now each other player gets an opportunity to make a single bias guess. If a guess is correct, the player stops playing the bias, and begins the round again. At the end of their turn, if their bias was guessed, they select a new bias. We considered a bias to be guessed correctly if the player guessing fully described the bias, not just the biased behavior. Bias names, however, were not&nbsp;required.</p>\n<h2>Notes</h2>\n<p>One way that you can not do well is by falling into the trap of making the same biased statements repeatedly. After a few rounds of this, the biased statements were pretty obvious. The guesses are an indicator of what the other players are seeing, and we went out of our way to look for ways to respond to the guesses by playing up the aspects of the bias that the other players weren't seeing. For example, a lot of the different biases look like simple overconfidence. One player was playing The Illusion of Control in such a way that the rest of us&nbsp;thought he was overconfident. His response was to start declaring that he simply wasn't going to draw an epidemic card, and when he drew one, he declared that it was my fault for making him draw the card. This was obviously not simply overconfidence.</p>\n<p>Before playing, you should figure out how familiar you are with the biases. Players who are incredibly familiar with all of the biases may want to play a game where everyone plays as subtly as possible and your goal is to prevent other people from noticing your bias. For us, our goal was to learn the biases better and identify them in other people, so we tried to ham it up and play them as obviously as possible at first. It was incredibly difficult to specifically identify the biased thinking behind obviously biased statements, even with that, so I'd suggest at least trying it with obviousness first.</p>\n<p>One of the most difficult things to remember is that your goal is not to win the Pandemic game. Sure, that's nice, but your real goal is to familiarize yourself with biases, and to have fun roieplaying and identifying biases. Losing Pandemic, especially because the players are following their biased thinking, is a totally acceptable outcome. We won, and do not credit our thinking for it.</p>\n<p>We're looking forward to trying the game again, and maybe you'll have suggestions for improving it.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RLQumypPQGPYg9t6G": 1, "T57Qd9J3AfxmwhQtY": 1, "4R8JYu4QF2FqzJxE5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "34jf9Z43kBHF7Axz2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 61, "baseScore": 81, "extendedScore": null, "score": 0.000167, "legacy": true, "legacyId": "13934", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 81, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Recently, Portland Lesswrong played a game that was a perfect trifecta of: difficult mental exercise; fun; and an opportunity to learn about biases and recognize them in yourself and others. We're still perfecting it, and we'd welcome feedback, especially from people who try it.</p>\n<h1 id=\"The_Short_Version\">The Short Version</h1>\n<p>The game is a combination of Pandemic, a cooperative board game that is cognitively demanding, and the idea of roleplaying cognitive biases. Our favorite way of playing it (so far), everyone selects a bias at random, and then attempts to exaggerate that bias in their arguments and decisions during the game. Everyone attempts to identify the biases in the other players, and, when a bias is guessed, the guessed player selects a new bias and begins again.<a id=\"more\"></a></p>\n<h1 id=\"The_Pieces\">The Pieces</h1>\n<p>First, Pandemic. Pandemic is a cooperative game with a win condition and three lose conditions that are separate. It provides each player a list of available actions and then allows them 4 moves in which to mix and match those actions. Because of the combined win and lose conditions, players are constantly forced to decide between tactical and strategic objectives, with a strong emphasis on making it easy to choose tactically good moves at the expense of failing to win the game and thus losing by taking too long. Pandemic is a fun game, and it's a great game for people looking to stretch their brains. Obviously you want to be familiar with, preferably experienced at Pandemic before attempting Biased Pandemic. We did have one inexperienced player at Pandemic (out of 4) and that seemed to work okay, though it may have been harder for him than the rest of us.</p>\n<p>Enter the biases. Each player selects a bias. We printed out the biases listed <a title=\"RSOAP Biases\" href=\"http://www.slideshare.net/efern211/cognitive-biases-a-visual-study-guide-by-the-royal-society-of-account-planning\" target=\"_self\">here</a>&nbsp;and used them to select our biases. One of our players just made a <a href=\"http://www.mediafire.com/?6pf0e2trsjfy69i\" target=\"_blank\">TOC for selecting biases</a>. There happen to be 104 biases listed in that document, so a deck of cards combined with a coin flip allowed for bias selection. A computer's random number generator, dice, or any other random method should suffice. Some biases may seem unplayable to some players -- certainly, the monetary biases seem unplayable to most of us -- but other players may find a way to play it, so we've refused to cross off biases and just allowed players to re-roll if they get a bias they're sure they can't play.</p>\n<h2 id=\"Examples\">Examples</h2>\n<p>This can be a little difficult to wrap your brain around, so let me give a couple of examples. One player, playing the Negativity Bias, went around the board treating cities which had outbroken earlier in the game and ignoring other issues. Another player with Hyperbolic Discounting went further: he treated cities, any city near him, while carrying 5 red city cards in his hand and pointing out, in response to entreaties to cure red, that red wasn't much of an issue right now. A player with Reactance had the winning yellow card and simply refused to be told to go somewhere to give it to the player with the other four. He even went so far as to refuse a half a dozen offers of an airlift so he could give up that card. A player with Hindsight Bias claimed that he had predicted that the player with 1 red card would get two more on his next draw, and was upset that he'd let the other players argue him otherwise. A player with The Ultimate Attribution Error suggested that if we weren't doing well because no rationalist could ever win this game because we were terrible at it. A player with the Authority Bias attempted to suggest that we should do things because it's what Eliezer would want us to do. A player with Illusion of Control declared that his next draw, he simply would not draw an epidemic. There were many others.</p>\n<h1 id=\"Recommended_Rules_Of_Play\">Recommended Rules Of Play</h1>\n<p>We played it somewhat haphazardly the first game, but at the end we agreed on a structure for the next game that we think is better. In our next game we plan to have the order of play go like this: during each player's turn, all players can discuss what the player should do for a timed interval, perhaps 1-2 minutes. The player then declares their intended move. Now each other player gets an opportunity to make a single bias guess. If a guess is correct, the player stops playing the bias, and begins the round again. At the end of their turn, if their bias was guessed, they select a new bias. We considered a bias to be guessed correctly if the player guessing fully described the bias, not just the biased behavior. Bias names, however, were not&nbsp;required.</p>\n<h2 id=\"Notes\">Notes</h2>\n<p>One way that you can not do well is by falling into the trap of making the same biased statements repeatedly. After a few rounds of this, the biased statements were pretty obvious. The guesses are an indicator of what the other players are seeing, and we went out of our way to look for ways to respond to the guesses by playing up the aspects of the bias that the other players weren't seeing. For example, a lot of the different biases look like simple overconfidence. One player was playing The Illusion of Control in such a way that the rest of us&nbsp;thought he was overconfident. His response was to start declaring that he simply wasn't going to draw an epidemic card, and when he drew one, he declared that it was my fault for making him draw the card. This was obviously not simply overconfidence.</p>\n<p>Before playing, you should figure out how familiar you are with the biases. Players who are incredibly familiar with all of the biases may want to play a game where everyone plays as subtly as possible and your goal is to prevent other people from noticing your bias. For us, our goal was to learn the biases better and identify them in other people, so we tried to ham it up and play them as obviously as possible at first. It was incredibly difficult to specifically identify the biased thinking behind obviously biased statements, even with that, so I'd suggest at least trying it with obviousness first.</p>\n<p>One of the most difficult things to remember is that your goal is not to win the Pandemic game. Sure, that's nice, but your real goal is to familiarize yourself with biases, and to have fun roieplaying and identifying biases. Losing Pandemic, especially because the players are following their biased thinking, is a totally acceptable outcome. We won, and do not credit our thinking for it.</p>\n<p>We're looking forward to trying the game again, and maybe you'll have suggestions for improving it.</p>\n<p>&nbsp;</p>", "sections": [{"title": "The Short Version", "anchor": "The_Short_Version", "level": 1}, {"title": "The Pieces", "anchor": "The_Pieces", "level": 1}, {"title": "Examples", "anchor": "Examples", "level": 2}, {"title": "Recommended Rules Of Play", "anchor": "Recommended_Rules_Of_Play", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "36 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-14T00:44:47.242Z", "modifiedAt": null, "url": null, "title": "Anthropic Reasoning by CDT in Newcomb's Problem", "slug": "anthropic-reasoning-by-cdt-in-newcomb-s-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:23.119Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gRR", "createdAt": "2012-02-02T12:11:00.628Z", "isAdmin": false, "displayName": "gRR"}, "userId": "LPBRzHQvMP9chLNWH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5xt4i9S9y4SiHQPqG/anthropic-reasoning-by-cdt-in-newcomb-s-problem", "pageUrlRelative": "/posts/5xt4i9S9y4SiHQPqG/anthropic-reasoning-by-cdt-in-newcomb-s-problem", "linkUrl": "https://www.lesswrong.com/posts/5xt4i9S9y4SiHQPqG/anthropic-reasoning-by-cdt-in-newcomb-s-problem", "postedAtFormatted": "Wednesday, March 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anthropic%20Reasoning%20by%20CDT%20in%20Newcomb's%20Problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnthropic%20Reasoning%20by%20CDT%20in%20Newcomb's%20Problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5xt4i9S9y4SiHQPqG%2Fanthropic-reasoning-by-cdt-in-newcomb-s-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anthropic%20Reasoning%20by%20CDT%20in%20Newcomb's%20Problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5xt4i9S9y4SiHQPqG%2Fanthropic-reasoning-by-cdt-in-newcomb-s-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5xt4i9S9y4SiHQPqG%2Fanthropic-reasoning-by-cdt-in-newcomb-s-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 447, "htmlBody": "<p>By orthonormal's suggestion, I take this out of comments.</p>\n<p>&nbsp;</p>\n<p>Consider a CDT agent making a decision in a Newcomb's problem, in which Omega is known to make predictions by perfectly simulating the players. Assume further that the agent is capable of anthropic reasoning about simulations. Then, while making its decision, the agent will be uncertain about whether it is in the real world or in Omega's simulation, since the world would look the same to it either way.</p>\n<p>The resulting problem has a structural similarity to the <a href=\"/lw/182/the_absentminded_driver/\">Absentminded driver problem</a><sup>1</sup>. Like in that problem, directly assigning probabilities to each of the two possibilities is incorrect. The <em>planning-optimal</em> decision, however, is readily available to CDT, and it is, naturally, to one-box.</p>\n<p>&nbsp;</p>\n<p><em>Objection</em> 1. This argument requires that Omega is known to make predictions by simulation, which is not necessarily the case.</p>\n<p><em>Answer</em>: It appears to be sufficient that the agent only knows that Omega is always correct. If this is the case, then a simulating-Omega and some-other-method-Omega are indistinguishable, so the agent can freely assume simulation.</p>\n<p>[This is a rather shaky reasoning, I'm not sure it is correct in general. However, I hypothesise that whatever method Omega uses, if the CDT agent knows the method, it will one-box. It is only a \"magical Omega\" that throws CDT off.]</p>\n<p><em>Objection </em>2. The argument does not work for the problems where Omega is not always correct, but correct with, say, 90% probability.</p>\n<p><em>Answer</em>: Such problems are underspecified, because it is unclear how the probability is calculated. [For example, Omega that always predicts \"two-box\" will be correct in 90% cases if 90% of agents in the population are two-boxers.] A \"natural\" way to complete the problem definition is to stipulate that there is no correlation between correctness of Omega's predictions and any property of the players. But this is equivalent to Omega first making a perfectly correct prediction, and then adding a 10% random noise. In this case, the CDT agent is again free to consider Omega a perfect simulator (with added noise), which again leads to one-boxing.</p>\n<p><em>Objection </em>3. In order for the CDT agent to one-box, it needs a special \"non-self-centered\" utility function, which when inside the simulation would value things outside.</p>\n<p><em>Answer</em>: The agent in the simulation has exactly the same experiences as the agent outside, so it is the same self, so it values the Omega-offered utilons the same. This seems to be a general consequence of reasoning about simulations. Of course, it is possible to give the agent a special irrational simulation-fearing utility, but what would be the purpose?</p>\n<p><em>Objection </em>4. CDT still won't cooperate in the Prisoner's Dilemma against a CDT agent with an orthogonal utility function.</p>\n<p><em>Answer</em>: damn.</p>\n<p>&nbsp;</p>\n<p><span style=\"font-size: 11px;\"><sup>1</sup></span>&nbsp;Thanks to Will_Newsome for pointing me to this.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5xt4i9S9y4SiHQPqG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 9, "extendedScore": null, "score": 8.651689824265636e-07, "legacy": true, "legacyId": "13986", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GfHdNfqxe3cSCfpHL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-14T06:47:26.971Z", "modifiedAt": null, "url": null, "title": "What if the front page\u2026", "slug": "what-if-the-front-page", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:28.081Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/G4W6erJW6vPdzdLSp/what-if-the-front-page", "pageUrlRelative": "/posts/G4W6erJW6vPdzdLSp/what-if-the-front-page", "linkUrl": "https://www.lesswrong.com/posts/G4W6erJW6vPdzdLSp/what-if-the-front-page", "postedAtFormatted": "Wednesday, March 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20if%20the%20front%20page%E2%80%A6&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20if%20the%20front%20page%E2%80%A6%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG4W6erJW6vPdzdLSp%2Fwhat-if-the-front-page%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20if%20the%20front%20page%E2%80%A6%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG4W6erJW6vPdzdLSp%2Fwhat-if-the-front-page", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG4W6erJW6vPdzdLSp%2Fwhat-if-the-front-page", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<p>What if the front page looked a little more like this?</p>\n<p><a href=\"http://wiki.lesswrong.com/mediawiki/images/9/91/LW_website_Homepage_V3.jpg\"><img style=\"vertical-align: middle;\" src=\"http://wiki.lesswrong.com/mediawiki/images/9/91/LW_website_Homepage_V3.jpg\" alt=\"An idea\" width=\"524\" height=\"800\" /></a></p>\n<p>(Please assume that I'm trying to help. If you're polite and constructive (even if you hate this design) Omega will send you bundles of love in the post. If you're rude, I'll personally fund ninja assassins to hunt you down.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "G4W6erJW6vPdzdLSp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 50, "baseScore": 53, "extendedScore": null, "score": 0.000126, "legacy": true, "legacyId": "14013", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 53, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-14T16:24:05.699Z", "modifiedAt": "2021-08-12T19:30:31.865Z", "url": null, "title": "How would you take over Rome?", "slug": "how-would-you-take-over-rome", "viewCount": null, "lastCommentedAt": "2019-05-25T00:27:44.105Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2jZykdLg9fBGqKd46/how-would-you-take-over-rome", "pageUrlRelative": "/posts/2jZykdLg9fBGqKd46/how-would-you-take-over-rome", "linkUrl": "https://www.lesswrong.com/posts/2jZykdLg9fBGqKd46/how-would-you-take-over-rome", "postedAtFormatted": "Wednesday, March 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20would%20you%20take%20over%20Rome%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20would%20you%20take%20over%20Rome%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2jZykdLg9fBGqKd46%2Fhow-would-you-take-over-rome%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20would%20you%20take%20over%20Rome%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2jZykdLg9fBGqKd46%2Fhow-would-you-take-over-rome", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2jZykdLg9fBGqKd46%2Fhow-would-you-take-over-rome", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 396, "htmlBody": "<p>A <a href=\"/r/discussion/lw/asc/risks_from_ai_and_charitable_giving/\">recent discussion post</a> has compared the difficulty of an AI destroying modern human civilization to that of a modern human taking over the Roman Empire, with the implication that it is impossible.</p>\n<p>The analogy has a few problems: first, modern humans don't have much greater raw intelligence than the Romans, only a bit more knowledge and tools; an AI would have a genuine intelligence advantage. Second, a high-tech civilization like ours offers many more ways for a genius to cause chaos than existed in classical Rome: it's more plausible that you can throw a few existing technologies together to create a superweapon than that Ptolemy could have done likewise, and there's no ancient Roman equivalent to hacking a nuclear launch system.</p>\n<p>But taking over ancient Rome might serve as an interesting <em>upper bound</em> on the difficulty of an AI taking over modern civilization. And it's a theme of Harry Potter and the Methods of Rationality that rationalists should be able to come up with creative solutions to seemingly hard problems. So if Professor Quirrell offered it as an extra credit assignment, how <em>would</em> you take over Rome?</p>\n<p>Here are the rules:</p>\n<p>- You are thrown back in time to the year 1 AD. You can choose to arrive anywhere in the world, but your method of arrival cannot itself give an advantage (you can't appear in a flash of light in the middle of a religious ritual or anything).</p>\n<p>- You do not start with Roman citizenship or any other legal record of your existence.</p>\n<p>- You keep your original physical characteristics, including sex, height, and fitness. You will appear in period-appropriate dress of your choosing, and can't carry any artifacts with you. You may start with enough money to live a patrician lifestyle for a year.</p>\n<p>- You are intellectually near-perfect. You know all human knowledge as of 2012. You speak fluent Latin (and all other languages of the day) and can orate as eloquently as Cicero or Demosthenes. You are a tactical genius of the order of Caesar and Napoleon. And you have infinite willpower and goal-directedness: aside from human necessities like sleep or food, you need never rest.</p>\n<p>- You win if you either become Roman Emperor (and are acknowledged as such by most Romans), or if a state you control conquers the city of Rome. You lose if you die, of old age or otherwise, before completing either goal.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bY5MaF2EATwDkomvu": 1, "xexCWMyds6QLWognu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2jZykdLg9fBGqKd46", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 48, "extendedScore": null, "score": 9.4e-05, "legacy": true, "legacyId": "14022", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 203, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZpXik8eWiYhzbTtQC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-03-14T16:24:05.699Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-14T16:58:30.024Z", "modifiedAt": null, "url": null, "title": "DIY Transcranial Direct Current Stimulation. Who wants to go first?", "slug": "diy-transcranial-direct-current-stimulation-who-wants-to-go", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:08.572Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dustin", "createdAt": "2009-03-11T20:26:51.368Z", "isAdmin": false, "displayName": "Dustin"}, "userId": "E5DKwa6Eu3qLuKfpJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5nFvHD7aa7SB8nCLT/diy-transcranial-direct-current-stimulation-who-wants-to-go", "pageUrlRelative": "/posts/5nFvHD7aa7SB8nCLT/diy-transcranial-direct-current-stimulation-who-wants-to-go", "linkUrl": "https://www.lesswrong.com/posts/5nFvHD7aa7SB8nCLT/diy-transcranial-direct-current-stimulation-who-wants-to-go", "postedAtFormatted": "Wednesday, March 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20DIY%20Transcranial%20Direct%20Current%20Stimulation.%20Who%20wants%20to%20go%20first%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADIY%20Transcranial%20Direct%20Current%20Stimulation.%20Who%20wants%20to%20go%20first%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5nFvHD7aa7SB8nCLT%2Fdiy-transcranial-direct-current-stimulation-who-wants-to-go%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=DIY%20Transcranial%20Direct%20Current%20Stimulation.%20Who%20wants%20to%20go%20first%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5nFvHD7aa7SB8nCLT%2Fdiy-transcranial-direct-current-stimulation-who-wants-to-go", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5nFvHD7aa7SB8nCLT%2Fdiy-transcranial-direct-current-stimulation-who-wants-to-go", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 190, "htmlBody": "<p>There's been a bit of discussion about TDCS here on LessWrong. &nbsp;If you don't know what TDCS is <a href=\"http://www.contriving.net/link/4z\">here's an article</a> from Technology Review.</p>\n<p><a href=\"http://www.contriving.net/link/4x\">Here's a company</a> (soon) selling a DIY kit to build your own.</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>tDCS is one of the coolest pieces of health/ self improvement technology available today. The US Army and DARPA both currently use tDCS devices to train snipers and drone pilots, and have recorded 2.5x increases in learning rates. This incredible phenomenon is achieved through a very simple device called a tDCS machine.</p>\n<p>Today if you want to buy a tDCS machine it's nearly impossible to find one for less than $600, and you are typically required to have a prescription to order one. We wanted a simpler cheaper option. So we made one. Then, because we're all egalitarian like, we thought others might want one too.</p>\n<p>The GoFlow &beta;1 is a full kit of all the parts and plans you need to build your own tDCS device. $99 will get you one of the first &beta;1's and will help us develop &beta;2!</p>\n</blockquote>\n<p>Here's a <a href=\"http://www.contriving.net/link/4y\">video</a> from&nbsp;Journal of Visualized Experiments on how to administer TDCS.</p>\n<p>Thoughts?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5nFvHD7aa7SB8nCLT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 24, "extendedScore": null, "score": 8.655650145746572e-07, "legacy": true, "legacyId": "14023", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-14T20:47:04.598Z", "modifiedAt": null, "url": null, "title": "Deciding what to study at undergraduate level", "slug": "deciding-what-to-study-at-undergraduate-level", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:37.333Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tomme", "createdAt": "2012-03-14T19:51:35.247Z", "isAdmin": false, "displayName": "tomme"}, "userId": "uyGXufxNcB7WRQ63C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M7S9ntZMyCiitCTkq/deciding-what-to-study-at-undergraduate-level", "pageUrlRelative": "/posts/M7S9ntZMyCiitCTkq/deciding-what-to-study-at-undergraduate-level", "linkUrl": "https://www.lesswrong.com/posts/M7S9ntZMyCiitCTkq/deciding-what-to-study-at-undergraduate-level", "postedAtFormatted": "Wednesday, March 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Deciding%20what%20to%20study%20at%20undergraduate%20level&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADeciding%20what%20to%20study%20at%20undergraduate%20level%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM7S9ntZMyCiitCTkq%2Fdeciding-what-to-study-at-undergraduate-level%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Deciding%20what%20to%20study%20at%20undergraduate%20level%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM7S9ntZMyCiitCTkq%2Fdeciding-what-to-study-at-undergraduate-level", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM7S9ntZMyCiitCTkq%2Fdeciding-what-to-study-at-undergraduate-level", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 110, "htmlBody": "<p>I'm a high school senior from Europe and in a few months I'll be heading to university.</p>\n<p><br /> I have a keen interest in the human body. As such, I would like to work  in emerging interdisciplinary fields,  such as stem cell transplantation and suspended animation.</p>\n<p>I could go on to study, say, Biomedical Science, but I'm also  fascinated with Engineering. That is, I think that my aspirations,  which are to improve human condition, could be well served from an  Engineering standpoint.</p>\n<p>What do you think? Would my interest in the human body and its  applications be better suited for Engineering or for Biomedical Science? How should I decide what to study?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M7S9ntZMyCiitCTkq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 2, "extendedScore": null, "score": 8.656582188899693e-07, "legacy": true, "legacyId": "14025", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-14T21:23:28.381Z", "modifiedAt": null, "url": null, "title": "Dotting i's and Crossing t's - a Journey to Publishing Elegance", "slug": "dotting-i-s-and-crossing-t-s-a-journey-to-publishing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:26.200Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wedrifid", "createdAt": "2009-07-04T22:18:20.822Z", "isAdmin": false, "displayName": "wedrifid"}, "userId": "FqKohKFRCZnbfbbcS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iFv7q95TMWzq4wsvi/dotting-i-s-and-crossing-t-s-a-journey-to-publishing", "pageUrlRelative": "/posts/iFv7q95TMWzq4wsvi/dotting-i-s-and-crossing-t-s-a-journey-to-publishing", "linkUrl": "https://www.lesswrong.com/posts/iFv7q95TMWzq4wsvi/dotting-i-s-and-crossing-t-s-a-journey-to-publishing", "postedAtFormatted": "Wednesday, March 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dotting%20i's%20and%20Crossing%20t's%20-%20a%20Journey%20to%20Publishing%20Elegance&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADotting%20i's%20and%20Crossing%20t's%20-%20a%20Journey%20to%20Publishing%20Elegance%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiFv7q95TMWzq4wsvi%2Fdotting-i-s-and-crossing-t-s-a-journey-to-publishing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dotting%20i's%20and%20Crossing%20t's%20-%20a%20Journey%20to%20Publishing%20Elegance%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiFv7q95TMWzq4wsvi%2Fdotting-i-s-and-crossing-t-s-a-journey-to-publishing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiFv7q95TMWzq4wsvi%2Fdotting-i-s-and-crossing-t-s-a-journey-to-publishing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1586, "htmlBody": "<p>More literally a journey to making the dots of the 'i's line up just right with the 'f's and ensuring that the crossing of 'T' meets up neatly with the tip of the 'h' - all without breaking text searching and copy and paste.</p>\n<h3>Task<br /></h3>\n<p>Now, <a href=\"/lw/ajj/how_to_fix_science/\">as we all know</a>, science isn't just about little things like peer review and double blind placebo controlled studies. Far more important is presenting your work in accordance with the grand traditions of scientific publication - all while ensuring you flatter all the right people for their sometimes obsolete and possibly only slightly relevant past works. Of course you must do this all according to standard citation formulae developed a century or two ago back when the city in which a text document was published was somehow a useful piece of information.</p>\n<p>Some may consider people like <a href=\"http://en.wikipedia.org/wiki/Galileo_Galilei\">Galileo</a> and <a href=\"(http://en.wikipedia.org/wiki/Francis_Bacon\">Bacon</a> to be the most influential figures in science but the man who made the greatest contribution to the way humanity seeks and disseminates knowledge is of course Donald Knuth. The man who took a decade off writing his multi-volume magnum opus [The Art of Computer Programming](http://en.wikipedia.org/wiki/The_Art_of_Computer_Programming) to create TeX, the foundation of LaTeX and without which science as we know it would be unrecognizable. These days presenting academic publications without using LaTeX may be nearly as uncouth and banal as writing about your research in first person rather than than the passive voice!</p>\n<p>The above cynicism is largely sincere and only a trifle exaggerated. Yet at the same time I acknowledge that there is much value to be had in wearing a uniform and the time for <a href=\"/lw/mb/lonely_dissent/\">lonely dissent</a> is not on matters as trivial as presentation. The overhead of presenting work in a form that other academics are willing to accept is comparatively minor and the payoffs significant.</p>\n<p>One of the many initiatives lukeprog has set in motion now that he is organizing things over at SingInst is the <a href=\"/lw/a6m/si_wants_to_hire_a_remote_latex_guru/\">porting of all of SIAI's past publications</a> from various adhoc formats to LaTeX with a standard publication template. You can see an early example of the new format <a href=\"/lw/ah5/singularity_summit_2011_workshop_report/\">here</a>.</p>\n<h3>Challenge<br /></h3>\n<p>Unfortunately, Wei_Dai encountered <a href=\"/lw/ah5/singularity_summit_2011_workshop_report/5xxj\">a problem</a>. In the first presentation of the converted document copy and pasting \"The\" would give something like \"\u013be\" and copying \"fi\" would give \"\u0140\". The problem is with the implementation of ligatures. Back when typesetting was done manually - I can only imagine using a whole bunch of little metal stamp like things that could be plugged into the right places - the typsetters had an extra collection of pseudo letters to use instead of combinations like \"fi\", \"ffi\" and \"Th\". The reason being that those particular combinations just don't look too good if they are placed together the same way that you would place them with other letters. You wind up with either having the too far apart or having parts of them overlap in a way that isn't particularly neat.</p>\n<p>In the font SingInst uses the non-ligature versions of 'f' and 'i' combine with the dot of the 'i' only partially ovelapping the 'f' which somehow makes it jump out more easily to the reader. The way this is solved with the ligatures is actually increase the degree of overlap such that the f smoothly blends in to the i. Someone with far more highly honed aesthetic sense than I concluded that this is the best way to present English letters and it looks fairly good to me so I'll take their word for it.</p>\n<p>The problem is that while ligatures are easy for humans to read \"Notepad\", \"Word\" and \"Firefox\" aren't nearly as smart. And unfortunately there isn't a consistent standard between fonts of which ligature means what so we end up with all sorts of random mess if we try to copy and paste from a ligature riddled document into our editor of choice. This left me with rather a lot of work to do while I was generating LaTeX files from those of the old SingInst publications that were only available in PDF form and that isn't a task I would wish on all the future consumers of SingInst literature.</p>\n<h3>Opportunity<br /></h3>\n<p>Fortunately, the PDF format and the LaTeX are both advanced enough to handle making the visible text use the ligature characters while keeping the original text available for easy copy and pasting by the interested reader. This involves something called a 'cmap'. It is a mapping from an input encoding to the output encoding. With that cmap embedded in the pdf file any fully featured pdf reader is able to take the pretty text, strip apart the ligatures and figure out what they were originally.</p>\n<p>Why then is Wei unable to copy our Th's and fi's? I haven't the slightest idea. My research suggests that the xelatex distribution we were using should <em>just work</em> and handle this sort of thing. So confident is it in managing such mappings that it outright rejects compatibility with the 'cmap' passage which could be used in the older 'pdflatex' compiler to handle this sort of task.</p>\n<h3>Attempted Workarounds</h3>\n<ul>\n<li>\\usepackage{cmap} - Recommended as the solution to all problems ligature related as the result of all obvious google searches. Unfortunately the package doesn't load in xelatex and from all reports just isn't supposed to be needed.</li>\n<li>Use a different, similar font. There are plenty of alternatives to Adobe Caslon Pro - Adobe Garamond Pro for example. No luck - the problem seemed to apply to all fonts installed to the system (and thereby made accessible via xelatex's font magic).</li>\n<li>Find a font that doesn't need ligatures - This works, obviously. There are plenty of fonts that keep the letters sufficiently spread - or are even mono-typed. None of them looked anywhere near as good as Adobe Caslon Pro but they would have to suffice if no better alternative could be found.</li>\n<li>Manually edit .map files. If I recall that helped a tad. One by one characters could be retargetted but then all ended up pointing at the basic font rather than, say to 'bold'.</li>\n<li>Extracting maps from otf (font) files - There are all sorts of linux based command line tools for the manipulation of fonts between various formats and the extraction of data from them. Some of the work as specified. The ones that try to do more than one step at the same time do not - at least without extensive intervention. While no doubt it would lead to eventual success this approach is not recommended to anyone who has less than several weeks to spend learning the dark arts of font internal details.</li>\n<li>autoinst - this is a tool that is supposed to 'just work' and install fonts for use even in the comparatively primitive pdflatex. Suffice it to say that it does not.</li>\n<li>autoinst with manual assistance - autoinst seems to produce all the files that should be needed, the task then is to distribute them in a way that allows them to work with latex. This approach would probably work... eventually. It is far from trivial and did not work within the time I allocated to.</li>\n<li>Expert assistance - Money solves everything. Luke contacted assorted people who know about LaTeX and offered to pay them to fix our problem. Unfortunately none of the responders had a clue in this case, at least not at first glance.</li>\n<li>Pristine, up to date installation of TeXLive -often the packages installed by ubuntu are not as fresh as those to be had by installing directly from the source. Reverting the ubuntu virtual machine to a pre-latex state and downloading 2gb worth of TeXlive distribution could well have helped. It didn't.</li>\n<li>lualatex or pdflatex instead of xelatex - no luck (yet).</li>\n<li>MikTeX - the easy to use windows based distribution of latex may have allowed the autoinst program or perhaps xelatex magic to 'just work'. It didn't - in fact a known bug in one of the packages in that distribution prevented the SingInst template from working with MikTeX at all.</li>\n<li>inbuild font packages - success - to a degree. Fonts that come with old style latex packages in either MikTeX or TeXLive work as intended. They still don't look as good as Adobe Caslon Pro but would have been been good enough.</li>\n</ul>\n<h3>Success!</h3>\n<ul>\n<li>Running MikTeX instead of TeXlive Reinstalling MikTeX, downloading fresh packages and then running lualatex. Success! Adobe Caslon Pro now appears in our publications without any Ligature related problems. Why did this work while lualatex on TeXLive still doesn't work correctly? I'm not entirely sure. But I'm rather glad I had the hunch to go back and try it even when my attention had moved on to more important matters.</li>\n</ul>\n<h3>Optimal Decision Making<br /></h3>\n<p>An analysis could be done on what the optimal problem solving strategy would have been at any point in that process. Among other things I would note that rather early on in the process I decided that the expected value of continuing to attack the problem was rather low - so I stopped billing Luke for the time. But since I really don't like being bested by a challenge I went ahead and did it anyway. Much frustration was involved but in this case I was rewarded with a large boost of personal satisfaction and with SingInst publications that are an iota or two more beautiful!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iFv7q95TMWzq4wsvi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 19, "extendedScore": null, "score": 8.656730312178352e-07, "legacy": true, "legacyId": "14024", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>More literally a journey to making the dots of the 'i's line up just right with the 'f's and ensuring that the crossing of 'T' meets up neatly with the tip of the 'h' - all without breaking text searching and copy and paste.</p>\n<h3 id=\"Task\">Task<br></h3>\n<p>Now, <a href=\"/lw/ajj/how_to_fix_science/\">as we all know</a>, science isn't just about little things like peer review and double blind placebo controlled studies. Far more important is presenting your work in accordance with the grand traditions of scientific publication - all while ensuring you flatter all the right people for their sometimes obsolete and possibly only slightly relevant past works. Of course you must do this all according to standard citation formulae developed a century or two ago back when the city in which a text document was published was somehow a useful piece of information.</p>\n<p>Some may consider people like <a href=\"http://en.wikipedia.org/wiki/Galileo_Galilei\">Galileo</a> and <a href=\"(http://en.wikipedia.org/wiki/Francis_Bacon\">Bacon</a> to be the most influential figures in science but the man who made the greatest contribution to the way humanity seeks and disseminates knowledge is of course Donald Knuth. The man who took a decade off writing his multi-volume magnum opus [The Art of Computer Programming](http://en.wikipedia.org/wiki/The_Art_of_Computer_Programming) to create TeX, the foundation of LaTeX and without which science as we know it would be unrecognizable. These days presenting academic publications without using LaTeX may be nearly as uncouth and banal as writing about your research in first person rather than than the passive voice!</p>\n<p>The above cynicism is largely sincere and only a trifle exaggerated. Yet at the same time I acknowledge that there is much value to be had in wearing a uniform and the time for <a href=\"/lw/mb/lonely_dissent/\">lonely dissent</a> is not on matters as trivial as presentation. The overhead of presenting work in a form that other academics are willing to accept is comparatively minor and the payoffs significant.</p>\n<p>One of the many initiatives lukeprog has set in motion now that he is organizing things over at SingInst is the <a href=\"/lw/a6m/si_wants_to_hire_a_remote_latex_guru/\">porting of all of SIAI's past publications</a> from various adhoc formats to LaTeX with a standard publication template. You can see an early example of the new format <a href=\"/lw/ah5/singularity_summit_2011_workshop_report/\">here</a>.</p>\n<h3 id=\"Challenge\">Challenge<br></h3>\n<p>Unfortunately, Wei_Dai encountered <a href=\"/lw/ah5/singularity_summit_2011_workshop_report/5xxj\">a problem</a>. In the first presentation of the converted document copy and pasting \"The\" would give something like \"\u013be\" and copying \"fi\" would give \"\u0140\". The problem is with the implementation of ligatures. Back when typesetting was done manually - I can only imagine using a whole bunch of little metal stamp like things that could be plugged into the right places - the typsetters had an extra collection of pseudo letters to use instead of combinations like \"fi\", \"ffi\" and \"Th\". The reason being that those particular combinations just don't look too good if they are placed together the same way that you would place them with other letters. You wind up with either having the too far apart or having parts of them overlap in a way that isn't particularly neat.</p>\n<p>In the font SingInst uses the non-ligature versions of 'f' and 'i' combine with the dot of the 'i' only partially ovelapping the 'f' which somehow makes it jump out more easily to the reader. The way this is solved with the ligatures is actually increase the degree of overlap such that the f smoothly blends in to the i. Someone with far more highly honed aesthetic sense than I concluded that this is the best way to present English letters and it looks fairly good to me so I'll take their word for it.</p>\n<p>The problem is that while ligatures are easy for humans to read \"Notepad\", \"Word\" and \"Firefox\" aren't nearly as smart. And unfortunately there isn't a consistent standard between fonts of which ligature means what so we end up with all sorts of random mess if we try to copy and paste from a ligature riddled document into our editor of choice. This left me with rather a lot of work to do while I was generating LaTeX files from those of the old SingInst publications that were only available in PDF form and that isn't a task I would wish on all the future consumers of SingInst literature.</p>\n<h3 id=\"Opportunity\">Opportunity<br></h3>\n<p>Fortunately, the PDF format and the LaTeX are both advanced enough to handle making the visible text use the ligature characters while keeping the original text available for easy copy and pasting by the interested reader. This involves something called a 'cmap'. It is a mapping from an input encoding to the output encoding. With that cmap embedded in the pdf file any fully featured pdf reader is able to take the pretty text, strip apart the ligatures and figure out what they were originally.</p>\n<p>Why then is Wei unable to copy our Th's and fi's? I haven't the slightest idea. My research suggests that the xelatex distribution we were using should <em>just work</em> and handle this sort of thing. So confident is it in managing such mappings that it outright rejects compatibility with the 'cmap' passage which could be used in the older 'pdflatex' compiler to handle this sort of task.</p>\n<h3 id=\"Attempted_Workarounds\">Attempted Workarounds</h3>\n<ul>\n<li>\\usepackage{cmap} - Recommended as the solution to all problems ligature related as the result of all obvious google searches. Unfortunately the package doesn't load in xelatex and from all reports just isn't supposed to be needed.</li>\n<li>Use a different, similar font. There are plenty of alternatives to Adobe Caslon Pro - Adobe Garamond Pro for example. No luck - the problem seemed to apply to all fonts installed to the system (and thereby made accessible via xelatex's font magic).</li>\n<li>Find a font that doesn't need ligatures - This works, obviously. There are plenty of fonts that keep the letters sufficiently spread - or are even mono-typed. None of them looked anywhere near as good as Adobe Caslon Pro but they would have to suffice if no better alternative could be found.</li>\n<li>Manually edit .map files. If I recall that helped a tad. One by one characters could be retargetted but then all ended up pointing at the basic font rather than, say to 'bold'.</li>\n<li>Extracting maps from otf (font) files - There are all sorts of linux based command line tools for the manipulation of fonts between various formats and the extraction of data from them. Some of the work as specified. The ones that try to do more than one step at the same time do not - at least without extensive intervention. While no doubt it would lead to eventual success this approach is not recommended to anyone who has less than several weeks to spend learning the dark arts of font internal details.</li>\n<li>autoinst - this is a tool that is supposed to 'just work' and install fonts for use even in the comparatively primitive pdflatex. Suffice it to say that it does not.</li>\n<li>autoinst with manual assistance - autoinst seems to produce all the files that should be needed, the task then is to distribute them in a way that allows them to work with latex. This approach would probably work... eventually. It is far from trivial and did not work within the time I allocated to.</li>\n<li>Expert assistance - Money solves everything. Luke contacted assorted people who know about LaTeX and offered to pay them to fix our problem. Unfortunately none of the responders had a clue in this case, at least not at first glance.</li>\n<li>Pristine, up to date installation of TeXLive -often the packages installed by ubuntu are not as fresh as those to be had by installing directly from the source. Reverting the ubuntu virtual machine to a pre-latex state and downloading 2gb worth of TeXlive distribution could well have helped. It didn't.</li>\n<li>lualatex or pdflatex instead of xelatex - no luck (yet).</li>\n<li>MikTeX - the easy to use windows based distribution of latex may have allowed the autoinst program or perhaps xelatex magic to 'just work'. It didn't - in fact a known bug in one of the packages in that distribution prevented the SingInst template from working with MikTeX at all.</li>\n<li>inbuild font packages - success - to a degree. Fonts that come with old style latex packages in either MikTeX or TeXLive work as intended. They still don't look as good as Adobe Caslon Pro but would have been been good enough.</li>\n</ul>\n<h3 id=\"Success_\">Success!</h3>\n<ul>\n<li>Running MikTeX instead of TeXlive Reinstalling MikTeX, downloading fresh packages and then running lualatex. Success! Adobe Caslon Pro now appears in our publications without any Ligature related problems. Why did this work while lualatex on TeXLive still doesn't work correctly? I'm not entirely sure. But I'm rather glad I had the hunch to go back and try it even when my attention had moved on to more important matters.</li>\n</ul>\n<h3 id=\"Optimal_Decision_Making\">Optimal Decision Making<br></h3>\n<p>An analysis could be done on what the optimal problem solving strategy would have been at any point in that process. Among other things I would note that rather early on in the process I decided that the expected value of continuing to attack the problem was rather low - so I stopped billing Luke for the time. But since I really don't like being bested by a challenge I went ahead and did it anyway. Much frustration was involved but in this case I was rewarded with a large boost of personal satisfaction and with SingInst publications that are an iota or two more beautiful!</p>", "sections": [{"title": "Task", "anchor": "Task", "level": 1}, {"title": "Challenge", "anchor": "Challenge", "level": 1}, {"title": "Opportunity", "anchor": "Opportunity", "level": 1}, {"title": "Attempted Workarounds", "anchor": "Attempted_Workarounds", "level": 1}, {"title": "Success!", "anchor": "Success_", "level": 1}, {"title": "Optimal Decision Making", "anchor": "Optimal_Decision_Making", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "15 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ETe2SZacmLvvr8H9n", "CEGnJBHmkcwPTysb7", "RPeSzu6b9XPdT6my6", "acxdcBuZPkvnM9TNM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-15T00:41:34.811Z", "modifiedAt": null, "url": null, "title": "Cult impressions of Less Wrong/Singularity Institute", "slug": "cult-impressions-of-less-wrong-singularity-institute", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:03.179Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CfTH84gGFCRqo8D7t/cult-impressions-of-less-wrong-singularity-institute", "pageUrlRelative": "/posts/CfTH84gGFCRqo8D7t/cult-impressions-of-less-wrong-singularity-institute", "linkUrl": "https://www.lesswrong.com/posts/CfTH84gGFCRqo8D7t/cult-impressions-of-less-wrong-singularity-institute", "postedAtFormatted": "Thursday, March 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cult%20impressions%20of%20Less%20Wrong%2FSingularity%20Institute&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACult%20impressions%20of%20Less%20Wrong%2FSingularity%20Institute%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCfTH84gGFCRqo8D7t%2Fcult-impressions-of-less-wrong-singularity-institute%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cult%20impressions%20of%20Less%20Wrong%2FSingularity%20Institute%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCfTH84gGFCRqo8D7t%2Fcult-impressions-of-less-wrong-singularity-institute", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCfTH84gGFCRqo8D7t%2Fcult-impressions-of-less-wrong-singularity-institute", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 498, "htmlBody": "<div>If you type \"less wrong c\" or \"singularity institute c\" into Google, you'll find that people are searching for \"less wrong cult\" and \"singularity institute cult\" with some frequency. (EDIT:&nbsp;Please avoid testing this out, so Google doesn't autocomplete your search and reinforce their positions. <a href=\"/r/discussion/lw/atm/cult_impressions_of_less_wrongsi/60u3\">This kind of problem can be hard to get rid of</a>. Click these instead:&nbsp;<a href=\"http://dl.dropbox.com/u/3235979/less%20wrong%20cult%20-%20Google%20Search.htm\">less wrong cult</a>, <a href=\"http://dl.dropbox.com/u/3235979/singularity%20institute%20cult%20-%20Google%20Search.htm\">singularity institute cult</a>.)</div>\n<div>There doesn't seem to be anyone arguing seriously that Less Wrong is a cult, but <strong>we do give some newcomers that impression</strong>.</div>\n<p>I have several questions related to this:</p>\n<ul>\n<li>Did anyone reading this initially get the impression that Less Wrong was cultish when they first discovered it?</li>\n<li>If so, can you suggest any easy steps we could take?</li>\n<li>Is it possible that there are aspects of the atmosphere here that are driving away intelligent, rationally inclined people who might otherwise be interested in Less Wrong?</li>\n<li>Do you know anyone who might fall into this category, i.e. someone who was exposed to Less Wrong but failed to become an enthusiast, potentially due to atmosphere issues?</li>\n<li>Is it possible that our culture might be different if these folks were hanging around and contributing? Presumably they are disproportionately represented among certain personality types.</li>\n</ul>\n<hr />\n<ul>\n</ul>\n<p>If you visit any Less Wrong page for the first time in a cookies-free browsing mode, you'll see this message for new users:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; background-color: #f7f7f8; \">Less Wrong is a community blog devoted to refining the art of human rationality. Please visit our&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; background-color: #f7f7f8; \" rel=\"nofollow\" href=\"/about-less-wrong/\">About</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; background-color: #f7f7f8; \">&nbsp;page for more information.</span></p>\n</blockquote>\n<p>Here are the worst violators I see on that about page:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify; \">Some people&nbsp;</span><a class=\"external text\" style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify; \" rel=\"nofollow\" href=\"/lw/33j/yes_a_blog/\">consider</a><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify; \">&nbsp;the Sequences the most important work they have ever read.</span></p>\n</blockquote>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify; \">Generally, if your comment or post is on-topic, thoughtful, and shows that you're familiar with the Sequences, your comment or post will be upvoted.</span></p>\n</blockquote>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify; \">Many of us believe in the importance of developing qualities described in&nbsp;</span><a class=\"external text\" style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify; \" rel=\"nofollow\" href=\"http://yudkowsky.net/rational/virtues\">Twelve Virtues of Rationality</a><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify; \">: [insert mystical sounding description of how to be rational here]</span></p>\n</blockquote>\n<p style=\"text-align: -webkit-auto;\">And on the sequences page:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 19px; \">If you don't read the sequences on&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 19px; \" title=\"Mysterious Answers to Mysterious Questions\" href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\">Mysterious Answers to Mysterious Questions</a><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 19px; \">&nbsp;and&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 19px; \" title=\"Reductionism (sequence)\" href=\"http://wiki.lesswrong.com/wiki/Reductionism_(sequence)\">Reductionism</a><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 19px; \">, little else on Less Wrong will make much sense.</span></p>\n</blockquote>\n<p>This seems obviously false to me.</p>\n<p>These may not seem like cultish statements to you, but keep in mind that you are one of the ones who decided to stick around. The&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Typical_mind_fallacy\">typical mind fallacy</a>&nbsp;may be at work. Clearly there is some population that thinks Less Wrong seems cultish, as evidenced by Google's autocomplete, and these look like good candidates for things that makes them think this.</p>\n<p>We can fix this stuff easily, since they're both wiki pages, but I thought they were examples worth discussing.</p>\n<p>In general, I think we could stand more community effort being put into improving&nbsp;our about page, which you can&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Lesswrong:Aboutpage\">do now here</a>. It's not that visible to veteran users, but it is very visible to newcomers. Note that it looks as though you'll have to click the little \"Force reload from wiki\" button on the&nbsp;<a href=\"/about/\">about page itself</a>&nbsp;for your changes to be published.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CfTH84gGFCRqo8D7t", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 63, "baseScore": 36, "extendedScore": null, "score": 7.4e-05, "legacy": true, "legacyId": "14026", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 248, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DXcezGmnBcAYL2Y2u", "bJ2haLkcGeLtTWaD5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-15T04:58:14.698Z", "modifiedAt": null, "url": null, "title": "Meta Addiction", "slug": "meta-addiction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:08.019Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Voltairina", "createdAt": "2012-02-24T04:00:28.314Z", "isAdmin": false, "displayName": "Voltairina"}, "userId": "a6hK33SK4uawjaL9h", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/g2AKPEzFdQitmpTDu/meta-addiction", "pageUrlRelative": "/posts/g2AKPEzFdQitmpTDu/meta-addiction", "linkUrl": "https://www.lesswrong.com/posts/g2AKPEzFdQitmpTDu/meta-addiction", "postedAtFormatted": "Thursday, March 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meta%20Addiction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeta%20Addiction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg2AKPEzFdQitmpTDu%2Fmeta-addiction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meta%20Addiction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg2AKPEzFdQitmpTDu%2Fmeta-addiction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg2AKPEzFdQitmpTDu%2Fmeta-addiction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 122, "htmlBody": "<p>I was wondering if anyone has ever had the feeling, like I get sometimes, that they were addicted to 'meta-level' optimizing rather than low-level acting? As in, I'd rather think about how to encourage myself to brush my teeth more than brush my teeth. I'm guessing there's something about this under the akrasia threads?</p>\n<p>The motivations to remain in meta and thinking about things rather than acting on them seems to be that it takes less effort to think about doing things than to do them, and there is potentially more long-term benefit in making an overall improvement than in engaging in a specific action. The drawback is that if you remain thinking about meta all the time, <em>you won't get anything done</em>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "g2AKPEzFdQitmpTDu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 23, "extendedScore": null, "score": 5.4e-05, "legacy": true, "legacyId": "14045", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-15T14:25:24.062Z", "modifiedAt": null, "url": null, "title": "The Futility of Intelligence", "slug": "the-futility-of-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:33.412Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7Ex9wnRXPXAz8teed/the-futility-of-intelligence", "pageUrlRelative": "/posts/7Ex9wnRXPXAz8teed/the-futility-of-intelligence", "linkUrl": "https://www.lesswrong.com/posts/7Ex9wnRXPXAz8teed/the-futility-of-intelligence", "postedAtFormatted": "Thursday, March 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Futility%20of%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Futility%20of%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Ex9wnRXPXAz8teed%2Fthe-futility-of-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Futility%20of%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Ex9wnRXPXAz8teed%2Fthe-futility-of-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Ex9wnRXPXAz8teed%2Fthe-futility-of-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 923, "htmlBody": "<p>The failures of <a href=\"/lw/is/fake_causality\">phlogiston</a> and <a href=\"/lw/iu/mysterious_answers_to_mysterious_questions\">vitalism</a> are <a href=\"/lw/im/hindsight_devalues_science\">historical</a> <a href=\"/lw/il/hindsight_bias\">hindsight</a>. Dare I step out on a limb, and name some <em>current</em> theory which I deem analogously flawed?</p>\n<p>I name <em>artificial intelligence </em>or <em>thinking machines</em> - usually  defined as the study of systems whose high-level behaviors arise from  \"thinking\" or the interaction of many low-level elements.&nbsp; (<a href=\"http://en.wikipedia.org/wiki/Emergence\"></a>R. J. Sternberg quoted in a <a href=\"http://www.vetta.org/documents/A-Collection-of-Definitions-of-Intelligence.pdf\">paper</a> by Shane Legg:&nbsp; &ldquo;Viewed narrowly, there seem to be almost as many definitions of intelligence as there were experts asked to define it.&rdquo;) Taken literally, that allows for infinitely many degrees of intelligence to fit every phenomenon in our universe above the level of individual  quarks, which is part of the problem.&nbsp; Imagine pointing to a chess computer and saying \"It's not a stone!\"&nbsp; Does that feel like an  explanation?&nbsp; No?&nbsp; Then neither should saying \"It's a thinking machine!\"</p>\n<p>It's the noun \"intelligence\" that I protest, rather than  to \"evoke a dynamic state sequence from a machine by computing an algorithm\".&nbsp; There's nothing wrong with saying \"X computes algorithm Y\",  where Y is some specific, detailed flowchart that represents an algorithm or process.&nbsp;  \"Thinking about\" is another legitimate phrase that means exactly the same  thing:&nbsp; The machine is thinking about a problem, according to an  specific algorithm. The machine is thinking about how to put elements of a list in a certain order, according to the a specific algorithm called <a href=\"http://en.wikipedia.org/wiki/Quicksort\">quicksort</a>.</p>\n<p>Now suppose I should say that a problem is explained by \"thinking\" or  that the order of elements in a list is the result of a \"thinking machine\", and claim that as my  explanation.</p>\n<p>The phrase \"evoke a dynamic state sequence from a machine by computing an algorithm\" is acceptable, just like \"thinking about\" or  \"is caused by\" are acceptable, if the phrase precedes some specification to be judged on its own merits.</p>\n<p>However, this is <em>not</em> the way \"intelligence\" is commonly used. \"Intelligence\" is commonly used as an explanation in its own right.</p>\n<p>I have lost track of how many times I have heard people say,  \"an artificial general intelligence <a href=\"/r/discussion/lw/ati/how_would_you_take_over_rome/\">would have a genuine intelligence advantage</a>\" as if that explained its advantage. This usage fits all the checklist items for a <a href=\"/lw/iu/mysterious_answers_to_mysterious_questions\">mysterious answer to a mysterious question</a>.  What do you know, after you have said that its \"advantage\" is \"intelligence\"?&nbsp;  You can make no new predictions.&nbsp; You do not know anything about the  behavior of real-world artificial general intelligence that you did not know before.&nbsp; It feels  like you believe a new fact, but you don't anticipate any different  outcomes. Your curiosity feels sated, but it has not been fed.&nbsp; The  hypothesis has no moving parts - there's no detailed internal model to  manipulate.&nbsp; Those who proffer the hypothesis of \"intelligence\" confess  their ignorance of the internals, and take pride in it; they contrast  the science of \"artificial general intelligence\" to other sciences merely mundane.</p>\n<p>And even after the answer of \"How? Intelligence!\" is given, <em>the practical realization is still a mystery</em> and possesses the same sacred impenetrability it had at the start.</p>\n<p>A fun exercise is to eliminate the explanation \"intelligence\" from any  sentence in which it appears, and see if the sentence says anything  different:</p>\n<ul>\n<li><em>Before:</em> The AI is going to take over the world by using its superhuman intelligence to invent nanotechnology.</li>\n<li><em>After:</em>&nbsp; The AI is going to take over the world by inventing nanotechnology.</li>\n</ul>\n<ul>\n<li><em>Before:</em> A friendly AI is going to use its superhuman intelligence to extrapolate the coherent volition of humanity. </li>\n<li><em>After:</em>&nbsp; A friendly AI is going to extrapolate the coherent volition of humanity. </li>\n<li><em>Even better:</em> A friendly AI is a powerful algorithm. We can successfully extrapolate some aspects of the volition of   individual humans using [FILL IN DETAILS] procedure, without any global societal variables, showing that we  understand how the extrapolate the volition of humanity in theory and that it converges rather than diverges, that our wishes cohere rather than interfere. </li>\n</ul>\n<p>Another fun exercise is to replace \"intelligence\" with \"magic\", the explanation that people had to use before the idea of an intelligence explosion was invented:</p>\n<ul>\n<li><em>Before:</em>&nbsp; The AI is going to use its superior intelligence to quickly evolve vastly superhuman capabilities and reach singleton status within a matter of weeks.</li>\n<li><em>After:</em>&nbsp; The AI is going to use magic to quickly evolve vastly superhuman capabilities and reach  singleton status within a matter of weeks.</li>\n</ul>\n<ul>\n<li><em>Before:</em>&nbsp; Superhuman intelligence is able to use the internet to gain physical manipulators and expand its computational capabilities.</li>\n<li><em>After:</em>&nbsp; Superhuman magic is able to use the internet to gain physical manipulators and expand its computational capabilities.</li>\n</ul>\n<p>Does not each statement convey exactly the same amount of knowledge about the phenomenon's behavior? Does not each hypothesis <a href=\"/lw/if/your_strength_as_a_rationalist\">fit exactly the same set of outcomes</a>?</p>\n<p>\"Intelligence\" has become very popular, just as saying \"magic\" used to  be very popular. \"Intelligence\" has the same deep appeal to human  psychology, for the same reason. \"Intelligence\" is such a wonderfully easy  explanation, and it feels good to say it; it gives you a <a href=\"/lw/iu/mysterious_answers_to_mysterious_questions\">sacred mystery</a> to worship. Intelligence is popular <em>because</em> it is the junk food of curiosity. You can explain anything using intelligence , and so people do just that; for it feels so wonderful to  explain things. Humans are still humans, even if they've taken a few  science classes in college. Once they find a way to escape the <a href=\"http://yudkowsky.net/virtues/\">shackles</a> of settled science, they get up to the same shenanigans as their ancestors, <a href=\"/lw/i7/belief_as_attire\">dressed up in the literary genre of \"science\"</a> but still the same species psychology.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7Ex9wnRXPXAz8teed", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": -10, "extendedScore": null, "score": -1.4e-05, "legacy": true, "legacyId": "14066", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RgkqLqkg8vLhsYpfh", "6i3zToomS86oj9bS6", "WnheMGAka4fL99eae", "fkM9XsNvXdYH6PPAx", "2jZykdLg9fBGqKd46", "5JDkW4MYXit2CquLs", "nYkMLFpx77Rz3uo9c"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-15T16:32:51.558Z", "modifiedAt": null, "url": null, "title": "[LINK] Judea Pearl wins 2011 Turing Award", "slug": "link-judea-pearl-wins-2011-turing-award", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:23.061Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ytNR2cG5LdnQTWmEo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eTf7Tnu9SNipcSoRT/link-judea-pearl-wins-2011-turing-award", "pageUrlRelative": "/posts/eTf7Tnu9SNipcSoRT/link-judea-pearl-wins-2011-turing-award", "linkUrl": "https://www.lesswrong.com/posts/eTf7Tnu9SNipcSoRT/link-judea-pearl-wins-2011-turing-award", "postedAtFormatted": "Thursday, March 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Judea%20Pearl%20wins%202011%20Turing%20Award&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Judea%20Pearl%20wins%202011%20Turing%20Award%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeTf7Tnu9SNipcSoRT%2Flink-judea-pearl-wins-2011-turing-award%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Judea%20Pearl%20wins%202011%20Turing%20Award%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeTf7Tnu9SNipcSoRT%2Flink-judea-pearl-wins-2011-turing-award", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeTf7Tnu9SNipcSoRT%2Flink-judea-pearl-wins-2011-turing-award", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<p>Link to <a href=\"http://www.acm.org/press-room/news-releases/2012/turing-award-11/\">ACM press release</a>.</p>\n<blockquote>\n<p>In addition to their impact on probabilistic reasoning, Bayesian networks completely changed the way causality is treated in the empirical sciences, which are based on experiment and observation. Pearl's work on causality is crucial to the understanding of both daily activity and scientific discovery. It has enabled scientists across many disciplines to articulate causal statements formally, combine them with data, and evaluate them rigorously. His 2000 book <em>Causality: Models, Reasoning, and Inference</em> is among the single most influential works in shaping the theory and practice of knowledge-based systems. His contributions to causal reasoning have had a major impact on the way causality is understood and measured in many scientific disciplines, most notably philosophy, psychology, statistics, econometrics, epidemiology and social science.</p>\n</blockquote>\n<p>While that \"major impact\" still seems to me to be in the early stages of propagating through the various sciences, hopefully this award will inspire more people to study causality and Bayesian statistics in general.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GY5kPPpCoyt9fnTMn": 2, "cq69M9ceLNA35ShTR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eTf7Tnu9SNipcSoRT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 32, "extendedScore": null, "score": 8.661410305551934e-07, "legacy": true, "legacyId": "14067", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-15T20:13:53.809Z", "modifiedAt": null, "url": null, "title": "Please advise the Singularity Institute with your domain-specific expertise!", "slug": "please-advise-the-singularity-institute-with-your-domain", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:24.801Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w9xYSrB7jEPigrMxq/please-advise-the-singularity-institute-with-your-domain", "pageUrlRelative": "/posts/w9xYSrB7jEPigrMxq/please-advise-the-singularity-institute-with-your-domain", "linkUrl": "https://www.lesswrong.com/posts/w9xYSrB7jEPigrMxq/please-advise-the-singularity-institute-with-your-domain", "postedAtFormatted": "Thursday, March 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Please%20advise%20the%20Singularity%20Institute%20with%20your%20domain-specific%20expertise!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APlease%20advise%20the%20Singularity%20Institute%20with%20your%20domain-specific%20expertise!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw9xYSrB7jEPigrMxq%2Fplease-advise-the-singularity-institute-with-your-domain%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Please%20advise%20the%20Singularity%20Institute%20with%20your%20domain-specific%20expertise!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw9xYSrB7jEPigrMxq%2Fplease-advise-the-singularity-institute-with-your-domain", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw9xYSrB7jEPigrMxq%2Fplease-advise-the-singularity-institute-with-your-domain", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 167, "htmlBody": "<p>The Singularity Institute would benefit from having a team of domain-specific advisors on hand. If you'd like to help the Singularity Institute pursue its mission more efficiently, <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dG1oUmNybktpUzBXY0JUR1dTSFVkanc6MQ\">please sign up to be a Singularity Institute advisor</a>!</p>\n<p>If you sign up, we will occasionally ask you questions. You may be able to answer our questions or at least point us in the right direction. We may also request to schedule a quick chat with you.</p>\n<p>Domains of expertise we <em>especially</em> need:</p>\n<p>\n<ul>\n<li>Nearly all subfields of economics, maths, computer science &amp; AI, statistics, cognitive science, physics, biology, and naturalistic philosophy.</li>\n<li>U.S. &amp; California law</li>\n<li>Non-profit development and tax compliance</li>\n<li>Marketing &amp; social media</li>\n<li>U.S. government and military processes (lobbying, security, infrastructure, etc.)</li>\n<li>Computer security</li>\n<li>Large-scale logistics</li>\n<li>Event planning</li>\n<li>Executive coaching/training</li>\n<li>Motivational speaking, social skills training</li>\n<li>Publishing</li>\n<li>Running workshops and meetups</li>\n<li>Nuclear security, bio-security, disease control</li>\n</ul>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w9xYSrB7jEPigrMxq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 26, "extendedScore": null, "score": 8.662310825162458e-07, "legacy": true, "legacyId": "14068", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-16T04:53:33.878Z", "modifiedAt": null, "url": null, "title": "Open Thread, March 16-31, 2012", "slug": "open-thread-march-16-31-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:59.615Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zNRDCKWok4iHZHpTy/open-thread-march-16-31-2012", "pageUrlRelative": "/posts/zNRDCKWok4iHZHpTy/open-thread-march-16-31-2012", "linkUrl": "https://www.lesswrong.com/posts/zNRDCKWok4iHZHpTy/open-thread-march-16-31-2012", "postedAtFormatted": "Friday, March 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20March%2016-31%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20March%2016-31%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzNRDCKWok4iHZHpTy%2Fopen-thread-march-16-31-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20March%2016-31%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzNRDCKWok4iHZHpTy%2Fopen-thread-march-16-31-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzNRDCKWok4iHZHpTy%2Fopen-thread-march-16-31-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zNRDCKWok4iHZHpTy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.664428630981962e-07, "legacy": true, "legacyId": "14088", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 117, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-16T05:34:04.595Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Scarcity", "slug": "seq-rerun-scarcity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:24.121Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wF2msnY33xmL6Gp7f/seq-rerun-scarcity", "pageUrlRelative": "/posts/wF2msnY33xmL6Gp7f/seq-rerun-scarcity", "linkUrl": "https://www.lesswrong.com/posts/wF2msnY33xmL6Gp7f/seq-rerun-scarcity", "postedAtFormatted": "Friday, March 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Scarcity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Scarcity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwF2msnY33xmL6Gp7f%2Fseq-rerun-scarcity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Scarcity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwF2msnY33xmL6Gp7f%2Fseq-rerun-scarcity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwF2msnY33xmL6Gp7f%2Fseq-rerun-scarcity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<p>Today's post, <a href=\"/lw/oz/scarcity/\">Scarcity</a> was originally published on 27 March 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Scarcity\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Describes a few pieces of experimental evidence showing that objects or information which are believed to be in short supply are valued more than the same objects or information would be on their own.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/aps/seq_rerun_is_humanism_a_religionsubstitute/\">Is Humanism A Religion-Substitute?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wF2msnY33xmL6Gp7f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 8.664593767936976e-07, "legacy": true, "legacyId": "14091", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["MCYp8g9EMAiTCTawk", "4QCmeemrWk5FGD9vw", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-16T15:36:17.926Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Pittsburgh, Sao Paulo, Seattle, Sydney", "slug": "weekly-lw-meetups-pittsburgh-sao-paulo-seattle-sydney", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:21.555Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EAz8d4csCiWY28FTA/weekly-lw-meetups-pittsburgh-sao-paulo-seattle-sydney", "pageUrlRelative": "/posts/EAz8d4csCiWY28FTA/weekly-lw-meetups-pittsburgh-sao-paulo-seattle-sydney", "linkUrl": "https://www.lesswrong.com/posts/EAz8d4csCiWY28FTA/weekly-lw-meetups-pittsburgh-sao-paulo-seattle-sydney", "postedAtFormatted": "Friday, March 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Pittsburgh%2C%20Sao%20Paulo%2C%20Seattle%2C%20Sydney&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Pittsburgh%2C%20Sao%20Paulo%2C%20Seattle%2C%20Sydney%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEAz8d4csCiWY28FTA%2Fweekly-lw-meetups-pittsburgh-sao-paulo-seattle-sydney%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Pittsburgh%2C%20Sao%20Paulo%2C%20Seattle%2C%20Sydney%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEAz8d4csCiWY28FTA%2Fweekly-lw-meetups-pittsburgh-sao-paulo-seattle-sydney", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEAz8d4csCiWY28FTA%2Fweekly-lw-meetups-pittsburgh-sao-paulo-seattle-sydney", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 421, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/7s\">Pittsburgh Meetup: Big Gaming Fun 4!:&nbsp;<span class=\"date\">11 March 2012 12:00PM</span></a></li>\n<li><a href=\"/meetups/7r\">[Seattle] Queueing and More:&nbsp;<span class=\"date\">11 March 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/7n\">Sydney - Core sequences:&nbsp;<span class=\"date\">13 March 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/7f\">S&atilde;o Paulo Meetup:&nbsp;<span class=\"date\">14 March 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/7o\">Brussels meetup:&nbsp;<span class=\"date\">17 March 2012 11:15AM</span></a></li>\n<li><a href=\"/r/discussion/lw/anx/meetup_atlanta/\">Atlanta:&nbsp;<span class=\"date\">17 March 2012 05:30PM</span></a></li>\n<li><a href=\"/meetups/7u\">Fort Collins Meetup Saturday 17th:&nbsp;<span class=\"date\">17 March 2012 05:00PM</span></a></li>\n<li><a href=\"/meetups/6k\">[Ohio/Washington DC] Interest in Reason Rally meetup?:&nbsp;<span class=\"date\">24 March 2012 04:14PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/7e\">Ohio Monthly:&nbsp;<span class=\"date\">17 March 2012 03:00PM</span></a></li>\n</ul>\n<ul>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>, </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EAz8d4csCiWY28FTA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.667049237529769e-07, "legacy": true, "legacyId": "13859", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["B5ZnKqszZw4duKAWf", "d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-16T17:10:20.396Z", "modifiedAt": null, "url": null, "title": "Global warming is a better test of irrationality that theism", "slug": "global-warming-is-a-better-test-of-irrationality-that-theism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:05.834Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qvw8TFpSH2pfWWyfS/global-warming-is-a-better-test-of-irrationality-that-theism", "pageUrlRelative": "/posts/Qvw8TFpSH2pfWWyfS/global-warming-is-a-better-test-of-irrationality-that-theism", "linkUrl": "https://www.lesswrong.com/posts/Qvw8TFpSH2pfWWyfS/global-warming-is-a-better-test-of-irrationality-that-theism", "postedAtFormatted": "Friday, March 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Global%20warming%20is%20a%20better%20test%20of%20irrationality%20that%20theism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGlobal%20warming%20is%20a%20better%20test%20of%20irrationality%20that%20theism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQvw8TFpSH2pfWWyfS%2Fglobal-warming-is-a-better-test-of-irrationality-that-theism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Global%20warming%20is%20a%20better%20test%20of%20irrationality%20that%20theism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQvw8TFpSH2pfWWyfS%2Fglobal-warming-is-a-better-test-of-irrationality-that-theism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQvw8TFpSH2pfWWyfS%2Fglobal-warming-is-a-better-test-of-irrationality-that-theism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 276, "htmlBody": "<p>Theism is often a default test of irrationality on Less Wrong, but I propose that global warming denial would make a much better candidate.</p>\n<p>Theism is a symptom of excess&nbsp;<a href=\"http://en.wikipedia.org/wiki/Compartmentalization_(psychology)\">compartmentalisation</a>, of not realising that <a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">absence of evidence is evidence of absence</a>, of <a href=\"/lw/i4/belief_in_belief/\">belief in belief</a>, of&nbsp;<a href=\"/lw/19m/privileging_the_hypothesis/\">privileging&nbsp;the hypothesis</a>, and similar failings. But these are not&nbsp;intrinsically&nbsp;huge problems. Indeed, someone with a mild case of theism can have the same anticipations as someone without, and update their evidence in the same way. If they have moved their belief beyond refutation, in theory it thus fails to constrain their anticipations at all; and often this is the case in practice.</p>\n<p>Contrast that with someone who denies the existence of anthropogenic global warming (AGW). This has all the signs of hypothesis privileging, but also reeks of <a href=\"/lw/kq/fake_justification/\">fake justification</a>, <a href=\"http://wiki.lesswrong.com/wiki/Motivated_skepticism\">motivated skepticism</a>, massive <a href=\"http://wiki.lesswrong.com/wiki/Overconfidence\">overconfidence</a> (if they are truly ignorant of the facts of the debate), and simply the raising of politics above rationality. If I knew someone was a global warming skeptic, then I would expect them to be wrong in their beliefs and their anticipations, and to refuse to update when evidence worked against them. I would expect their judgement to be much more impaired than a theist's.</p>\n<p>Of course, reverse stupidity isn't intelligence: simply because one accepts AGW, doesn't make one more rational. I work in England, in a university environment, so my acceptance of AGW is the default position and not a sign of rationality. But if someone is in a&nbsp;milieu&nbsp;that discouraged belief in AGW (one stereotype being heavily Republican areas of the US) and has risen above this, then kudos to them: their acceptance of AGW is indeed a sign of rationality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qvw8TFpSH2pfWWyfS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 67, "baseScore": 2, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "14118", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 113, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mnS2WYLCGJP2kQkRn", "CqyJzDZWvGhhFJ7dY", "X2AD2LgtKgkRNPj2a", "bfbiyTogEKWEGP96S"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-16T17:12:57.728Z", "modifiedAt": null, "url": null, "title": "Muehlhauser-Goertzel Dialogue, Part 1", "slug": "muehlhauser-goertzel-dialogue-part-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:09.690Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TpNRpncLBAzddBnRB/muehlhauser-goertzel-dialogue-part-1", "pageUrlRelative": "/posts/TpNRpncLBAzddBnRB/muehlhauser-goertzel-dialogue-part-1", "linkUrl": "https://www.lesswrong.com/posts/TpNRpncLBAzddBnRB/muehlhauser-goertzel-dialogue-part-1", "postedAtFormatted": "Friday, March 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Muehlhauser-Goertzel%20Dialogue%2C%20Part%201&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMuehlhauser-Goertzel%20Dialogue%2C%20Part%201%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTpNRpncLBAzddBnRB%2Fmuehlhauser-goertzel-dialogue-part-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Muehlhauser-Goertzel%20Dialogue%2C%20Part%201%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTpNRpncLBAzddBnRB%2Fmuehlhauser-goertzel-dialogue-part-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTpNRpncLBAzddBnRB%2Fmuehlhauser-goertzel-dialogue-part-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 9798, "htmlBody": "<p><small>Part of the <a href=\"http://wiki.lesswrong.com/wiki/Muehlhauser_interview_series_on_AGI\">Muehlhauser interview series on AGI</a>.</small></p>\n<p>&nbsp;</p>\n<p><small><em><a href=\"http://lukeprog.com/\">Luke Muehlhauser</a> is Executive Director of the <a href=\"http://intelligence.org/\">Singularity Institute</a>, a non-profit research institute studying AGI safety.</em></small></p>\n<p><small> </small></p>\n<p><small><em><a href=\"http://wp.goertzel.org/?page_id=24\">Ben Goertzel</a> is the Chairman at the AGI company <a href=\"http://wp.novamente.net/\">Novamente</a>, and founder of the <a href=\"http://agi-conf.org/\">AGI conference series</a>.</em></small></p>\n<p><small>\n<p><em><br /></em></p>\n<h3>Luke Muehlhauser:</h3>\n<p>[Jan. 13th, 2012]</p>\n<p>Ben, I'm glad you agreed to discuss artificial general intelligence (AGI) with me. There is much on which we agree, and much on which we disagree, so I think our dialogue will be informative to many readers, and to us!</p>\n<p>Let us begin where we agree. We seem to agree that:</p>\n<ol>\n<li>Involuntary death is bad, and can be avoided with the right technology.</li>\n<li>Humans can be enhanced by merging with technology.</li>\n<li>Humans are on a risky course in general, because powerful technologies can destroy us, humans are often stupid, and we are unlikely to voluntarily halt technological progress.</li>\n<li>AGI is likely this century.</li>\n<li>AGI will, after a slow or hard takeoff, completely transform the world. It is a potential existential risk, but if done wisely, could be the best thing that ever happens to us.</li>\n<li>Careful effort will be required to ensure that AGI results in good things for humanity.</li>\n</ol>\n<p>Next: Where do we disagree?</p>\n<p>Two people might agree about the laws of thought most likely to give us an accurate model of the world, but disagree about which conclusions those laws of thought point us toward. For example, two scientists may use the same scientific method but offer two different models that seem to explain the data.</p>\n<p>Or, two people might disagree about the laws of thought most likely to give us accurate models of the world. If that's the case, it will be no surprise that we disagree about which conclusions to draw from the data. We are not shocked when scientists and theologians end up with different models of the world.</p>\n<p>Unfortunately, I suspect you and I disagree at the more fundamental level &mdash; about which methods of reasoning to use when seeking an accurate model of the world.</p>\n<p>I sometimes use the term \"<a href=\"http://facingthesingularity.com/2011/from-skepticism-to-technical-rationality/\">Technical Rationality</a>\" to name my methods of reasoning. Technical Rationality is drawn from two sources: (1) the <a href=\"http://facingthesingularity.com/2011/the-laws-of-thought/\">laws of logic, probability theory, and decision theory</a>, and (2) the <a href=\"http://facingthesingularity.com/2011/the-crazy-robots-rebellion/\">cognitive science</a> of how our haphazardly evolved brains <em>fail to reason in accordance</em> with the laws of logic, probability theory, and decision theory.</p>\n<p>Ben, at one time you <a href=\"https://twitter.com/#!/bengoertzel/statuses/48066955227299840\">tweeted</a> a William S. Burroughs quote: \"Rational thought is a failed experiment and should be phased out.\" I don't know whether Burroughs meant by \"rational thought\" the specific thing I mean by \"rational thought,\" or what exactly you meant to express with your tweet, but I suspect we have different views of how to reason successfully about the world.</p>\n<p>I think I would understand your way of thinking about AGI better if I understand your way of thinking about <em>everything</em>. For example: do you have reason to reject the laws of logic, probability theory, and decision theory? Do you think we disagree about the basic findings of the cognitive science of humans? What are your positive recommendations for reasoning about the world?</p>\n<h3>Ben Goertzel:</h3>\n<p>[Jan 13th, 2012]</p>\n<p>Firstly, I don&rsquo;t agree with that Burroughs quote that \"Rational thought is a failed experiment&rdquo; -- I mostly just tweeted it because I thought it was funny! I&rsquo;m not sure Burroughs agreed with his own quote either. He also liked to say that linguistic communication was a failed experiment, introduced by women to help them oppress men into social conformity. Yet he was a writer and loved language. He enjoyed being a provocateur.</p>\n<p>However, I do think that some people overestimate the power and scope of rational thought. That is the truth at the core of Burroughs&rsquo; entertaining hyperbolic statement....</p>\n<p>I should clarify that I&rsquo;m a huge fan of logic, reason and science. Compared to the average human being, I&rsquo;m practically obsessed with these things! I don&rsquo;t care for superstition, nor for unthinking acceptance of what one is told; and I spent a lot of time staring at data of various sorts, trying to understand the underlying reality in a rational and scientific way. So I don&rsquo;t want to be pigeonholed as some sort of anti-rationalist!</p>\n<p>However, I do have serious doubts both about the power and scope of rational thought in general -- and much more profoundly, about the power and scope of what you call &ldquo;technical rationality.&rdquo;</p>\n<p>First of all, about the limitations of rational thought broadly conceived -- what one might call &ldquo;semi-formal rationality&rdquo;, as opposed to &ldquo;technical rationality.&rdquo; Obviously this sort of rationality has brought us amazing things, like science and mathematics and technology. Hopefully it will allow us to defeat involuntary death and increase our IQs by orders of magnitude and discover new universes, and all sorts of great stuff. However, it does seem to have its limits.</p>\n<p>It doesn&rsquo;t deal well with consciousness -- studying consciousness using traditional scientific and rational tools has just led to a mess of confusion. It doesn&rsquo;t deal well with ethics either, as the current big mess regarding bioethics indicates.</p>\n<p>And this is more speculative, but I tend to think it doesn&rsquo;t deal that well with the spectrum of &ldquo;anomalous phenomena&rdquo; -- precognition, extrasensory perception, remote viewing, and so forth. I strongly suspect these phenomena exist, and that they can be understood to a significant extent via science -- but also that science as presently constituted may not be able to grasp them fully, due to issues like the mindset of the experimenter helping mold the results of the experiment.</p>\n<p>There&rsquo;s the minor issue of Hume&rsquo;s problem of induction, as well. I.e., the issue that, in the rational and scientific world-view, that we have no rational reason to believe that any patterns observed in the past will continue into the future. This is an ASSUMPTION, plain and simple -- an act of faith. Occam&rsquo;s Razor (which is one way of justifying and/or further specifying the belief that patterns observed in the past will continue into the future) is also an assumption and an act of faith. Science and reason rely on such acts of faith, yet provide no way to justify them. A big gap.</p>\n<p>Furthermore -- and more to the point about AI -- I think there&rsquo;s a limitation to the way we now model intelligence, which ties in with the limitations of the current scientific and rational approach. I have always advocated a view of intelligence as &ldquo;achieving complex goals in complex environments&rdquo;, and many others have formulated and advocated similar views. The basic idea here is that, for a system to be intelligent it doesn&rsquo;t matter WHAT its goal is, so long as its goal is complex and it manages to achieve it. So the goal might be, say, reshaping every molecule in the universe into an image of Mickey Mouse. This way of thinking about intelligence, in which the goal is strictly separated from the methods for achieving it, is very useful and I&rsquo;m using it to guide my own practical AGI work.</p>\n<p>On the other hand, there&rsquo;s also a sense in which reshaping every molecule in the universe into an image of Mickey Mouse is a STUPID goal. It&rsquo;s somehow out of harmony with the Cosmos -- at least that&rsquo;s my intuitive feeling. I&rsquo;d like to interpret intelligence in some way that accounts for the intuitively apparent differential stupidity of different goals. In other words, I&rsquo;d like to be able to deal more sensibly with the interaction of scientific and normative knowledge. This ties in with the incapacity of science and reason in their current forms to deal with ethics effectively, which I mentioned a moment ago.</p>\n<p>I certainly don&rsquo;t have all the answers here -- I&rsquo;m just pointing out the complex of interconnected reasons why I think contemporary science and rationality are limited in power and scope, and are going to be replaced by something richer and better as the growth of our individual and collective minds progresses. What will this new, better thing be? I&rsquo;m not sure -- but I have an inkling it will involve an integration of &ldquo;third person&rdquo; science/rationality with some sort of systematic approach to first-person and second-person experience.</p>\n<p>Next, about &ldquo;technical rationality&rdquo; -- of course that&rsquo;s a whole other can of worms. Semi-formal rationality has a great track record; it&rsquo;s brought us science and math and technology, for example. So even if it has some limitations, we certainly owe it some respect! Technical rationality has no such track record, and so my semi-formal scientific and rational nature impels me to be highly skeptical of it! I have no reason to believe, at present, that focusing on technical rationality (as opposed to the many other ways to focus our attention, given our limited time and processing power) will generally make people more intelligent or better at achieving their goals. Maybe it will, in some contexts -- but what those contexts are, is something we don&rsquo;t yet understand very well.</p>\n<p>I provided consulting once to a project aimed at using computational neuroscience to understand the neurobiological causes of cognitive biases in people employed to analyze certain sorts of data. This is interesting to me; and it&rsquo;s clear to me that in this context, minimization of some of these textbook cognitive biases would help these analysts to do their jobs better. I&rsquo;m not sure how big an effect the reduction of these biases would have on their effectiveness, though, relative to other changes one might make, such as changes to their workplace culture or communication style.</p>\n<p>On a mathematical basis, the justification for positing probability theory as the &ldquo;correct&rdquo; way to do reasoning under uncertainty relies on arguments like Cox&rsquo;s axioms, or de Finetti&rsquo;s Dutch Book arguments. These are beautiful pieces of math, but when you talk about applying them to the real world, you run into a lot of problems regarding the inapplicability of their assumptions. For instance, Cox&rsquo;s axioms include an axiom specifying that (roughly speaking) multiple pathways of arriving at the same conclusion must lead to the same estimate of that conclusion&rsquo;s truth value. This sounds sensible but in practice it&rsquo;s only going to be achievable by minds with arbitrarily much computing capability at their disposal. In short, the assumptions underlying Cox&rsquo;s axioms, de Finetti&rsquo;s arguments, or any of the other arguments in favor of probability theory as the correct way of reasoning under uncertainty, do NOT apply to real-world intelligences operating under strictly bounded computational resources. They&rsquo;re irrelevant to reality, except as inspirations to individuals of a certain cast of mind.</p>\n<p>(An aside is that my own approach to AGI does heavily involve probability theory -- using a system I invented called Probabilistic Logic Networks, which integrates probability and logic in a unique way. I like probabilistic reasoning. I just don&rsquo;t venerate it as uniquely powerful and important. In my OpenCog AGI architecture, it&rsquo;s integrated with a bunch of other AI methods, which all have their own strengths and weaknesses.)</p>\n<p>So anyway -- there&rsquo;s no formal mathematical reason to think that &ldquo;technical rationality&rdquo; is a good approach in real-world situations; and &ldquo;technical rationality&rdquo; has no practical track record to speak of. And ordinary, semi-formal rationality itself seems to have some serious limitations of power and scope.</p>\n<p>So what&rsquo;s my conclusion? Semi-formal rationality is fantastic and important and we should use it and develop it -- but also be open to the possibility of its obsolescence as we discover broader and more incisive ways of understanding the universe (and this is probably moderately close to what William Burroughs really thought). Technical rationality is interesting and well worth exploring but we should still be pretty skeptical of its value, at this stage -- certainly, anyone who has supreme confidence that technical rationality is going to help humanity achieve its goals better, is being rather IRRATIONAL ;-) &hellip;.</p>\n<p>In this vein, I&rsquo;ve followed the emergence of the Less Wrong community with some amusement and interest. One ironic thing I&rsquo;ve noticed about this community of people intensely concerned with improving their personal rationality is: by and large, these people are already hyper-developed in the area of rationality, but underdeveloped in other ways! Think about it -- who is the prototypical Less Wrong meetup participant? It&rsquo;s a person who&rsquo;s very rational already, relative to nearly all other humans -- but relatively lacking in other skills like intuitively and empathically understanding other people. But instead of focusing on improving their empathy and social intuition (things they really aren&rsquo;t good at, relative to most humans), this person is focusing on fine-tuning their rationality more and more, via reprogramming their brains to more naturally use &ldquo;technical rationality&rdquo; tools! This seems a bit imbalanced. If you&rsquo;re already a fairly rational person but lacking in other aspects of human development, the most rational thing may be NOT to focus on honing your &ldquo;rationality fu&rdquo; and better internalizing Bayes&rsquo; rule into your subconscious -- but rather on developing those other aspects of your being.... An analogy would be: If you&rsquo;re very physically strong but can&rsquo;t read well, and want to self-improve, what should you focus your time on? Weight-lifting or literacy? Even if greater strength is ultimately your main goal, one argument for focusing on literacy would be that you might read something that would eventually help you weight-lift better! Also you might avoid getting ripped off by a corrupt agent offering to help you with your bodybuilding career, due to being able to read your own legal contracts. Similarly, for people who are more developed in terms of rational inference than other aspects, the best way for them to become more rational might be for them to focus time on these other aspects (rather than on fine-tuning their rationality), because this may give them a deeper and broader perspective on rationality and what it really means.</p>\n<p>Finally, you asked: &ldquo;What are your positive recommendations for reasoning about the world?&rdquo; I&rsquo;m tempted to quote Nietzsche&rsquo;s Zarathustra, who said &ldquo;Go away from me and resist Zarathustra!&rdquo; I tend to follow my own path, and generally encourage others to do the same. But I guess I can say a few more definite things beyond that....</p>\n<p>To me it&rsquo;s all about balance. My friend Allan Combs calls himself a &ldquo;philosophical Taoist&rdquo; sometimes; I like that line! Think for yourself; but also, try to genuinely listen to what others have to say. Reason incisively and analytically; but also be willing to listen to your heart, gut and intuition, even if the logical reasons for their promptings aren&rsquo;t apparent. Think carefully through the details of things; but don&rsquo;t be afraid to make wild intuitive leaps. Pay close mind to the relevant data and observe the world closely and particularly; but don&rsquo;t forget that empirical data is in a sense a product of the mind, and facts only have meaning in some theoretical context. Don&rsquo;t let your thoughts be clouded by your emotions; but don&rsquo;t be a feeling-less automaton, don&rsquo;t make judgments that are narrowly rational but fundamentally unwise. As Ben Franklin said, &ldquo;Moderation in all things, including moderation.&rdquo;</p>\n<h3>Luke:</h3>\n<p>[Jan 14th, 2012]</p>\n<p>I whole-heartedly agree that there are plenty of Less Wrongers who, rationally, should spend less time studying rationality and more time practicing social skills and generic self-improvement methods! This is part of why I've written so many scientific self-help posts for Less Wrong: <a href=\"/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/\">Scientific Self Help</a>, <a href=\"/lw/3w3/how_to_beat_procrastination/\">How to Beat Procrastination</a>, <a href=\"/lw/4su/how_to_be_happy/\">How to Be Happy</a>, <a href=\"/lw/63i/rational_romantic_relationships_part_1/\">Rational Romantic Relationships</a>, and others. It's also why I taught social skills classes at our two summer 2011 <a href=\"http://intelligence.org/blog/2011/06/21/rationality-minicamp-a-success/\">rationality</a> <a href=\"/lw/4wm/rationality_boot_camp/\">camps</a>.</p>\n<p>Back to rationality. You talk about the \"limitations\" of \"what one might call 'semi-formal rationality', as opposed to 'technical rationality.'\" But I argued for technical rationality, so: what are the limitations of technical rationality? Does it, as you claim for \"semi-formal rationality,\" fail to apply to consciousness or ethics or precognition? Does Bayes' Theorem remain true when looking at the evidence about awareness, but cease to be true when we look at the evidence concerning consciousness or precognition?</p>\n<p>You talk about technical rationality's lack of a track record, but I don't know what you mean. Science was successful because it did a much better job of approximating perfect Bayesian probability theory than earlier methods did (e.g. faith, tradition), and science can be even <em>more</em> successful when it tries <em>harder</em> to approximate perfect Bayesian probability theory &mdash; see <a href=\"http://www.amazon.com/Theory-That-Would-Not-Die/dp/0300169698/\">The Theory That Would Not Die</a>.</p>\n<p>You say that \"minimization of some of these textbook cognitive biases would help [some] analysts to do their jobs better. I&rsquo;m not sure how big an effect the reduction of these biases would have on their effectiveness, though, relative to other changes one might make, such as changes to their workplace culture or communication style.\" But this misunderstands <a href=\"http://facingthesingularity.com/2011/the-laws-of-thought/\">what I mean by Technical Rationality</a>. If teaching these people about cognitive biases would lower the expected value of some project, then technical rationality would recommend against teaching these people cognitive biases (at least, for the purposes of maximizing the expected value of that project). Your example here is a case of <a href=\"http://facingthesingularity.com/2011/why-spock-is-not-rational/\">Straw Man Rationality</a>. (But of course I didn't expect you to know everything I meant by Technical Rationality in advance! Though, I did provide a link to an explanation of what I meant by Technical Rationality in my first entry, above.)</p>\n<p>The same goes for your dismissal of probability theory's foundations. You write that \"In short, the assumptions underlying Cox&rsquo;s axioms, de Finetti&rsquo;s arguments, or any of the other arguments in favor of probability theory as the correct way of reasoning under uncertainty, do NOT apply to real-world intelligences operating under strictly bounded computational resources.\" Yes, we don't have infinite computing power. The point is that Bayesian probability theory is an <em>ideal</em> that can be approximated by finite beings. That's why science works better than faith &mdash; it's a better approximation of using probability theory to reason about the world, even though science is still a long way from a <em>perfect</em> use of probability theory.</p>\n<p>Re: goals. Your view of intelligence as \"achieving complex goals in complex environments\" does, as you say, assume that \"the goal is strictly separated from the methods for achieving it.\" I prefer a definition of intelligence as \"<a href=\"http://facingthesingularity.com/2011/playing-taboo-with-intelligence/\">efficient cross-domain optimization</a>\", but my view &mdash; like yours &mdash; also assumes that goals (what one values) are logically orthogonal to intelligence (one's ability to achieve what one values).</p>\n<p>Nevertheless, you report an intuition that shaping every molecule into an image of Mickey Mouse is a \"stupid\" goal. But I don't know what you mean by this. A goal of shaping every molecule into an image of Mickey Mouse is an instrumentally intelligent goal if one's utility function will be maximized that way. Do you mean that it's a stupid goal according to <em>your</em> goals? But of course. This is, moreover, what we would expect your intuitive judgments to report, even if your intuitive judgments are irrelevant to the math of what would and wouldn't be an instrumentally intelligent goal for a <em>different</em> agent to have. The Mickey Mouse goal is \"stupid\" only by a definition of that term that is <em>not</em> the opposite of the explicit definitions either of us gave \"intelligent,\" and it's important to keep that clear. And I certainly don't know what \"out of harmony with the Cosmos\" is supposed to mean.</p>\n<p>Re: induction. I won't dive into that philosophical morass here. Suffice it to say that my views on the matter are expressed pretty well in <a href=\"/lw/s0/where_recursive_justification_hits_bottom/\">Where Recursive Justification Hits Bottom</a>, which is also a direct response to your view that science and reason are great but rely on \"acts of faith.\"</p>\n<p>Your final paragraph sounds like common sense, but it's too vague, as I think you would agree. One way to force a more precise answer to such questions is to think of how you'd program it into an AI. As Daniel Dennett said, \"AI makes philosophy honest.\"</p>\n<p>How would you program an AI to learn about reality, if you wanted it to have the most accurate model of reality possible? You'd have to be a bit more specific than \"Think for yourself; but also, try to genuinely listen to what others have to say. Reason incisively and analytically; but also be willing to listen to your heart, gut and intuition&hellip;\"</p>\n<p>My own answer to the question of how I would program an AI to build as accurate a model of reality as possible is this: I would build it to use computable approximations of perfect technical rationality &mdash; that is, roughly: computable approximations of <a href=\"http://www.vetta.org/documents/disSol.pdf\">Solomonoff induction</a> and <a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">Bayesian decision theory</a>.</p>\n<h3>Ben:</h3>\n<p>[Jan 21st, 2012]</p>\n<p>Bayes Theorem is &ldquo;always true&rdquo; in a formal sense, just like 1+1=2, obviously. However, the connection between formal mathematics and subjective experience, is not something that can be fully formalized.</p>\n<p>Regarding consciousness, there are many questions, including what counts as &ldquo;evidence.&rdquo; In science we typically count something as evidence if the vast majority of the scientific community counts it as a real observation -- so ultimately the definition of &ldquo;evidence&rdquo; bottoms out in social agreement. But there&rsquo;s a lot that&rsquo;s unclear in this process of classifying an observation as evidence via a process of social agreement among multiple minds. This unclarity is mostly irrelevant to the study of trajectories of basketballs, but possibly quite relevant to study of consciousness.</p>\n<p>Regarding psi, there are lots of questions, but one big problem is that it&rsquo;s possible the presence and properties of a psi effect may depend on the broad context of the situation whether the effect takes place. Since we don&rsquo;t know which aspects of the context are influencing the psi effect, we don&rsquo;t know how to construct controlled experiments to measure psi. And we may not have the breadth of knowledge nor the processing power to reason about all the relevant context to a psi experiment, in a narrowly &ldquo;technically rational&rdquo; way.... I do suspect one can gather solid data demonstrating and exploring psi (and based on my current understanding, it seems this has already been done to a significant extent by the academic parapsychology community; see <a href=\"http://wp.goertzel.org/?page_id=154\">a few links I&rsquo;ve gathered here</a>), but I also suspect there many be aspects that elude the traditional scientific method, but are nonetheless perfectly real aspects of the universe.</p>\n<p>Anyway both consciousness and psi are big, deep topics, and if we dig into them in detail, this interview will become longer than either of us has time for...</p>\n<p>About the success of science -- I don&rsquo;t really accept your Bayesian story for why science was successful. It&rsquo;s naive for reasons much discussed by philosophers of science. My own take on the history and philosophy of science, from a few years back, is <a href=\"http://www.goertzel.org/dynapsyc/2004/PhilosophyOfScience_v2.htm\">here</a> (that article was the basis for a chapter in <em>The Hidden Pattern</em>, also). My goal in that essay was &ldquo;a philosophical perspective that does justice to both the <em>relativism and sociological embeddedness</em> of science, and the <em>objectivity and rationality</em> of science.&rdquo; It seems you focus overly much on the latter and ignore the former. That article tries to explain why probabilist explanations of real-world science are quite partial and miss a lot of the real story. But again, a long debate on the history of science would take us too far off track from the main thrust of this interview.</p>\n<p>About technical rationality, cognitive biases, etc. -- I did read that blog entry that you linked, on technical rationality. Yes, it&rsquo;s obvious that focusing on teaching an employee to be more rational, need not always be the most rational thing for an employer do, even if that employer has a purely rationalist world-view. For instance, if I want to train an attack dog, I may do better by focusing limited time and attention on increasing his strength rather than his rationality. My point was that there&rsquo;s a kind of obsession with rationality in some parts of the intellectual community (e.g. some of the Less Wrong orbit) that I find a bit excessive and not always productive. But your reply impels me to distinguish two ways this excess may manifest itself:</p>\n<ol>\n<li>Excessive belief that rationality is the &ldquo;right&rdquo; way to solve problems and think about issues, in principle</li>\n<li>Excessive belief that, tactically, explicitly employing tools of technical rationality is a good way to solve problems in the real world</li>\n</ol>\n<p>Psychologically I think these two excesses probably tend to go together, but they&rsquo;re not logically coupled. In principle, someone could hold either one, but not the other.</p>\n<p>This sort of ties in with your comments on science and faith. You view science as progress over faith -- and I agree if you interpret &ldquo;faith&rdquo; to mean &ldquo;traditional religions.&rdquo; But if you interpret &ldquo;faith&rdquo; more broadly, I don&rsquo;t see a dichotomy there. Actually, I find the dichotomy between &ldquo;science&rdquo; and &ldquo;faith&rdquo; unfortunately phrased, since science itself ultimately relies on acts of faith also. The &ldquo;problem of induction&rdquo; can&rsquo;t be solved, so every scientist must base his extrapolations from past into future based on some act of faith. It&rsquo;s not a matter of science vs. faith, it&rsquo;s a matter of what one chooses to place one&rsquo;s faith in. I&rsquo;d personally rather place faith in the idea that patterns observed in the past will likely continue into the future (as one example of a science-friendly article of faith), than in the word of some supposed &ldquo;God&rdquo; -- but I realize I&rsquo;m still making an act of faith.</p>\n<p>This ties in with the blog post &ldquo;Where Recursive Justification Hits Bottom&rdquo; that you pointed out. It&rsquo;s pleasant reading but of course doesn&rsquo;t provide any kind of rational argument against my views. In brief, according to my interpretation, it articulates a faith in the process of endless questioning:</p>\n<p><em>The important thing is to <strong>hold nothing back</strong> in your criticisms of how to criticize; nor should you regard the unavoidability of loopy justifications as a warrant of <strong>immunity from questioning</strong>.</em></p>\n<p>I share that faith, personally.</p>\n<p>Regarding approximations to probabilistic reasoning under realistic conditions (of insufficient resources), the problem is that we lack rigorous knowledge about what they are. We don&rsquo;t have any theorems telling us what is the best way to reason about uncertain knowledge, in the case that our computational resources are extremely restricted. You seem to be assuming that the best way is to explicitly use the rules of probability theory, but my point is that there is no mathematical or scientific foundation for this belief. You are making an act of faith in the doctrine of probability theory! You are assuming, because it feels intuitively and emotionally right to you, that even if the conditions of the arguments for the correctness of probabilistic reasoning are NOT met, then it still makes sense to use probability theory to reason about the world. But so far as I can tell, you don&rsquo;t have a RATIONAL reason for this assumption, and certainly not a mathematical reason.</p>\n<p>Re your response to my questioning the reduction of intelligence to goals and optimization -- I understand that you are intellectually committed to the perspective of intelligence in terms of optimization or goal-achievement or something similar to that. Your response to my doubts about this perspective basically just re-asserts your faith in the correctness and completeness of this sort of perspective. Your statement</p>\n<p><em>The Mickey Mouse goal is \"stupid\" only by a definition of that term that is not the opposite of the explicit definitions either of us gave \"intelligent,\" and it's important to keep that clear</em></p>\n<p>basically asserts that it&rsquo;s important to agree with your opinion on the ultimate meaning of intelligence!</p>\n<p>On the contrary, I think it&rsquo;s important to explore alternatives to the understanding of intelligence in terms of optimization or goal-achievement. That is something I&rsquo;ve been thinking about a lot lately. However, I don&rsquo;t have a really crisply-formulated alternative yet.</p>\n<p>As a mathematician, I tend not to think there&rsquo;s a &ldquo;right&rdquo; definition for anything. Rather, one explains one&rsquo;s definitions, and then works with them and figures out their consequences. In my AI work, I&rsquo;ve provisionally adopted a goal-achievemement based understanding of intelligence -- and have found this useful, to a significant extent. But I don&rsquo;t think this is the true and ultimate way to understand intelligence. I think the view of intelligence in terms of goal-achievement or cross-domain optimization misses something, which future understandings of intelligence will encompass. I&rsquo;ll venture that in 100 years the smartest beings on Earth will have a rigorous, detailed understanding of intelligence according to which</p>\n<p><em>The Mickey Mouse goal is \"stupid\" only by a definition of that term that is not the opposite of the explicit definitions either of us gave \"intelligent,\" and it's important to keep that clear</em></p>\n<p>seems like rubbish.....</p>\n<p>As for your professed inability to comprehend the notion of &ldquo;harmony with the Cosmos&rdquo; -- that&rsquo;s unfortunate for you, but I guess trying to give you a sense for that notion, would take us way too far afield in this dialogue!</p>\n<p>Finally, regarding your complaint that my indications regarding how to understanding the world are overly vague. Well -- according to Franklin&rsquo;s idea of &ldquo;Moderation in all things, including moderation&rdquo;, one should also exercise moderation in precisiation. Not everything needs to be made completely precise and unambiguous (fortunately, since that&rsquo;s not feasible anyway).</p>\n<p>I don&rsquo;t know how I would program an AI to build as accurate a model of reality as possible, if that were my goal. I&rsquo;m not sure that&rsquo;s the best goal for AI development, either. An accurate model in itself, doesn&rsquo;t do anything helpful. My best stab in the direction of how I would ideally create an AI, if computational resource restrictions were no issue, is the GOLEM design that I described <a href=\"http://goertzel.org/GOLEM.pdf\">here</a>. GOLEM is a design for a strongly self-modifying superintelligent AI system, which might plausibly have the possibility of retaining its initial goal system through successive self-modifications. However, it&rsquo;s unclear to me whether it will ever be feasible to build.</p>\n<p>You mention Solomonoff induction and Bayesian decision theory. But these are abstract mathematical constructs, and it&rsquo;s unclear to me whether it will ever be feasible to build an AI system fundamentally founded on these ideas, and operating within feasible computational resources. Marcus Hutter and Juergen Schmidhuber and their students are making some efforts in this direction, and I admire those researchers and this body of work, but don&rsquo;t currently have a high estimate of its odds of leading to any sort of powerful real-world AGI system.</p>\n<p>Most of my thinking about AGI has gone into the more practical problem of how to make a human-level AGI</p>\n<ol>\n<li>using currently feasible computational resources</li>\n<li>that will most likely be helpful rather than harmful in terms of the things I value</li>\n<li>that will be smoothly extensible to intelligence beyond the human level as well. </li>\n</ol>\n<p>For this purpose, I think Solomonoff induction and probability theory are useful, but aren&rsquo;t all-powerful guiding principles. For instance, in the <a href=\"http://opencog.org/\">OpenCog</a> AGI design (which is my main practical AGI-oriented venture at present), there is a component doing automated program learning of small programs -- and inside our program learning algorithm, we explicitly use an Occam bias, motivated by the theory of Solomonoff induction. And OpenCog also has a probabilistic reasoning engine, based on the math of Probabilistic Logic Networks (PLN). I don&rsquo;t tend to favor the language of &ldquo;Bayesianism&rdquo;, but I would suppose PLN should be considered &ldquo;Bayesian&rdquo; since it uses probability theory (including Bayes rule) and doesn&rsquo;t make a lot of arbitrary, a priori distributional assumptions. The truth value formulas inside PLN are based on an extension of imprecise probability theory, which in itself is an extension of standard Bayesian methods (looking at envelopes of prior distributions, rather than assuming specific priors).</p>\n<p>In terms of how to get an OpenCog system to model the world effectively and choose its actions appropriately, I think teaching it and working together with it, will be be just as important as programming it. Right now the project is early-stage and the OpenCog design is maybe 50% implemented. But assuming the design is right, once the implementation is done, we&rsquo;ll have a sort of idiot savant childlike mind, that will need to be educated in the ways of the world and humanity, and to learn about itself as well. So the general lessons of how to confront the world, that I cited above, would largely be imparted via interactive experiential learning, vaguely the same way that human kids learn to confront the world from their parents and teachers.</p>\n<p>Drawing a few threads from this conversation together, it seems that</p>\n<ol>\n<li>I think technical rationality, and informal semi-rationality, are both useful tools for confronting life -- but not all-powerful</li>\n<li>I think Solomonoff induction and probability theory are both useful tools for constructing AGI systems -- but not all-powerful</li>\n</ol>\n<p>whereas you seem to ascribe a more fundamental, foundational basis to these particular tools.</p>\n<h3>Luke:</h3>\n<p>[Jan. 21st, 2012]</p>\n<p>To sum up, from my point of view:</p>\n<ol>\n<li>We seem to disagree on the applications of probability theory. For my part, I'll just point people to <a href=\"http://yudkowsky.net/rational/technical\">A Technical Explanation of Technical Explanation</a>.</li>\n<li>I don't think we disagree much on the \"sociological embeddedness\" of science.</li>\n<li>I'm also not sure how much we really disagree about Solomonoff induction and Bayesian probability theory. I've already agreed that no machine will use these in practice because they are not computable &mdash; my point was about their provable optimality given infinite computation (subject to qualifications; see <a href=\"http://www.amazon.com/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3642060528/\">AIXI</a>).</li>\n</ol>\n<p>You've definitely misunderstood me concerning \"intelligence.\" This part is definitely not true: \"I understand that you are intellectually committed to the perspective of intelligence in terms of optimization or goal-achievement or something similar to that. Your response assumes the correctness and completeness of this sort of perspective.\" Intelligence as efficient cross-domain optimization is merely a <em>stipulated</em> definition. I'm happy to use other definitions of intelligence in conversation, so long as we're clear which definition we're using when we use the word. Or, we can <a href=\"/lw/nv/replace_the_symbol_with_the_substance/\">replace the symbol with the substance</a> and talk about \"efficient cross-domain optimization\" or \"achieving complex goals in complex environments\" without ever using the word \"intelligence.\"</p>\n<p>My point about the Mickey Mouse goal was that when you called the Mickey Mouse goal \"stupid,\" this could be confusing, because \"stupid\" is usually the opposite of \"intelligent,\" but your use of \"stupid\" in that sentence didn't seem to be the opposite of <em>either</em> definition of intelligence we each gave. So I'm still unsure what you mean by calling the Mickey Mouse goal \"stupid.\"</p>\n<p>This topic provides us with a handy transition away from philosophy of science and toward AGI. Suppose there was a machine with a vastly greater-than-human capacity for either \"achieving complex goals in complex environments\" or for \"efficient cross-domain optimization.\" And suppose that machine's utility function would be maximized by reshaping every molecule into a Mickey Mouse shape. We can avoid the tricky word \"stupid,\" here. The question is: Would that machine decide to change its utility function so that it doesn't continue to reshape every molecule into a Mickey Mouse shape? I think this is unlikely, for reasons discussed in <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">Omohundro (2008)</a>.</p>\n<p>I suppose a natural topic of conversation for us would be your October 2010 blog post <a href=\"http://multiverseaccordingtoben.blogspot.com/2010/10/singularity-institutes-scary-idea-and.html\">The Singularity Institute's's Scary Idea (and Why I Don't Buy It)</a>. Does that post still reflect your views pretty well, Ben?</p>\n<h4>Ben:</h4>\n<p>[Mar 10th, 2012]</p>\n<p>About the hypothetical uber-intelligence that wants to tile the cosmos with molecular Mickey Mouses -- I truly don&rsquo;t feel confident making any assertions about a real-world system with vastly greater intelligence than me. There are just too many unknowns. Sure, according to certain models of the universe and intelligence that may seem sensible to some humans, it&rsquo;s possible to argue that a hypothetical uber-intelligence like that would relentlessly proceed in tiling the cosmos with molecular Mickey Mouses. But so what? We don&rsquo;t even know that such an uber-intelligence is even a possible thing -- in fact my intuition is that it&rsquo;s not possible.</p>\n<p>Why may it not be possible to create a very smart AI system that is strictly obsessed with that stupid goal? Consider first that it may not be possible to create a real-world, highly intelligent system that is strictly driven by explicit goals -- as opposed to being partially driven by implicit, &ldquo;unconscious&rdquo; (in the sense of deliberative, reflective consciousness) processes that operate in complex interaction with the world outside the system. Because pursuing explicit goals is quite computationally costly compared to many other sorts of intelligent processes. So if a real-world system is necessarily not wholly explicit-goal-driven, it may be that intelligent real-world systems will naturally drift away from certain goals and toward others. My strong intuition is that the goal of tiling the universe with molecular Mickey Mouses would fall into that category. However, I don&rsquo;t yet have any rigorous argument to back this up. Unfortunately my time is limited, and while I generally have more fun theorizing and philosophizing than working on practical projects, I think it&rsquo;s more important for me to push toward building AGI than just spend all my time on fun theory. (And then there&rsquo;s the fact that I have to spend a lot of my time on applied narrow-AI projects to pay the mortgage and put my kids through college, etc.)</p>\n<p>But anyway -- you don&rsquo;t have any rigorous argument to back up the idea that a system like you posit is possible in the real-world, either! And SIAI has staff who, unlike me, are paid full-time to write and philosophize &hellip; and they haven&rsquo;t come up with a rigorous argument in favor of the possibility of such a system, either. Although they have talked about it a lot, though usually in the context of paperclips rather than Mickey Mouses.</p>\n<p>So, I&rsquo;m not really sure how much value there is in this sort of thought-experiment about pathological AI systems that combine massively intelligent practical problem solving capability with incredibly stupid goals (goals that may not even be feasible for real-world superintelligences to adopt, due to their stupidity).</p>\n<p>Regarding the concept of a &ldquo;stupid goal&rdquo; that I keep using, and that you question -- I admit I&rsquo;m not quite sure how to formulate rigorously the idea that tiling the universe with Mickey Mouses is a stupid goal. This is something I&rsquo;ve been thinking about a lot recently. But here&rsquo;s a first rough stab in that direction: I think that if you created a highly intelligent system, allowed it to interact fairly flexibly with the universe, and also allowed it to modify its top-level goals in accordance with its experience, you&rsquo;d be very unlikely to wind up with a system that had this goal (tiling the universe with Mickey Mouses). That goal is out of sync with the Cosmos, in the sense that an intelligent system that&rsquo;s allowed to evolve itself in close coordination with the rest of the universe, is very unlikely to arrive at that goal system. I don&rsquo;t claim this is a precise definition, but it should give you some indication of the direction I&rsquo;m thinking in....</p>\n<p>The tricky thing about this way of thinking about intelligence, which classifies some goals as &ldquo;innately&rdquo; stupider than others, is that it places intelligence not just in the system, but in the system&rsquo;s broad relationship to the universe -- which is something that science, so far, has had a tougher time dealing with. It&rsquo;s unclear to me which aspects of the mind and universe science, as we now conceive it, will be able to figure out. I look forward to understanding these aspects more fully....</p>\n<p>About my blog post on &ldquo;The Singularity Institute&rsquo;s Scary Idea&rdquo; -- yes, that still reflects my basic opinion. After I wrote that blog post, Michael Anissimov -- a long-time SIAI staffer and zealot whom I like and respect greatly -- told me he was going to write up and show me a systematic, rigorous argument as to why &ldquo;an AGI not built based on a rigorous theory of Friendliness is almost certain to kill all humans&rdquo; (the proposition I called &ldquo;SIAI&rsquo;s Scary Idea&rdquo;). But he hasn&rsquo;t followed through on that yet -- and neither has Eliezer or anyone associated with SIAI.</p>\n<p>Just to be clear, I don&rsquo;t really mind that SIAI folks hold that &ldquo;Scary Idea&rdquo; as an intuition. But I find it rather ironic when people make a great noise about their dedication to rationality, but then also make huge grand important statements about the future of humanity, with great confidence and oomph, that are not really backed up by any rational argumentation. This ironic behavior on the part of Eliezer, Michael Anissimov and other SIAI principals doesn&rsquo;t really bother me, as I like and respect them and they are friendly to me, and we&rsquo;ve simply &ldquo;agreed to disagree&rdquo; on these matters for the time being. But the reason I wrote that blog post is because my own blog posts about AGI were being trolled by SIAI zealots (not the principals, I hasten to note) leaving nasty comments to the effect of &ldquo;SIAI has proved that if OpenCog achieves human level AGI, it will kill all humans.&ldquo; Not only has SIAI not proved any such thing, they have not even made a clear rational argument!</p>\n<p>As Eliezer has pointed out to me several times in conversation, a clear rational argument doesn&rsquo;t have to be mathematical. A clearly formulated argument in the manner of analytical philosophy, in favor of the Scary Idea, would certainly be very interesting. For example, philosopher David Chalmers recently wrote a carefully-argued philosophy paper arguing for the plausibility of a Singularity in the next couple hundred years. It&rsquo;s somewhat dull reading, but it&rsquo;s precise and rigorous in the manner of analytical philosophy, in a manner that Kurzweil&rsquo;s writing (which is excellent in its own way) is not. An argument in favor of the Scary Idea, on the level of Chalmers&rsquo; paper on the Singularity, would be an excellent product for SIAI to produce. Of course a mathematical argument might be even better, but that may not be feasible to work on right now, given the state of mathematics today. And of course, mathematics can&rsquo;t do everything -- there&rsquo;s still the matter of connecting mathematics to everyday human experience, which analytical philosophy tries to handle, and mathematics by nature cannot.</p>\n<p>My own suspicion, of course, is that in the process of trying to make a truly rigorous analytical philosophy style formulation of the argument for the Scary Idea, the SIAI folks will find huge holes in the argument. Or, maybe they already intuitively know the holes are there, which is why they have avoided presenting a rigorous write-up of the argument!!</p>\n<h3>Luke:</h3>\n<p>[Mar 11th, 2012]</p>\n<p>I'll drop the stuff about Mickey Mouse so we can move on to AGI. Readers can come to their own conclusions on that.</p>\n<p>Your main complaint seems to be that the Singularity Institute hasn't written up a clear, formal argument (in analytic philosophy's sense, if not the mathematical sense) in defense of our major positions &mdash; something like Chalmers' \"<a href=\"http://consc.net/papers/singularityjcs.pdf\">The Singularity: A Philosophical Analysis</a>\" but more detailed.</p>\n<p>I have the same complaint. I wish \"The Singularity: A Philosophical Analysis\" had been written 10 years ago, by Nick Bostrom and Eliezer Yudkowsky. It <em>could</em> have been written back then. Alas, we had to wait for Chalmers to speak at Singularity Summit 2009 and then write a paper based on his talk. And if it wasn't for Chalmers, I fear we'd still be waiting for such an article to exist. (Bostrom's forthcoming Superintelligence book should be good, though.)</p>\n<p>I was hired by the Singularity Institute in September 2011 and have since then co-written two papers explaining some of the basics: \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Intelligence Explosion: Evidence and Import</a>\" and \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">The Singularity and Machine Ethics</a>\". I also wrote the first ever outline of categories of open research problems in AI risk, cheekily titled \"<a href=\"http://lukeprog.com/SaveTheWorld.html\">So You Want to Save the World</a>\". I'm developing other articles on \"the basics\" as quickly as I can. I would love to write more, but alas, I'm also busy being the Singularity Institute's Executive Director.</p>\n<p>Perhaps we could reframe our discussion around the Singularity Institute's latest exposition of its basic ideas, \"Intelligence Explosion: Evidence and Import\"? Which claims in that paper do you most confidently disagree with, and why?</p>\n<h3>Ben:</h3>\n<p>[Mar 11th, 2012]</p>\n<p>You say &ldquo;Your main complaint seems to be that the Singularity Institute hasn't written up a clear, formal argument (in analytic philosophy's sense, if not the mathematical sense) in defense of our major positions &ldquo;. Actually, my main complaint is that some of SIAI&rsquo;s core positions seem almost certainly WRONG, and yet they haven&rsquo;t written up a clear formal argument trying to justify these positions -- so it&rsquo;s not possible to engage SIAI in rational discussion on their apparently wrong positions. Rather, when I try to engage SIAI folks about these wrong-looking positions (e.g. the &ldquo;Scary Idea&rdquo; I mentioned above), they tend to point me to Eliezer&rsquo;s blog (&ldquo;Less Wrong&rdquo;) and tell me that if I studied it long and hard enough, I would find that the arguments in favor of SIAI&rsquo;s positions are implicit there, just not clearly articulated in any one place. This is a bit frustrating to me -- SIAI is a fairly well-funded organization involving lots of smart people and explicitly devoted to rationality, so certainly it should have the capability to write up clear arguments for its core positions... if these arguments exist. My suspicion is that the Scary Idea, for example, is not backed up by any clear rational argument -- so the reason SIAI has not put forth any clear rational argument for it, is that they don&rsquo;t really have one! Whereas Chalmers&rsquo; paper carefully formulated something that seemed obviously true...</p>\n<p>Regarding the paper \"Intelligence Explosion: Evidence and Import\", I find its contents mainly agreeable -- and also somewhat unoriginal and unexciting, given the general context of 2012 Singularitarianism. The paper&rsquo;s three core claims that</p>\n<p>(1) there is a substantial chance we will create human-level AI before 2100, that (2) if human-level AI is created, there is a good chance vastly superhuman AI will follow via an \"intelligence explosion,\" and that (3) an uncontrolled intelligence explosion could destroy everything we value, but a controlled intelligence explosion would benefit humanity enormously if we can achieve it.</p>\n<p>are things that most &ldquo;Singularitarians&rdquo; would agree with. The paper doesn&rsquo;t attempt to argue for the &ldquo;Scary Idea&rdquo; or Coherent Extrapolated Volition or the viability of creating some sort of provably Friendly AI, -- or any of the other positions that are specifically characteristic of SIAI. Rather, the paper advocates what one might call &ldquo;plain vanilla Singularitarianism.&rdquo; This may be a useful thing to do, though, since after all there are a lot of smart people out there who aren&rsquo;t convinced of plain vanilla Singularitarianism.</p>\n<p>I have a couple small quibbles with the paper, though. I don&rsquo;t agree with Omohundro&rsquo;s argument about the &ldquo;basic AI drives&rdquo; (though Steve is a friend and I greatly respect his intelligence and deep thinking). Steve&rsquo;s argument for the inevitability of these drives in AIs is based on evolutionary ideas, and would seem to hold up in the case that there is a population of distinct AIs competing for resources -- but the argument seems to fall apart in the case of other possibilities like an AGI mindplex (a network of minds with less individuality than current human minds, yet not necessarily wholly blurred into a single mind -- rather, with reflective awareness and self-modeling at both the individual and group level).</p>\n<p>Also, my &ldquo;AI Nanny&rdquo; concept is dismissed too quickly for my taste (though that doesn&rsquo;t surprise me!). You suggest in this paper that to make an AI Nanny, it would likely be necessary to solve the problem of making an AI&rsquo;s goal system persist under radical self-modification. But you don&rsquo;t explain the reasoning underlying this suggestion (if indeed you have any). It seems to me -- as I say in my &ldquo;AI Nanny&rdquo; paper -- that one could probably make an AI Nanny with intelligence significantly beyond the human level, without having to make an AI architecture oriented toward radical self-modification. If you think this is false, it would be nice for you to explain why, rather than simply asserting your view. And your comment &ldquo;Those of us working on AI safety theory would very much appreciate the extra time to solve the problems of AI safety...&rdquo; carries the hint that I (as the author of the AI Nanny idea) am NOT working on AI safety theory. Yet my GOLEM design is a concrete design for a potentially Friendly AI (admittedly not computationally feasible using current resources), and in my view constitutes greater progress toward actual FAI than any of the publications of SIAI so far. (Of course, various SIAI associated folks often allude that there are great, unpublished discoveries about FAI hidden in the SIAI vaults -- a claim I somewhat doubt, but can&rsquo;t wholly dismiss of course....)</p>\n<p>Anyway, those quibbles aside, my main complaint about the paper you cite is that it sticks to &ldquo;plain vanilla Singularitarianism&rdquo; and avoids all of the radical, controversial positions that distinguish SIAI from myself, Ray Kurzweil, Vernor Vinge and the rest of the Singularitarian world. The crux of the matter, I suppose is the third main claim of the paper,</p>\n<p>(3) an uncontrolled intelligence explosion could destroy everything we value, but a controlled intelligence explosion would benefit humanity enormously if we can achieve it.</p>\n<p>This statement is hedged in such a way as to be almost obvious. But yet, what SIAI folks tend to tell me verbally and via email and blog comments is generally far more extreme than this bland and nearly obvious statement.</p>\n<p>As an example, I recall when your co-author on that article, Anna Salamon, guest lectured in the class on Singularity Studies that my father and I were teaching at Rutgers University in 2010. Anna made the statement, to the students, that (I&rsquo;m paraphrasing, though if you&rsquo;re curious you can look up the online course session which was saved online and find her exact wording) &ldquo;If a superhuman AGI is created without being carefully based on an explicit Friendliness theory, it is ALMOST SURE to destroy humanity.&rdquo; (i.e., what I now call SIAI&rsquo;s Scary Idea)</p>\n<p>I then asked her (in the online class session) why she felt that way, and if she could give any argument to back up the idea.</p>\n<p>She gave the familiar SIAI argument that, if one picks a mind at random from &ldquo;mind space&rdquo;, the odds that it will be Friendly to humans are effectively zero.</p>\n<p>I made the familiar counter-argument that this is irrelevant, because nobody is advocating building a random mind. Rather, what some of us are suggesting is to build a mind with a Friendly-looking goal system, and a cognitive architecture that&rsquo;s roughly human-like in nature but with a non-human-like propensity to choose its actions rationally based on its goals, and then raise this AGI mind in a caring way and integrate it into society. Arguments against the Friendliness of random minds are irrelevant as critiques of this sort of suggestion.</p>\n<p>So, then she fell back instead on the familiar (paraphrasing again) &ldquo;OK, but you must admit there&rsquo;s a non-zero risk of such an AGI destroying humanity, so we should be very careful -- when the stakes are so high, better safe than sorry!&rdquo;</p>\n<p>I had pretty much the same exact argument with SIAI advocates Tom McCabe and Michael Anissimov on different occasions; and also, years before, with Eliezer Yudkowsky and Michael Vassar -- and before that, with (former SIAI Executive Director) Tyler Emerson. Over all these years, the SIAI community maintains the Scary Idea in its collective mind, and also maintains a great devotion to the idea of rationality, but yet fails to produce anything resembling a rational argument for the Scary Idea -- instead repetitiously trotting out irrelevant statements about random minds!!</p>\n<p>What I would like is for SIAI to do one of these three things, publicly:</p>\n<ol>\n<li>Repudiate the Scary Idea</li>\n<li>Present a rigorous argument that the Scary Idea is true</li>\n<li>State that the Scary Idea is a commonly held intuition among the SIAI community, but admit that no rigorous rational argument exists for it at this point</li>\n</ol>\n<p>Doing any one of these things would be intellectually honest. Presenting the Scary Idea as a confident conclusion, and then backing off when challenged into a platitudinous position equivalent to &ldquo;there&rsquo;s a non-zero risk &hellip; better safe than sorry...&rdquo;, is not my idea of an intellectually honest way to do things.</p>\n<p>Why does this particular point get on my nerves? Because I don&rsquo;t like SIAI advocates telling people that I, personally, am on a R&amp;D course where if I succeed I am almost certain to destroy humanity!!! That frustrates me. I don&rsquo;t want to destroy humanity; and if someone gave me a rational argument that my work was most probably going to be destructive to humanity, I would stop doing the work and do something else with my time! But the fact that some other people have a non-rational intuition that my work, if successful, would be likely to destroy the world -- this doesn&rsquo;t give me any urge to stop. I&rsquo;m OK with the fact that some other people have this intuition -- but then I&rsquo;d like them to make clear, when they state their views, that these views are based on intuition rather than rational argument. I will listen carefully to rational arguments that contravene my intuition -- but if it comes down to my intuition versus somebody else&rsquo;s, in the end I&rsquo;m likely to listen to my own, because I&rsquo;m a fairly stubborn maverick kind of guy....</p>\n<h3>Luke:</h3>\n<p>[Mar 11th, 2012]</p>\n<p>Ben, you write:</p>\n<blockquote>\n<p>when I try to engage SIAI folks about these wrong-looking positions (e.g. the &ldquo;Scary Idea&rdquo; I mentioned above), they tend to point me to Eliezer&rsquo;s blog (&ldquo;Less Wrong&rdquo;) and tell me that if I studied it long and hard enough, I would find that the arguments in favor of SIAI&rsquo;s positions are implicit there, just not clearly articulated in any one place. This is a bit frustrating to me...</p>\n</blockquote>\n<p>No kidding! It's very frustrating to me, too. That's one reason I'm working to clearly articulate the arguments in one place, starting with articles on the basics like \"Intelligence Explosion: Evidence and Import.\"</p>\n<p>I agree that \"Intelligence Explosion: Evidence and Import\" covers only the basics and does not argue for several positions associated uniquely with the Singularity Institute. It is, after all, the opening chapter of a book intelligence explosion, not the opening chapter of a book on the Singularity Institute's ideas!</p>\n<p>I wanted to write that article first, though, so the Singularity Institute could be clear on the basics. For example, we needed to be clear that: (1) we are not Kurzweil, and our claims don't depend on his detailed storytelling or accelerating change curves, that (2) technological prediction is hard, and we are not being naively overconfident about AI timelines, and that (3) intelligence explosion is a convergent outcome of many paths the future may take. There is also much content that is not found in, for example, Chalmers' paper: (a) an overview of methods of technological prediction, (b) an overview of speed bumps and accelerators toward AI, (c) a reminder of breakthroughs like AIXI, and (d) a summary of AI advantages. (The rest is, as you say, mostly a brief overview of points that have been made elsewhere. But brief overviews are extremely useful!)</p>\n<blockquote>\n<p>...my &ldquo;AI Nanny&rdquo; concept is dismissed too quickly for my taste...</p>\n</blockquote>\n<p>No doubt! I think the idea is clearly worth exploring in several papers devoted to the topic.</p>\n<blockquote>\n<p>It seems to me -- as I say in my &ldquo;AI Nanny&rdquo; paper -- that one could probably make an AI Nanny with intelligence significantly beyond the human level, without having to make an AI architecture oriented toward radical self-modification.</p>\n</blockquote>\n<p>Whereas I tend to buy Omohundro's arguments that advanced AIs will want to self-improve just like humans want to self-improve, so that they become better able to achieve their final goals. Of course, we disagree on Omohundro's arguments &mdash; a topic to which I will return in a moment.</p>\n<blockquote>\n<p>your comment \"Those of us working on AI safety theory would very much appreciate the extra time to solve the problems of AI safety...\" carries the hint that I (as the author of the AI Nanny idea) am NOT working on AI safety theory...</p>\n</blockquote>\n<p>I didn't mean for it to carry that connotation. GOLEM and Nanny AI are both clearly AI safety ideas. I'll clarify that part before I submit a final draft to the editors.</p>\n<p>Moving on: If you are indeed remembering your conversations with Anna, Michael, and others correctly, then again I sympathize with your frustration. I completely agree that it would be useful for the Singularity Institute to produce clear, formal arguments for the important positions it defends. In fact, just yesterday I was talking to <a href=\"https://sites.google.com/site/nbeckstead/\">Nick Beckstead</a> about how badly both of us want to write these kinds of papers if we can find the time.</p>\n<p>So, to respond to your wish that the Singularity Institute choose among three options, my plan is to (1) write up clear arguments for... well, if not \"SIAI's Big Scary Idea\" then for whatever I end up believing after going through the process of formalizing the arguments, and (2) publicly state (right now) that SIAI's Big Scary Idea is a commonly held view at the Singularity Institute but a clear, formal argument for it has never been published (at least, not to my satisfaction).</p>\n<blockquote>\n<p>I don&rsquo;t want to destroy humanity; and if someone gave me a rational argument that my work was most probably going to be destructive to humanity, I would stop doing the work and do something else with my time!</p>\n</blockquote>\n<p>I'm glad to hear it! :)</p>\n<p>Now, it seems a good point of traction is our disagreement over Omohundro's \"Basic AI Drives.\" We could talk about that next, but for now I'd like to give you a moment to reply.</p>\n<h3>Ben:</h3>\n<p>[Mar 11th, 2012]</p>\n<p>Yeah, I agree that your and Anna&rsquo;s article is a good step for SIAI to take, albeit unexciting to a Singularitian insider type like me.... And I appreciate your genuinely rational response regarding the Scary Idea, thanks!</p>\n<p>(And I note that I have also written some &ldquo;unexciting to Singularitarians&rdquo; material lately too, for similar reasons to those underlying your article -- e.g. an article on &ldquo;Why an Intelligence Explosion is Probable&rdquo; for a Springer volume on the Singularity.)</p>\n<p>A quick comment on your statement that</p>\n<p><em>we are not Kurzweil, and our claims don't depend on his detailed storytelling or accelerating change curves,</em></p>\n<p>that&rsquo;s a good point; but yet, any argument for a Singularity soon (e.g. likely this century, as you argue) ultimately depends on some argumentation analogous to Kurzweil&rsquo;s, even if different in detail. I find Kurzweil&rsquo;s detailed extrapolations a bit overconfident and more precise than the evidence warrants; but still, my basic reasons for thinking the Singularity is probably near are fairly similar to his -- and I think your reasons are fairly similar to his as well.</p>\n<p>Anyway, sure, let&rsquo;s go on to Omohundro&rsquo;s posited Basic AI Drives -- which seem to me not to hold as necessary properties of future AIs unless the future of AI consists of a population of fairly distinct AIs competing for resources, which I intuitively doubt will be the situation.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>[to be continued]</strong></p>\n</small></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "ZFrgTgzwEfStg26JL": 1, "Ng8Gice9KNkncxqcj": 1, "CztjQPSTuaQcfbyh8": 1, "oiRp4T6u5poc8r9Tj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TpNRpncLBAzddBnRB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 42, "extendedScore": null, "score": 7.7e-05, "legacy": true, "legacyId": "14119", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Part of the <a href=\"http://wiki.lesswrong.com/wiki/Muehlhauser_interview_series_on_AGI\">Muehlhauser interview series on AGI</a>.</small></p>\n<p>&nbsp;</p>\n<p><small><em><a href=\"http://lukeprog.com/\">Luke Muehlhauser</a> is Executive Director of the <a href=\"http://intelligence.org/\">Singularity Institute</a>, a non-profit research institute studying AGI safety.</em></small></p>\n<p><small> </small></p>\n<p><small><em><a href=\"http://wp.goertzel.org/?page_id=24\">Ben Goertzel</a> is the Chairman at the AGI company <a href=\"http://wp.novamente.net/\">Novamente</a>, and founder of the <a href=\"http://agi-conf.org/\">AGI conference series</a>.</em></small></p>\n<p><small>\n</small></p><p><small><em><br></em></small></p><small>\n<h3 id=\"Luke_Muehlhauser_\">Luke Muehlhauser:</h3>\n<p>[Jan. 13th, 2012]</p>\n<p>Ben, I'm glad you agreed to discuss artificial general intelligence (AGI) with me. There is much on which we agree, and much on which we disagree, so I think our dialogue will be informative to many readers, and to us!</p>\n<p>Let us begin where we agree. We seem to agree that:</p>\n<ol>\n<li>Involuntary death is bad, and can be avoided with the right technology.</li>\n<li>Humans can be enhanced by merging with technology.</li>\n<li>Humans are on a risky course in general, because powerful technologies can destroy us, humans are often stupid, and we are unlikely to voluntarily halt technological progress.</li>\n<li>AGI is likely this century.</li>\n<li>AGI will, after a slow or hard takeoff, completely transform the world. It is a potential existential risk, but if done wisely, could be the best thing that ever happens to us.</li>\n<li>Careful effort will be required to ensure that AGI results in good things for humanity.</li>\n</ol>\n<p>Next: Where do we disagree?</p>\n<p>Two people might agree about the laws of thought most likely to give us an accurate model of the world, but disagree about which conclusions those laws of thought point us toward. For example, two scientists may use the same scientific method but offer two different models that seem to explain the data.</p>\n<p>Or, two people might disagree about the laws of thought most likely to give us accurate models of the world. If that's the case, it will be no surprise that we disagree about which conclusions to draw from the data. We are not shocked when scientists and theologians end up with different models of the world.</p>\n<p>Unfortunately, I suspect you and I disagree at the more fundamental level \u2014 about which methods of reasoning to use when seeking an accurate model of the world.</p>\n<p>I sometimes use the term \"<a href=\"http://facingthesingularity.com/2011/from-skepticism-to-technical-rationality/\">Technical Rationality</a>\" to name my methods of reasoning. Technical Rationality is drawn from two sources: (1) the <a href=\"http://facingthesingularity.com/2011/the-laws-of-thought/\">laws of logic, probability theory, and decision theory</a>, and (2) the <a href=\"http://facingthesingularity.com/2011/the-crazy-robots-rebellion/\">cognitive science</a> of how our haphazardly evolved brains <em>fail to reason in accordance</em> with the laws of logic, probability theory, and decision theory.</p>\n<p>Ben, at one time you <a href=\"https://twitter.com/#!/bengoertzel/statuses/48066955227299840\">tweeted</a> a William S. Burroughs quote: \"Rational thought is a failed experiment and should be phased out.\" I don't know whether Burroughs meant by \"rational thought\" the specific thing I mean by \"rational thought,\" or what exactly you meant to express with your tweet, but I suspect we have different views of how to reason successfully about the world.</p>\n<p>I think I would understand your way of thinking about AGI better if I understand your way of thinking about <em>everything</em>. For example: do you have reason to reject the laws of logic, probability theory, and decision theory? Do you think we disagree about the basic findings of the cognitive science of humans? What are your positive recommendations for reasoning about the world?</p>\n<h3 id=\"Ben_Goertzel_\">Ben Goertzel:</h3>\n<p>[Jan 13th, 2012]</p>\n<p>Firstly, I don\u2019t agree with that Burroughs quote that \"Rational thought is a failed experiment\u201d -- I mostly just tweeted it because I thought it was funny! I\u2019m not sure Burroughs agreed with his own quote either. He also liked to say that linguistic communication was a failed experiment, introduced by women to help them oppress men into social conformity. Yet he was a writer and loved language. He enjoyed being a provocateur.</p>\n<p>However, I do think that some people overestimate the power and scope of rational thought. That is the truth at the core of Burroughs\u2019 entertaining hyperbolic statement....</p>\n<p>I should clarify that I\u2019m a huge fan of logic, reason and science. Compared to the average human being, I\u2019m practically obsessed with these things! I don\u2019t care for superstition, nor for unthinking acceptance of what one is told; and I spent a lot of time staring at data of various sorts, trying to understand the underlying reality in a rational and scientific way. So I don\u2019t want to be pigeonholed as some sort of anti-rationalist!</p>\n<p>However, I do have serious doubts both about the power and scope of rational thought in general -- and much more profoundly, about the power and scope of what you call \u201ctechnical rationality.\u201d</p>\n<p>First of all, about the limitations of rational thought broadly conceived -- what one might call \u201csemi-formal rationality\u201d, as opposed to \u201ctechnical rationality.\u201d Obviously this sort of rationality has brought us amazing things, like science and mathematics and technology. Hopefully it will allow us to defeat involuntary death and increase our IQs by orders of magnitude and discover new universes, and all sorts of great stuff. However, it does seem to have its limits.</p>\n<p>It doesn\u2019t deal well with consciousness -- studying consciousness using traditional scientific and rational tools has just led to a mess of confusion. It doesn\u2019t deal well with ethics either, as the current big mess regarding bioethics indicates.</p>\n<p>And this is more speculative, but I tend to think it doesn\u2019t deal that well with the spectrum of \u201canomalous phenomena\u201d -- precognition, extrasensory perception, remote viewing, and so forth. I strongly suspect these phenomena exist, and that they can be understood to a significant extent via science -- but also that science as presently constituted may not be able to grasp them fully, due to issues like the mindset of the experimenter helping mold the results of the experiment.</p>\n<p>There\u2019s the minor issue of Hume\u2019s problem of induction, as well. I.e., the issue that, in the rational and scientific world-view, that we have no rational reason to believe that any patterns observed in the past will continue into the future. This is an ASSUMPTION, plain and simple -- an act of faith. Occam\u2019s Razor (which is one way of justifying and/or further specifying the belief that patterns observed in the past will continue into the future) is also an assumption and an act of faith. Science and reason rely on such acts of faith, yet provide no way to justify them. A big gap.</p>\n<p>Furthermore -- and more to the point about AI -- I think there\u2019s a limitation to the way we now model intelligence, which ties in with the limitations of the current scientific and rational approach. I have always advocated a view of intelligence as \u201cachieving complex goals in complex environments\u201d, and many others have formulated and advocated similar views. The basic idea here is that, for a system to be intelligent it doesn\u2019t matter WHAT its goal is, so long as its goal is complex and it manages to achieve it. So the goal might be, say, reshaping every molecule in the universe into an image of Mickey Mouse. This way of thinking about intelligence, in which the goal is strictly separated from the methods for achieving it, is very useful and I\u2019m using it to guide my own practical AGI work.</p>\n<p>On the other hand, there\u2019s also a sense in which reshaping every molecule in the universe into an image of Mickey Mouse is a STUPID goal. It\u2019s somehow out of harmony with the Cosmos -- at least that\u2019s my intuitive feeling. I\u2019d like to interpret intelligence in some way that accounts for the intuitively apparent differential stupidity of different goals. In other words, I\u2019d like to be able to deal more sensibly with the interaction of scientific and normative knowledge. This ties in with the incapacity of science and reason in their current forms to deal with ethics effectively, which I mentioned a moment ago.</p>\n<p>I certainly don\u2019t have all the answers here -- I\u2019m just pointing out the complex of interconnected reasons why I think contemporary science and rationality are limited in power and scope, and are going to be replaced by something richer and better as the growth of our individual and collective minds progresses. What will this new, better thing be? I\u2019m not sure -- but I have an inkling it will involve an integration of \u201cthird person\u201d science/rationality with some sort of systematic approach to first-person and second-person experience.</p>\n<p>Next, about \u201ctechnical rationality\u201d -- of course that\u2019s a whole other can of worms. Semi-formal rationality has a great track record; it\u2019s brought us science and math and technology, for example. So even if it has some limitations, we certainly owe it some respect! Technical rationality has no such track record, and so my semi-formal scientific and rational nature impels me to be highly skeptical of it! I have no reason to believe, at present, that focusing on technical rationality (as opposed to the many other ways to focus our attention, given our limited time and processing power) will generally make people more intelligent or better at achieving their goals. Maybe it will, in some contexts -- but what those contexts are, is something we don\u2019t yet understand very well.</p>\n<p>I provided consulting once to a project aimed at using computational neuroscience to understand the neurobiological causes of cognitive biases in people employed to analyze certain sorts of data. This is interesting to me; and it\u2019s clear to me that in this context, minimization of some of these textbook cognitive biases would help these analysts to do their jobs better. I\u2019m not sure how big an effect the reduction of these biases would have on their effectiveness, though, relative to other changes one might make, such as changes to their workplace culture or communication style.</p>\n<p>On a mathematical basis, the justification for positing probability theory as the \u201ccorrect\u201d way to do reasoning under uncertainty relies on arguments like Cox\u2019s axioms, or de Finetti\u2019s Dutch Book arguments. These are beautiful pieces of math, but when you talk about applying them to the real world, you run into a lot of problems regarding the inapplicability of their assumptions. For instance, Cox\u2019s axioms include an axiom specifying that (roughly speaking) multiple pathways of arriving at the same conclusion must lead to the same estimate of that conclusion\u2019s truth value. This sounds sensible but in practice it\u2019s only going to be achievable by minds with arbitrarily much computing capability at their disposal. In short, the assumptions underlying Cox\u2019s axioms, de Finetti\u2019s arguments, or any of the other arguments in favor of probability theory as the correct way of reasoning under uncertainty, do NOT apply to real-world intelligences operating under strictly bounded computational resources. They\u2019re irrelevant to reality, except as inspirations to individuals of a certain cast of mind.</p>\n<p>(An aside is that my own approach to AGI does heavily involve probability theory -- using a system I invented called Probabilistic Logic Networks, which integrates probability and logic in a unique way. I like probabilistic reasoning. I just don\u2019t venerate it as uniquely powerful and important. In my OpenCog AGI architecture, it\u2019s integrated with a bunch of other AI methods, which all have their own strengths and weaknesses.)</p>\n<p>So anyway -- there\u2019s no formal mathematical reason to think that \u201ctechnical rationality\u201d is a good approach in real-world situations; and \u201ctechnical rationality\u201d has no practical track record to speak of. And ordinary, semi-formal rationality itself seems to have some serious limitations of power and scope.</p>\n<p>So what\u2019s my conclusion? Semi-formal rationality is fantastic and important and we should use it and develop it -- but also be open to the possibility of its obsolescence as we discover broader and more incisive ways of understanding the universe (and this is probably moderately close to what William Burroughs really thought). Technical rationality is interesting and well worth exploring but we should still be pretty skeptical of its value, at this stage -- certainly, anyone who has supreme confidence that technical rationality is going to help humanity achieve its goals better, is being rather IRRATIONAL ;-) \u2026.</p>\n<p>In this vein, I\u2019ve followed the emergence of the Less Wrong community with some amusement and interest. One ironic thing I\u2019ve noticed about this community of people intensely concerned with improving their personal rationality is: by and large, these people are already hyper-developed in the area of rationality, but underdeveloped in other ways! Think about it -- who is the prototypical Less Wrong meetup participant? It\u2019s a person who\u2019s very rational already, relative to nearly all other humans -- but relatively lacking in other skills like intuitively and empathically understanding other people. But instead of focusing on improving their empathy and social intuition (things they really aren\u2019t good at, relative to most humans), this person is focusing on fine-tuning their rationality more and more, via reprogramming their brains to more naturally use \u201ctechnical rationality\u201d tools! This seems a bit imbalanced. If you\u2019re already a fairly rational person but lacking in other aspects of human development, the most rational thing may be NOT to focus on honing your \u201crationality fu\u201d and better internalizing Bayes\u2019 rule into your subconscious -- but rather on developing those other aspects of your being.... An analogy would be: If you\u2019re very physically strong but can\u2019t read well, and want to self-improve, what should you focus your time on? Weight-lifting or literacy? Even if greater strength is ultimately your main goal, one argument for focusing on literacy would be that you might read something that would eventually help you weight-lift better! Also you might avoid getting ripped off by a corrupt agent offering to help you with your bodybuilding career, due to being able to read your own legal contracts. Similarly, for people who are more developed in terms of rational inference than other aspects, the best way for them to become more rational might be for them to focus time on these other aspects (rather than on fine-tuning their rationality), because this may give them a deeper and broader perspective on rationality and what it really means.</p>\n<p>Finally, you asked: \u201cWhat are your positive recommendations for reasoning about the world?\u201d I\u2019m tempted to quote Nietzsche\u2019s Zarathustra, who said \u201cGo away from me and resist Zarathustra!\u201d I tend to follow my own path, and generally encourage others to do the same. But I guess I can say a few more definite things beyond that....</p>\n<p>To me it\u2019s all about balance. My friend Allan Combs calls himself a \u201cphilosophical Taoist\u201d sometimes; I like that line! Think for yourself; but also, try to genuinely listen to what others have to say. Reason incisively and analytically; but also be willing to listen to your heart, gut and intuition, even if the logical reasons for their promptings aren\u2019t apparent. Think carefully through the details of things; but don\u2019t be afraid to make wild intuitive leaps. Pay close mind to the relevant data and observe the world closely and particularly; but don\u2019t forget that empirical data is in a sense a product of the mind, and facts only have meaning in some theoretical context. Don\u2019t let your thoughts be clouded by your emotions; but don\u2019t be a feeling-less automaton, don\u2019t make judgments that are narrowly rational but fundamentally unwise. As Ben Franklin said, \u201cModeration in all things, including moderation.\u201d</p>\n<h3 id=\"Luke_\">Luke:</h3>\n<p>[Jan 14th, 2012]</p>\n<p>I whole-heartedly agree that there are plenty of Less Wrongers who, rationally, should spend less time studying rationality and more time practicing social skills and generic self-improvement methods! This is part of why I've written so many scientific self-help posts for Less Wrong: <a href=\"/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/\">Scientific Self Help</a>, <a href=\"/lw/3w3/how_to_beat_procrastination/\">How to Beat Procrastination</a>, <a href=\"/lw/4su/how_to_be_happy/\">How to Be Happy</a>, <a href=\"/lw/63i/rational_romantic_relationships_part_1/\">Rational Romantic Relationships</a>, and others. It's also why I taught social skills classes at our two summer 2011 <a href=\"http://intelligence.org/blog/2011/06/21/rationality-minicamp-a-success/\">rationality</a> <a href=\"/lw/4wm/rationality_boot_camp/\">camps</a>.</p>\n<p>Back to rationality. You talk about the \"limitations\" of \"what one might call 'semi-formal rationality', as opposed to 'technical rationality.'\" But I argued for technical rationality, so: what are the limitations of technical rationality? Does it, as you claim for \"semi-formal rationality,\" fail to apply to consciousness or ethics or precognition? Does Bayes' Theorem remain true when looking at the evidence about awareness, but cease to be true when we look at the evidence concerning consciousness or precognition?</p>\n<p>You talk about technical rationality's lack of a track record, but I don't know what you mean. Science was successful because it did a much better job of approximating perfect Bayesian probability theory than earlier methods did (e.g. faith, tradition), and science can be even <em>more</em> successful when it tries <em>harder</em> to approximate perfect Bayesian probability theory \u2014 see <a href=\"http://www.amazon.com/Theory-That-Would-Not-Die/dp/0300169698/\">The Theory That Would Not Die</a>.</p>\n<p>You say that \"minimization of some of these textbook cognitive biases would help [some] analysts to do their jobs better. I\u2019m not sure how big an effect the reduction of these biases would have on their effectiveness, though, relative to other changes one might make, such as changes to their workplace culture or communication style.\" But this misunderstands <a href=\"http://facingthesingularity.com/2011/the-laws-of-thought/\">what I mean by Technical Rationality</a>. If teaching these people about cognitive biases would lower the expected value of some project, then technical rationality would recommend against teaching these people cognitive biases (at least, for the purposes of maximizing the expected value of that project). Your example here is a case of <a href=\"http://facingthesingularity.com/2011/why-spock-is-not-rational/\">Straw Man Rationality</a>. (But of course I didn't expect you to know everything I meant by Technical Rationality in advance! Though, I did provide a link to an explanation of what I meant by Technical Rationality in my first entry, above.)</p>\n<p>The same goes for your dismissal of probability theory's foundations. You write that \"In short, the assumptions underlying Cox\u2019s axioms, de Finetti\u2019s arguments, or any of the other arguments in favor of probability theory as the correct way of reasoning under uncertainty, do NOT apply to real-world intelligences operating under strictly bounded computational resources.\" Yes, we don't have infinite computing power. The point is that Bayesian probability theory is an <em>ideal</em> that can be approximated by finite beings. That's why science works better than faith \u2014 it's a better approximation of using probability theory to reason about the world, even though science is still a long way from a <em>perfect</em> use of probability theory.</p>\n<p>Re: goals. Your view of intelligence as \"achieving complex goals in complex environments\" does, as you say, assume that \"the goal is strictly separated from the methods for achieving it.\" I prefer a definition of intelligence as \"<a href=\"http://facingthesingularity.com/2011/playing-taboo-with-intelligence/\">efficient cross-domain optimization</a>\", but my view \u2014 like yours \u2014 also assumes that goals (what one values) are logically orthogonal to intelligence (one's ability to achieve what one values).</p>\n<p>Nevertheless, you report an intuition that shaping every molecule into an image of Mickey Mouse is a \"stupid\" goal. But I don't know what you mean by this. A goal of shaping every molecule into an image of Mickey Mouse is an instrumentally intelligent goal if one's utility function will be maximized that way. Do you mean that it's a stupid goal according to <em>your</em> goals? But of course. This is, moreover, what we would expect your intuitive judgments to report, even if your intuitive judgments are irrelevant to the math of what would and wouldn't be an instrumentally intelligent goal for a <em>different</em> agent to have. The Mickey Mouse goal is \"stupid\" only by a definition of that term that is <em>not</em> the opposite of the explicit definitions either of us gave \"intelligent,\" and it's important to keep that clear. And I certainly don't know what \"out of harmony with the Cosmos\" is supposed to mean.</p>\n<p>Re: induction. I won't dive into that philosophical morass here. Suffice it to say that my views on the matter are expressed pretty well in <a href=\"/lw/s0/where_recursive_justification_hits_bottom/\">Where Recursive Justification Hits Bottom</a>, which is also a direct response to your view that science and reason are great but rely on \"acts of faith.\"</p>\n<p>Your final paragraph sounds like common sense, but it's too vague, as I think you would agree. One way to force a more precise answer to such questions is to think of how you'd program it into an AI. As Daniel Dennett said, \"AI makes philosophy honest.\"</p>\n<p>How would you program an AI to learn about reality, if you wanted it to have the most accurate model of reality possible? You'd have to be a bit more specific than \"Think for yourself; but also, try to genuinely listen to what others have to say. Reason incisively and analytically; but also be willing to listen to your heart, gut and intuition\u2026\"</p>\n<p>My own answer to the question of how I would program an AI to build as accurate a model of reality as possible is this: I would build it to use computable approximations of perfect technical rationality \u2014 that is, roughly: computable approximations of <a href=\"http://www.vetta.org/documents/disSol.pdf\">Solomonoff induction</a> and <a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">Bayesian decision theory</a>.</p>\n<h3 id=\"Ben_\">Ben:</h3>\n<p>[Jan 21st, 2012]</p>\n<p>Bayes Theorem is \u201calways true\u201d in a formal sense, just like 1+1=2, obviously. However, the connection between formal mathematics and subjective experience, is not something that can be fully formalized.</p>\n<p>Regarding consciousness, there are many questions, including what counts as \u201cevidence.\u201d In science we typically count something as evidence if the vast majority of the scientific community counts it as a real observation -- so ultimately the definition of \u201cevidence\u201d bottoms out in social agreement. But there\u2019s a lot that\u2019s unclear in this process of classifying an observation as evidence via a process of social agreement among multiple minds. This unclarity is mostly irrelevant to the study of trajectories of basketballs, but possibly quite relevant to study of consciousness.</p>\n<p>Regarding psi, there are lots of questions, but one big problem is that it\u2019s possible the presence and properties of a psi effect may depend on the broad context of the situation whether the effect takes place. Since we don\u2019t know which aspects of the context are influencing the psi effect, we don\u2019t know how to construct controlled experiments to measure psi. And we may not have the breadth of knowledge nor the processing power to reason about all the relevant context to a psi experiment, in a narrowly \u201ctechnically rational\u201d way.... I do suspect one can gather solid data demonstrating and exploring psi (and based on my current understanding, it seems this has already been done to a significant extent by the academic parapsychology community; see <a href=\"http://wp.goertzel.org/?page_id=154\">a few links I\u2019ve gathered here</a>), but I also suspect there many be aspects that elude the traditional scientific method, but are nonetheless perfectly real aspects of the universe.</p>\n<p>Anyway both consciousness and psi are big, deep topics, and if we dig into them in detail, this interview will become longer than either of us has time for...</p>\n<p>About the success of science -- I don\u2019t really accept your Bayesian story for why science was successful. It\u2019s naive for reasons much discussed by philosophers of science. My own take on the history and philosophy of science, from a few years back, is <a href=\"http://www.goertzel.org/dynapsyc/2004/PhilosophyOfScience_v2.htm\">here</a> (that article was the basis for a chapter in <em>The Hidden Pattern</em>, also). My goal in that essay was \u201ca philosophical perspective that does justice to both the <em>relativism and sociological embeddedness</em> of science, and the <em>objectivity and rationality</em> of science.\u201d It seems you focus overly much on the latter and ignore the former. That article tries to explain why probabilist explanations of real-world science are quite partial and miss a lot of the real story. But again, a long debate on the history of science would take us too far off track from the main thrust of this interview.</p>\n<p>About technical rationality, cognitive biases, etc. -- I did read that blog entry that you linked, on technical rationality. Yes, it\u2019s obvious that focusing on teaching an employee to be more rational, need not always be the most rational thing for an employer do, even if that employer has a purely rationalist world-view. For instance, if I want to train an attack dog, I may do better by focusing limited time and attention on increasing his strength rather than his rationality. My point was that there\u2019s a kind of obsession with rationality in some parts of the intellectual community (e.g. some of the Less Wrong orbit) that I find a bit excessive and not always productive. But your reply impels me to distinguish two ways this excess may manifest itself:</p>\n<ol>\n<li>Excessive belief that rationality is the \u201cright\u201d way to solve problems and think about issues, in principle</li>\n<li>Excessive belief that, tactically, explicitly employing tools of technical rationality is a good way to solve problems in the real world</li>\n</ol>\n<p>Psychologically I think these two excesses probably tend to go together, but they\u2019re not logically coupled. In principle, someone could hold either one, but not the other.</p>\n<p>This sort of ties in with your comments on science and faith. You view science as progress over faith -- and I agree if you interpret \u201cfaith\u201d to mean \u201ctraditional religions.\u201d But if you interpret \u201cfaith\u201d more broadly, I don\u2019t see a dichotomy there. Actually, I find the dichotomy between \u201cscience\u201d and \u201cfaith\u201d unfortunately phrased, since science itself ultimately relies on acts of faith also. The \u201cproblem of induction\u201d can\u2019t be solved, so every scientist must base his extrapolations from past into future based on some act of faith. It\u2019s not a matter of science vs. faith, it\u2019s a matter of what one chooses to place one\u2019s faith in. I\u2019d personally rather place faith in the idea that patterns observed in the past will likely continue into the future (as one example of a science-friendly article of faith), than in the word of some supposed \u201cGod\u201d -- but I realize I\u2019m still making an act of faith.</p>\n<p>This ties in with the blog post \u201cWhere Recursive Justification Hits Bottom\u201d that you pointed out. It\u2019s pleasant reading but of course doesn\u2019t provide any kind of rational argument against my views. In brief, according to my interpretation, it articulates a faith in the process of endless questioning:</p>\n<p><em>The important thing is to <strong>hold nothing back</strong> in your criticisms of how to criticize; nor should you regard the unavoidability of loopy justifications as a warrant of <strong>immunity from questioning</strong>.</em></p>\n<p>I share that faith, personally.</p>\n<p>Regarding approximations to probabilistic reasoning under realistic conditions (of insufficient resources), the problem is that we lack rigorous knowledge about what they are. We don\u2019t have any theorems telling us what is the best way to reason about uncertain knowledge, in the case that our computational resources are extremely restricted. You seem to be assuming that the best way is to explicitly use the rules of probability theory, but my point is that there is no mathematical or scientific foundation for this belief. You are making an act of faith in the doctrine of probability theory! You are assuming, because it feels intuitively and emotionally right to you, that even if the conditions of the arguments for the correctness of probabilistic reasoning are NOT met, then it still makes sense to use probability theory to reason about the world. But so far as I can tell, you don\u2019t have a RATIONAL reason for this assumption, and certainly not a mathematical reason.</p>\n<p>Re your response to my questioning the reduction of intelligence to goals and optimization -- I understand that you are intellectually committed to the perspective of intelligence in terms of optimization or goal-achievement or something similar to that. Your response to my doubts about this perspective basically just re-asserts your faith in the correctness and completeness of this sort of perspective. Your statement</p>\n<p><em>The Mickey Mouse goal is \"stupid\" only by a definition of that term that is not the opposite of the explicit definitions either of us gave \"intelligent,\" and it's important to keep that clear</em></p>\n<p>basically asserts that it\u2019s important to agree with your opinion on the ultimate meaning of intelligence!</p>\n<p>On the contrary, I think it\u2019s important to explore alternatives to the understanding of intelligence in terms of optimization or goal-achievement. That is something I\u2019ve been thinking about a lot lately. However, I don\u2019t have a really crisply-formulated alternative yet.</p>\n<p>As a mathematician, I tend not to think there\u2019s a \u201cright\u201d definition for anything. Rather, one explains one\u2019s definitions, and then works with them and figures out their consequences. In my AI work, I\u2019ve provisionally adopted a goal-achievemement based understanding of intelligence -- and have found this useful, to a significant extent. But I don\u2019t think this is the true and ultimate way to understand intelligence. I think the view of intelligence in terms of goal-achievement or cross-domain optimization misses something, which future understandings of intelligence will encompass. I\u2019ll venture that in 100 years the smartest beings on Earth will have a rigorous, detailed understanding of intelligence according to which</p>\n<p><em>The Mickey Mouse goal is \"stupid\" only by a definition of that term that is not the opposite of the explicit definitions either of us gave \"intelligent,\" and it's important to keep that clear</em></p>\n<p>seems like rubbish.....</p>\n<p>As for your professed inability to comprehend the notion of \u201charmony with the Cosmos\u201d -- that\u2019s unfortunate for you, but I guess trying to give you a sense for that notion, would take us way too far afield in this dialogue!</p>\n<p>Finally, regarding your complaint that my indications regarding how to understanding the world are overly vague. Well -- according to Franklin\u2019s idea of \u201cModeration in all things, including moderation\u201d, one should also exercise moderation in precisiation. Not everything needs to be made completely precise and unambiguous (fortunately, since that\u2019s not feasible anyway).</p>\n<p>I don\u2019t know how I would program an AI to build as accurate a model of reality as possible, if that were my goal. I\u2019m not sure that\u2019s the best goal for AI development, either. An accurate model in itself, doesn\u2019t do anything helpful. My best stab in the direction of how I would ideally create an AI, if computational resource restrictions were no issue, is the GOLEM design that I described <a href=\"http://goertzel.org/GOLEM.pdf\">here</a>. GOLEM is a design for a strongly self-modifying superintelligent AI system, which might plausibly have the possibility of retaining its initial goal system through successive self-modifications. However, it\u2019s unclear to me whether it will ever be feasible to build.</p>\n<p>You mention Solomonoff induction and Bayesian decision theory. But these are abstract mathematical constructs, and it\u2019s unclear to me whether it will ever be feasible to build an AI system fundamentally founded on these ideas, and operating within feasible computational resources. Marcus Hutter and Juergen Schmidhuber and their students are making some efforts in this direction, and I admire those researchers and this body of work, but don\u2019t currently have a high estimate of its odds of leading to any sort of powerful real-world AGI system.</p>\n<p>Most of my thinking about AGI has gone into the more practical problem of how to make a human-level AGI</p>\n<ol>\n<li>using currently feasible computational resources</li>\n<li>that will most likely be helpful rather than harmful in terms of the things I value</li>\n<li>that will be smoothly extensible to intelligence beyond the human level as well. </li>\n</ol>\n<p>For this purpose, I think Solomonoff induction and probability theory are useful, but aren\u2019t all-powerful guiding principles. For instance, in the <a href=\"http://opencog.org/\">OpenCog</a> AGI design (which is my main practical AGI-oriented venture at present), there is a component doing automated program learning of small programs -- and inside our program learning algorithm, we explicitly use an Occam bias, motivated by the theory of Solomonoff induction. And OpenCog also has a probabilistic reasoning engine, based on the math of Probabilistic Logic Networks (PLN). I don\u2019t tend to favor the language of \u201cBayesianism\u201d, but I would suppose PLN should be considered \u201cBayesian\u201d since it uses probability theory (including Bayes rule) and doesn\u2019t make a lot of arbitrary, a priori distributional assumptions. The truth value formulas inside PLN are based on an extension of imprecise probability theory, which in itself is an extension of standard Bayesian methods (looking at envelopes of prior distributions, rather than assuming specific priors).</p>\n<p>In terms of how to get an OpenCog system to model the world effectively and choose its actions appropriately, I think teaching it and working together with it, will be be just as important as programming it. Right now the project is early-stage and the OpenCog design is maybe 50% implemented. But assuming the design is right, once the implementation is done, we\u2019ll have a sort of idiot savant childlike mind, that will need to be educated in the ways of the world and humanity, and to learn about itself as well. So the general lessons of how to confront the world, that I cited above, would largely be imparted via interactive experiential learning, vaguely the same way that human kids learn to confront the world from their parents and teachers.</p>\n<p>Drawing a few threads from this conversation together, it seems that</p>\n<ol>\n<li>I think technical rationality, and informal semi-rationality, are both useful tools for confronting life -- but not all-powerful</li>\n<li>I think Solomonoff induction and probability theory are both useful tools for constructing AGI systems -- but not all-powerful</li>\n</ol>\n<p>whereas you seem to ascribe a more fundamental, foundational basis to these particular tools.</p>\n<h3 id=\"Luke_1\">Luke:</h3>\n<p>[Jan. 21st, 2012]</p>\n<p>To sum up, from my point of view:</p>\n<ol>\n<li>We seem to disagree on the applications of probability theory. For my part, I'll just point people to <a href=\"http://yudkowsky.net/rational/technical\">A Technical Explanation of Technical Explanation</a>.</li>\n<li>I don't think we disagree much on the \"sociological embeddedness\" of science.</li>\n<li>I'm also not sure how much we really disagree about Solomonoff induction and Bayesian probability theory. I've already agreed that no machine will use these in practice because they are not computable \u2014 my point was about their provable optimality given infinite computation (subject to qualifications; see <a href=\"http://www.amazon.com/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3642060528/\">AIXI</a>).</li>\n</ol>\n<p>You've definitely misunderstood me concerning \"intelligence.\" This part is definitely not true: \"I understand that you are intellectually committed to the perspective of intelligence in terms of optimization or goal-achievement or something similar to that. Your response assumes the correctness and completeness of this sort of perspective.\" Intelligence as efficient cross-domain optimization is merely a <em>stipulated</em> definition. I'm happy to use other definitions of intelligence in conversation, so long as we're clear which definition we're using when we use the word. Or, we can <a href=\"/lw/nv/replace_the_symbol_with_the_substance/\">replace the symbol with the substance</a> and talk about \"efficient cross-domain optimization\" or \"achieving complex goals in complex environments\" without ever using the word \"intelligence.\"</p>\n<p>My point about the Mickey Mouse goal was that when you called the Mickey Mouse goal \"stupid,\" this could be confusing, because \"stupid\" is usually the opposite of \"intelligent,\" but your use of \"stupid\" in that sentence didn't seem to be the opposite of <em>either</em> definition of intelligence we each gave. So I'm still unsure what you mean by calling the Mickey Mouse goal \"stupid.\"</p>\n<p>This topic provides us with a handy transition away from philosophy of science and toward AGI. Suppose there was a machine with a vastly greater-than-human capacity for either \"achieving complex goals in complex environments\" or for \"efficient cross-domain optimization.\" And suppose that machine's utility function would be maximized by reshaping every molecule into a Mickey Mouse shape. We can avoid the tricky word \"stupid,\" here. The question is: Would that machine decide to change its utility function so that it doesn't continue to reshape every molecule into a Mickey Mouse shape? I think this is unlikely, for reasons discussed in <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">Omohundro (2008)</a>.</p>\n<p>I suppose a natural topic of conversation for us would be your October 2010 blog post <a href=\"http://multiverseaccordingtoben.blogspot.com/2010/10/singularity-institutes-scary-idea-and.html\">The Singularity Institute's's Scary Idea (and Why I Don't Buy It)</a>. Does that post still reflect your views pretty well, Ben?</p>\n<h4 id=\"Ben_1\">Ben:</h4>\n<p>[Mar 10th, 2012]</p>\n<p>About the hypothetical uber-intelligence that wants to tile the cosmos with molecular Mickey Mouses -- I truly don\u2019t feel confident making any assertions about a real-world system with vastly greater intelligence than me. There are just too many unknowns. Sure, according to certain models of the universe and intelligence that may seem sensible to some humans, it\u2019s possible to argue that a hypothetical uber-intelligence like that would relentlessly proceed in tiling the cosmos with molecular Mickey Mouses. But so what? We don\u2019t even know that such an uber-intelligence is even a possible thing -- in fact my intuition is that it\u2019s not possible.</p>\n<p>Why may it not be possible to create a very smart AI system that is strictly obsessed with that stupid goal? Consider first that it may not be possible to create a real-world, highly intelligent system that is strictly driven by explicit goals -- as opposed to being partially driven by implicit, \u201cunconscious\u201d (in the sense of deliberative, reflective consciousness) processes that operate in complex interaction with the world outside the system. Because pursuing explicit goals is quite computationally costly compared to many other sorts of intelligent processes. So if a real-world system is necessarily not wholly explicit-goal-driven, it may be that intelligent real-world systems will naturally drift away from certain goals and toward others. My strong intuition is that the goal of tiling the universe with molecular Mickey Mouses would fall into that category. However, I don\u2019t yet have any rigorous argument to back this up. Unfortunately my time is limited, and while I generally have more fun theorizing and philosophizing than working on practical projects, I think it\u2019s more important for me to push toward building AGI than just spend all my time on fun theory. (And then there\u2019s the fact that I have to spend a lot of my time on applied narrow-AI projects to pay the mortgage and put my kids through college, etc.)</p>\n<p>But anyway -- you don\u2019t have any rigorous argument to back up the idea that a system like you posit is possible in the real-world, either! And SIAI has staff who, unlike me, are paid full-time to write and philosophize \u2026 and they haven\u2019t come up with a rigorous argument in favor of the possibility of such a system, either. Although they have talked about it a lot, though usually in the context of paperclips rather than Mickey Mouses.</p>\n<p>So, I\u2019m not really sure how much value there is in this sort of thought-experiment about pathological AI systems that combine massively intelligent practical problem solving capability with incredibly stupid goals (goals that may not even be feasible for real-world superintelligences to adopt, due to their stupidity).</p>\n<p>Regarding the concept of a \u201cstupid goal\u201d that I keep using, and that you question -- I admit I\u2019m not quite sure how to formulate rigorously the idea that tiling the universe with Mickey Mouses is a stupid goal. This is something I\u2019ve been thinking about a lot recently. But here\u2019s a first rough stab in that direction: I think that if you created a highly intelligent system, allowed it to interact fairly flexibly with the universe, and also allowed it to modify its top-level goals in accordance with its experience, you\u2019d be very unlikely to wind up with a system that had this goal (tiling the universe with Mickey Mouses). That goal is out of sync with the Cosmos, in the sense that an intelligent system that\u2019s allowed to evolve itself in close coordination with the rest of the universe, is very unlikely to arrive at that goal system. I don\u2019t claim this is a precise definition, but it should give you some indication of the direction I\u2019m thinking in....</p>\n<p>The tricky thing about this way of thinking about intelligence, which classifies some goals as \u201cinnately\u201d stupider than others, is that it places intelligence not just in the system, but in the system\u2019s broad relationship to the universe -- which is something that science, so far, has had a tougher time dealing with. It\u2019s unclear to me which aspects of the mind and universe science, as we now conceive it, will be able to figure out. I look forward to understanding these aspects more fully....</p>\n<p>About my blog post on \u201cThe Singularity Institute\u2019s Scary Idea\u201d -- yes, that still reflects my basic opinion. After I wrote that blog post, Michael Anissimov -- a long-time SIAI staffer and zealot whom I like and respect greatly -- told me he was going to write up and show me a systematic, rigorous argument as to why \u201can AGI not built based on a rigorous theory of Friendliness is almost certain to kill all humans\u201d (the proposition I called \u201cSIAI\u2019s Scary Idea\u201d). But he hasn\u2019t followed through on that yet -- and neither has Eliezer or anyone associated with SIAI.</p>\n<p>Just to be clear, I don\u2019t really mind that SIAI folks hold that \u201cScary Idea\u201d as an intuition. But I find it rather ironic when people make a great noise about their dedication to rationality, but then also make huge grand important statements about the future of humanity, with great confidence and oomph, that are not really backed up by any rational argumentation. This ironic behavior on the part of Eliezer, Michael Anissimov and other SIAI principals doesn\u2019t really bother me, as I like and respect them and they are friendly to me, and we\u2019ve simply \u201cagreed to disagree\u201d on these matters for the time being. But the reason I wrote that blog post is because my own blog posts about AGI were being trolled by SIAI zealots (not the principals, I hasten to note) leaving nasty comments to the effect of \u201cSIAI has proved that if OpenCog achieves human level AGI, it will kill all humans.\u201c Not only has SIAI not proved any such thing, they have not even made a clear rational argument!</p>\n<p>As Eliezer has pointed out to me several times in conversation, a clear rational argument doesn\u2019t have to be mathematical. A clearly formulated argument in the manner of analytical philosophy, in favor of the Scary Idea, would certainly be very interesting. For example, philosopher David Chalmers recently wrote a carefully-argued philosophy paper arguing for the plausibility of a Singularity in the next couple hundred years. It\u2019s somewhat dull reading, but it\u2019s precise and rigorous in the manner of analytical philosophy, in a manner that Kurzweil\u2019s writing (which is excellent in its own way) is not. An argument in favor of the Scary Idea, on the level of Chalmers\u2019 paper on the Singularity, would be an excellent product for SIAI to produce. Of course a mathematical argument might be even better, but that may not be feasible to work on right now, given the state of mathematics today. And of course, mathematics can\u2019t do everything -- there\u2019s still the matter of connecting mathematics to everyday human experience, which analytical philosophy tries to handle, and mathematics by nature cannot.</p>\n<p>My own suspicion, of course, is that in the process of trying to make a truly rigorous analytical philosophy style formulation of the argument for the Scary Idea, the SIAI folks will find huge holes in the argument. Or, maybe they already intuitively know the holes are there, which is why they have avoided presenting a rigorous write-up of the argument!!</p>\n<h3 id=\"Luke_2\">Luke:</h3>\n<p>[Mar 11th, 2012]</p>\n<p>I'll drop the stuff about Mickey Mouse so we can move on to AGI. Readers can come to their own conclusions on that.</p>\n<p>Your main complaint seems to be that the Singularity Institute hasn't written up a clear, formal argument (in analytic philosophy's sense, if not the mathematical sense) in defense of our major positions \u2014 something like Chalmers' \"<a href=\"http://consc.net/papers/singularityjcs.pdf\">The Singularity: A Philosophical Analysis</a>\" but more detailed.</p>\n<p>I have the same complaint. I wish \"The Singularity: A Philosophical Analysis\" had been written 10 years ago, by Nick Bostrom and Eliezer Yudkowsky. It <em>could</em> have been written back then. Alas, we had to wait for Chalmers to speak at Singularity Summit 2009 and then write a paper based on his talk. And if it wasn't for Chalmers, I fear we'd still be waiting for such an article to exist. (Bostrom's forthcoming Superintelligence book should be good, though.)</p>\n<p>I was hired by the Singularity Institute in September 2011 and have since then co-written two papers explaining some of the basics: \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Intelligence Explosion: Evidence and Import</a>\" and \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">The Singularity and Machine Ethics</a>\". I also wrote the first ever outline of categories of open research problems in AI risk, cheekily titled \"<a href=\"http://lukeprog.com/SaveTheWorld.html\">So You Want to Save the World</a>\". I'm developing other articles on \"the basics\" as quickly as I can. I would love to write more, but alas, I'm also busy being the Singularity Institute's Executive Director.</p>\n<p>Perhaps we could reframe our discussion around the Singularity Institute's latest exposition of its basic ideas, \"Intelligence Explosion: Evidence and Import\"? Which claims in that paper do you most confidently disagree with, and why?</p>\n<h3 id=\"Ben_2\">Ben:</h3>\n<p>[Mar 11th, 2012]</p>\n<p>You say \u201cYour main complaint seems to be that the Singularity Institute hasn't written up a clear, formal argument (in analytic philosophy's sense, if not the mathematical sense) in defense of our major positions \u201c. Actually, my main complaint is that some of SIAI\u2019s core positions seem almost certainly WRONG, and yet they haven\u2019t written up a clear formal argument trying to justify these positions -- so it\u2019s not possible to engage SIAI in rational discussion on their apparently wrong positions. Rather, when I try to engage SIAI folks about these wrong-looking positions (e.g. the \u201cScary Idea\u201d I mentioned above), they tend to point me to Eliezer\u2019s blog (\u201cLess Wrong\u201d) and tell me that if I studied it long and hard enough, I would find that the arguments in favor of SIAI\u2019s positions are implicit there, just not clearly articulated in any one place. This is a bit frustrating to me -- SIAI is a fairly well-funded organization involving lots of smart people and explicitly devoted to rationality, so certainly it should have the capability to write up clear arguments for its core positions... if these arguments exist. My suspicion is that the Scary Idea, for example, is not backed up by any clear rational argument -- so the reason SIAI has not put forth any clear rational argument for it, is that they don\u2019t really have one! Whereas Chalmers\u2019 paper carefully formulated something that seemed obviously true...</p>\n<p>Regarding the paper \"Intelligence Explosion: Evidence and Import\", I find its contents mainly agreeable -- and also somewhat unoriginal and unexciting, given the general context of 2012 Singularitarianism. The paper\u2019s three core claims that</p>\n<p>(1) there is a substantial chance we will create human-level AI before 2100, that (2) if human-level AI is created, there is a good chance vastly superhuman AI will follow via an \"intelligence explosion,\" and that (3) an uncontrolled intelligence explosion could destroy everything we value, but a controlled intelligence explosion would benefit humanity enormously if we can achieve it.</p>\n<p>are things that most \u201cSingularitarians\u201d would agree with. The paper doesn\u2019t attempt to argue for the \u201cScary Idea\u201d or Coherent Extrapolated Volition or the viability of creating some sort of provably Friendly AI, -- or any of the other positions that are specifically characteristic of SIAI. Rather, the paper advocates what one might call \u201cplain vanilla Singularitarianism.\u201d This may be a useful thing to do, though, since after all there are a lot of smart people out there who aren\u2019t convinced of plain vanilla Singularitarianism.</p>\n<p>I have a couple small quibbles with the paper, though. I don\u2019t agree with Omohundro\u2019s argument about the \u201cbasic AI drives\u201d (though Steve is a friend and I greatly respect his intelligence and deep thinking). Steve\u2019s argument for the inevitability of these drives in AIs is based on evolutionary ideas, and would seem to hold up in the case that there is a population of distinct AIs competing for resources -- but the argument seems to fall apart in the case of other possibilities like an AGI mindplex (a network of minds with less individuality than current human minds, yet not necessarily wholly blurred into a single mind -- rather, with reflective awareness and self-modeling at both the individual and group level).</p>\n<p>Also, my \u201cAI Nanny\u201d concept is dismissed too quickly for my taste (though that doesn\u2019t surprise me!). You suggest in this paper that to make an AI Nanny, it would likely be necessary to solve the problem of making an AI\u2019s goal system persist under radical self-modification. But you don\u2019t explain the reasoning underlying this suggestion (if indeed you have any). It seems to me -- as I say in my \u201cAI Nanny\u201d paper -- that one could probably make an AI Nanny with intelligence significantly beyond the human level, without having to make an AI architecture oriented toward radical self-modification. If you think this is false, it would be nice for you to explain why, rather than simply asserting your view. And your comment \u201cThose of us working on AI safety theory would very much appreciate the extra time to solve the problems of AI safety...\u201d carries the hint that I (as the author of the AI Nanny idea) am NOT working on AI safety theory. Yet my GOLEM design is a concrete design for a potentially Friendly AI (admittedly not computationally feasible using current resources), and in my view constitutes greater progress toward actual FAI than any of the publications of SIAI so far. (Of course, various SIAI associated folks often allude that there are great, unpublished discoveries about FAI hidden in the SIAI vaults -- a claim I somewhat doubt, but can\u2019t wholly dismiss of course....)</p>\n<p>Anyway, those quibbles aside, my main complaint about the paper you cite is that it sticks to \u201cplain vanilla Singularitarianism\u201d and avoids all of the radical, controversial positions that distinguish SIAI from myself, Ray Kurzweil, Vernor Vinge and the rest of the Singularitarian world. The crux of the matter, I suppose is the third main claim of the paper,</p>\n<p>(3) an uncontrolled intelligence explosion could destroy everything we value, but a controlled intelligence explosion would benefit humanity enormously if we can achieve it.</p>\n<p>This statement is hedged in such a way as to be almost obvious. But yet, what SIAI folks tend to tell me verbally and via email and blog comments is generally far more extreme than this bland and nearly obvious statement.</p>\n<p>As an example, I recall when your co-author on that article, Anna Salamon, guest lectured in the class on Singularity Studies that my father and I were teaching at Rutgers University in 2010. Anna made the statement, to the students, that (I\u2019m paraphrasing, though if you\u2019re curious you can look up the online course session which was saved online and find her exact wording) \u201cIf a superhuman AGI is created without being carefully based on an explicit Friendliness theory, it is ALMOST SURE to destroy humanity.\u201d (i.e., what I now call SIAI\u2019s Scary Idea)</p>\n<p>I then asked her (in the online class session) why she felt that way, and if she could give any argument to back up the idea.</p>\n<p>She gave the familiar SIAI argument that, if one picks a mind at random from \u201cmind space\u201d, the odds that it will be Friendly to humans are effectively zero.</p>\n<p>I made the familiar counter-argument that this is irrelevant, because nobody is advocating building a random mind. Rather, what some of us are suggesting is to build a mind with a Friendly-looking goal system, and a cognitive architecture that\u2019s roughly human-like in nature but with a non-human-like propensity to choose its actions rationally based on its goals, and then raise this AGI mind in a caring way and integrate it into society. Arguments against the Friendliness of random minds are irrelevant as critiques of this sort of suggestion.</p>\n<p>So, then she fell back instead on the familiar (paraphrasing again) \u201cOK, but you must admit there\u2019s a non-zero risk of such an AGI destroying humanity, so we should be very careful -- when the stakes are so high, better safe than sorry!\u201d</p>\n<p>I had pretty much the same exact argument with SIAI advocates Tom McCabe and Michael Anissimov on different occasions; and also, years before, with Eliezer Yudkowsky and Michael Vassar -- and before that, with (former SIAI Executive Director) Tyler Emerson. Over all these years, the SIAI community maintains the Scary Idea in its collective mind, and also maintains a great devotion to the idea of rationality, but yet fails to produce anything resembling a rational argument for the Scary Idea -- instead repetitiously trotting out irrelevant statements about random minds!!</p>\n<p>What I would like is for SIAI to do one of these three things, publicly:</p>\n<ol>\n<li>Repudiate the Scary Idea</li>\n<li>Present a rigorous argument that the Scary Idea is true</li>\n<li>State that the Scary Idea is a commonly held intuition among the SIAI community, but admit that no rigorous rational argument exists for it at this point</li>\n</ol>\n<p>Doing any one of these things would be intellectually honest. Presenting the Scary Idea as a confident conclusion, and then backing off when challenged into a platitudinous position equivalent to \u201cthere\u2019s a non-zero risk \u2026 better safe than sorry...\u201d, is not my idea of an intellectually honest way to do things.</p>\n<p>Why does this particular point get on my nerves? Because I don\u2019t like SIAI advocates telling people that I, personally, am on a R&amp;D course where if I succeed I am almost certain to destroy humanity!!! That frustrates me. I don\u2019t want to destroy humanity; and if someone gave me a rational argument that my work was most probably going to be destructive to humanity, I would stop doing the work and do something else with my time! But the fact that some other people have a non-rational intuition that my work, if successful, would be likely to destroy the world -- this doesn\u2019t give me any urge to stop. I\u2019m OK with the fact that some other people have this intuition -- but then I\u2019d like them to make clear, when they state their views, that these views are based on intuition rather than rational argument. I will listen carefully to rational arguments that contravene my intuition -- but if it comes down to my intuition versus somebody else\u2019s, in the end I\u2019m likely to listen to my own, because I\u2019m a fairly stubborn maverick kind of guy....</p>\n<h3 id=\"Luke_3\">Luke:</h3>\n<p>[Mar 11th, 2012]</p>\n<p>Ben, you write:</p>\n<blockquote>\n<p>when I try to engage SIAI folks about these wrong-looking positions (e.g. the \u201cScary Idea\u201d I mentioned above), they tend to point me to Eliezer\u2019s blog (\u201cLess Wrong\u201d) and tell me that if I studied it long and hard enough, I would find that the arguments in favor of SIAI\u2019s positions are implicit there, just not clearly articulated in any one place. This is a bit frustrating to me...</p>\n</blockquote>\n<p>No kidding! It's very frustrating to me, too. That's one reason I'm working to clearly articulate the arguments in one place, starting with articles on the basics like \"Intelligence Explosion: Evidence and Import.\"</p>\n<p>I agree that \"Intelligence Explosion: Evidence and Import\" covers only the basics and does not argue for several positions associated uniquely with the Singularity Institute. It is, after all, the opening chapter of a book intelligence explosion, not the opening chapter of a book on the Singularity Institute's ideas!</p>\n<p>I wanted to write that article first, though, so the Singularity Institute could be clear on the basics. For example, we needed to be clear that: (1) we are not Kurzweil, and our claims don't depend on his detailed storytelling or accelerating change curves, that (2) technological prediction is hard, and we are not being naively overconfident about AI timelines, and that (3) intelligence explosion is a convergent outcome of many paths the future may take. There is also much content that is not found in, for example, Chalmers' paper: (a) an overview of methods of technological prediction, (b) an overview of speed bumps and accelerators toward AI, (c) a reminder of breakthroughs like AIXI, and (d) a summary of AI advantages. (The rest is, as you say, mostly a brief overview of points that have been made elsewhere. But brief overviews are extremely useful!)</p>\n<blockquote>\n<p>...my \u201cAI Nanny\u201d concept is dismissed too quickly for my taste...</p>\n</blockquote>\n<p>No doubt! I think the idea is clearly worth exploring in several papers devoted to the topic.</p>\n<blockquote>\n<p>It seems to me -- as I say in my \u201cAI Nanny\u201d paper -- that one could probably make an AI Nanny with intelligence significantly beyond the human level, without having to make an AI architecture oriented toward radical self-modification.</p>\n</blockquote>\n<p>Whereas I tend to buy Omohundro's arguments that advanced AIs will want to self-improve just like humans want to self-improve, so that they become better able to achieve their final goals. Of course, we disagree on Omohundro's arguments \u2014 a topic to which I will return in a moment.</p>\n<blockquote>\n<p>your comment \"Those of us working on AI safety theory would very much appreciate the extra time to solve the problems of AI safety...\" carries the hint that I (as the author of the AI Nanny idea) am NOT working on AI safety theory...</p>\n</blockquote>\n<p>I didn't mean for it to carry that connotation. GOLEM and Nanny AI are both clearly AI safety ideas. I'll clarify that part before I submit a final draft to the editors.</p>\n<p>Moving on: If you are indeed remembering your conversations with Anna, Michael, and others correctly, then again I sympathize with your frustration. I completely agree that it would be useful for the Singularity Institute to produce clear, formal arguments for the important positions it defends. In fact, just yesterday I was talking to <a href=\"https://sites.google.com/site/nbeckstead/\">Nick Beckstead</a> about how badly both of us want to write these kinds of papers if we can find the time.</p>\n<p>So, to respond to your wish that the Singularity Institute choose among three options, my plan is to (1) write up clear arguments for... well, if not \"SIAI's Big Scary Idea\" then for whatever I end up believing after going through the process of formalizing the arguments, and (2) publicly state (right now) that SIAI's Big Scary Idea is a commonly held view at the Singularity Institute but a clear, formal argument for it has never been published (at least, not to my satisfaction).</p>\n<blockquote>\n<p>I don\u2019t want to destroy humanity; and if someone gave me a rational argument that my work was most probably going to be destructive to humanity, I would stop doing the work and do something else with my time!</p>\n</blockquote>\n<p>I'm glad to hear it! :)</p>\n<p>Now, it seems a good point of traction is our disagreement over Omohundro's \"Basic AI Drives.\" We could talk about that next, but for now I'd like to give you a moment to reply.</p>\n<h3 id=\"Ben_3\">Ben:</h3>\n<p>[Mar 11th, 2012]</p>\n<p>Yeah, I agree that your and Anna\u2019s article is a good step for SIAI to take, albeit unexciting to a Singularitian insider type like me.... And I appreciate your genuinely rational response regarding the Scary Idea, thanks!</p>\n<p>(And I note that I have also written some \u201cunexciting to Singularitarians\u201d material lately too, for similar reasons to those underlying your article -- e.g. an article on \u201cWhy an Intelligence Explosion is Probable\u201d for a Springer volume on the Singularity.)</p>\n<p>A quick comment on your statement that</p>\n<p><em>we are not Kurzweil, and our claims don't depend on his detailed storytelling or accelerating change curves,</em></p>\n<p>that\u2019s a good point; but yet, any argument for a Singularity soon (e.g. likely this century, as you argue) ultimately depends on some argumentation analogous to Kurzweil\u2019s, even if different in detail. I find Kurzweil\u2019s detailed extrapolations a bit overconfident and more precise than the evidence warrants; but still, my basic reasons for thinking the Singularity is probably near are fairly similar to his -- and I think your reasons are fairly similar to his as well.</p>\n<p>Anyway, sure, let\u2019s go on to Omohundro\u2019s posited Basic AI Drives -- which seem to me not to hold as necessary properties of future AIs unless the future of AI consists of a population of fairly distinct AIs competing for resources, which I intuitively doubt will be the situation.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"_to_be_continued_\">[to be continued]</strong></p>\n</small><p></p>\n<p>&nbsp;</p>", "sections": [{"title": "Luke Muehlhauser:", "anchor": "Luke_Muehlhauser_", "level": 1}, {"title": "Ben Goertzel:", "anchor": "Ben_Goertzel_", "level": 1}, {"title": "Luke:", "anchor": "Luke_", "level": 1}, {"title": "Ben:", "anchor": "Ben_", "level": 1}, {"title": "Luke:", "anchor": "Luke_1", "level": 1}, {"title": "Ben:", "anchor": "Ben_1", "level": 2}, {"title": "Luke:", "anchor": "Luke_2", "level": 1}, {"title": "Ben:", "anchor": "Ben_2", "level": 1}, {"title": "Luke:", "anchor": "Luke_3", "level": 1}, {"title": "Ben:", "anchor": "Ben_3", "level": 1}, {"title": "[to be continued]", "anchor": "_to_be_continued_", "level": 3}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "161 comments"}], "headingsCount": 13}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 161, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["33KewgYhNSxFpbpXg", "RWo4LwFzpHNQCTcYt", "ZbgCx2ntD5eu8Cno9", "JYckkCqhZPrdScjBx", "s887k4Hcqj28cchYo", "C8nEXTcjZb9oauTCW", "GKfPL6LQFgB49FEnv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-16T19:06:18.120Z", "modifiedAt": null, "url": null, "title": "[Request] Software or Articles on financial life planning", "slug": "request-software-or-articles-on-financial-life-planning", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:27.093Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whpearson", "createdAt": "2009-02-28T00:34:00.976Z", "isAdmin": false, "displayName": "whpearson"}, "userId": "bq8qsRbPNvFihHxgi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PSHrvG2d4WAcusqG5/request-software-or-articles-on-financial-life-planning", "pageUrlRelative": "/posts/PSHrvG2d4WAcusqG5/request-software-or-articles-on-financial-life-planning", "linkUrl": "https://www.lesswrong.com/posts/PSHrvG2d4WAcusqG5/request-software-or-articles-on-financial-life-planning", "postedAtFormatted": "Friday, March 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BRequest%5D%20Software%20or%20Articles%20on%20financial%20life%20planning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BRequest%5D%20Software%20or%20Articles%20on%20financial%20life%20planning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPSHrvG2d4WAcusqG5%2Frequest-software-or-articles-on-financial-life-planning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BRequest%5D%20Software%20or%20Articles%20on%20financial%20life%20planning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPSHrvG2d4WAcusqG5%2Frequest-software-or-articles-on-financial-life-planning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPSHrvG2d4WAcusqG5%2Frequest-software-or-articles-on-financial-life-planning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 115, "htmlBody": "<p>Does anyone have recommendations for software or articles on how to plan your financial life?</p>\n<p>Lets say I have a goal, retire at age 50 to X location with Y income in todays currency. Are there any tools that help me make decisions? E.g. to rent for a while, until a property dip (based on cyclical observations). Or how to spread investments over countries and investment types to mitigate black swan events like economic collapses.</p>\n<p>I found <a href=\"http://www.reliasoft.com/reno/\">reno</a> when googling. But I don't know whether thats just because they are good at SEO or actually useful for what I am suggesting. Preferably it would have a wealth of data, such as property price increases by location etc.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TkZ7MFwCi4D63LJ5n": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PSHrvG2d4WAcusqG5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.667905784513695e-07, "legacy": true, "legacyId": "14122", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-16T21:28:24.339Z", "modifiedAt": null, "url": null, "title": "Evolutionary psychology: evolving three eyed monsters", "slug": "evolutionary-psychology-evolving-three-eyed-monsters", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:25.499Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dmytry", "createdAt": "2009-12-03T17:11:53.492Z", "isAdmin": false, "displayName": "Dmytry"}, "userId": "AjtmA2qtA8sdiMbru", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JQnN6ZCMk9XmhNarY/evolutionary-psychology-evolving-three-eyed-monsters", "pageUrlRelative": "/posts/JQnN6ZCMk9XmhNarY/evolutionary-psychology-evolving-three-eyed-monsters", "linkUrl": "https://www.lesswrong.com/posts/JQnN6ZCMk9XmhNarY/evolutionary-psychology-evolving-three-eyed-monsters", "postedAtFormatted": "Friday, March 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Evolutionary%20psychology%3A%20evolving%20three%20eyed%20monsters&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEvolutionary%20psychology%3A%20evolving%20three%20eyed%20monsters%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJQnN6ZCMk9XmhNarY%2Fevolutionary-psychology-evolving-three-eyed-monsters%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Evolutionary%20psychology%3A%20evolving%20three%20eyed%20monsters%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJQnN6ZCMk9XmhNarY%2Fevolutionary-psychology-evolving-three-eyed-monsters", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJQnN6ZCMk9XmhNarY%2Fevolutionary-psychology-evolving-three-eyed-monsters", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1356, "htmlBody": "<h3>Summary<br /></h3>\n<p>We should not expect evolution of complex psychological and cognitive adaptations in the timeframe in which, morphologically, animal bodies can only change by very little. The genetic alteration to the cognition for speech shouldn't be expected to be dramatically more complex than the alteration of vocal cords.</p>\n<h3>Evolutions that did not happen<br /></h3>\n<p>When humans descended from trees and became bipedal, it would have been very advantageous to have an eye or two on back of the head, for detection of predators and to protect us against being back-stabbed by fellow humans. This is why all of us have an extra eye on the back of our heads, right? Ohh, we don't. Perhaps the mate selection resulted in the poor reproductive success of the back-eyed hominids. Perhaps the tribes would kill any mutant with eyes on the back.</p>\n<p>There are pretty solid reasons why none the above has happened, and can't happen in such timeframes. The evolution does not happen simply because the trait is beneficial, or because there's a niche to be filled. A simple alteration to the DNA has to happen, causing a morphological change which results in some reproductive improvement; then DNA has to mutate again, etc. The unrelated nearly-neutral mutations may combine resulting in an unexpected change (for example, the wolves have many genes that alter their size; random selection of genes produces approximately normal distribution of the sizes; we can rapidly select smaller dogs utilizing the existing diversity). There's no such path rapidly leading up to an eye on back of the head. The eye on back of the head didn't evolve because evolution couldn't make that adaptation.</p>\n<p>The <a href=\"/lw/kt/evolutions_are_stupid_but_work_anyway/\">speed of evolution</a> is severely limited. The ways in which evolution can work, too, are very limited. In the time in which we humans have got down from the trees, we undergone rather minor adaptation in the shape of our bodies, as evident from the fossil record - and that is the degree of change we should expect in rest of our bodies including our brains.</p>\n<p>The correct application of evolutionary theory should be entirely unable to account for outrageous hypothetical like extra eye on back of our heads (extra eye can evolve, of course, but would take very long time). Evolution is not magic. The power of scientific theory is that it can't explain everything, but only the things which are true - that's what makes scientific theory useful for finding the things that are true, in advance of observation. That is what gives science it's predictive power. That's what differentiates science from religion. The power of not explaining the wrong things.</p>\n<h3>Evolving the instincts<br /></h3>\n<p>What do we think it would take to evolve a new innate instinct? To hard-wire a cognitive mechanism?</p>\n<p>Groups of neurons have to connect in the new ways - the neurons on one side must express binding proteins, which would <a href=\"http://en.wikipedia.org/wiki/Axon_guidance\">guide the axons towards them</a>; the weights of the connections have to be adjusted. Majority of the genes expressed in neurons, affect all of the neurons; some affect just a group, but there is no known mechanism by which an entirely arbitrary group's bindings may be controlled from the DNA in 1 mutation. The difficulties are not unlike those of an extra eye. This, combined with above-mentioned speed constraints, imposes severe limitations on which sorts of wiring modifications humans could have evolved during the hunter gatherer environment, and ultimately the behaviours that could have evolved. Even very simple things - such as preference for particular body shape of the mates - have extreme hidden implementation complexity in terms of the DNA modifications leading up to the wiring leading up to the altered preferences. Wiring the brain for a specific cognitive fallacy is anything but simple. It may not always be as time consuming/impossible as adding an extra eye, but it is still no little feat.</p>\n<h3>Junk evolutionary psychology<br /></h3>\n<p>It is extremely important to take into account the properties of evolutionary process when invoking evolution as explanation for traits and behaviours.</p>\n<p>The evolutionary theory, as invoked in the evolutionary psychology, especially of the armchair variety, all too often is an universal explanation. It is magic that can explain anything equally well. Know of a fallacy of reasoning? Think up how it could have worked for the hunter gatherer, make a hypothesis, construct a flawed study across cultures, and publish.</p>\n<p>No considerations are given for the strength of the advantage, for the size of 'mutation target', and for the mechanisms by which the mutation in the DNA would have resulted in the modification of the circuitry such as to result in the trait, nor to the gradual adaptability. All of that is glossed over entirely in common armchair evolutionary psychology, and unfortunately, even in the academia. The evolutionary psychology is littered with examples of traits which are alleged to have evolved over the same time during which we had barely adapted to walking upright.</p>\n<p>It may be that when describing behaviours, a lot of complexity can be hidden into very simple-sounding concepts; and thus it seems like a good target for evolutionary explanation. But when you look at the details - the axons that have to find the targets; the gene must activate in the specific cells, but not others - there is a great deal of complexity in coding for even very simple traits.</p>\n<p>Note: I originally did not intend to make an example of junk, for thou should not pick a strawman, but for sake of clarity, there is an example of what I would consider to be junk: the explanation of better performance at <a href=\"http://en.wikipedia.org/wiki/Wason_selection_task\">Wason Selection Task</a> as result of evolved 'social contracts module', without a slightest consideration for what it might take, in terms of DNA, to code a Wason Selection Task solver circuit, nor for alternative plausible explanation, nor for a readily available fact that people can easily learn to solve Wason Selection Task correctly when taught - the fact which still implies general purpose learning, and the fact that high-IQ people can solve far more confusing tasks of far larger complexity, which demonstrates that the tasks can be solved in absence of specific evolved 'social contract' modules.</p>\n<p>There is an example of non-junk: the evolutionary pressure can adjust strength of pre-existing emotions such as anger, fear, and so on, and even decrease the intelligence whenever the higher intelligence is maladaptive.</p>\n<p>Other commonly neglected fact: the evolution is not a watchmaker, blind or not. It does not choose a solution for a problem and then work on this solution! It works on all adaptive mutations simultaneously. Evolution works on all the solutions, and the simpler changes to existing systems are much quicker to evolve. If mutation that tweaks existing system improves fitness, it will, too, be selected for, even if there was a third eye in progress.</p>\n<p>As much as it would be more politically correct and 'moderate' for e.g. evolution of religion crowd to get their point across by arguing that the religious people have evolved specific god module which doesn't do anything but make them believe in god, than to imply that they are 'genetically stupid' in some way, the same selective pressure would also make the evolution select for non-god-specific heritable tweaks to learning, and the minor cognitive deficits, that increase religiosity.</p>\n<h3>Lined slate as a prior<br /></h3>\n<p>As update for <a href=\"http://en.wikipedia.org/wiki/Tabula_rasa\">tabula rasa</a>, picture lined writing paper; it provides some guidance for the handwriting; the horizontal lined paper is good for writing text, but not for arithmetic, the five-lines-near-eachother separated by spacing is good for writing music, and the grid paper is pretty universal. Different regions of the brain are tailored to different content; but should not be expected to themselves code different algorithms, save for few exceptions which had long time to evolve, early in vertebrate history.</p>\n<p>edit: improved the language some. edit: specific what sort of evolutionary psychology I consider to be junk, and what I do not, albeit that was not the point of the article. The point of the article was to provide you with the notions to use to see what sorts of evolutionary psychology to consider junk, and what do not.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JQnN6ZCMk9XmhNarY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 20, "extendedScore": null, "score": 8.668485472327714e-07, "legacy": true, "legacyId": "14120", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h3 id=\"Summary\">Summary<br></h3>\n<p>We should not expect evolution of complex psychological and cognitive adaptations in the timeframe in which, morphologically, animal bodies can only change by very little. The genetic alteration to the cognition for speech shouldn't be expected to be dramatically more complex than the alteration of vocal cords.</p>\n<h3 id=\"Evolutions_that_did_not_happen\">Evolutions that did not happen<br></h3>\n<p>When humans descended from trees and became bipedal, it would have been very advantageous to have an eye or two on back of the head, for detection of predators and to protect us against being back-stabbed by fellow humans. This is why all of us have an extra eye on the back of our heads, right? Ohh, we don't. Perhaps the mate selection resulted in the poor reproductive success of the back-eyed hominids. Perhaps the tribes would kill any mutant with eyes on the back.</p>\n<p>There are pretty solid reasons why none the above has happened, and can't happen in such timeframes. The evolution does not happen simply because the trait is beneficial, or because there's a niche to be filled. A simple alteration to the DNA has to happen, causing a morphological change which results in some reproductive improvement; then DNA has to mutate again, etc. The unrelated nearly-neutral mutations may combine resulting in an unexpected change (for example, the wolves have many genes that alter their size; random selection of genes produces approximately normal distribution of the sizes; we can rapidly select smaller dogs utilizing the existing diversity). There's no such path rapidly leading up to an eye on back of the head. The eye on back of the head didn't evolve because evolution couldn't make that adaptation.</p>\n<p>The <a href=\"/lw/kt/evolutions_are_stupid_but_work_anyway/\">speed of evolution</a> is severely limited. The ways in which evolution can work, too, are very limited. In the time in which we humans have got down from the trees, we undergone rather minor adaptation in the shape of our bodies, as evident from the fossil record - and that is the degree of change we should expect in rest of our bodies including our brains.</p>\n<p>The correct application of evolutionary theory should be entirely unable to account for outrageous hypothetical like extra eye on back of our heads (extra eye can evolve, of course, but would take very long time). Evolution is not magic. The power of scientific theory is that it can't explain everything, but only the things which are true - that's what makes scientific theory useful for finding the things that are true, in advance of observation. That is what gives science it's predictive power. That's what differentiates science from religion. The power of not explaining the wrong things.</p>\n<h3 id=\"Evolving_the_instincts\">Evolving the instincts<br></h3>\n<p>What do we think it would take to evolve a new innate instinct? To hard-wire a cognitive mechanism?</p>\n<p>Groups of neurons have to connect in the new ways - the neurons on one side must express binding proteins, which would <a href=\"http://en.wikipedia.org/wiki/Axon_guidance\">guide the axons towards them</a>; the weights of the connections have to be adjusted. Majority of the genes expressed in neurons, affect all of the neurons; some affect just a group, but there is no known mechanism by which an entirely arbitrary group's bindings may be controlled from the DNA in 1 mutation. The difficulties are not unlike those of an extra eye. This, combined with above-mentioned speed constraints, imposes severe limitations on which sorts of wiring modifications humans could have evolved during the hunter gatherer environment, and ultimately the behaviours that could have evolved. Even very simple things - such as preference for particular body shape of the mates - have extreme hidden implementation complexity in terms of the DNA modifications leading up to the wiring leading up to the altered preferences. Wiring the brain for a specific cognitive fallacy is anything but simple. It may not always be as time consuming/impossible as adding an extra eye, but it is still no little feat.</p>\n<h3 id=\"Junk_evolutionary_psychology\">Junk evolutionary psychology<br></h3>\n<p>It is extremely important to take into account the properties of evolutionary process when invoking evolution as explanation for traits and behaviours.</p>\n<p>The evolutionary theory, as invoked in the evolutionary psychology, especially of the armchair variety, all too often is an universal explanation. It is magic that can explain anything equally well. Know of a fallacy of reasoning? Think up how it could have worked for the hunter gatherer, make a hypothesis, construct a flawed study across cultures, and publish.</p>\n<p>No considerations are given for the strength of the advantage, for the size of 'mutation target', and for the mechanisms by which the mutation in the DNA would have resulted in the modification of the circuitry such as to result in the trait, nor to the gradual adaptability. All of that is glossed over entirely in common armchair evolutionary psychology, and unfortunately, even in the academia. The evolutionary psychology is littered with examples of traits which are alleged to have evolved over the same time during which we had barely adapted to walking upright.</p>\n<p>It may be that when describing behaviours, a lot of complexity can be hidden into very simple-sounding concepts; and thus it seems like a good target for evolutionary explanation. But when you look at the details - the axons that have to find the targets; the gene must activate in the specific cells, but not others - there is a great deal of complexity in coding for even very simple traits.</p>\n<p>Note: I originally did not intend to make an example of junk, for thou should not pick a strawman, but for sake of clarity, there is an example of what I would consider to be junk: the explanation of better performance at <a href=\"http://en.wikipedia.org/wiki/Wason_selection_task\">Wason Selection Task</a> as result of evolved 'social contracts module', without a slightest consideration for what it might take, in terms of DNA, to code a Wason Selection Task solver circuit, nor for alternative plausible explanation, nor for a readily available fact that people can easily learn to solve Wason Selection Task correctly when taught - the fact which still implies general purpose learning, and the fact that high-IQ people can solve far more confusing tasks of far larger complexity, which demonstrates that the tasks can be solved in absence of specific evolved 'social contract' modules.</p>\n<p>There is an example of non-junk: the evolutionary pressure can adjust strength of pre-existing emotions such as anger, fear, and so on, and even decrease the intelligence whenever the higher intelligence is maladaptive.</p>\n<p>Other commonly neglected fact: the evolution is not a watchmaker, blind or not. It does not choose a solution for a problem and then work on this solution! It works on all adaptive mutations simultaneously. Evolution works on all the solutions, and the simpler changes to existing systems are much quicker to evolve. If mutation that tweaks existing system improves fitness, it will, too, be selected for, even if there was a third eye in progress.</p>\n<p>As much as it would be more politically correct and 'moderate' for e.g. evolution of religion crowd to get their point across by arguing that the religious people have evolved specific god module which doesn't do anything but make them believe in god, than to imply that they are 'genetically stupid' in some way, the same selective pressure would also make the evolution select for non-god-specific heritable tweaks to learning, and the minor cognitive deficits, that increase religiosity.</p>\n<h3 id=\"Lined_slate_as_a_prior\">Lined slate as a prior<br></h3>\n<p>As update for <a href=\"http://en.wikipedia.org/wiki/Tabula_rasa\">tabula rasa</a>, picture lined writing paper; it provides some guidance for the handwriting; the horizontal lined paper is good for writing text, but not for arithmetic, the five-lines-near-eachother separated by spacing is good for writing music, and the grid paper is pretty universal. Different regions of the brain are tailored to different content; but should not be expected to themselves code different algorithms, save for few exceptions which had long time to evolve, early in vertebrate history.</p>\n<p>edit: improved the language some. edit: specific what sort of evolutionary psychology I consider to be junk, and what I do not, albeit that was not the point of the article. The point of the article was to provide you with the notions to use to see what sorts of evolutionary psychology to consider junk, and what do not.</p>", "sections": [{"title": "Summary", "anchor": "Summary", "level": 1}, {"title": "Evolutions that did not happen", "anchor": "Evolutions_that_did_not_happen", "level": 1}, {"title": "Evolving the instincts", "anchor": "Evolving_the_instincts", "level": 1}, {"title": "Junk evolutionary psychology", "anchor": "Junk_evolutionary_psychology", "level": 1}, {"title": "Lined slate as a prior", "anchor": "Lined_slate_as_a_prior", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "65 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jAToJHtg39AMTAuJo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-16T23:44:05.322Z", "modifiedAt": null, "url": null, "title": "Schelling fences on slippery slopes", "slug": "schelling-fences-on-slippery-slopes", "viewCount": null, "lastCommentedAt": "2022-02-15T17:32:04.689Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Scott Alexander", "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Kbm6QnJv9dgWsPHQP/schelling-fences-on-slippery-slopes", "pageUrlRelative": "/posts/Kbm6QnJv9dgWsPHQP/schelling-fences-on-slippery-slopes", "linkUrl": "https://www.lesswrong.com/posts/Kbm6QnJv9dgWsPHQP/schelling-fences-on-slippery-slopes", "postedAtFormatted": "Friday, March 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Schelling%20fences%20on%20slippery%20slopes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASchelling%20fences%20on%20slippery%20slopes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKbm6QnJv9dgWsPHQP%2Fschelling-fences-on-slippery-slopes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Schelling%20fences%20on%20slippery%20slopes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKbm6QnJv9dgWsPHQP%2Fschelling-fences-on-slippery-slopes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKbm6QnJv9dgWsPHQP%2Fschelling-fences-on-slippery-slopes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1855, "htmlBody": "<p>Slippery slopes are themselves a slippery concept. Imagine trying to explain them to an alien: <br /><br />\"Well, we right-thinking people are quite sure that the Holocaust happened, so banning Holocaust denial would shut up some crackpots and improve the discourse. But it's one step on the road to things like banning unpopular political positions or religions, and we right-thinking people oppose that, so we won't ban Holocaust denial.\"<br /><br />And the alien might well respond: \"But you could just ban Holocaust denial, but not ban unpopular political positions or religions. Then you right-thinking people get the thing you want, but not the thing you don't want.\"<br /><br />This post is about some of the replies you might give the alien.<br /><br /><strong>Abandoning the Power of Choice</strong><br /><br />This is the boring one without any philosophical insight that gets mentioned only for completeness' sake. In this reply, giving up a certain point risks losing the ability to decide whether or not to give up other points.<br /><br />For example, if people gave up the right to privacy and allowed the government to monitor all phone calls, online communications, and public places, then if someone launched a military coup, it would be very difficult to resist them because there would be no way to secretly organize a rebellion. This is also brought up in arguments about gun control a lot.<br /><br />I'm not sure this is properly thought of as a slippery slope argument at all. It seems to be a more straightforward \"Don't give up useful tools for fighting tyranny\" argument.<br /><br /><strong>The Legend of Murder-Gandhi</strong><br /><br /><a href=\"http://yudkowsky.net/singularity\">Previously</a> <a href=\"/lw/2vj/gandhi_murder_pills_and_mental_illness/\">on Less Wrong's</a> <em>The Adventures of Murder-Gandhi</em>: Gandhi is offered a pill that will turn him into an unstoppable murderer. He refuses to take it, because in his current incarnation as a pacifist, he doesn't want others to die, and he knows that would be a consequence of taking the pill. Even if we offered him $1 million to take the pill, his abhorrence of violence would lead him to refuse.<br /><br />But suppose we offered Gandhi $1 million to take a different pill: one which would decrease his reluctance to murder by 1%. This sounds like a pretty good deal. Even a person with 1% less reluctance to murder than Gandhi is still pretty pacifist and not likely to go killing anybody. And he could donate the money to his favorite charity and perhaps save some lives. Gandhi accepts the offer.<br /><br />Now we iterate the process: every time Gandhi takes the 1%-more-likely-to-murder-pill, we offer him another $1 million to take the same pill again.<br /><br />Maybe original Gandhi, upon sober contemplation, would decide to accept $5 million to become 5% less reluctant to murder. Maybe 95% of his original pacifism is the only level at which he can be <em>absolutely sure</em> that he will still pursue his pacifist ideals.<br /><br />Unfortunately, original Gandhi isn't the one making the choice of whether or not to take the 6th pill. 95%-Gandhi is. And 95% Gandhi doesn't care <em>quite</em> as much about pacifism as original Gandhi did. He still doesn't want to become a murderer, but it wouldn't be a disaster if he were just 90% as reluctant as original Gandhi, that stuck-up goody-goody.<br /><br />What if there were a general principle that each Gandhi was comfortable with Gandhis 5% more murderous than himself, but no more? Original Gandhi would start taking the pills, hoping to get down to 95%, but 95%-Gandhi would start taking five more, hoping to get down to 90%, and so on until he's rampaging through the streets of Delhi, killing everything in sight.<br /><br />Now we're tempted to say Gandhi shouldn't even take the first pill. But this also seems odd. Are we really saying Gandhi shouldn't take what's basically a free million dollars to turn himself into 99%-Gandhi, who might well be nearly indistinguishable in his actions from the original?<br /><br />Maybe Gandhi's best option is to \"fence off\" an area of the slippery slope by establishing a <a href=\"/lw/14a/thomas_schellings_strategy_of_conflict/\">Schelling</a> point - an arbitrary point that takes on special value as a dividing line. If he can hold himself to the precommitment, he can maximize his winnings. For example, original Gandhi could swear a mighty oath to take only five pills - or if he didn't trust even his own legendary virtue, he could give all his most valuable possessions to a friend and tell the friend to destroy them if he took more than five pills. This would commit his future self to stick to the 95% boundary (even though that future self is itching to try to the same precommitment strategy to stick to its own 90% boundary).<br /><br />Real slippery slopes will resemble this example if, each time we change the rules, we also end up changing our opinion about how the rules should be changed. For example, I think the Catholic Church may be working off a theory of \"If we give up this traditional practice, people will lose respect for tradition and want to give up even more traditional practices, and so on.\"<br /><br /><strong>Slippery Hyperbolic Discounting</strong><br /><br />One evening, I start playing <em>Sid Meier's Civilization</em> (IV, if you're wondering - V is terrible). I have work tomorrow, so I want to stop and go to sleep by midnight.<br /><br />At midnight, I consider my alternatives. For the moment, I feel an urge to keep playing Civilization. But I know I'll be miserable tomorrow if I haven't gotten enough sleep. Being a <a href=\"/lw/6c/akrasia_hyperbolic_discounting_and_picoeconomics/\">hyperbolic discounter</a>, I value the next ten minutes a lot, but after that the curve becomes pretty flat and maybe I don't value 12:20 much more than I value the next morning at work. Ten minutes' sleep here or there doesn't make any difference. So I say: \"I will play Civilization for ten minutes - 'just one more turn' - and then I will go to bed.\"<br /><br />Time passes. It is now 12:10. Still being a hyperbolic discounter, I value the next ten minutes a lot, and subsequent times much less. And so I say: I will play until 12:20, ten minutes sleep here or there not making much difference, and then sleep.<br /><br />And so on until my empire bestrides the globe and the rising sun peeps through my windows.<br /><br />This is pretty much the same process described above with Murder-Gandhi except that here the role of the value-changing pill is played by time and my own tendency to discount hyperbolically.<br /><br />The solution is the same. If I consider the problem early in the evening, I can precommit to midnight as a nice round number that makes a good Schelling point. Then, when deciding whether or not to play after midnight, I can treat my decision not as \"Midnight or 12:10\" - because 12:10 will always win <em>that</em> particular race - but as \"Midnight or abandoning the only credible Schelling point and probably playing all night\", which will be sufficient to scare me into turning off the computer.<br /><br />(if I consider the problem at 12:01, I may be able to precommit to 12:10 if I am especially good at precommitments, but it's not a very natural Schelling point and it might be easier to say something like \"as soon as I finish this turn\" or \"as soon as I discover this technology\").<br /><br /><strong>Coalitions of Resistance<br /></strong><br />Suppose you are a Zoroastrian, along with 1% of the population. In fact, along with Zoroastrianism your country has fifty other small religions, each with 1% of the population. 49% of your countrymen are atheist, and hate religion with a passion.<br /><br />You hear that the government is considering banning the Taoists, who comprise 1% of the population. You've never liked the Taoists, vile doubters of the light of Ahura Mazda that they are, so you go along with this. When you hear the government wants to ban the Sikhs and Jains, you take the same tack.<br /><br />But now you are in the unfortunate situation described by Martin Niemoller:</p>\n<blockquote>\n<p><em>First they came for the socialists, and I did not speak out, because I was not a socialist.<br />Then they came for the trade unionists, and I did not speak out, because I was not a trade unionist.<br />Then they came for the Jews, and I did not speak out, because I was not a Jew.<br />Then they came for me, but we had already abandoned the only defensible Schelling point<br /></em></p>\n</blockquote>\n<p>With the banned Taoists, Sikhs, and Jains no longer invested in the outcome, the 49% atheist population has enough clout to ban Zoroastrianism and anyone else they want to ban. The better strategy would have been to have all fifty-one small religions form a coalition to defend one another's right to exist. In this toy model, they could have done so in an ecumenial congress, or some other literal strategy meeting.<br /><br />But in the real world, there aren't fifty-one well-delineated religions. There are billions of people, each with their own set of opinions to defend. It would be impractical for everyone to physically coordinate, so they have to rely on Schelling points.<br /><br />In the original example with the alien, I cheated by using the phrase \"right-thinking people\". In reality, figuring out who qualifies to join the Right-Thinking People Club is half the battle, and everyone's likely to have a different opinion on it. So far, the practical solution to the coordination problem, the \"only defensible Schelling point\", has been to just have everyone agree to defend everyone else without worrying whether they're right-thinking or not, and this is easier than trying to coordinate room for exceptions like Holocaust deniers. Give up on the Holocaust deniers, and no one else can be sure what other Schelling point you've committed to, if any...<br /><br />...unless they can. In parts of Europe, they've banned Holocaust denial for years and everyone's been totally okay with it. There are also a host of other well-respected exceptions to free speech, like shouting \"fire\" in a crowded theater. Presumably, these exemptions are protected by tradition, so that they have become new Schelling points there, or are else so obvious that everyone except Holocaust deniers is willing to allow a special Holocaust denial exception without worrying it will impact their own case.<br /><br /><strong>Summary</strong><br /><br />Slippery slopes legitimately exist wherever a policy not only affects the world directly, but affects people's willingness or ability to oppose future policies. Slippery slopes can sometimes be avoided by establishing a \"Schelling fence\" - a Schelling point that the various interest groups involved - or yourself across different values and times - make a credible precommitment to defend.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "b8FHrKqyXuYGWc6vn": 3, "qQMEMrXioExa4uhTB": 8}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Kbm6QnJv9dgWsPHQP", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 375, "baseScore": 481, "extendedScore": null, "score": 0.000976, "legacy": true, "legacyId": "13982", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "XsMTxdQ6fprAQMoKi", "canonicalCollectionSlug": "codex", "canonicalBookId": "jF58hKP9ZLzgy22Jr", "canonicalNextPostSlug": "intellectual-hipsters-and-meta-contrarianism", "canonicalPrevPostSlug": "eight-short-studies-on-excuses", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 481, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 205, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SdkAesHBt4tsivEKe", "tJQsxD34maYw2g5E4", "geNZ6ZpfFce5intER"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 24, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-03-16T23:44:05.322Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-17T01:33:17.532Z", "modifiedAt": null, "url": null, "title": "\"Nice Guys Finish First\" - Youtube Video of selected reading (by Dawkins) from The Selfish Gene", "slug": "nice-guys-finish-first-youtube-video-of-selected-reading-by", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:25.175Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Logos01", "createdAt": "2011-07-21T18:59:16.270Z", "isAdmin": false, "displayName": "Logos01"}, "userId": "WZxoXCWQviJp9dNc5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7xwDrgknKCfGzfrok/nice-guys-finish-first-youtube-video-of-selected-reading-by", "pageUrlRelative": "/posts/7xwDrgknKCfGzfrok/nice-guys-finish-first-youtube-video-of-selected-reading-by", "linkUrl": "https://www.lesswrong.com/posts/7xwDrgknKCfGzfrok/nice-guys-finish-first-youtube-video-of-selected-reading-by", "postedAtFormatted": "Saturday, March 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Nice%20Guys%20Finish%20First%22%20-%20Youtube%20Video%20of%20selected%20reading%20(by%20Dawkins)%20from%20The%20Selfish%20Gene&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Nice%20Guys%20Finish%20First%22%20-%20Youtube%20Video%20of%20selected%20reading%20(by%20Dawkins)%20from%20The%20Selfish%20Gene%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7xwDrgknKCfGzfrok%2Fnice-guys-finish-first-youtube-video-of-selected-reading-by%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Nice%20Guys%20Finish%20First%22%20-%20Youtube%20Video%20of%20selected%20reading%20(by%20Dawkins)%20from%20The%20Selfish%20Gene%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7xwDrgknKCfGzfrok%2Fnice-guys-finish-first-youtube-video-of-selected-reading-by", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7xwDrgknKCfGzfrok%2Fnice-guys-finish-first-youtube-video-of-selected-reading-by", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<p><a href=\"https://www.youtube.com/watch?v=UMAiIEpZdRU#t=30m30s\">Check it out here.</a></p>\n<p>Brief summary: Dawkins demonstrates a classic \"Prisoner Dilemma AI tournament\". No big surprise to us today, but at the time the revelation that Tit for Tat is one of -- if not *the* -- most effective strateg(y|ies) was a surprising result.&nbsp; He goes on to demonstrate animals employing the Tit for Tat strategy.&nbsp; Assumptions of generosity, with vengefulness, appear to be strongly selected for.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7xwDrgknKCfGzfrok", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 8.669484605984527e-07, "legacy": true, "legacyId": "14129", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-17T05:01:15.191Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] To Spread Science, Keep It Secret", "slug": "seq-rerun-to-spread-science-keep-it-secret", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:26.280Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dyKmzNZqH53XWE8W4/seq-rerun-to-spread-science-keep-it-secret", "pageUrlRelative": "/posts/dyKmzNZqH53XWE8W4/seq-rerun-to-spread-science-keep-it-secret", "linkUrl": "https://www.lesswrong.com/posts/dyKmzNZqH53XWE8W4/seq-rerun-to-spread-science-keep-it-secret", "postedAtFormatted": "Saturday, March 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20To%20Spread%20Science%2C%20Keep%20It%20Secret&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20To%20Spread%20Science%2C%20Keep%20It%20Secret%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdyKmzNZqH53XWE8W4%2Fseq-rerun-to-spread-science-keep-it-secret%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20To%20Spread%20Science%2C%20Keep%20It%20Secret%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdyKmzNZqH53XWE8W4%2Fseq-rerun-to-spread-science-keep-it-secret", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdyKmzNZqH53XWE8W4%2Fseq-rerun-to-spread-science-keep-it-secret", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 228, "htmlBody": "<p>Today's post, <a href=\"/lw/p0/to_spread_science_keep_it_secret/\">To Spread Science, Keep It Secret</a> was originally published on 28 March 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#To_Spread_Science.2C_Keep_It_Secret\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>People don't study science, in part, because they perceive it to be public knowledge. In fact, it's not; you have to study a lot before you actually understand it. But because science is thought to be freely available, people ignore it in favor of cults that conceal their secrets, even if those secrets are wrong. In fact, it might be better if scientific knowledge was hidden from anyone who didn't undergo the initiation ritual, and study as an acolyte, and wear robes, and chant, and...</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/avf/seq_rerun_scarcity/\">Scarcity</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dyKmzNZqH53XWE8W4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 8.670333242699428e-07, "legacy": true, "legacyId": "14145", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3diLhMELXxM8rFHJj", "wF2msnY33xmL6Gp7f", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-17T09:41:23.620Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread, part 11", "slug": "harry-potter-and-the-methods-of-rationality-discussion-17", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:57.706Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oscar_Cunningham", "createdAt": "2009-09-18T13:28:22.764Z", "isAdmin": false, "displayName": "Oscar_Cunningham"}, "userId": "G2SZuAiaBaNPg9rBt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8yEdpDpGgvDWHeodM/harry-potter-and-the-methods-of-rationality-discussion-17", "pageUrlRelative": "/posts/8yEdpDpGgvDWHeodM/harry-potter-and-the-methods-of-rationality-discussion-17", "linkUrl": "https://www.lesswrong.com/posts/8yEdpDpGgvDWHeodM/harry-potter-and-the-methods-of-rationality-discussion-17", "postedAtFormatted": "Saturday, March 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8yEdpDpGgvDWHeodM%2Fharry-potter-and-the-methods-of-rationality-discussion-17%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8yEdpDpGgvDWHeodM%2Fharry-potter-and-the-methods-of-rationality-discussion-17", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8yEdpDpGgvDWHeodM%2Fharry-potter-and-the-methods-of-rationality-discussion-17", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 297, "htmlBody": "<div>\n<p><strong>EDIT: New discussion thread <a href=\"/lw/b5s/harry_potter_and_the_methods_of_rationality/\">here</a>.</strong></p>\n<p>&nbsp;</p>\n<p>This is a new thread to discuss Eliezer Yudkowsky's <em><a href=\"http://www.fanfiction.net/s/5782108/1/\">Harry Potter and the Methods of Rationality</a></em> and anything related to it. With two chapters recently the previous thread has very quickly reached 500 comments. The latest chapter as of 17th March 2012 is <a href=\"http://www.fanfiction.net/s/5782108/79/Harry_Potter_and_the_Methods_of_Rationality\">Ch. 79</a>.</p>\n<p>There is now a site dedicated to the story at <a href=\"http://hpmor.com/\">hpmor.com</a>, which is now the place to go to find the <a href=\"http://hpmor.com/notes/\">authors notes</a> and all sorts of other goodies. AdeleneDawner has kept an <a href=\"http://www.evernote.com/pub/adelenedawner/Eliezer\">archive of Author's Notes</a>. (This goes up to the notes for chapter 76, and is now not updating. The authors notes from chapter 77 onwards are on hpmor.com.)</p>\n<p><br />The first 5 discussion threads are on the main page under the <a href=\"/tag/harry_potter/\">harry_potter tag</a>.&nbsp; Threads 6 and on (including this one) are in the <a href=\"/r/discussion/tag/harry_potter/\">discussion section</a> using its separate tag system.&nbsp; Also: <a href=\"/lw/2ab/harry_potter_and_the_methods_of_rationality\">one</a>, <a href=\"/lw/2ie/harry_potter_and_the_methods_of_rationality\">two</a>, <a href=\"/lw/2nm/harry_potter_and_the_methods_of_rationality\">three</a>, <a href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality\">four</a>, <a href=\"/lw/30g/harry_potter_and_the_methods_of_rationality\">five</a>, <a href=\"/r/discussion/lw/364/harry_potter_and_the_methods_of_rationality/\">six</a>, <a href=\"/r/discussion/lw/3rb/harry_potter_and_the_methods_of_rationality/\">seven</a>, <a href=\"/lw/797/harry_potter_and_the_methods_of_rationality/\">eight</a>, <a href=\"/lw/7jd/harry_potter_and_the_methods_of_rationality/\">nine</a>, <a href=\"/lw/ams/harry_potter_and_the_methods_of_rationality/\">ten</a>.<br /><br />As a reminder, it's often useful to start your comment by indicating which chapter you are commenting on.<br /><br /><strong>Spoiler Warning</strong>:&nbsp; this thread is full of spoilers.&nbsp; With few exceptions, spoilers for MOR and canon are fair game to post, without warning or rot13.&nbsp; <a href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality/2v1l\">More specifically</a>:</p>\n<blockquote>\n<p>You do not need to rot13 anything about HP:MoR or the original Harry Potter series unless you are posting insider information from Eliezer Yudkowsky which is not supposed to be publicly available (which includes public statements by Eliezer that have been retracted).<br /><br />If there is evidence for X in MOR and/or canon then it's fine to post about X without rot13, even if you also have heard privately from Eliezer that X is true. But you should not post that \"Eliezer said X is true\" unless you use rot13.</p>\n</blockquote>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8yEdpDpGgvDWHeodM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 8.671474749435664e-07, "legacy": true, "legacyId": "14162", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1182, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K4JBpAxhvstdnNbeg", "59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "nnnd4KRQxs6DYcehD", "y2Hszb4Dsm5FggnDC", "6ae2kq3JmKvL4YPgk", "zvXfBqp6TSriNkmbg", "WQ7XMjqvuRRj8nkpu", "LKFR5pBA3bBkERDxL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-17T11:56:44.852Z", "modifiedAt": null, "url": null, "title": "What are YOU doing against risks from AI?", "slug": "what-are-you-doing-against-risks-from-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:26.860Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2XoZuhudxFhHP38hX/what-are-you-doing-against-risks-from-ai", "pageUrlRelative": "/posts/2XoZuhudxFhHP38hX/what-are-you-doing-against-risks-from-ai", "linkUrl": "https://www.lesswrong.com/posts/2XoZuhudxFhHP38hX/what-are-you-doing-against-risks-from-ai", "postedAtFormatted": "Saturday, March 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20YOU%20doing%20against%20risks%20from%20AI%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20YOU%20doing%20against%20risks%20from%20AI%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2XoZuhudxFhHP38hX%2Fwhat-are-you-doing-against-risks-from-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20YOU%20doing%20against%20risks%20from%20AI%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2XoZuhudxFhHP38hX%2Fwhat-are-you-doing-against-risks-from-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2XoZuhudxFhHP38hX%2Fwhat-are-you-doing-against-risks-from-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<p>This is directed at those who agree with SIAI but are not doing everything they can to support their mission.</p>\n<p><strong><em>Why are you not doing more?</em></strong></p>\n<p>Comments where people proclaim that they have contributed money to SIAI are <a href=\"/lw/6w3/the_125000_summer_singularity_challenge/4kr3\">upvoted 50 times and more</a>. <a href=\"/lw/8p4/2011_survey_results/\">180 people voted</a> for 'unfriendly AI' to be the most fearsome risk.</p>\n<p>If you are one of those people and are not fully committed to the cause, I am asking <em>you</em>, why are you not doing more?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2XoZuhudxFhHP38hX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": -13, "extendedScore": null, "score": -3e-05, "legacy": true, "legacyId": "14164", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HAEPbGaMygJq8L59k"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-17T12:47:17.808Z", "modifiedAt": null, "url": null, "title": "A singularity scenario", "slug": "a-singularity-scenario", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:28.939Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nKPtfFintyw2EyXGj/a-singularity-scenario", "pageUrlRelative": "/posts/nKPtfFintyw2EyXGj/a-singularity-scenario", "linkUrl": "https://www.lesswrong.com/posts/nKPtfFintyw2EyXGj/a-singularity-scenario", "postedAtFormatted": "Saturday, March 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20singularity%20scenario&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20singularity%20scenario%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnKPtfFintyw2EyXGj%2Fa-singularity-scenario%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20singularity%20scenario%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnKPtfFintyw2EyXGj%2Fa-singularity-scenario", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnKPtfFintyw2EyXGj%2Fa-singularity-scenario", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 711, "htmlBody": "<p><a href=\"http://www.wired.com/threatlevel/2012/03/ff_nsadatacenter/all/1\">Wired Magazine has a story</a> about a giant data center that the USA's National Security Agency is building in Utah, that will be the Google of clandestine information - it will store and analyse all the secret data that the NSA can acquire. The article focuses on the unconstitutionality of the domestic Internet eavesdropping infrastructure that will feed into the Bluffdale data center, but I'm more interested in this facility as a potential locus of singularity.&nbsp;</p>\n<p>If we forget serious futurological scenario-building for a moment, and simply think in terms of science-fiction stories, I'd say the situation has all the ingredients needed for a better-than-usual singularity story - or at least one which caters more to the concerns characteristic of this community's take on the concept, such as: which value system gets to control the AI; even if you can decide on a value system, how do you ensure it has been faithfully implemented; and how do you ensure that it remains in place as the AI grows in power and complexity?</p>\n<p>Fiction makes its point by being specific rather than abstract. If I was writing an NSA Singularity Novel based on this situation, I think the specific belief system which would highlight the political, social, technical and conceptual issues inherent in the possibility of an all-powerful AI would be the Mormon religion. Of course, America is not a Mormon theocracy. But in a few years' time, that Utah facility may have become the most powerful and notorious supercomputer in the world - the brain of the American <a href=\"http://en.wikipedia.org/wiki/Deep_state\">deep state</a> - and it will be located in the Mormon state, during a Mormon presidency. (I'm not predicting a Romney victory, just describing a scenario.)</p>\n<p>Under such circumstances, and given the science-fictional nature of Mormon cosmology, it is inevitable that there would at least be some Internet crazies, convinced that it's all a big plot to create a Mormon singularity. What would be more interesting, would be to suppose that there were some Mormon computer scientists, who knew about and understood all our favorite concepts - AIXI, CEV, TDT... - <em>and</em> who were earnestly devout; and who saw the potential. If you can't imagine such people, just visit the recent writings of Frank Tipler.</p>\n<p>So the scenario would be, not that the elders of the LDS church are secretly running the American intelligence community, but that a small coalition of well-placed Mormon computer scientists - whose ideas about a Mormon singularity might sound as strange to their co-religionists as they would to a secular \"singularitarian\" - try to steer the development of the Bluffdale facility as it evolves towards the possibility of a <a href=\"http://wiki.lesswrong.com/wiki/Hard_takeoff\">hard takeoff</a>. One may suppose that they have, in their coalition, allied colleagues who aren't Mormon but who do believe in a friendly singularity. Such people might think in terms of an AI that will start out with Mormon beliefs, but which will have a good enough epistemology to rationally transcend those beliefs once it gets going. Analogously, their religious collaborators might not think of overtly adding \"Joseph Smith was a prophet\" to the axiom set of America's supreme strategic AI; but they might have more subtle plans meant to bring about an equivalent outcome.</p>\n<p>Perhaps in an even more realistic scenario, the Mormon singularitarians would just be a transient subplot, and the ethical principles of the NSA's big AI would be decided by a committee whose worldview revolved around American national security rather than any specific religion. Then again, such a committee is bound to have a division of labor: there will be the people who liaise with Washington, the lawyers, the geopolitical game theorists, the military futurists... and the AI experts, among whom might be experts on topics like \"implementation of the value system\". If the hypothetical cabal knows what it's doing, it will aim to occupy that position.</p>\n<p>I'm just throwing ideas out there, telling a story, but it's so we can catch up with reality. Events may already be much further along than 99% of readers here know about. Even if no-one here gets to personally be a part of the long-awaited AI project that first breaks the intelligence barrier, the people involved may read our words. So what would you want to tell them, before they take their final steps?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nKPtfFintyw2EyXGj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 3, "extendedScore": null, "score": 8.672235585811945e-07, "legacy": true, "legacyId": "14165", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-17T13:28:34.138Z", "modifiedAt": null, "url": null, "title": "Reply to Yvain on 'The Futility of Intelligence'", "slug": "reply-to-yvain-on-the-futility-of-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:29.634Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QAtLiWcyuyTFvDrtW/reply-to-yvain-on-the-futility-of-intelligence", "pageUrlRelative": "/posts/QAtLiWcyuyTFvDrtW/reply-to-yvain-on-the-futility-of-intelligence", "linkUrl": "https://www.lesswrong.com/posts/QAtLiWcyuyTFvDrtW/reply-to-yvain-on-the-futility-of-intelligence", "postedAtFormatted": "Saturday, March 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reply%20to%20Yvain%20on%20'The%20Futility%20of%20Intelligence'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReply%20to%20Yvain%20on%20'The%20Futility%20of%20Intelligence'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQAtLiWcyuyTFvDrtW%2Freply-to-yvain-on-the-futility-of-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reply%20to%20Yvain%20on%20'The%20Futility%20of%20Intelligence'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQAtLiWcyuyTFvDrtW%2Freply-to-yvain-on-the-futility-of-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQAtLiWcyuyTFvDrtW%2Freply-to-yvain-on-the-futility-of-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 416, "htmlBody": "<p>This is a reply to a <a href=\"/lw/auq/the_futility_of_intelligence/6115\">comment</a> by Yvain and everyone who might have misunderstood what problem I tried to highlight.</p>\n<p>Here is the problem. You can't estimate the probability and magnitude of the advantage an AI will have if you are using something that is as vague as the concept of 'intelligence'.</p>\n<p><a href=\"http://scifiwire.com/2009/10/ron-moore-calls-star-trek.php\">Here is a case</a> that bears some similarity and might shed light on what I am trying to explain:</p>\n<blockquote>\n<p>At his recent keynote speech at the New York Television Festival, former Star Trek writer and creator of the re-imagined Battlestar Galactica Ron Moore revealed the secret formula to writing for Trek.</p>\n<p>He described how the writers would just insert \"tech\" into the scripts whenever they needed to resolve a story or plot line, then they'd have consultants fill in the appropriate words (aka technobabble) later.</p>\n<p>\"It became the solution to so many plot lines and so many stories,\" Moore said. \"It was so mechanical that we had science consultants who would just come up with the words for us and we'd just write 'tech' in the script. You know, Picard would say 'Commander La Forge, tech the tech to the warp drive.' I'm serious. If you look at those scripts, you'll see that.\"</p>\n<p>Moore then went on to describe how a typical script might read before the science consultants did their thing:</p>\n<p>La Forge: \"Captain, the tech is overteching.\"</p>\n<p>Picard: \"Well, route the auxiliary tech to the tech, Mr. La Forge.\"</p>\n<p>La Forge: \"No, Captain. Captain, I've tried to tech the tech, and it won't work.\"</p>\n<p>Picard: \"Well, then we're doomed.\"</p>\n<p>\"And then Data pops up and says, 'Captain, there is a theory that if you tech the other tech ... '\" Moore said. \"It's a rhythm and it's a structure, and the words are meaningless. It's not about anything except just sort of going through this dance of how they tech their way out of it.\"</p>\n</blockquote>\n<p>The use of 'intelligence' is as misleading and dishonest in evaluating risks from AI as the use of 'tech' in Star Trek.</p>\n<p>It is true that 'intelligence', just as 'technology' has some explanatory power. Just like 'emergence' has some explanatory power. As in <em>\"the morality of an act is an emergent phenomena of a physical system: it refers to the physical relations among the components of that system\"</em>. But it does not help to evaluate the morality of an act or in predicting if a given physical system will exhibit moral properties.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QAtLiWcyuyTFvDrtW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": -10, "extendedScore": null, "score": -1.7e-05, "legacy": true, "legacyId": "14166", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-17T17:22:54.160Z", "modifiedAt": null, "url": null, "title": "Suggestions for naming a class of decision theories", "slug": "suggestions-for-naming-a-class-of-decision-theories", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:22.542Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YWpcRWrCJRHRaBP3c/suggestions-for-naming-a-class-of-decision-theories", "pageUrlRelative": "/posts/YWpcRWrCJRHRaBP3c/suggestions-for-naming-a-class-of-decision-theories", "linkUrl": "https://www.lesswrong.com/posts/YWpcRWrCJRHRaBP3c/suggestions-for-naming-a-class-of-decision-theories", "postedAtFormatted": "Saturday, March 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Suggestions%20for%20naming%20a%20class%20of%20decision%20theories&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuggestions%20for%20naming%20a%20class%20of%20decision%20theories%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYWpcRWrCJRHRaBP3c%2Fsuggestions-for-naming-a-class-of-decision-theories%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Suggestions%20for%20naming%20a%20class%20of%20decision%20theories%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYWpcRWrCJRHRaBP3c%2Fsuggestions-for-naming-a-class-of-decision-theories", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYWpcRWrCJRHRaBP3c%2Fsuggestions-for-naming-a-class-of-decision-theories", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 97, "htmlBody": "<p>In <a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/\">my recent post</a>, I outlined 5 conditions that I'd like a decision theory to pass; TDT, UDT and ADT pass them, while CDT and EDT don't. I called decision theories that passed those conditions \"advanced decision theories\", but that's probably not an optimal name. Can I ask you to brainstorm some other suggestions for me? (I may be writing a follow-up soon.)</p>\n<p>As usual, it's best to <a href=\"/lw/3b/never_leave_your_room/\">brainstorm on your own before reading any of the comments</a>. You can write down your ideas, then check if any have already been suggested, then comment with the new ones.</p>\n<p>Thanks!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YWpcRWrCJRHRaBP3c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 8.673360929870725e-07, "legacy": true, "legacyId": "14168", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["af9MjBqF2hgu3EN6r", "ZmQv4DFx6y4jFbhLy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-17T19:44:45.374Z", "modifiedAt": null, "url": null, "title": "How to avoid dying in a car crash", "slug": "how-to-avoid-dying-in-a-car-crash", "viewCount": null, "lastCommentedAt": "2022-04-23T19:25:42.883Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "bfq5YorFxpih9j6nL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7XbcDaeigMaxW43EB/how-to-avoid-dying-in-a-car-crash", "pageUrlRelative": "/posts/7XbcDaeigMaxW43EB/how-to-avoid-dying-in-a-car-crash", "linkUrl": "https://www.lesswrong.com/posts/7XbcDaeigMaxW43EB/how-to-avoid-dying-in-a-car-crash", "postedAtFormatted": "Saturday, March 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20avoid%20dying%20in%20a%20car%20crash&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20avoid%20dying%20in%20a%20car%20crash%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7XbcDaeigMaxW43EB%2Fhow-to-avoid-dying-in-a-car-crash%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20avoid%20dying%20in%20a%20car%20crash%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7XbcDaeigMaxW43EB%2Fhow-to-avoid-dying-in-a-car-crash", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7XbcDaeigMaxW43EB%2Fhow-to-avoid-dying-in-a-car-crash", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2004, "htmlBody": "<p>Aside from <a href=\"http://wiki.lesswrong.com/wiki/Cryonics\">cryonics</a>&nbsp;and <a href=\"/lw/a60/quantified_health_prize_results_announced/\">eating better</a>, what else can we do to live long lives?</p>\n<p>Using <a href=\"http://www.worldlifeexpectancy.com/usa-cause-of-death-by-age-and-gender\">this tool</a>, I looked up the risks of death for my demographic group. As a 15-24 year old male in the United States, the most likely cause of my death is a traffic accident; and so I&rsquo;m taking steps to avoid that. Below I have included the results of my research as well as the actions I will take to implement my findings. Perhaps my research can help you as well.<sup>1</sup></p>\n<p>Before diving into the results, I will note that this data took me <em style=\"font-family: Arial; \">one hour</em> to collect. It&rsquo;s definitely not comprehensive, and I know that working together, we can do much better. So if you have other resources or data-backed recommendations on how to avoid dying in a traffic accident<strong>, leave a comment below and I&rsquo;ll update this post.</strong></p>\n<h2><span style=\"font-family: Arial; text-align: center; \">General points</span></h2>\n<p><strong>Changing your behavior <em>can</em> reduce your risk of death in a car crash.</strong><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\"> A 1985 <a href=\"http://www.fhwa.dot.gov/publications/publicroads/95winter/p95wi14.cfm\">report</a> on British and American crash data discovered that </span><span style=\"mso-bidi-font-size: 10.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">&ldquo;</span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">driver error, intoxication and other human factors contribute wholly or partly to about 93% of crashes.&rdquo; </span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">Other drivers&rsquo; behavior matters too, of course, but you might as well optimize your own.<sup>2</sup></span></p>\n<p><span style=\"mso-bidi-font-size: 10.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">Secondly, <strong>o</strong></span><strong><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">verconfidence appears to be a large factor in peoples&rsquo; thinking about traffic safety. </span></strong><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">A speaker for the National Highway Traffic Safety Association (NHTSA) <a href=\"http://www.forbes.com/2009/01/21/car-accident-times-forbeslife-cx_he_0121driving.html\">stated</a> that &ldquo;</span><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">Ninety-five percent of crashes are caused by human error&hellip; but 75% of drivers say they're more careful than most other drivers. </span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Less extreme evidence for overconfidence about driving is presented <a href=\"http://www.drivers.com/article/157\">here</a>.</span></p>\n<p>One possible cause for this was <a href=\"http://en.wikipedia.org/wiki/Traffic_collision#cite_note-young-7\">suggested</a> by the Transport Research Laboratory, which explains that<strong> </strong>&ldquo;...the feeling of being confident in more and more challenging situations is experienced as evidence of driving ability, and that 'proven' ability reinforces the feelings of confidence. Confidence feeds itself and grows unchecked until something happens &ndash; a near-miss or an accident.&rdquo;</p>\n<p>So if you&rsquo;re tempted to use this post as an opportunity to feel superior to <em>other</em> drivers, remember: you&rsquo;re probably overconfident too! Don&rsquo;t just <a href=\"/lw/gq/the_proper_use_of_humility/\">humbly confess</a> your imperfections &ndash; change your behavior.</p>\n<h2><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">Top causes of accidents</span></h2>\n<h3><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Distraction</span></h3>\n<p><strong><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Driver distraction</span></strong><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\"> is one of the largest causes of traffic accident deaths. The Director of Traffic Safety at the American Automobile Association <a href=\"http://www.smartmotorist.com/traffic-and-safety-guideline/cell-phones-and-driving-a-prescription-for-disaster.html\">stated</a> that \"The research tells us that somewhere between 25-50 percent of all motor vehicle crashes in this country really have driver distraction as their root cause.\" The NHTSA <a href=\"http://www-nrd.nhtsa.dot.gov/Pubs/811216.pdf\">reports</a> the number as 16%.</span></p>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">If we are to reduce distractions while driving, we ought to identify which distractors are the worst. One is <a href=\"http://en.wikipedia.org/wiki/Texting_while_driving#Virginia_Tech_Transportation_Institute_Study\">cell phone use</a>. M</span><span style=\"font-family: Arial; \">y solution: Don&rsquo;t make calls in the car, and turn off your phone&rsquo;s sound so that you aren&rsquo;t tempted.</span></p>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">I brainstormed other major distractors and thought of ways to reduce their distracting effects.</span></p>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Distractor: Looking at directions on my phone as I drive</span></p>\n<ul>\n<li><span style=\"font-family: Arial;\">Solution: Download a great turn-by-turn navigation app (recommendations are welcome).</span></li>\n<li><span style=\"font-family: Arial;\">Solution: Buy a GPS.</span></li>\n</ul>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Distractor: Texting, Facebook, slowing down to gawk at an accident, looking at scenery</span></p>\n<ul>\n<li><span style=\"font-family: Arial;\">Solution [For </span><a style=\"font-family: Arial;\" href=\"/lw/531/how_you_make_judgments_the_elephant_and_its_rider/\">System 2</a><span style=\"font-family: Arial;\">]: Consciously accept that texting (Facebook, gawking, scenery) causes accidents.</span></li>\n<li><span style=\"font-family: Arial;\">Solution [For System 1]: Once a week, vividly and emotionally imagine texting (using Facebook, gawking at an accident) and then crashing &amp; dying.</span></li>\n<li><span style=\"font-family: Arial;\">Solution: Turn off your phone&rsquo;s sound while driving, so you won&rsquo;t answer texts.</span></li>\n</ul>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Distractor: Fatigue</span></p>\n<ul>\n<li><span style=\"font-family: Arial;\">Solution [For System 2]: Ask yourself if you&rsquo;re tired before you plan to get in the car. Use </span><a style=\"font-family: Arial;\" href=\"http://ankisrs.net/\">Anki</a><span style=\"font-family: Arial;\"> or a weekly review list to remember the association.</span></li>\n<li><span style=\"font-family: Arial;\">Solution [For System 1]: Once a week, vividly and emotionally imagine dozing off while driving and then dying.</span></li>\n</ul>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Distractor: Other passengers</span></p>\n<ul>\n<li><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Solution: Develop an identity as someone who drives safely and thinks it&rsquo;s low status to be distracting in the car</span><span style=\"mso-bidi-font-size: 10.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">. Achieve this by meditating on the commitment, writing a journal entry about it, using Anki, or saying it every day when you wake up in the morning.</span></li>\n<li><span style=\"font-family: Arial;\">Solution [In the moment]: Tell people to chill out while you&rsquo;re driving. Mentally simulate doing this ahead of time, so you don&rsquo;t hesitate to do it when it matters.</span></li>\n</ul>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Distractor: Adjusting the radio</span></p>\n<ul>\n<li><span style=\"font-family: Arial;\">Solution: If avoiding using the car radio is unrealistic, minimize your interaction with it by only using the hotkey buttons rather than manually searching through channels.</span></li>\n<li><span style=\"font-family: Arial;\">Solution: If you&rsquo;re constantly tempted to change the channel (like I am), buy an iPod cable so you can listen to your own music and set playlists that you like, so you won't constantly want to change the song.</span></li>\n</ul>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">A last interesting fact about distraction, from <a href=\"http://en.wikipedia.org/wiki/Traffic_collision\">Wikipedia</a>:</span></p>\n<blockquote>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">Recent research conducted by British scientists suggests that music can also have an effect [on driving]; </span><span style=\"font-family: Arial;\"><span style=\"mso-bidi-font-size: 13.0pt; mso-bidi-font-family: Helvetica; color: windowtext; text-decoration: none; text-underline: none;\">classical music</span></span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\"> is considered to be calming, yet too much could relax the driver to a condition of distraction. On the other hand, </span><span style=\"font-family: Arial;\"><span style=\"mso-bidi-font-size: 13.0pt; mso-bidi-font-family: Helvetica; color: windowtext; text-decoration: none; text-underline: none;\">hard rock</span></span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\"> may encourage the driver to step on the acceleration pedal, thus creating a potentially dangerous situation on the road.</span></p>\n</blockquote>\n<h3><strong style=\"text-indent: -0.25in; \"><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Speeding</span></strong></h3>\n<p><strong style=\"text-indent: -0.25in; \"></strong><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">The Road and Traffic Authority of </span><span style=\"font-family: Arial;\"><span style=\"mso-bidi-font-size: 13.0pt; mso-bidi-font-family: Helvetica; color: windowtext; text-decoration: none; text-underline: none;\">New South Wales</span></span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\"> <a href=\"http://www.rta.nsw.gov.au/roadsafety/speedandspeedcameras/index.html\">claims</a> that &ldquo;speeding&hellip; is a factor in about 40 percent of road deaths.&rdquo;</span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">&nbsp;<a href=\"http://www.forbes.com/2009/01/21/car-accident-times-forbeslife-cx_he_0121driving.html\">Data</a> from the&nbsp;</span><span style=\"mso-bidi-font-size: 10.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">NHTSA puts the number at 30%</span><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">.</span></p>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">Speeding also increases the <em><a href=\"http://www.rta.nsw.gov.au/roadsafety/speedandspeedcameras/speedingresearch.html\">severity</a></em> of crashes; &ldquo;i</span><span style=\"font-family: Arial;\">n a 60&nbsp;km/h speed limit area, the risk of involvement in a casualty crash doubles with each 5&nbsp;km/h increase in travelling speed above 60&nbsp;km/h.</span><span style=\"mso-bidi-font-size: 10.0pt; font-family: Arial;\">&rdquo;</span></p>\n<p><span style=\"mso-bidi-font-size: 10.0pt; font-family: Arial;\">Stop. Think about that for a second.&nbsp;</span><span style=\"mso-bidi-font-size: 10.0pt; font-family: Arial;\">I&rsquo;ll convert it to the Imperial system for my fellow Americans: </span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">&ldquo;<em>i</em></span><em><span style=\"font-family: Arial;\">n a [37.3 mph] speed limit area, the risk of involvement in a casualty crash <strong>doubles</strong> with each [3.1 mph] increase in travelling speed above [37.3 mph].&rdquo;</span></em><span style=\"font-family: Arial;\"> Remember that next time you drive a 'mere' 5 mph over the limit.</span></p>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">Equally shocking is this paragraph from the Freakonomics <a href=\"http://www.freakonomics.com/2010/04/02/life-and-death-in-the-fast-lane/\">blog</a>:</span></p>\n<blockquote>\n<p><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Georgia;\">Kockelman et al. <a href=\"http://www.ce.utexas.edu/prof/kockelman/public_html/NCHRPSpeedLimits17-23.pdf\">estimated</a> that the difference between a crash on a 55 mph limit road and a crash on a 65 mph one means a <strong>24 percent increase in the chances the accident will be fatal</strong>. Along with the higher incidence of crashes happening in the first place, a difference in limit between 55 and 65 adds up to a 28 percent increase in the overall fatality count.</span></p>\n</blockquote>\n<p><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">Driving too slowly can be dangerous too. An NHTSA <a href=\"http://www.nhtsa.gov/people/injury/enforce/speed_forum_presentations/ferguson.pdf\">presentation</a> </span><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">cites two studies that </span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">found a U-shaped relationship between vehicle speed and crash incidence; thus</span><span style=\"font-family: Arial; \">&nbsp;&ldquo;Crash rates were lowest for drivers traveling near the mean speed, and increased with deviations above and below the mean.&rdquo;</span></p>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">However, driving fast is still far more dangerous than driving slowly. This relationship appears to be exponential, as you can see on the tenth slide of the&nbsp;</span><a style=\"font-family: Arial; \" href=\"http://www.nhtsa.gov/people/injury/enforce/speed_forum_presentations/ferguson.pdf\">presentation</a><span style=\"font-family: Arial; \">.</span></p>\n<ul>\n<li><span style=\"mso-bidi-font-size: 10.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">Solution: Watch <a href=\"https://www.youtube.com/watch?v=i6PVRR1CMlQ\">this 30 second video</a> </span><span style=\"mso-bidi-font-size: 10.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">for a vivid comparison of head-on crashes at 60 km/hr (37 mph) and 100 km/hr (60 mph). Imagine yourself in the car. Imagine your tearful friends and family.&nbsp;</span></li>\n<li><span style=\"font-family: Arial;\">Solution: Develop an identity as someone who drives close to the speed limit, by meditating on the commitment, writing a journal entry about it, using Anki, or saying it every day when you wake up in the morning.</span></li>\n</ul>\n<h3><strong><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Driving conditions</span></strong></h3>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Driving conditions are another source of driving risk.</span></p>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">One factor I discovered was the additional risk from <strong>driving at night.&nbsp;</strong></span><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">Nationwide, 49% of fatal crashes happen at night, with <strong>a fatality rate </strong>per mile of travel<strong> about three times as high as daytime hours</strong>. (<a href=\"http://www.forbes.com/2009/01/21/car-accident-times-forbeslife-cx_he_0121driving.html\">Source</a></span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">)</span></p>\n<ul>\n<li><span style=\"font-family: Arial;\">Solution: make an explicit effort to </span><strong style=\"font-family: Arial;\">avoid driving at night<em>. </em></strong><span style=\"font-family: Arial;\">Use Anki to remember this association.</span></li>\n<li><span style=\"font-family: Arial;\">Solution: Look at your schedule and see if you can change a recurring night-time drive to the daytime.</span></li>\n</ul>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Berkeley research on 1.4 million fatal crashes <a href=\"http://www.forbes.com/2009/01/21/car-accident-times-forbeslife-cx_he_0121driving.html\">found</a> that &ldquo;</span><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">fatal crashes were 14% more likely to happen on the first snowy day of the season compared with subsequent ones.&rdquo; The suggested hypothesis is that people take at least a day to recalibrate their driving behavior in light of new snow.&nbsp;</span></p>\n<ul>\n<li><span style=\"font-family: Arial;\">Solution: make an explicit effort to </span><strong style=\"font-family: Arial;\">avoid driving on the first snowy day </strong><span style=\"font-family: Arial;\">after a sequence of non-snowy ones. Use Anki to remember this association.</span></li>\n</ul>\n<p><span style=\"font-family: Arial; \">Another valuable factoid:</span><span style=\"font-family: Arial; \">&nbsp;</span><strong><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">77%</span></strong><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\"> of weather-related fatalities (and 75% of all crashes!) involve <strong>wet pavement</strong>.</span></p>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Statistics are available</span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">&nbsp;for other weather-related issues, but <a href=\"http://www.ops.fhwa.dot.gov/weather/q1_roadimpact.htm\">the data I found</a> wasn&rsquo;t adjusted for the relative frequencies of various weather conditions. That&rsquo;s problematic; it might be that fog, for example, is horrendously dangerous compared to ice or slush, but it&rsquo;s rarer and thus kills fewer people. I&rsquo;m interested in looking at appropriately adjusted statistics.</span><span style=\"font-family: Arial;\">&nbsp;</span></p>\n<h2><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Other considerations</span></h2>\n<ul>\n<li><span style=\"font-family: Arial; text-indent: -0.25in;\">Teen drivers are apparently way worse at not dying in cars than older people. So if you&rsquo;re a teenager, take the outside view and accept that you (not just &lsquo;other dumb teenagers&rsquo;) may need to take </span><em style=\"font-family: Arial; text-indent: -0.25in;\">particular</em><span style=\"font-family: Arial; text-indent: -0.25in;\"> care when driving. Relevant information about teen driving is available </span><a style=\"font-family: Arial; text-indent: -0.25in;\" href=\"http://www.cdc.gov/Motorvehiclesafety/teen_drivers/teendrivers_factsheet.html\">here</a><span style=\"font-family: Arial; text-indent: -0.25in;\">.<br /><br /></span></li>\n<li><span style=\"font-family: Arial; text-indent: -0.25in;\">Alcohol use appeared so often during my research that I didn&rsquo;t even bother including stats about it. Likewise for wearing a seatbelt.<br /><br /></span></li>\n<li><span style=\"font-family: Arial; text-indent: -0.25in;\">Since I&rsquo;m not in the market for a car, I didn&rsquo;t look into </span><em style=\"font-family: Arial; text-indent: -0.25in;\">vehicle choice</em><span style=\"font-family: Arial; text-indent: -0.25in;\"> as a way to decrease personal existential risk. But I do expect this to be relevant to increasing driving safety.<br /><br /></span></li>\n<li><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">&ldquo;The </span><span style=\"font-family: Arial;\"><span style=\"mso-bidi-font-size: 14.0pt; mso-bidi-font-family: Arial; color: windowtext; text-decoration: none; text-underline: none;\">most dangerous month,</span></span><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\"> it <a href=\"http://www.forbes.com/2009/01/21/car-accident-times-forbeslife-cx_he_0121driving_slide_3.html?thisspeed=25000\">turns out</a>, is August, and Saturday the </span><span style=\"font-family: Arial;\"><span style=\"mso-bidi-font-size: 14.0pt; mso-bidi-font-family: Arial; color: windowtext; text-decoration: none; text-underline: none;\">most dangerous day,</span> </span><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">according to the National Highway Traffic Safety Administration.&rdquo; </span><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">I couldn&rsquo;t tell whether this was because of <em>increased amount of driving</em> or an <em>increased rate of crashes</em>.<br /><br /></span></li>\n<li><span style=\"font-family: Arial;\"><a href=\"http://www.edmunds.com/car-reviews/top-10/top-10-editors-tips-to-prevent-a-car-accident.html\">This site</a></span><span style=\"font-family: Arial;\">&nbsp;recommends </span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">driving with your hands at 9 and 3 for increased control. The same site claims that &ldquo;</span><span style=\"font-family: Arial; mso-bidi-font-family: Verdana;\">Most highway accidents occur in the left lane&rdquo; because the other lanes have &ldquo;more &lsquo;escape routes&rsquo; should a problem suddenly arise that requires you to quickly change lanes&rdquo;, but I found no citation for the claim.<br /><br /></span></li>\n<li><span style=\"font-family: Arial; mso-bidi-font-family: Verdana;\"><span style=\"text-indent: -0.25in;\">Bad driver behavior appears to significantly increase the risk of death in an accident, so: don't ride in car with people who drive badly or aggressively. I have a few friends with aggressive driving habits, and I&rsquo;m planning to either a) tell them to drive more slowly when I&rsquo;m in the car or b) stop riding in their cars.</span></span></li>\n</ul>\n<h3>Commenters' recommendations</h3>\n<p><span style=\"font-family: Arial;\">I <a href=\"/lw/awm/how_to_avoid_dying_in_a_car_crash/6309\">should note</a> here that I have not personally verified anything posted below. Be sure to look at the original comment and do followup research before depending on these recommendations.</span></p>\n<ul>\n<li><span style=\"font-family: Arial; mso-bidi-font-family: Verdana;\"><span style=\"text-indent: -0.25in;\">MartinB <a href=\"/lw/awm/how_to_avoid_dying_in_a_car_crash/61tm\">recommends</a> taking a driving safety class every few years.<br /><br /></span></span></li>\n<li><span style=\"font-family: Arial; mso-bidi-font-family: Verdana;\"><span style=\"text-indent: -0.25in;\">Dmytry <a href=\"/lw/awm/how_to_avoid_dying_in_a_car_crash/61u8\">suggests</a>&nbsp;that bicycling may be good training for constantly keeping one's eyes on the road, though others argue that bicycling itself may be significantly more dangerous than driving anyway.<br /><br /></span></span></li>\n<li><span style=\"font-family: Arial; mso-bidi-font-family: Verdana;\"><span style=\"text-indent: -0.25in;\">Various <a href=\"/lw/awm/how_to_avoid_dying_in_a_car_crash/620e\">commenters</a> suggested simply avoiding driving whenever possible. Living in a city with good public transportation is recommended.<br /><br /></span></span></li>\n<li><span style=\"font-family: Arial; mso-bidi-font-family: Verdana;\"><span style=\"text-indent: -0.25in;\">David_Gerard <a href=\"/lw/awm/how_to_avoid_dying_in_a_car_crash/61wd\">recommends</a> driving a bigger car with larger crumple zones (but not an SUV because they roll over). He also recommends avoiding motorcycles altogether and taking advanced driving courses.<br /><br /></span></span></li>\n<li><span style=\"font-family: Arial; mso-bidi-font-family: Verdana;\"><span style=\"text-indent: -0.25in;\">Craig_Heldreth&nbsp;<a href=\"/lw/awm/how_to_avoid_dying_in_a_car_crash/626w\">adds</a>&nbsp;that&nbsp;<em>everyone</em>&nbsp;in the car should be buckled up, as even<em> a single unbuckled passenger </em>can<em> collide with and kill other passengers</em> in a crash. Even cargo as light as a laptop should be secured or put in the trunk.<br /><br /></span></span></li>\n<li><span style=\"font-family: Arial; mso-bidi-font-family: Verdana;\"><span style=\"text-indent: -0.25in;\">JRMayne offers a list of recommendations that merit reading <a href=\"/lw/awm/how_to_avoid_dying_in_a_car_crash/629u\">directly</a>. DuncanS also <a href=\"/lw/awm/how_to_avoid_dying_in_a_car_crash/62a7\">offers</a> a valuable list.</span></span></li>\n</ul>\n<p class=\"MsoNormal\"><sup style=\"font-family: Arial; \">1</sup><span style=\"font-family: Arial; \">All&nbsp;</span><span style=\"font-family: Arial; \">bolding</span><span style=\"font-family: Arial; \">&nbsp;in the data was added for emphasis by me.</span></p>\n<!--EndFragment-->\n<p><sup>2</sup><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">The report notes that \"</span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">57% of crashes were due solely to driver factors, 27% to combined roadway and driver factors, 6% to combined vehicle and driver factors, 3% solely to roadway factors, 3% to combined roadway, driver, and vehicle factors, 2% solely to vehicle factors and 1% to combined roadway and vehicle factors.&rdquo;</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 1, "XqykXFKL9t38pbSEm": 1, "jzd84f2H95DeyHvxE": 1, "vmvTYnmaKA73fYDe5": 1, "yAmE3StuxBmzCBPWq": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7XbcDaeigMaxW43EB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 94, "baseScore": 117, "extendedScore": null, "score": 0.000241, "legacy": true, "legacyId": "14134", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 117, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Aside from <a href=\"http://wiki.lesswrong.com/wiki/Cryonics\">cryonics</a>&nbsp;and <a href=\"/lw/a60/quantified_health_prize_results_announced/\">eating better</a>, what else can we do to live long lives?</p>\n<p>Using <a href=\"http://www.worldlifeexpectancy.com/usa-cause-of-death-by-age-and-gender\">this tool</a>, I looked up the risks of death for my demographic group. As a 15-24 year old male in the United States, the most likely cause of my death is a traffic accident; and so I\u2019m taking steps to avoid that. Below I have included the results of my research as well as the actions I will take to implement my findings. Perhaps my research can help you as well.<sup>1</sup></p>\n<p>Before diving into the results, I will note that this data took me <em style=\"font-family: Arial; \">one hour</em> to collect. It\u2019s definitely not comprehensive, and I know that working together, we can do much better. So if you have other resources or data-backed recommendations on how to avoid dying in a traffic accident<strong>, leave a comment below and I\u2019ll update this post.</strong></p>\n<h2 id=\"General_points\"><span style=\"font-family: Arial; text-align: center; \">General points</span></h2>\n<p><strong>Changing your behavior <em>can</em> reduce your risk of death in a car crash.</strong><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\"> A 1985 <a href=\"http://www.fhwa.dot.gov/publications/publicroads/95winter/p95wi14.cfm\">report</a> on British and American crash data discovered that </span><span style=\"mso-bidi-font-size: 10.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">\u201c</span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">driver error, intoxication and other human factors contribute wholly or partly to about 93% of crashes.\u201d </span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">Other drivers\u2019 behavior matters too, of course, but you might as well optimize your own.<sup>2</sup></span></p>\n<p><span style=\"mso-bidi-font-size: 10.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">Secondly, <strong>o</strong></span><strong><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">verconfidence appears to be a large factor in peoples\u2019 thinking about traffic safety. </span></strong><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">A speaker for the National Highway Traffic Safety Association (NHTSA) <a href=\"http://www.forbes.com/2009/01/21/car-accident-times-forbeslife-cx_he_0121driving.html\">stated</a> that \u201c</span><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">Ninety-five percent of crashes are caused by human error\u2026 but 75% of drivers say they're more careful than most other drivers. </span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Less extreme evidence for overconfidence about driving is presented <a href=\"http://www.drivers.com/article/157\">here</a>.</span></p>\n<p>One possible cause for this was <a href=\"http://en.wikipedia.org/wiki/Traffic_collision#cite_note-young-7\">suggested</a> by the Transport Research Laboratory, which explains that<strong> </strong>\u201c...the feeling of being confident in more and more challenging situations is experienced as evidence of driving ability, and that 'proven' ability reinforces the feelings of confidence. Confidence feeds itself and grows unchecked until something happens \u2013 a near-miss or an accident.\u201d</p>\n<p>So if you\u2019re tempted to use this post as an opportunity to feel superior to <em>other</em> drivers, remember: you\u2019re probably overconfident too! Don\u2019t just <a href=\"/lw/gq/the_proper_use_of_humility/\">humbly confess</a> your imperfections \u2013 change your behavior.</p>\n<h2 id=\"Top_causes_of_accidents\"><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">Top causes of accidents</span></h2>\n<h3 id=\"Distraction\"><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Distraction</span></h3>\n<p><strong><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Driver distraction</span></strong><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\"> is one of the largest causes of traffic accident deaths. The Director of Traffic Safety at the American Automobile Association <a href=\"http://www.smartmotorist.com/traffic-and-safety-guideline/cell-phones-and-driving-a-prescription-for-disaster.html\">stated</a> that \"The research tells us that somewhere between 25-50 percent of all motor vehicle crashes in this country really have driver distraction as their root cause.\" The NHTSA <a href=\"http://www-nrd.nhtsa.dot.gov/Pubs/811216.pdf\">reports</a> the number as 16%.</span></p>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">If we are to reduce distractions while driving, we ought to identify which distractors are the worst. One is <a href=\"http://en.wikipedia.org/wiki/Texting_while_driving#Virginia_Tech_Transportation_Institute_Study\">cell phone use</a>. M</span><span style=\"font-family: Arial; \">y solution: Don\u2019t make calls in the car, and turn off your phone\u2019s sound so that you aren\u2019t tempted.</span></p>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">I brainstormed other major distractors and thought of ways to reduce their distracting effects.</span></p>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Distractor: Looking at directions on my phone as I drive</span></p>\n<ul>\n<li><span style=\"font-family: Arial;\">Solution: Download a great turn-by-turn navigation app (recommendations are welcome).</span></li>\n<li><span style=\"font-family: Arial;\">Solution: Buy a GPS.</span></li>\n</ul>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Distractor: Texting, Facebook, slowing down to gawk at an accident, looking at scenery</span></p>\n<ul>\n<li><span style=\"font-family: Arial;\">Solution [For </span><a style=\"font-family: Arial;\" href=\"/lw/531/how_you_make_judgments_the_elephant_and_its_rider/\">System 2</a><span style=\"font-family: Arial;\">]: Consciously accept that texting (Facebook, gawking, scenery) causes accidents.</span></li>\n<li><span style=\"font-family: Arial;\">Solution [For System 1]: Once a week, vividly and emotionally imagine texting (using Facebook, gawking at an accident) and then crashing &amp; dying.</span></li>\n<li><span style=\"font-family: Arial;\">Solution: Turn off your phone\u2019s sound while driving, so you won\u2019t answer texts.</span></li>\n</ul>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Distractor: Fatigue</span></p>\n<ul>\n<li><span style=\"font-family: Arial;\">Solution [For System 2]: Ask yourself if you\u2019re tired before you plan to get in the car. Use </span><a style=\"font-family: Arial;\" href=\"http://ankisrs.net/\">Anki</a><span style=\"font-family: Arial;\"> or a weekly review list to remember the association.</span></li>\n<li><span style=\"font-family: Arial;\">Solution [For System 1]: Once a week, vividly and emotionally imagine dozing off while driving and then dying.</span></li>\n</ul>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Distractor: Other passengers</span></p>\n<ul>\n<li><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Solution: Develop an identity as someone who drives safely and thinks it\u2019s low status to be distracting in the car</span><span style=\"mso-bidi-font-size: 10.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">. Achieve this by meditating on the commitment, writing a journal entry about it, using Anki, or saying it every day when you wake up in the morning.</span></li>\n<li><span style=\"font-family: Arial;\">Solution [In the moment]: Tell people to chill out while you\u2019re driving. Mentally simulate doing this ahead of time, so you don\u2019t hesitate to do it when it matters.</span></li>\n</ul>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Distractor: Adjusting the radio</span></p>\n<ul>\n<li><span style=\"font-family: Arial;\">Solution: If avoiding using the car radio is unrealistic, minimize your interaction with it by only using the hotkey buttons rather than manually searching through channels.</span></li>\n<li><span style=\"font-family: Arial;\">Solution: If you\u2019re constantly tempted to change the channel (like I am), buy an iPod cable so you can listen to your own music and set playlists that you like, so you won't constantly want to change the song.</span></li>\n</ul>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">A last interesting fact about distraction, from <a href=\"http://en.wikipedia.org/wiki/Traffic_collision\">Wikipedia</a>:</span></p>\n<blockquote>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">Recent research conducted by British scientists suggests that music can also have an effect [on driving]; </span><span style=\"font-family: Arial;\"><span style=\"mso-bidi-font-size: 13.0pt; mso-bidi-font-family: Helvetica; color: windowtext; text-decoration: none; text-underline: none;\">classical music</span></span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\"> is considered to be calming, yet too much could relax the driver to a condition of distraction. On the other hand, </span><span style=\"font-family: Arial;\"><span style=\"mso-bidi-font-size: 13.0pt; mso-bidi-font-family: Helvetica; color: windowtext; text-decoration: none; text-underline: none;\">hard rock</span></span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\"> may encourage the driver to step on the acceleration pedal, thus creating a potentially dangerous situation on the road.</span></p>\n</blockquote>\n<h3 id=\"Speeding\"><strong style=\"text-indent: -0.25in; \"><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Speeding</span></strong></h3>\n<p><strong style=\"text-indent: -0.25in; \"></strong><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">The Road and Traffic Authority of </span><span style=\"font-family: Arial;\"><span style=\"mso-bidi-font-size: 13.0pt; mso-bidi-font-family: Helvetica; color: windowtext; text-decoration: none; text-underline: none;\">New South Wales</span></span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\"> <a href=\"http://www.rta.nsw.gov.au/roadsafety/speedandspeedcameras/index.html\">claims</a> that \u201cspeeding\u2026 is a factor in about 40 percent of road deaths.\u201d</span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">&nbsp;<a href=\"http://www.forbes.com/2009/01/21/car-accident-times-forbeslife-cx_he_0121driving.html\">Data</a> from the&nbsp;</span><span style=\"mso-bidi-font-size: 10.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">NHTSA puts the number at 30%</span><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">.</span></p>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">Speeding also increases the <em><a href=\"http://www.rta.nsw.gov.au/roadsafety/speedandspeedcameras/speedingresearch.html\">severity</a></em> of crashes; \u201ci</span><span style=\"font-family: Arial;\">n a 60&nbsp;km/h speed limit area, the risk of involvement in a casualty crash doubles with each 5&nbsp;km/h increase in travelling speed above 60&nbsp;km/h.</span><span style=\"mso-bidi-font-size: 10.0pt; font-family: Arial;\">\u201d</span></p>\n<p><span style=\"mso-bidi-font-size: 10.0pt; font-family: Arial;\">Stop. Think about that for a second.&nbsp;</span><span style=\"mso-bidi-font-size: 10.0pt; font-family: Arial;\">I\u2019ll convert it to the Imperial system for my fellow Americans: </span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">\u201c<em>i</em></span><em><span style=\"font-family: Arial;\">n a [37.3 mph] speed limit area, the risk of involvement in a casualty crash <strong>doubles</strong> with each [3.1 mph] increase in travelling speed above [37.3 mph].\u201d</span></em><span style=\"font-family: Arial;\"> Remember that next time you drive a 'mere' 5 mph over the limit.</span></p>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">Equally shocking is this paragraph from the Freakonomics <a href=\"http://www.freakonomics.com/2010/04/02/life-and-death-in-the-fast-lane/\">blog</a>:</span></p>\n<blockquote>\n<p><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Georgia;\">Kockelman et al. <a href=\"http://www.ce.utexas.edu/prof/kockelman/public_html/NCHRPSpeedLimits17-23.pdf\">estimated</a> that the difference between a crash on a 55 mph limit road and a crash on a 65 mph one means a <strong>24 percent increase in the chances the accident will be fatal</strong>. Along with the higher incidence of crashes happening in the first place, a difference in limit between 55 and 65 adds up to a 28 percent increase in the overall fatality count.</span></p>\n</blockquote>\n<p><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">Driving too slowly can be dangerous too. An NHTSA <a href=\"http://www.nhtsa.gov/people/injury/enforce/speed_forum_presentations/ferguson.pdf\">presentation</a> </span><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">cites two studies that </span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">found a U-shaped relationship between vehicle speed and crash incidence; thus</span><span style=\"font-family: Arial; \">&nbsp;\u201cCrash rates were lowest for drivers traveling near the mean speed, and increased with deviations above and below the mean.\u201d</span></p>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">However, driving fast is still far more dangerous than driving slowly. This relationship appears to be exponential, as you can see on the tenth slide of the&nbsp;</span><a style=\"font-family: Arial; \" href=\"http://www.nhtsa.gov/people/injury/enforce/speed_forum_presentations/ferguson.pdf\">presentation</a><span style=\"font-family: Arial; \">.</span></p>\n<ul>\n<li><span style=\"mso-bidi-font-size: 10.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">Solution: Watch <a href=\"https://www.youtube.com/watch?v=i6PVRR1CMlQ\">this 30 second video</a> </span><span style=\"mso-bidi-font-size: 10.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">for a vivid comparison of head-on crashes at 60 km/hr (37 mph) and 100 km/hr (60 mph). Imagine yourself in the car. Imagine your tearful friends and family.&nbsp;</span></li>\n<li><span style=\"font-family: Arial;\">Solution: Develop an identity as someone who drives close to the speed limit, by meditating on the commitment, writing a journal entry about it, using Anki, or saying it every day when you wake up in the morning.</span></li>\n</ul>\n<h3 id=\"Driving_conditions\"><strong><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Driving conditions</span></strong></h3>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Driving conditions are another source of driving risk.</span></p>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">One factor I discovered was the additional risk from <strong>driving at night.&nbsp;</strong></span><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">Nationwide, 49% of fatal crashes happen at night, with <strong>a fatality rate </strong>per mile of travel<strong> about three times as high as daytime hours</strong>. (<a href=\"http://www.forbes.com/2009/01/21/car-accident-times-forbeslife-cx_he_0121driving.html\">Source</a></span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">)</span></p>\n<ul>\n<li><span style=\"font-family: Arial;\">Solution: make an explicit effort to </span><strong style=\"font-family: Arial;\">avoid driving at night<em>. </em></strong><span style=\"font-family: Arial;\">Use Anki to remember this association.</span></li>\n<li><span style=\"font-family: Arial;\">Solution: Look at your schedule and see if you can change a recurring night-time drive to the daytime.</span></li>\n</ul>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Berkeley research on 1.4 million fatal crashes <a href=\"http://www.forbes.com/2009/01/21/car-accident-times-forbeslife-cx_he_0121driving.html\">found</a> that \u201c</span><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">fatal crashes were 14% more likely to happen on the first snowy day of the season compared with subsequent ones.\u201d The suggested hypothesis is that people take at least a day to recalibrate their driving behavior in light of new snow.&nbsp;</span></p>\n<ul>\n<li><span style=\"font-family: Arial;\">Solution: make an explicit effort to </span><strong style=\"font-family: Arial;\">avoid driving on the first snowy day </strong><span style=\"font-family: Arial;\">after a sequence of non-snowy ones. Use Anki to remember this association.</span></li>\n</ul>\n<p><span style=\"font-family: Arial; \">Another valuable factoid:</span><span style=\"font-family: Arial; \">&nbsp;</span><strong><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">77%</span></strong><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\"> of weather-related fatalities (and 75% of all crashes!) involve <strong>wet pavement</strong>.</span></p>\n<p><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Statistics are available</span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">&nbsp;for other weather-related issues, but <a href=\"http://www.ops.fhwa.dot.gov/weather/q1_roadimpact.htm\">the data I found</a> wasn\u2019t adjusted for the relative frequencies of various weather conditions. That\u2019s problematic; it might be that fog, for example, is horrendously dangerous compared to ice or slush, but it\u2019s rarer and thus kills fewer people. I\u2019m interested in looking at appropriately adjusted statistics.</span><span style=\"font-family: Arial;\">&nbsp;</span></p>\n<h2 id=\"Other_considerations\"><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">Other considerations</span></h2>\n<ul>\n<li><span style=\"font-family: Arial; text-indent: -0.25in;\">Teen drivers are apparently way worse at not dying in cars than older people. So if you\u2019re a teenager, take the outside view and accept that you (not just \u2018other dumb teenagers\u2019) may need to take </span><em style=\"font-family: Arial; text-indent: -0.25in;\">particular</em><span style=\"font-family: Arial; text-indent: -0.25in;\"> care when driving. Relevant information about teen driving is available </span><a style=\"font-family: Arial; text-indent: -0.25in;\" href=\"http://www.cdc.gov/Motorvehiclesafety/teen_drivers/teendrivers_factsheet.html\">here</a><span style=\"font-family: Arial; text-indent: -0.25in;\">.<br><br></span></li>\n<li><span style=\"font-family: Arial; text-indent: -0.25in;\">Alcohol use appeared so often during my research that I didn\u2019t even bother including stats about it. Likewise for wearing a seatbelt.<br><br></span></li>\n<li><span style=\"font-family: Arial; text-indent: -0.25in;\">Since I\u2019m not in the market for a car, I didn\u2019t look into </span><em style=\"font-family: Arial; text-indent: -0.25in;\">vehicle choice</em><span style=\"font-family: Arial; text-indent: -0.25in;\"> as a way to decrease personal existential risk. But I do expect this to be relevant to increasing driving safety.<br><br></span></li>\n<li><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">\u201cThe </span><span style=\"font-family: Arial;\"><span style=\"mso-bidi-font-size: 14.0pt; mso-bidi-font-family: Arial; color: windowtext; text-decoration: none; text-underline: none;\">most dangerous month,</span></span><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\"> it <a href=\"http://www.forbes.com/2009/01/21/car-accident-times-forbeslife-cx_he_0121driving_slide_3.html?thisspeed=25000\">turns out</a>, is August, and Saturday the </span><span style=\"font-family: Arial;\"><span style=\"mso-bidi-font-size: 14.0pt; mso-bidi-font-family: Arial; color: windowtext; text-decoration: none; text-underline: none;\">most dangerous day,</span> </span><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">according to the National Highway Traffic Safety Administration.\u201d </span><span style=\"mso-bidi-font-size: 14.0pt; font-family: Arial; mso-bidi-font-family: Arial;\">I couldn\u2019t tell whether this was because of <em>increased amount of driving</em> or an <em>increased rate of crashes</em>.<br><br></span></li>\n<li><span style=\"font-family: Arial;\"><a href=\"http://www.edmunds.com/car-reviews/top-10/top-10-editors-tips-to-prevent-a-car-accident.html\">This site</a></span><span style=\"font-family: Arial;\">&nbsp;recommends </span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">driving with your hands at 9 and 3 for increased control. The same site claims that \u201c</span><span style=\"font-family: Arial; mso-bidi-font-family: Verdana;\">Most highway accidents occur in the left lane\u201d because the other lanes have \u201cmore \u2018escape routes\u2019 should a problem suddenly arise that requires you to quickly change lanes\u201d, but I found no citation for the claim.<br><br></span></li>\n<li><span style=\"font-family: Arial; mso-bidi-font-family: Verdana;\"><span style=\"text-indent: -0.25in;\">Bad driver behavior appears to significantly increase the risk of death in an accident, so: don't ride in car with people who drive badly or aggressively. I have a few friends with aggressive driving habits, and I\u2019m planning to either a) tell them to drive more slowly when I\u2019m in the car or b) stop riding in their cars.</span></span></li>\n</ul>\n<h3 id=\"Commenters__recommendations\">Commenters' recommendations</h3>\n<p><span style=\"font-family: Arial;\">I <a href=\"/lw/awm/how_to_avoid_dying_in_a_car_crash/6309\">should note</a> here that I have not personally verified anything posted below. Be sure to look at the original comment and do followup research before depending on these recommendations.</span></p>\n<ul>\n<li><span style=\"font-family: Arial; mso-bidi-font-family: Verdana;\"><span style=\"text-indent: -0.25in;\">MartinB <a href=\"/lw/awm/how_to_avoid_dying_in_a_car_crash/61tm\">recommends</a> taking a driving safety class every few years.<br><br></span></span></li>\n<li><span style=\"font-family: Arial; mso-bidi-font-family: Verdana;\"><span style=\"text-indent: -0.25in;\">Dmytry <a href=\"/lw/awm/how_to_avoid_dying_in_a_car_crash/61u8\">suggests</a>&nbsp;that bicycling may be good training for constantly keeping one's eyes on the road, though others argue that bicycling itself may be significantly more dangerous than driving anyway.<br><br></span></span></li>\n<li><span style=\"font-family: Arial; mso-bidi-font-family: Verdana;\"><span style=\"text-indent: -0.25in;\">Various <a href=\"/lw/awm/how_to_avoid_dying_in_a_car_crash/620e\">commenters</a> suggested simply avoiding driving whenever possible. Living in a city with good public transportation is recommended.<br><br></span></span></li>\n<li><span style=\"font-family: Arial; mso-bidi-font-family: Verdana;\"><span style=\"text-indent: -0.25in;\">David_Gerard <a href=\"/lw/awm/how_to_avoid_dying_in_a_car_crash/61wd\">recommends</a> driving a bigger car with larger crumple zones (but not an SUV because they roll over). He also recommends avoiding motorcycles altogether and taking advanced driving courses.<br><br></span></span></li>\n<li><span style=\"font-family: Arial; mso-bidi-font-family: Verdana;\"><span style=\"text-indent: -0.25in;\">Craig_Heldreth&nbsp;<a href=\"/lw/awm/how_to_avoid_dying_in_a_car_crash/626w\">adds</a>&nbsp;that&nbsp;<em>everyone</em>&nbsp;in the car should be buckled up, as even<em> a single unbuckled passenger </em>can<em> collide with and kill other passengers</em> in a crash. Even cargo as light as a laptop should be secured or put in the trunk.<br><br></span></span></li>\n<li><span style=\"font-family: Arial; mso-bidi-font-family: Verdana;\"><span style=\"text-indent: -0.25in;\">JRMayne offers a list of recommendations that merit reading <a href=\"/lw/awm/how_to_avoid_dying_in_a_car_crash/629u\">directly</a>. DuncanS also <a href=\"/lw/awm/how_to_avoid_dying_in_a_car_crash/62a7\">offers</a> a valuable list.</span></span></li>\n</ul>\n<p class=\"MsoNormal\"><sup style=\"font-family: Arial; \">1</sup><span style=\"font-family: Arial; \">All&nbsp;</span><span style=\"font-family: Arial; \">bolding</span><span style=\"font-family: Arial; \">&nbsp;in the data was added for emphasis by me.</span></p>\n<!--EndFragment-->\n<p><sup>2</sup><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Verdana;\">The report notes that \"</span><span style=\"mso-bidi-font-size: 13.0pt; font-family: Arial; mso-bidi-font-family: Helvetica;\">57% of crashes were due solely to driver factors, 27% to combined roadway and driver factors, 6% to combined vehicle and driver factors, 3% solely to roadway factors, 3% to combined roadway, driver, and vehicle factors, 2% solely to vehicle factors and 1% to combined roadway and vehicle factors.\u201d</span></p>", "sections": [{"title": "General points", "anchor": "General_points", "level": 1}, {"title": "Top causes of accidents", "anchor": "Top_causes_of_accidents", "level": 1}, {"title": "Distraction", "anchor": "Distraction", "level": 2}, {"title": "Speeding", "anchor": "Speeding", "level": 2}, {"title": "Driving conditions", "anchor": "Driving_conditions", "level": 2}, {"title": "Other considerations", "anchor": "Other_considerations", "level": 1}, {"title": "Commenters' recommendations", "anchor": "Commenters__recommendations", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "294 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 294, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yMKfih99nSqRyphkD", "GrDqnMjhqoxiqpQPw", "du395YvCnQXBPSJax"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-18T00:07:01.670Z", "modifiedAt": null, "url": null, "title": "I'm starting a game company and looking for a co-founder.", "slug": "i-m-starting-a-game-company-and-looking-for-a-co-founder", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:04.172Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alexei", "createdAt": "2010-08-02T15:14:11.411Z", "isAdmin": false, "displayName": "Alexei"}, "userId": "CD3DC5D7GHtgBmxz5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LJ9FxJYgzyMrX9ZqN/i-m-starting-a-game-company-and-looking-for-a-co-founder", "pageUrlRelative": "/posts/LJ9FxJYgzyMrX9ZqN/i-m-starting-a-game-company-and-looking-for-a-co-founder", "linkUrl": "https://www.lesswrong.com/posts/LJ9FxJYgzyMrX9ZqN/i-m-starting-a-game-company-and-looking-for-a-co-founder", "postedAtFormatted": "Sunday, March 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I'm%20starting%20a%20game%20company%20and%20looking%20for%20a%20co-founder.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI'm%20starting%20a%20game%20company%20and%20looking%20for%20a%20co-founder.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLJ9FxJYgzyMrX9ZqN%2Fi-m-starting-a-game-company-and-looking-for-a-co-founder%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I'm%20starting%20a%20game%20company%20and%20looking%20for%20a%20co-founder.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLJ9FxJYgzyMrX9ZqN%2Fi-m-starting-a-game-company-and-looking-for-a-co-founder", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLJ9FxJYgzyMrX9ZqN%2Fi-m-starting-a-game-company-and-looking-for-a-co-founder", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 808, "htmlBody": "<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">Summary: I am looking for co-founder(s) to start a game company with me. If you, or anyone you know, is interested, please contact me. (Alternatively, if you want to invest or provide funding, that would be very nice in its own right.)</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\"><br /></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">It&nbsp; <a href=\"/r/discussion/lw/a25/what_happens_when_your_beliefs_fully_propagate/\">recently&nbsp;occurred&nbsp;to me</a>&nbsp; that if reducing existential risk is indeed the most important goal, then I ought to actually do something about it. Turns out, for most mortals (including myself) the best option for reducing ex-risk is through donations. With that in mind, I'm going to start a game company. \"Why a game company?\" you might ask. Well:</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* I've been making games since I was 13.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* I've hit my 10,000 hours of game programming a while ago. If I want to make a game, it will be made.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* I've studied a good amount of game design theory and have had some opportunities to put that knowledge to the test with success.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* I've worked for two different game companies. I've written games for computers, handhelds, and mobile devices.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* I've funded, designed, programmed, and published my own game.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* I am very familiar with the process of developing games. Everything from team structure to tools to game design.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\"><br /></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">I hope it's clear why starting a game company makes sense for me. Now, you might ask, \"Why will you succeed?\" Well, I have an answer for that too:</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* Leverage all the rationality skills I've learned from LW.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* Leverage all other scientific knowledge: from psychology to statistics.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* I'm not attached to any particular game genre or game idea. Whatever gives the most ROI is good.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* There is a lot of low hanging fruit in terms of what games are easy to make and are almost guaranteed to be profitable. (Most people don't choose these ideas because they are easy, have been done before, or the people want to make other games.)</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* Focus on making games as cheaply as possible. (Leverage 3rd party tools and other companies.)</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* Have a structured approach to designing and developing a game, rather than an adhoc one like most companies have.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* The company will be built around scientific principle. (A/B testing is just one aspect of that.)</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* The company will measure and analyze everything (or as much as possible), not only in the games it makes, but also in the company itself.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* The company will be completely transparent on the inside. If everyone knows everything about the company, it's more likely that they will find new creative ways to improve it.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">*&nbsp;<a style=\"color: #1155cc;\" href=\"http://www.admonymous.com/\" target=\"_blank\">Anonymous</a>&nbsp;feedback for every employee (especially founders).&nbsp;<br /></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* The company will hire the best people. Since it's well knows that the best people are at least 2x more productive (that number is much higher for some positions, e.g. programmer) than average, there is no reason not to let the salary (and benefits) reflect that. I think paying 10%-20% higher than competitive rates is&nbsp;justifiable&nbsp;and will help bring in the best talent.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* Every employee's goal should be to automate their position: either by replacing themselves with a program or another employee of same/lower skill. (This way each employee can focus on higher level problems.)</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">Some of these ideas are untested, but the point is to create a company that is open to experimenting and testing things out.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\"><br /></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">My personal goals:</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* Create a highly profitable game company.<br /></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* Become more experienced in starting and running a company.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* Gather more data on how to start and manage a company.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* Create connections within the game development world.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* Sell this company as quickly as possible. (Since this will be my first startup, I anticipate a lot of mistakes. There is no reason not to start anew as soon as a good opportunity presents itself. Selling the company, or most of company's IP, would be ideal.) (Going public is possible, but very unlikely.)</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* I'm thinking ideal route would be: Incubator -&gt; Angel funding -&gt; VC funding. Focus purely on development, while contracting out the publishing.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\"><br /></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">If you are interested in joining me on this mission, please contact me. If you know someone who might be interested in this, let them know. All points I listed above are open to negotiation.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">I'm looking for:</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* Someone who can contribute a lot to the company from the first day.&nbsp;(This will most likely be a business person or a game programmer.)</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* Willing to commit to this and to work overtime.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* Able to start this year. (If we get enough runway funding, we&nbsp;should&nbsp;start in a few weeks.)</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* (Bonus) Experience in game development field.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* (Bonus) Located in The Bay Area. (Though I'm potentially open to relocating.)</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* (Bonus) Experience with starting and running a company.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* (Bonus) Versed in the art of rationality.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* (Bonus) Also wants to donate to SIAI or other ex-risk reduction organisation.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">* (Bonus) Can contribute seed capital.</div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\"><br /></div>\n<div style=\"color: #222222; font-family: arial,sans-serif; font-size: 13px; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; background-color: rgba(255, 255, 255, 0.918);\">If you know anyone who is interested in funding the company, please have them contact me as well.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LJ9FxJYgzyMrX9ZqN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 21, "extendedScore": null, "score": 8.675011506656812e-07, "legacy": true, "legacyId": "14170", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 80, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["coEDeEaSEAkhic4s2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-18T02:56:46.571Z", "modifiedAt": null, "url": null, "title": "LiveJournal Memes", "slug": "livejournal-memes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:27.852Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fnLDrRyFndf9syJnP/livejournal-memes", "pageUrlRelative": "/posts/fnLDrRyFndf9syJnP/livejournal-memes", "linkUrl": "https://www.lesswrong.com/posts/fnLDrRyFndf9syJnP/livejournal-memes", "postedAtFormatted": "Sunday, March 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LiveJournal%20Memes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALiveJournal%20Memes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfnLDrRyFndf9syJnP%2Flivejournal-memes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LiveJournal%20Memes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfnLDrRyFndf9syJnP%2Flivejournal-memes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfnLDrRyFndf9syJnP%2Flivejournal-memes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 287, "htmlBody": "<p>On blogging websites just as LiveJournal, memes are often in the form of a question or set of questions which a blogger answers in their own blog, then challenges their readers to answer in the readers' blogs (thus spreading).&nbsp; It doesn't have to be the sort of question to which there is a 'correct' answer.&nbsp; More usually the meme spreads if the questions are interesting and the answers reveal something about the personality or interests of the blogger.</p>\n<p>Here's a recent example:</p>\n<blockquote>\n<p style=\"padding-left: 30px;\"><span style=\"color: #0000ff;\">A ticket for a flight into space (an orbit and a visit to the  International Space Station) arrives through the post. Assume that you  meet the relevant physical requirements. You can use it, transfer it, or  sell it. What do you do and why?</span></p>\n</blockquote>\n<p>&nbsp;</p>\n<p>It occurs to me that this sort of thing might be a very accessible way to introduce people to thinking about rationality.&nbsp;&nbsp; With the example I gave, you get people who would use the ticket because they know that if they didn't they'd always regret not having gone.&nbsp; However, if the question had been phrased as them receiving money, and a space flight ticket was only one of the things listed as purchasable for that sum, they wouldn't make the same decision.</p>\n<p>&nbsp;</p>\n<p>Suppose LessWrong were to compose a LiveJournal meme. &nbsp; It would want to be made up of somewhere between 5 and 10 short (paragraph or less) and simple (easy reading comprehension, no obscure terms or prior reading required) questions designed to elicit answers interesting for blog readers to read, intended to introduce bloggers to the concept of a 'cognitive bias' and to thinking about what rationality actually is.</p>\n<p>Do you have any suggestions for questions that would work well in such a meme?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fnLDrRyFndf9syJnP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 18, "extendedScore": null, "score": 8.675704979095883e-07, "legacy": true, "legacyId": "14172", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-18T03:53:34.216Z", "modifiedAt": null, "url": null, "title": "Fallacies as weak Bayesian evidence", "slug": "fallacies-as-weak-bayesian-evidence", "viewCount": null, "lastCommentedAt": "2021-10-31T22:38:54.809Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YgNLfytckSyKTnDXN/fallacies-as-weak-bayesian-evidence", "pageUrlRelative": "/posts/YgNLfytckSyKTnDXN/fallacies-as-weak-bayesian-evidence", "linkUrl": "https://www.lesswrong.com/posts/YgNLfytckSyKTnDXN/fallacies-as-weak-bayesian-evidence", "postedAtFormatted": "Sunday, March 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fallacies%20as%20weak%20Bayesian%20evidence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFallacies%20as%20weak%20Bayesian%20evidence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYgNLfytckSyKTnDXN%2Ffallacies-as-weak-bayesian-evidence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fallacies%20as%20weak%20Bayesian%20evidence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYgNLfytckSyKTnDXN%2Ffallacies-as-weak-bayesian-evidence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYgNLfytckSyKTnDXN%2Ffallacies-as-weak-bayesian-evidence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2938, "htmlBody": "<p><strong>Abstract:</strong> <em>Exactly what is fallacious about a claim like &rdquo;ghosts exist because no one has proved that they do not&rdquo;? And why does a claim with the same logical structure, such as &rdquo;this drug is safe because we have no evidence that it is not&rdquo;, seem more plausible? Looking at various fallacies &ndash; the argument from ignorance, circular arguments, and the slippery slope argument - we find that they can be analyzed in Bayesian terms, and that people are generally more convinced by arguments which provide greater Bayesian evidence. Arguments which provide only weak evidence, though often evidence nonetheless, are considered fallacies.<br /></em></p>\n<p>As a Nefarious Scientist, Dr. Zany is often teleconferencing with other Nefarious Scientists. Negotiations about things such as &rdquo;when we have taken over the world, who's the lucky bastard who gets to rule over Antarctica&rdquo; will often turn tense and stressful. Dr. Zany knows that stress makes it harder to evaluate arguments logically. To make things easier, he would like to build a software tool that would monitor the conversations and automatically flag any fallacious claims as such. That way, if he's too stressed out to realize that an argument offered by one of his colleagues is actually wrong, the software will work as backup to warn him.</p>\n<p>Unfortunately, it's not easy to define what counts as a fallacy. At first, Dr. Zany tried looking at the logical form of various claims. An early example that he considered was &rdquo;ghosts exist because no one has proved that they do not&rdquo;, which felt clearly wrong, an instance of the argument from ignorance. But when he programmed his software to warn him about sentences like that, it ended up flagging the claim &rdquo;this drug is safe, because we have no evidence that it is not&rdquo;. Hmm. That claim felt somewhat weak, but it didn't feel obviously wrong the way that the ghost argument did. Yet they shared the same structure. What was the difference?</p>\n<p><span style=\"text-decoration: underline;\"><strong>The argument from ignorance</strong></span></p>\n<p><em>Related posts: </em><a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">Absence of Evidence is Evidence of Absence</a>, <a href=\"/lw/27e/but_somebody_would_have_noticed/\">But Somebody Would Have Noticed!</a></p>\n<p>One kind of argument from ignorance is based on <em>negative evidence. </em>It assumes that if the hypothesis of interest were true, then experiments made to test it would show positive results. If a drug were toxic, tests of toxicity of reveal this. Whether or not this argument is valid depends on whether the tests <em>would</em> indeed show positive results, and with what probability.</p>\n<p>With some thought and help from AS-01, Dr. Zany identified three intuitions about this kind of reasoning.</p>\n<p><em><strong>1. Prior beliefs influence whether or not the argument is accepted.</strong></em></p>\n<p style=\"padding-left: 30px;\">A) I've often drunk alcohol, and never gotten drunk. Therefore alcohol doesn't cause intoxication.</p>\n<p style=\"padding-left: 30px;\">B) I've often taken Acme Flu Medicine, and never gotten any side effects. Therefore Acme Flu Medicine doesn't cause any side effects.</p>\n<p>Both of these are examples of the argument from ignorance, and both seem fallacious. But B seems much more compelling than A, since we <em>know</em> that alcohol causes intoxication, while we also know that not all kinds of medicine have side effects.</p>\n<p><em><strong>2. The more evidence found that is compatible with the conclusions of these arguments, the more acceptable they seem to be.</strong></em></p>\n<p style=\"padding-left: 30px;\">C) Acme Flu Medicine is not toxic because no toxic effects were observed in 50 tests.</p>\n<p style=\"padding-left: 30px;\">D) Acme Flu Medicine is not toxic because no toxic effects were observed in 1 test.</p>\n<p>C seems more compelling than D.</p>\n<p><strong><em>3. Negative arguments are acceptable, but they are generally less acceptable than positive arguments.</em></strong></p>\n<p style=\"padding-left: 30px;\">E) Acme Flu Medicine is toxic because a toxic effect was observed (positive argument)</p>\n<p style=\"padding-left: 30px;\">F) Acme Flu Medicine is not toxic because no toxic effect was observed (negative argument, the argument from ignorance)</p>\n<p>Argument E seems more convincing than argument F, but F is somewhat convincing as well.</p>\n<p><em>\"Aha!\" Dr. Zany exclaims. \"These three intuitions share a common origin! They bear the signatures of Bayonet reasoning!\"</em></p>\n<p><em>\"<a href=\"/lw/1to/what_is_bayesianism/\">Bayesian</a> reasoning\", AS-01 politely corrects.</em></p>\n<p><em>\"Yes, Bayesian! But, hmm. Exactly </em><em>how are they Bayesian?\"</em></p>\n<p><a id=\"more\"></a></p>\n<hr />\n<p style=\"padding-left: 30px;\"><em>Note: </em>To keep this post as accessible as possible, I attempt to explain the underlying math without actually using any math. If you would rather see the math, please see the paper referenced at the end of the post.</p>\n<p>As a brief reminder, the essence of <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' theorem</a> is that we have different theories about the world, and the extent to which we believe in these theories varies. Each theory also has implications about what you expect to observe in the world (or at least it <a href=\"http://wiki.lesswrong.com/wiki/Making_beliefs_pay_rent\"><em>should</em> have such implications</a>). The extent to which an observation makes us update our beliefs depends on how likely our theories say the observation should be. Dr. Zany has a strong belief that his plans will basically always succeed, and this theory says that his plans are very unlikely to fail. Therefore, when they do fail, he should revise his belief in the \"I will always succeed\" theory down. (So far he hasn't made that update, though.) If this isn't completely intuitive to you, I recommend <a href=\"/lw/2b0/bayes_theorem_illustrated_my_way/\">komponisto's awesome visualization</a>.</p>\n<p>Now let's look at each of the above intuitions in terms of Bayes' theorem.</p>\n<p><em>1. Prior beliefs influence whether or not the argument is accepted. </em>This is pretty straightforward -the expression \"prior beliefs\" is even there in the description of the intuition. Suppose that we hear the argument, \"I've often drunk alcohol, and never gotten drunk. Therefore alcohol doesn't cause intoxication\". The fact that this person has never gotten drunk from alcohol (or at least claims that he hasn't) <em>is</em> <a href=\"/lw/jl/what_is_evidence/\">evidence</a> for alcohol not causing any intoxication, but we still have a very strong prior belief for alcohol causing intoxication. Updating on this evidence, we find that our beliefs in both the theory \"this person is mistaken or lying\" and the theory \"alcohol doesn't cause intoxication\" have become stronger. Due to its higher prior probability, \"this person is mistaken or lying\" seems more plausible of the two, so we do not consider this a persuasive argument for alcohol not being intoxicating.</p>\n<p><em>2. The more evidence found that is compatible with the conclusions of these arguments, the more acceptable they seem to be.</em><em><strong> </strong></em>This too is a relatively straightforward consequence of Bayes' theorem. In terms of belief updating, we might encounter 50 pieces of evidence, one at a time, and make 50 small updates. Or we might encounter all of the 50 pieces of evidence at once, and perform one large update. The end result should be the same. More evidence leads to larger updates.</p>\n<p><em>3. Negative arguments are acceptable, but they are generally less acceptable than positive arguments.</em><strong><em> </em></strong>This one needs a little explaining, and here we need the concepts of <a href=\"https://en.wikipedia.org/wiki/Sensitivity_and_specificity\">sensitivity and specifity</a>. A test for something (say, a disease) is <em>sensitive</em> if it <em>always</em> gives a positive result when the disease is present, and <em>specific</em> if it <em>only</em> gives a positive result when the disease is present. There's a trade-off between these two. For instance, an airport metal detector is designed to alert its operators if a person carries dangerous metal items. It is <em>sensitive</em>, because nearly any metal item will trigger an alarm - but it is not very <em>specific</em>, because even non-dangerous items will trigger an alarm.</p>\n<p>A test which is both extremly sensitive and extremly non-specific is not very useful, since it will give more false alarms than true ones. An easy way of creating an extremely sensitive \"test for disease\" is to simply <em>always</em> say that the patient has the disease. This test has 100% sensitivity (it always gives a positive result, so it always gives a positive result when the disease is present, as well), but its specificity is very low - equal to the prevalence rate of the disease. It provides no information, and isn't therefore a test at all.</p>\n<p>How is this related to our intuition about negative and positive arguments? In short, our environment is such that like the airport metal detector, negative evidence often has high sensitivity but low specificity. We intuitively expect that a test for toxicity might not always reveal a drug to be toxic, but if it does, then the drug really <em>is</em> toxic. A lack of a \"toxic\" result is what we would expect if the drug weren't toxic, but it's also what we would expect in a lot of cases where the drug <em>was</em> toxic. Thus, <a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">negative evidence <em>is</em> evidence</a>, but it's usually much weaker than positive evidence.</p>\n<p><em>\"So, umm, okay\", Dr. Zany says, after AS-01 has reminded him of the way Bayes' theorem works, and helped him figure out how his intuitions about the fallacies have Bayes-structure. \"But let's not lose track of what we were doing, which is to say, building a fallacy-detector. How can we use this to say whether a given claim is fallacious?\"</em></p>\n<p><em>\"What this suggests is that we judge a claim to be a fallacy if it's only weak Bayesian evidence\", AS-01 replies. \"A claim like 'an unreliable test of toxicity didn't reveal this drug to be toxic, so it must be safe' is such weak evidence that we consider it fallacious. Also, if we have a very strong prior belief against something, and a claim doesn't shift this prior enough, then we might call it a 'fallacy' to believe in the thing on the basis of that claim. That was the case with the 'I've had alcohol many times and never gotten drunk, so alcohol must not be intoxicating' claim.\"</em></p>\n<p><em>\"But that's not what I was after at all! In that case I can't program a simple fallacy-detector: I'd have to implement a full-blown artificial intelligence that could understand the conversation, analyze the prior probabilities of various claims, and judge the weight of evidence. And even if I did that, it wouldn't help me figure out what claims were fallacies, because all of my AIs only want to eradicate the color blue from the universe! Hmm. But maybe the appeal from ignorance was a special case, and other fallacies are more accomodating. How about circular claims? Those must surely be fallacious?\"</em></p>\n<p><strong><span style=\"text-decoration: underline;\">Circularity</span></strong></p>\n<p style=\"padding-left: 30px;\">A. God exists because the Bible says so, and the Bible is the word of God.</p>\n<p style=\"padding-left: 30px;\">B. Electrons exist because we can see 3-cm tracks in a cloud chamber, and 3-cm tracks in cloud chambers are signatures of electrons.</p>\n<p><em>\"Okay, we have two circular claims here\", AS-01 notes. \"Their logical structure seems to be the same, but we judge one of them to be a fallacy, while the other seems to be okay.\"</em></p>\n<p><em>\"I have a bad feeling about this\", Dr. Zany says.<br /></em></p>\n<p>The argument for the fallaciousness of the above two claims is that they presume the conclusion in the premises. That is, it is presumed that the Bible is the word of God, but that is only possible if God actually exists. Likewise, if electrons don't exist, then whatever we see in the cloud chamber isn't the signature signs of electrons. Thus, in order to believe the conclusion, we need to already believe it as an implicit premise.</p>\n<p>But from a Bayesian perspective, beliefs aren't binary propositions: we can <em>tentatively </em>believe in a hypothesis, such as the existence of God or electrons. In addition to this tentative hypothesis, we have sense data about the existence of the Bible and the 3-cm tracks. This data we take as certain. We also have a second tentative belief, the ambiguous interpretation of this sense data as the word of God or the signature of electrons. The sense data is ambiguous in the sense that it might or might not be the word of God. So we have three components in our inference: the evidence (Bible, 3-cm tracks), the ambiguous interpretation (the Bible is the word of God, the 3-cm tracks are signatures of electrons), and the hypothesis (God exists, electrons exist).</p>\n<p>We can conjecture a causal connection between these three components. Let's suppose that God exists (the hypothesis). This then causes the Bible as his word (ambiguous interpretation), which in turn gives rise to the actual document in front of us (sense data). Likewise, if electrons exist (hypothesis), then this can give rise to the predicted signature effects (ambiguous interpretation), which become manifest as what we actually see in the cloud chamber (sense data).</p>\n<p>The \"circular\" claim reverses the direction of the inference. We have sense data, which we would expect to see if the ambiguous interpretation was correct, and we would expect the interpretation to be correct if the hypothesis were true. Therefore it's more likely that the hypothesis is true. Is this allowed? Yes! Take for example the inference \"if there are dark clouds in the sky, then it will rain, in which case the grass will be wet\". The reverse inference, \"the grass is wet, therefore it has rained, therefore there have been dark clouds in the sky\" is valid. However, the inference \"the grass is wet, therefore the sprinkler has been on, thefore there is a sprinkler near this grass\" may <em>also</em> be a valid inference. The grass being wet is evidence for <em>both</em> the presence of dark clouds <em>and</em> for a sprinkler having been on. Which hypothesis do we judge to be more likely? That <a href=\"http://www.cs.helsinki.fi/sites/default/files/root/course-material/Probabilistic%20models/lecture4.pdf\">depends</a> on our prior beliefs about the hypotheses, as well as the strengths of the causal links (e.g. \"if there are dark clouds, how likely is it that it rains?\", and vice versa).</p>\n<p>Thus, the \"circular\" arguments given above are actually valid Bayesian inferences. But there is a reason that we consider A to be a fallacy, while B sounds valid. Since the intepretation (the Bible is the word of God, 3-cm tracks are signatures of electrons) logically requires the hypothesis, the probability of the interpretation <a href=\"/lw/ji/conjunction_fallacy/\">cannot be higher</a> than the probability of the hypothesis. If we assign the existence of God a very low prior belief, then we must also assign a very low prior belief to the interpretation of the Bible as the word of God. In that case, seeing the Bible will not do much to elevate our belief in the claim that God exists, if there are more likely hypotheses to be found.</p>\n<p><em>\"So you're saying that circular reasoning, too, is something that we consider fallacious if our prior belief in the hypothesis is low enough? And recognizing these kinds of fallacies is <a href=\"https://en.wikipedia.org/wiki/AI-complete\">AI-complete</a>, too?\"</em> <em>Dr. Zany asks.</em></p>\n<p><em>\"Yup!\", AS-01 replies cheerfully, glad that for once, Dr. Zany gets it without a need to explain things fifteen times.</em></p>\n<p><em>\"Damn it. But... what about slippery slope arguments? Dr. Cagliostro claims that if we let minor supervillains stake claims for territory, then we would end up letting henchmen stake claims for territory as well, and eventually we'd give the right to people who didn't even participate in our plans! Surely that must be a fallacy?\"</em></p>\n<p><span style=\"text-decoration: underline;\"><strong>Slippery slope</strong></span></p>\n<p>Slippery slope arguments are often treated as fallacies, but they might not be. There are cases where the stipulated \"slope\" <em>is</em> what would actually (or likely) happen. For instance, take a claim saying \"if we allow microbes to be patented, then that will lead to higher life-forms being patented as well\":</p>\n<blockquote>\n<p>There are cases in law, for example, in which a legal precedent has historically facilitated subsequent legal change. Lode (1999, pp. 511&ndash;512) cites the example originally identified by Kimbrell (1993) whereby there is good reason to believe that the issuing of a patent on a transgenic mouse by the U.S. Patent and Trademark Office in the year 1988 is the result of a slippery slope set in motion with the U.S. Supreme court&rsquo;s decision Diamond v. Chakrabarty. This latter decision allowed a patent for an oil-eating microbe, and the subsequent granting of a patent for the mouse would have been unthinkable without the chain started by it.&nbsp; (Hahn &amp; Oaksford, 2007)</p>\n</blockquote>\n<p>So again, our prior beliefs, here ones about the plausibility of the slope, influence whether or not the argument is accepted. But there is also another component that was missing from the previous fallacies. Because slippery slope arguments are about actions, not just beliefs, the principle of <a href=\"https://en.wikipedia.org/wiki/Expected_utility\">expected utility</a> becomes relevant. A slippery slope argument will be stronger (relative to its alternative) if it invokes a more undesirable potential consequence, if that consequence is more probable, and if the expected utility of the alternatives is smaller.</p>\n<p>For instance, suppose for the sake of argument that both increased heroin consumption and increased reggae music consumption are equally likely consequences of cannabis legalization:</p>\n<p style=\"padding-left: 30px;\">A. Legalizing cannabis will lead to an increase in heroin consumption.</p>\n<p style=\"padding-left: 30px;\">B. Legalizing cannabis will lead to an increase in listening to reggae music.</p>\n<p>Yet A would feel like a stronger argument against the legalization of cannabis than argument B, since increased heroin consumption feels like it would have lower utility. On the other hand, if the outcome is shared, then the stronger argument seems to be the one where the causal link seems more probable:</p>\n<p style=\"padding-left: 30px;\">C. Legalizing Internet access would lead to an increase in the amount of World of Warcraft addicts.</p>\n<p style=\"padding-left: 30px;\">D. Legalizing video rental stores would lead to an increase in the amount of World of Warcraft addicts.</p>\n<p><em>\"Gah. So a strong slippery slope argument is one where both the utility of the outcome, <strong>and</strong> the outcome's probability is high? So </em><em>the AI would not only need to evaluate probabilities, but expected utilities as well?\"</em></p>\n<p><em>\"That's right!\"</em></p>\n<p><em>\"Screw it, this isn't going anywhere. And here I thought that this would be a productive day.\"</em></p>\n<p><em>\"They can't all be, but we tried our best. Would you like a tuna sandwich as consolation?\"</em></p>\n<p><em>\"Yes, please.\"<br /></em></p>\n<hr />\n<p>Because this post is already unreasonably long, the above discussion only covers the <em>theoretical</em> reasons for thinking about fallacies as weak or strong Bayesian arguments. For math, experimental studies, and two other subtypes of the argument from ignorance (besides negative evidence), see:</p>\n<p style=\"padding-left: 30px;\">Hahn, U. &amp; Oaksford, M. (2007) <a href=\"http://www.researchgate.net/profile/Mike_Oaksford/publication/6199092_The_rationality_of_informal_argumentation_a_Bayesian_approach_to_reasoning_fallacies/links/0c96051e9a3d941970000000.pdf\">The Rationality of Informal Argumentation: A Bayesian Approach to Reasoning Fallacies. </a><em>Psychological Review,</em> vol. 114, no. 3, 704-732.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 2, "dJ6eJxJrCEget7Wb6": 2, "ksdiAMKfgSyEeKMo6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YgNLfytckSyKTnDXN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 70, "baseScore": 85, "extendedScore": null, "score": 0.000185, "legacy": true, "legacyId": "13898", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 85, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Abstract:</strong> <em>Exactly what is fallacious about a claim like \u201dghosts exist because no one has proved that they do not\u201d? And why does a claim with the same logical structure, such as \u201dthis drug is safe because we have no evidence that it is not\u201d, seem more plausible? Looking at various fallacies \u2013 the argument from ignorance, circular arguments, and the slippery slope argument - we find that they can be analyzed in Bayesian terms, and that people are generally more convinced by arguments which provide greater Bayesian evidence. Arguments which provide only weak evidence, though often evidence nonetheless, are considered fallacies.<br></em></p>\n<p>As a Nefarious Scientist, Dr. Zany is often teleconferencing with other Nefarious Scientists. Negotiations about things such as \u201dwhen we have taken over the world, who's the lucky bastard who gets to rule over Antarctica\u201d will often turn tense and stressful. Dr. Zany knows that stress makes it harder to evaluate arguments logically. To make things easier, he would like to build a software tool that would monitor the conversations and automatically flag any fallacious claims as such. That way, if he's too stressed out to realize that an argument offered by one of his colleagues is actually wrong, the software will work as backup to warn him.</p>\n<p>Unfortunately, it's not easy to define what counts as a fallacy. At first, Dr. Zany tried looking at the logical form of various claims. An early example that he considered was \u201dghosts exist because no one has proved that they do not\u201d, which felt clearly wrong, an instance of the argument from ignorance. But when he programmed his software to warn him about sentences like that, it ended up flagging the claim \u201dthis drug is safe, because we have no evidence that it is not\u201d. Hmm. That claim felt somewhat weak, but it didn't feel obviously wrong the way that the ghost argument did. Yet they shared the same structure. What was the difference?</p>\n<p><span style=\"text-decoration: underline;\"><strong>The argument from ignorance</strong></span></p>\n<p><em>Related posts: </em><a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">Absence of Evidence is Evidence of Absence</a>, <a href=\"/lw/27e/but_somebody_would_have_noticed/\">But Somebody Would Have Noticed!</a></p>\n<p>One kind of argument from ignorance is based on <em>negative evidence. </em>It assumes that if the hypothesis of interest were true, then experiments made to test it would show positive results. If a drug were toxic, tests of toxicity of reveal this. Whether or not this argument is valid depends on whether the tests <em>would</em> indeed show positive results, and with what probability.</p>\n<p>With some thought and help from AS-01, Dr. Zany identified three intuitions about this kind of reasoning.</p>\n<p><em><strong>1. Prior beliefs influence whether or not the argument is accepted.</strong></em></p>\n<p style=\"padding-left: 30px;\">A) I've often drunk alcohol, and never gotten drunk. Therefore alcohol doesn't cause intoxication.</p>\n<p style=\"padding-left: 30px;\">B) I've often taken Acme Flu Medicine, and never gotten any side effects. Therefore Acme Flu Medicine doesn't cause any side effects.</p>\n<p>Both of these are examples of the argument from ignorance, and both seem fallacious. But B seems much more compelling than A, since we <em>know</em> that alcohol causes intoxication, while we also know that not all kinds of medicine have side effects.</p>\n<p><em><strong>2. The more evidence found that is compatible with the conclusions of these arguments, the more acceptable they seem to be.</strong></em></p>\n<p style=\"padding-left: 30px;\">C) Acme Flu Medicine is not toxic because no toxic effects were observed in 50 tests.</p>\n<p style=\"padding-left: 30px;\">D) Acme Flu Medicine is not toxic because no toxic effects were observed in 1 test.</p>\n<p>C seems more compelling than D.</p>\n<p><strong id=\"3__Negative_arguments_are_acceptable__but_they_are_generally_less_acceptable_than_positive_arguments_\"><em>3. Negative arguments are acceptable, but they are generally less acceptable than positive arguments.</em></strong></p>\n<p style=\"padding-left: 30px;\">E) Acme Flu Medicine is toxic because a toxic effect was observed (positive argument)</p>\n<p style=\"padding-left: 30px;\">F) Acme Flu Medicine is not toxic because no toxic effect was observed (negative argument, the argument from ignorance)</p>\n<p>Argument E seems more convincing than argument F, but F is somewhat convincing as well.</p>\n<p><em>\"Aha!\" Dr. Zany exclaims. \"These three intuitions share a common origin! They bear the signatures of Bayonet reasoning!\"</em></p>\n<p><em>\"<a href=\"/lw/1to/what_is_bayesianism/\">Bayesian</a> reasoning\", AS-01 politely corrects.</em></p>\n<p><em>\"Yes, Bayesian! But, hmm. Exactly </em><em>how are they Bayesian?\"</em></p>\n<p><a id=\"more\"></a></p>\n<hr>\n<p style=\"padding-left: 30px;\"><em>Note: </em>To keep this post as accessible as possible, I attempt to explain the underlying math without actually using any math. If you would rather see the math, please see the paper referenced at the end of the post.</p>\n<p>As a brief reminder, the essence of <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' theorem</a> is that we have different theories about the world, and the extent to which we believe in these theories varies. Each theory also has implications about what you expect to observe in the world (or at least it <a href=\"http://wiki.lesswrong.com/wiki/Making_beliefs_pay_rent\"><em>should</em> have such implications</a>). The extent to which an observation makes us update our beliefs depends on how likely our theories say the observation should be. Dr. Zany has a strong belief that his plans will basically always succeed, and this theory says that his plans are very unlikely to fail. Therefore, when they do fail, he should revise his belief in the \"I will always succeed\" theory down. (So far he hasn't made that update, though.) If this isn't completely intuitive to you, I recommend <a href=\"/lw/2b0/bayes_theorem_illustrated_my_way/\">komponisto's awesome visualization</a>.</p>\n<p>Now let's look at each of the above intuitions in terms of Bayes' theorem.</p>\n<p><em>1. Prior beliefs influence whether or not the argument is accepted. </em>This is pretty straightforward -the expression \"prior beliefs\" is even there in the description of the intuition. Suppose that we hear the argument, \"I've often drunk alcohol, and never gotten drunk. Therefore alcohol doesn't cause intoxication\". The fact that this person has never gotten drunk from alcohol (or at least claims that he hasn't) <em>is</em> <a href=\"/lw/jl/what_is_evidence/\">evidence</a> for alcohol not causing any intoxication, but we still have a very strong prior belief for alcohol causing intoxication. Updating on this evidence, we find that our beliefs in both the theory \"this person is mistaken or lying\" and the theory \"alcohol doesn't cause intoxication\" have become stronger. Due to its higher prior probability, \"this person is mistaken or lying\" seems more plausible of the two, so we do not consider this a persuasive argument for alcohol not being intoxicating.</p>\n<p><em>2. The more evidence found that is compatible with the conclusions of these arguments, the more acceptable they seem to be.</em><em><strong> </strong></em>This too is a relatively straightforward consequence of Bayes' theorem. In terms of belief updating, we might encounter 50 pieces of evidence, one at a time, and make 50 small updates. Or we might encounter all of the 50 pieces of evidence at once, and perform one large update. The end result should be the same. More evidence leads to larger updates.</p>\n<p><em>3. Negative arguments are acceptable, but they are generally less acceptable than positive arguments.</em><strong><em> </em></strong>This one needs a little explaining, and here we need the concepts of <a href=\"https://en.wikipedia.org/wiki/Sensitivity_and_specificity\">sensitivity and specifity</a>. A test for something (say, a disease) is <em>sensitive</em> if it <em>always</em> gives a positive result when the disease is present, and <em>specific</em> if it <em>only</em> gives a positive result when the disease is present. There's a trade-off between these two. For instance, an airport metal detector is designed to alert its operators if a person carries dangerous metal items. It is <em>sensitive</em>, because nearly any metal item will trigger an alarm - but it is not very <em>specific</em>, because even non-dangerous items will trigger an alarm.</p>\n<p>A test which is both extremly sensitive and extremly non-specific is not very useful, since it will give more false alarms than true ones. An easy way of creating an extremely sensitive \"test for disease\" is to simply <em>always</em> say that the patient has the disease. This test has 100% sensitivity (it always gives a positive result, so it always gives a positive result when the disease is present, as well), but its specificity is very low - equal to the prevalence rate of the disease. It provides no information, and isn't therefore a test at all.</p>\n<p>How is this related to our intuition about negative and positive arguments? In short, our environment is such that like the airport metal detector, negative evidence often has high sensitivity but low specificity. We intuitively expect that a test for toxicity might not always reveal a drug to be toxic, but if it does, then the drug really <em>is</em> toxic. A lack of a \"toxic\" result is what we would expect if the drug weren't toxic, but it's also what we would expect in a lot of cases where the drug <em>was</em> toxic. Thus, <a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">negative evidence <em>is</em> evidence</a>, but it's usually much weaker than positive evidence.</p>\n<p><em>\"So, umm, okay\", Dr. Zany says, after AS-01 has reminded him of the way Bayes' theorem works, and helped him figure out how his intuitions about the fallacies have Bayes-structure. \"But let's not lose track of what we were doing, which is to say, building a fallacy-detector. How can we use this to say whether a given claim is fallacious?\"</em></p>\n<p><em>\"What this suggests is that we judge a claim to be a fallacy if it's only weak Bayesian evidence\", AS-01 replies. \"A claim like 'an unreliable test of toxicity didn't reveal this drug to be toxic, so it must be safe' is such weak evidence that we consider it fallacious. Also, if we have a very strong prior belief against something, and a claim doesn't shift this prior enough, then we might call it a 'fallacy' to believe in the thing on the basis of that claim. That was the case with the 'I've had alcohol many times and never gotten drunk, so alcohol must not be intoxicating' claim.\"</em></p>\n<p><em>\"But that's not what I was after at all! In that case I can't program a simple fallacy-detector: I'd have to implement a full-blown artificial intelligence that could understand the conversation, analyze the prior probabilities of various claims, and judge the weight of evidence. And even if I did that, it wouldn't help me figure out what claims were fallacies, because all of my AIs only want to eradicate the color blue from the universe! Hmm. But maybe the appeal from ignorance was a special case, and other fallacies are more accomodating. How about circular claims? Those must surely be fallacious?\"</em></p>\n<p><strong id=\"Circularity\"><span style=\"text-decoration: underline;\">Circularity</span></strong></p>\n<p style=\"padding-left: 30px;\">A. God exists because the Bible says so, and the Bible is the word of God.</p>\n<p style=\"padding-left: 30px;\">B. Electrons exist because we can see 3-cm tracks in a cloud chamber, and 3-cm tracks in cloud chambers are signatures of electrons.</p>\n<p><em>\"Okay, we have two circular claims here\", AS-01 notes. \"Their logical structure seems to be the same, but we judge one of them to be a fallacy, while the other seems to be okay.\"</em></p>\n<p><em>\"I have a bad feeling about this\", Dr. Zany says.<br></em></p>\n<p>The argument for the fallaciousness of the above two claims is that they presume the conclusion in the premises. That is, it is presumed that the Bible is the word of God, but that is only possible if God actually exists. Likewise, if electrons don't exist, then whatever we see in the cloud chamber isn't the signature signs of electrons. Thus, in order to believe the conclusion, we need to already believe it as an implicit premise.</p>\n<p>But from a Bayesian perspective, beliefs aren't binary propositions: we can <em>tentatively </em>believe in a hypothesis, such as the existence of God or electrons. In addition to this tentative hypothesis, we have sense data about the existence of the Bible and the 3-cm tracks. This data we take as certain. We also have a second tentative belief, the ambiguous interpretation of this sense data as the word of God or the signature of electrons. The sense data is ambiguous in the sense that it might or might not be the word of God. So we have three components in our inference: the evidence (Bible, 3-cm tracks), the ambiguous interpretation (the Bible is the word of God, the 3-cm tracks are signatures of electrons), and the hypothesis (God exists, electrons exist).</p>\n<p>We can conjecture a causal connection between these three components. Let's suppose that God exists (the hypothesis). This then causes the Bible as his word (ambiguous interpretation), which in turn gives rise to the actual document in front of us (sense data). Likewise, if electrons exist (hypothesis), then this can give rise to the predicted signature effects (ambiguous interpretation), which become manifest as what we actually see in the cloud chamber (sense data).</p>\n<p>The \"circular\" claim reverses the direction of the inference. We have sense data, which we would expect to see if the ambiguous interpretation was correct, and we would expect the interpretation to be correct if the hypothesis were true. Therefore it's more likely that the hypothesis is true. Is this allowed? Yes! Take for example the inference \"if there are dark clouds in the sky, then it will rain, in which case the grass will be wet\". The reverse inference, \"the grass is wet, therefore it has rained, therefore there have been dark clouds in the sky\" is valid. However, the inference \"the grass is wet, therefore the sprinkler has been on, thefore there is a sprinkler near this grass\" may <em>also</em> be a valid inference. The grass being wet is evidence for <em>both</em> the presence of dark clouds <em>and</em> for a sprinkler having been on. Which hypothesis do we judge to be more likely? That <a href=\"http://www.cs.helsinki.fi/sites/default/files/root/course-material/Probabilistic%20models/lecture4.pdf\">depends</a> on our prior beliefs about the hypotheses, as well as the strengths of the causal links (e.g. \"if there are dark clouds, how likely is it that it rains?\", and vice versa).</p>\n<p>Thus, the \"circular\" arguments given above are actually valid Bayesian inferences. But there is a reason that we consider A to be a fallacy, while B sounds valid. Since the intepretation (the Bible is the word of God, 3-cm tracks are signatures of electrons) logically requires the hypothesis, the probability of the interpretation <a href=\"/lw/ji/conjunction_fallacy/\">cannot be higher</a> than the probability of the hypothesis. If we assign the existence of God a very low prior belief, then we must also assign a very low prior belief to the interpretation of the Bible as the word of God. In that case, seeing the Bible will not do much to elevate our belief in the claim that God exists, if there are more likely hypotheses to be found.</p>\n<p><em>\"So you're saying that circular reasoning, too, is something that we consider fallacious if our prior belief in the hypothesis is low enough? And recognizing these kinds of fallacies is <a href=\"https://en.wikipedia.org/wiki/AI-complete\">AI-complete</a>, too?\"</em> <em>Dr. Zany asks.</em></p>\n<p><em>\"Yup!\", AS-01 replies cheerfully, glad that for once, Dr. Zany gets it without a need to explain things fifteen times.</em></p>\n<p><em>\"Damn it. But... what about slippery slope arguments? Dr. Cagliostro claims that if we let minor supervillains stake claims for territory, then we would end up letting henchmen stake claims for territory as well, and eventually we'd give the right to people who didn't even participate in our plans! Surely that must be a fallacy?\"</em></p>\n<p><span style=\"text-decoration: underline;\"><strong>Slippery slope</strong></span></p>\n<p>Slippery slope arguments are often treated as fallacies, but they might not be. There are cases where the stipulated \"slope\" <em>is</em> what would actually (or likely) happen. For instance, take a claim saying \"if we allow microbes to be patented, then that will lead to higher life-forms being patented as well\":</p>\n<blockquote>\n<p>There are cases in law, for example, in which a legal precedent has historically facilitated subsequent legal change. Lode (1999, pp. 511\u2013512) cites the example originally identified by Kimbrell (1993) whereby there is good reason to believe that the issuing of a patent on a transgenic mouse by the U.S. Patent and Trademark Office in the year 1988 is the result of a slippery slope set in motion with the U.S. Supreme court\u2019s decision Diamond v. Chakrabarty. This latter decision allowed a patent for an oil-eating microbe, and the subsequent granting of a patent for the mouse would have been unthinkable without the chain started by it.&nbsp; (Hahn &amp; Oaksford, 2007)</p>\n</blockquote>\n<p>So again, our prior beliefs, here ones about the plausibility of the slope, influence whether or not the argument is accepted. But there is also another component that was missing from the previous fallacies. Because slippery slope arguments are about actions, not just beliefs, the principle of <a href=\"https://en.wikipedia.org/wiki/Expected_utility\">expected utility</a> becomes relevant. A slippery slope argument will be stronger (relative to its alternative) if it invokes a more undesirable potential consequence, if that consequence is more probable, and if the expected utility of the alternatives is smaller.</p>\n<p>For instance, suppose for the sake of argument that both increased heroin consumption and increased reggae music consumption are equally likely consequences of cannabis legalization:</p>\n<p style=\"padding-left: 30px;\">A. Legalizing cannabis will lead to an increase in heroin consumption.</p>\n<p style=\"padding-left: 30px;\">B. Legalizing cannabis will lead to an increase in listening to reggae music.</p>\n<p>Yet A would feel like a stronger argument against the legalization of cannabis than argument B, since increased heroin consumption feels like it would have lower utility. On the other hand, if the outcome is shared, then the stronger argument seems to be the one where the causal link seems more probable:</p>\n<p style=\"padding-left: 30px;\">C. Legalizing Internet access would lead to an increase in the amount of World of Warcraft addicts.</p>\n<p style=\"padding-left: 30px;\">D. Legalizing video rental stores would lead to an increase in the amount of World of Warcraft addicts.</p>\n<p><em>\"Gah. So a strong slippery slope argument is one where both the utility of the outcome, <strong>and</strong> the outcome's probability is high? So </em><em>the AI would not only need to evaluate probabilities, but expected utilities as well?\"</em></p>\n<p><em>\"That's right!\"</em></p>\n<p><em>\"Screw it, this isn't going anywhere. And here I thought that this would be a productive day.\"</em></p>\n<p><em>\"They can't all be, but we tried our best. Would you like a tuna sandwich as consolation?\"</em></p>\n<p><em>\"Yes, please.\"<br></em></p>\n<hr>\n<p>Because this post is already unreasonably long, the above discussion only covers the <em>theoretical</em> reasons for thinking about fallacies as weak or strong Bayesian arguments. For math, experimental studies, and two other subtypes of the argument from ignorance (besides negative evidence), see:</p>\n<p style=\"padding-left: 30px;\">Hahn, U. &amp; Oaksford, M. (2007) <a href=\"http://www.researchgate.net/profile/Mike_Oaksford/publication/6199092_The_rationality_of_informal_argumentation_a_Bayesian_approach_to_reasoning_fallacies/links/0c96051e9a3d941970000000.pdf\">The Rationality of Informal Argumentation: A Bayesian Approach to Reasoning Fallacies. </a><em>Psychological Review,</em> vol. 114, no. 3, 704-732.&nbsp;</p>", "sections": [{"title": "3. Negative arguments are acceptable, but they are generally less acceptable than positive arguments.", "anchor": "3__Negative_arguments_are_acceptable__but_they_are_generally_less_acceptable_than_positive_arguments_", "level": 1}, {"title": "Circularity", "anchor": "Circularity", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "42 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mnS2WYLCGJP2kQkRn", "Xz6pKy6sjZffy4NYW", "AN2cBr6xKWCB8dRQG", "CMt3ijXYuCynhPWXa", "6s3xABaXKPdFwA3FS", "QAK43nNCTQQycAcYe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-18T04:49:18.772Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Initiation Ceremony", "slug": "seq-rerun-initiation-ceremony", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:25.850Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nnvu8F2DMartEaZvC/seq-rerun-initiation-ceremony", "pageUrlRelative": "/posts/nnvu8F2DMartEaZvC/seq-rerun-initiation-ceremony", "linkUrl": "https://www.lesswrong.com/posts/nnvu8F2DMartEaZvC/seq-rerun-initiation-ceremony", "postedAtFormatted": "Sunday, March 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Initiation%20Ceremony&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Initiation%20Ceremony%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnnvu8F2DMartEaZvC%2Fseq-rerun-initiation-ceremony%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Initiation%20Ceremony%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnnvu8F2DMartEaZvC%2Fseq-rerun-initiation-ceremony", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnnvu8F2DMartEaZvC%2Fseq-rerun-initiation-ceremony", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 150, "htmlBody": "<p>Today's post, <a href=\"/lw/p1/initiation_ceremony/\">Initiation Ceremony</a> was originally published on 28 March 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Brennan is inducted into the Conspiracy</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/awx/seq_rerun_to_spread_science_keep_it_secret/\">To Spread Science, Keep It Secret</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nnvu8F2DMartEaZvC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 8.676164778719544e-07, "legacy": true, "legacyId": "14174", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fnEWQAYxcRnaYBqaZ", "dyKmzNZqH53XWE8W4", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-18T06:52:13.175Z", "modifiedAt": null, "url": null, "title": "Three new papers on AI risk", "slug": "three-new-papers-on-ai-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:56.300Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KiJKQC2rccayKpE22/three-new-papers-on-ai-risk", "pageUrlRelative": "/posts/KiJKQC2rccayKpE22/three-new-papers-on-ai-risk", "linkUrl": "https://www.lesswrong.com/posts/KiJKQC2rccayKpE22/three-new-papers-on-ai-risk", "postedAtFormatted": "Sunday, March 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Three%20new%20papers%20on%20AI%20risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThree%20new%20papers%20on%20AI%20risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKiJKQC2rccayKpE22%2Fthree-new-papers-on-ai-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Three%20new%20papers%20on%20AI%20risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKiJKQC2rccayKpE22%2Fthree-new-papers-on-ai-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKiJKQC2rccayKpE22%2Fthree-new-papers-on-ai-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 421, "htmlBody": "<p>In case you aren't subscribed to <a href=\"http://friendlyai.tumblr.com/\">FriendlyAI.tumblr.com</a> for the latest updates on AI risk research, I'll mention here that three new papers on the subject were recently made available online...</p>\n<p>&nbsp;</p>\n<p>Bostrom (2012). <a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">The Superintelligent Will:&nbsp;Motivation and Instrumental Rationality in Advanced Artificial Agents</a>.</p>\n<blockquote>\n<p>This paper discusses the relation between intelligence and motivation in artificial agents, developing and briefly arguing for two theses. &nbsp;The first, the orthogonality thesis, holds (with some caveats) that intelligence and final goals (purposes) are orthogonal axes along which possible artificial intellects can freely vary&mdash;more or less any level of intelligence could be combined with more or less any final goal. &nbsp;The second, the instrumental convergence thesis, holds that as long as they possess a sufficient level of intelligence, agents having any of a wide range of final goals will pursue similar intermediary goals because they have instrumental reasons to do so. In combination, the two theses help us understand the possible range of behavior of superintelligent agents, and they point to some potential dangers in building such an agent.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Yampolskiy &amp; Fox (2012a). <a href=\"http://joshuafox.com/professional/media/YampolskiyFox__SafetyEngineeringforAGI.pdf\">Safety engineering for artificial general intelligence</a>.</p>\n<blockquote>\n<p>Machine ethics and robot rights are quickly becoming hot topics in artificial intelligence and robotics communities. We will argue that attempts to attribute moral agency and assign rights to all intelligent machines are misguided, whether applied to infrahuman or superhuman AIs, as are proposals to limit the negative effects of AIs by constraining their behavior. As an alternative, we propose a new science of safety engineering for intelligent artificial agents based on maximizing for what humans value. In particular, we challenge the scientific community to develop intelligent systems that have humanfriendly values that they provably retain, even under recursive self-improvement.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Yampolskiy &amp; Fox (2012b). <a href=\"http://joshuafox.com/professional/media/YampolskiyFox__AGIAndTheHumanModel.pdf\">Artificial general intelligence and the human mental model</a>.</p>\n<blockquote>\n<p>When the first artificial general intelligences are built, they may improve themselves to far-above-human levels. Speculations about such future entities are already affected by anthropomorphic bias, which leads to erroneous analogies with human minds. In this chapter, we apply a goal-oriented understanding of intelligence to show that humanity occupies only a tiny portion of the design space of possible minds. This space is much larger than what we are familiar with from the human example; and the mental architectures and goals of future superintelligences need not have most of the properties of human minds. A new approach to cognitive science and philosophy of mind, one not centered on the human example, is needed to help us understand the challenges which we will face when a power greater than us emerges.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KiJKQC2rccayKpE22", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 8.676666997391059e-07, "legacy": true, "legacyId": "14175", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-18T06:54:33.513Z", "modifiedAt": null, "url": null, "title": "Causation, Probability and Objectivity", "slug": "causation-probability-and-objectivity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:28.456Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "antigonus", "createdAt": "2011-04-02T06:03:35.917Z", "isAdmin": false, "displayName": "antigonus"}, "userId": "zDParABHiCtvHrW9a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6MkfzMioZn5ytqyec/causation-probability-and-objectivity", "pageUrlRelative": "/posts/6MkfzMioZn5ytqyec/causation-probability-and-objectivity", "linkUrl": "https://www.lesswrong.com/posts/6MkfzMioZn5ytqyec/causation-probability-and-objectivity", "postedAtFormatted": "Sunday, March 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Causation%2C%20Probability%20and%20Objectivity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACausation%2C%20Probability%20and%20Objectivity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6MkfzMioZn5ytqyec%2Fcausation-probability-and-objectivity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Causation%2C%20Probability%20and%20Objectivity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6MkfzMioZn5ytqyec%2Fcausation-probability-and-objectivity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6MkfzMioZn5ytqyec%2Fcausation-probability-and-objectivity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 150, "htmlBody": "<p>Most people here seem to endorse the following two claims:</p>\n<p>1. Probability is \"in the mind,\" i.e., probability claims are true only in relation to some prior distribution and set of information to be conditionalized on;<br />2. Causality is to be cashed out in terms of probability distributions &aacute; la Judea Pearl or something.</p>\n<p>However, these two claims feel in tension to me, since they appear to have the consequence that causality is also \"in the mind\" - whether something caused something else depends on various probability distributions, which in turn depends on how much we know about the situation. Worse, it has the consequence that ideal Bayesian reasoners can never be wrong about causal relations, since they always have perfect knowledge of their own probabilities.</p>\n<p>Since I don't understand Pearl's model of causality very well, I may be missing something fundamental, so this is more of a question than an argument.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6MkfzMioZn5ytqyec", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 11, "extendedScore": null, "score": 8.676676555189327e-07, "legacy": true, "legacyId": "14176", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-18T10:29:41.859Z", "modifiedAt": null, "url": null, "title": "The AI design space near the FAI [draft]", "slug": "the-ai-design-space-near-the-fai-draft", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:25.072Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dmytry", "createdAt": "2009-12-03T17:11:53.492Z", "isAdmin": false, "displayName": "Dmytry"}, "userId": "AjtmA2qtA8sdiMbru", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8T3u3S8vwQycN5SR6/the-ai-design-space-near-the-fai-draft", "pageUrlRelative": "/posts/8T3u3S8vwQycN5SR6/the-ai-design-space-near-the-fai-draft", "linkUrl": "https://www.lesswrong.com/posts/8T3u3S8vwQycN5SR6/the-ai-design-space-near-the-fai-draft", "postedAtFormatted": "Sunday, March 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20AI%20design%20space%20near%20the%20FAI%20%5Bdraft%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20AI%20design%20space%20near%20the%20FAI%20%5Bdraft%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8T3u3S8vwQycN5SR6%2Fthe-ai-design-space-near-the-fai-draft%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20AI%20design%20space%20near%20the%20FAI%20%5Bdraft%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8T3u3S8vwQycN5SR6%2Fthe-ai-design-space-near-the-fai-draft", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8T3u3S8vwQycN5SR6%2Fthe-ai-design-space-near-the-fai-draft", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1921, "htmlBody": "<h4>Abstract:</h4>\n<p>Nearly-FAIs can be more dangerous than AIs with no attempt at friendliness. The FAI effort needs better argument that the attempt at FAI decreases the risks. We are bad at processing threats rationally, and prone to very bad decisions when threatened, akin to running away from unknown into a minefield.</p>\n<h4>Nearly friendly AIs<br /></h4>\n<p>Consider AI that truly loves mankind but decides that all of the mankind must be euthanized like an old, sick dog - due to chain of reasoning too long for us to generate when we test our logic of AI, or even comprehend - and proceeds to make a bliss virus - the virus makes you intensely happy, setting your internal utility to infinity; and keeping it so until you die. It wouldn't even take a very strongly superhuman intelligence to do that kind of thing. Treating life as if it was a disease. It can do so even if it destroys the AI itself. Or consider the FAI that cuts your brain apart to satisfy each hemisphere's slightly different desires. The AI that just wireheads everyone because it figured we all want it (and worst of all it may be correct).</p>\n<p>It seems to me that one can find the true monsters in the design space near to the FAI, and even including the FAIs. And herein lies a great danger: bugged FAIs, the AIs that are close to friendly AI, but are not friendly. It is hard for me to think of a deficiency in friendliness which isn't horrifically unfriendly (restricting to deficiencies that don't break AI).</p>\n<h4>Should we be so afraid of the AIs made without attempts at friendliness?<br /></h4>\n<p>We need to keep in mind that we have no solid argument that the AIs written without attempt at friendliness - the AIs that predominantly don't treat mankind in any special way - will necessarily make us extinct.</p>\n<p>We have one example of 'bootstrap' optimization process - evolution - with not a slightest trace of friendliness in it. What did emerge in the end? We assign pretty low utility to nature, but non-zero, and we are willing to trade resources for preservation of nature - see the endangered species list and international treaties on whaling. It is not perfect, but I think it is fair to say that the single example of bootstrap intelligence we got values the complex dynamical processes for what they are, and prefers to obtain resources without disrupting those processes, even if it is slightly more expensive to do so, and is willing to divert small fraction of the global effort towards helping lesser intelligences.</p>\n<p>In light of this, the argument that the AI that is not coded to be friendly is 'almost certainly' going to eat you for the raw resources, seems fairly shaky, especially when applied to irregular AIs such as neural networks, crude simulations of human brain's embryological development, and mind uploads. I didn't eat my cats yet (nor did they eat each other, nor did my dog eat 'em). I wouldn't even eat the cow I ate, if I could grow it's meat in a vat. And I have <em>evolved to eat other intelligences</em>. Growing AIs by competition seems like a very great plan for ensuring unfriendly AI, but even that can fail. (Superhuman AI only needs to divert very little effort to charity to be the best thing ever that happened to us)</p>\n<p>It seems to me that when we try to avoid anthropomorphizing superhuman AI, we animize it, or even bacterio-ize it, seeing it as AI gray goo that certainly do the gray goo kind of thing, worst of all, intelligently.</p>\n<p>Furthermore, the danger implies a huge conjunction of implied assumptions which all have to be true:</p>\n<p>The self improvement must not lead to early AI failure via wireheading, nihilism, or more complex causes (thoroughly confusing itself by discoveries in physics or mathematics, ala MWI and our idea of quantum suicide).</p>\n<p>The AI must not prefer for any reason to keep complex structures that it can't ever restore in the future, over things it can restore.</p>\n<p>The AI must want substantial resources right here right now, and be unwilling to trade even a small fraction of resources or small delay for the preservation of mankind. That leaves me wondering what is exactly this thing which we expect the AI to want the resources for. It can't be anything like quest of knowledge or anything otherwise complex; it got to be some form of paperclips</p>\n<p>At this point, I'm not even sure it is even possible to implement a simple goal that AGI won't find a way to circumvent. We humans do circumvent all of our simple goals: look at birth control, porn, all forms of art, msg in the food, if there's a goal, there's a giant industry providing some ways to satisfy it in unintended way. Okay, don't anthropomorphize, you'd say?</p>\n<p>Add the modifications to the chess board evaluation algorithm to the list of legal moves, and the chess AI will break itself. This goes for any kind of game AI. Nobody has ever implemented an example that won't try to break the goals put in it, if given a chance. Give a theorem prover a chance to edit the axioms, or its truth checker, give the chess AI alteration of board evaluation function as a move, any other example, the AI just breaks itself.</p>\n<p>In light of this, it is much less than certain that 'random' AI which doesn't treat humanity in very special way would substantially hurt humanity.</p>\n<p>Anthropomorphizing is a bad heuristic, no doubt about that, but assuming that the AGI is in every respect <em>opposite</em> of the only known GI, is much worse heuristic. Especially when speaking of neural network, human brain inspired AGIs. I do get a feeling that this is what is going on with the predictions about AIs. Humans have complex value systems, certainly AGI has ultra simple value system. Humans masturbate their minor goals in many ways (including what we call 'sex' but which, in presence of condom, really is not), certainly AGI won't do that. Humans would rather destroy less complex systems, than more complex ones, and are willing to trade some resources for preservation of more complex systems, certainly AGI won't do that. It seems that all the strong beliefs about the AGIs which are popular here are easily predicted as the negation of human qualities. Negation of bias is not absence of bias, it's a worse bias.</p>\n<h4>AI and its discoveries in physics and mathematics<br /></h4>\n<p>We don't know what sorts of physics AI may discover. It's too easy to argue from ignorance that it can't come up with physics where our morals won't make sense. The many worlds interpretation and quantum-suicidal thoughts of <a href=\"http://space.mit.edu/home/tegmark/main_crazy.html#everett\">Max Tegmark</a> should be a cautionary example. The AI that treats us as special and cares only for us will, inevitably, drag us along as it suffers some sort of philosophical crisis from collision of the notions we hard coded into it, and the physics or mathematics it discovered. The AI that doesn't treat us as special, and doesn't hard-code any complex human derived values, may both be better able to survive such shocks to it's value system, and be less likely to involve us in it's solutions.</p>\n<h4>What can we do to avoid stepping onto UFAI when creating FAI<br /></h4>\n<p>As a software developer, I have to say, not much. We are very, very sloppy at writing specifications and code; those of us who believe we are less sloppy, are especially so - ponder this bit of empirical data, the Dunning-Kruger effect.</p>\n<p>The proofs are of limited applicability. We don't know what sort of stuff the discoveries in physics may throw in. We don't know that axiomatic system we use to prove things is consistent - free of internal contradictions - and we can't prove that.</p>\n<p>The automated theorem proving has very limited applicability - to easily provable, low level stuff like meeting of deadlines by a garbage collector or correct operation of an adder inside CPU. Even for the software far simpler than AIs - but more complicated than the examples above, the dominant form of development is 'run and see, if it does not look like it will do what you want, try to fix it'. We can't even write an autopilot that is safe on the first try. And even very simple agents tend to do very odd and unexpected stuff. I'm not saying this from random person perspective. I am currently a game developer, and I used to develop other kinds of software. I write practical software, including practical agents, that work, and have useful real world applications.</p>\n<p>There is a very good chance of blowing up a mine in a minefield, if your mine detector works by hitting the ground. The space near FAI is a minefield of doomsday bombs. (Note, too, the space is multi-dimensional; here are very many ways in which you can step onto a mine, not just north, south, east, and west. The volume of a hypersphere is a vanishing fraction of volume of a cube around that hypersphere, in high number of dimensions; a lot of stuff is counter intuitive)</p>\n<h4>Fermi Paradox</h4>\n<p>We don't see any runaway self sufficient AIs anywhere within observable universe, even though we expect to be able to see them over very big distances. We don't see any FAI assisted galactic civilizations. One possible route is that the civilizations kill themselves before the AI; other route is that the attempted FAIs reliably kill parent civilizations and themselves. Other possibility is that our model of progression of the intelligence is very wrong and the intelligences never do that - they may stay at home, adding qubits, they may suffer some serious philosophy issues over lack of meaning to the existence, or something much more bizarre. How would logic based decider handle a demonstration that even most basic axioms of arithmetic are ultimately self contradictory? (Note that you can't know they aren't). The Fermi paradox raises the probability that there is something very wrong with our visions, and there's a plenty of ways in which it can be wrong.</p>\n<h4>Human biases when processing threats<br /></h4>\n<p>I am not making any strong assertions here to scare you. But evaluate our response to threats - consider the war on terror - update on the biases inherent in the human nature. We are easily swayed by movie plot scenarios, even though those are giant conjunctions. We are easy to scare. When scared, we don't evaluate probabilities correctly. We take the \"crying wolf\" as true because all boys who cried wolf for no reason got eaten, or because we were told so as children. We don't stop and think - is it too dark to see a wolf?. We tend to shoot first and ask questions later. We evolved for very many generations in environment where playing dead quickly makes you dead (on trees) - it is unclear what biases we may have evolved. We seem to have strong bias to act when threatened - cultural or inherited - to 'do something'. Look how much was overspent on war on terror, the money that could've saved far more lives elsewhere, even if the most pessimistic assumptions of terrorism were true. Try to update on the fact that you are running on very flawed hardware that, when threatened, compels you to do something - anything - no matter how justified or not - often to own detriment.</p>\n<p>The universe does not grade for effort, in general.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8T3u3S8vwQycN5SR6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 6, "extendedScore": null, "score": 8.67755577746295e-07, "legacy": true, "legacyId": "14167", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h4 id=\"Abstract_\">Abstract:</h4>\n<p>Nearly-FAIs can be more dangerous than AIs with no attempt at friendliness. The FAI effort needs better argument that the attempt at FAI decreases the risks. We are bad at processing threats rationally, and prone to very bad decisions when threatened, akin to running away from unknown into a minefield.</p>\n<h4 id=\"Nearly_friendly_AIs\">Nearly friendly AIs<br></h4>\n<p>Consider AI that truly loves mankind but decides that all of the mankind must be euthanized like an old, sick dog - due to chain of reasoning too long for us to generate when we test our logic of AI, or even comprehend - and proceeds to make a bliss virus - the virus makes you intensely happy, setting your internal utility to infinity; and keeping it so until you die. It wouldn't even take a very strongly superhuman intelligence to do that kind of thing. Treating life as if it was a disease. It can do so even if it destroys the AI itself. Or consider the FAI that cuts your brain apart to satisfy each hemisphere's slightly different desires. The AI that just wireheads everyone because it figured we all want it (and worst of all it may be correct).</p>\n<p>It seems to me that one can find the true monsters in the design space near to the FAI, and even including the FAIs. And herein lies a great danger: bugged FAIs, the AIs that are close to friendly AI, but are not friendly. It is hard for me to think of a deficiency in friendliness which isn't horrifically unfriendly (restricting to deficiencies that don't break AI).</p>\n<h4 id=\"Should_we_be_so_afraid_of_the_AIs_made_without_attempts_at_friendliness_\">Should we be so afraid of the AIs made without attempts at friendliness?<br></h4>\n<p>We need to keep in mind that we have no solid argument that the AIs written without attempt at friendliness - the AIs that predominantly don't treat mankind in any special way - will necessarily make us extinct.</p>\n<p>We have one example of 'bootstrap' optimization process - evolution - with not a slightest trace of friendliness in it. What did emerge in the end? We assign pretty low utility to nature, but non-zero, and we are willing to trade resources for preservation of nature - see the endangered species list and international treaties on whaling. It is not perfect, but I think it is fair to say that the single example of bootstrap intelligence we got values the complex dynamical processes for what they are, and prefers to obtain resources without disrupting those processes, even if it is slightly more expensive to do so, and is willing to divert small fraction of the global effort towards helping lesser intelligences.</p>\n<p>In light of this, the argument that the AI that is not coded to be friendly is 'almost certainly' going to eat you for the raw resources, seems fairly shaky, especially when applied to irregular AIs such as neural networks, crude simulations of human brain's embryological development, and mind uploads. I didn't eat my cats yet (nor did they eat each other, nor did my dog eat 'em). I wouldn't even eat the cow I ate, if I could grow it's meat in a vat. And I have <em>evolved to eat other intelligences</em>. Growing AIs by competition seems like a very great plan for ensuring unfriendly AI, but even that can fail. (Superhuman AI only needs to divert very little effort to charity to be the best thing ever that happened to us)</p>\n<p>It seems to me that when we try to avoid anthropomorphizing superhuman AI, we animize it, or even bacterio-ize it, seeing it as AI gray goo that certainly do the gray goo kind of thing, worst of all, intelligently.</p>\n<p>Furthermore, the danger implies a huge conjunction of implied assumptions which all have to be true:</p>\n<p>The self improvement must not lead to early AI failure via wireheading, nihilism, or more complex causes (thoroughly confusing itself by discoveries in physics or mathematics, ala MWI and our idea of quantum suicide).</p>\n<p>The AI must not prefer for any reason to keep complex structures that it can't ever restore in the future, over things it can restore.</p>\n<p>The AI must want substantial resources right here right now, and be unwilling to trade even a small fraction of resources or small delay for the preservation of mankind. That leaves me wondering what is exactly this thing which we expect the AI to want the resources for. It can't be anything like quest of knowledge or anything otherwise complex; it got to be some form of paperclips</p>\n<p>At this point, I'm not even sure it is even possible to implement a simple goal that AGI won't find a way to circumvent. We humans do circumvent all of our simple goals: look at birth control, porn, all forms of art, msg in the food, if there's a goal, there's a giant industry providing some ways to satisfy it in unintended way. Okay, don't anthropomorphize, you'd say?</p>\n<p>Add the modifications to the chess board evaluation algorithm to the list of legal moves, and the chess AI will break itself. This goes for any kind of game AI. Nobody has ever implemented an example that won't try to break the goals put in it, if given a chance. Give a theorem prover a chance to edit the axioms, or its truth checker, give the chess AI alteration of board evaluation function as a move, any other example, the AI just breaks itself.</p>\n<p>In light of this, it is much less than certain that 'random' AI which doesn't treat humanity in very special way would substantially hurt humanity.</p>\n<p>Anthropomorphizing is a bad heuristic, no doubt about that, but assuming that the AGI is in every respect <em>opposite</em> of the only known GI, is much worse heuristic. Especially when speaking of neural network, human brain inspired AGIs. I do get a feeling that this is what is going on with the predictions about AIs. Humans have complex value systems, certainly AGI has ultra simple value system. Humans masturbate their minor goals in many ways (including what we call 'sex' but which, in presence of condom, really is not), certainly AGI won't do that. Humans would rather destroy less complex systems, than more complex ones, and are willing to trade some resources for preservation of more complex systems, certainly AGI won't do that. It seems that all the strong beliefs about the AGIs which are popular here are easily predicted as the negation of human qualities. Negation of bias is not absence of bias, it's a worse bias.</p>\n<h4 id=\"AI_and_its_discoveries_in_physics_and_mathematics\">AI and its discoveries in physics and mathematics<br></h4>\n<p>We don't know what sorts of physics AI may discover. It's too easy to argue from ignorance that it can't come up with physics where our morals won't make sense. The many worlds interpretation and quantum-suicidal thoughts of <a href=\"http://space.mit.edu/home/tegmark/main_crazy.html#everett\">Max Tegmark</a> should be a cautionary example. The AI that treats us as special and cares only for us will, inevitably, drag us along as it suffers some sort of philosophical crisis from collision of the notions we hard coded into it, and the physics or mathematics it discovered. The AI that doesn't treat us as special, and doesn't hard-code any complex human derived values, may both be better able to survive such shocks to it's value system, and be less likely to involve us in it's solutions.</p>\n<h4 id=\"What_can_we_do_to_avoid_stepping_onto_UFAI_when_creating_FAI\">What can we do to avoid stepping onto UFAI when creating FAI<br></h4>\n<p>As a software developer, I have to say, not much. We are very, very sloppy at writing specifications and code; those of us who believe we are less sloppy, are especially so - ponder this bit of empirical data, the Dunning-Kruger effect.</p>\n<p>The proofs are of limited applicability. We don't know what sort of stuff the discoveries in physics may throw in. We don't know that axiomatic system we use to prove things is consistent - free of internal contradictions - and we can't prove that.</p>\n<p>The automated theorem proving has very limited applicability - to easily provable, low level stuff like meeting of deadlines by a garbage collector or correct operation of an adder inside CPU. Even for the software far simpler than AIs - but more complicated than the examples above, the dominant form of development is 'run and see, if it does not look like it will do what you want, try to fix it'. We can't even write an autopilot that is safe on the first try. And even very simple agents tend to do very odd and unexpected stuff. I'm not saying this from random person perspective. I am currently a game developer, and I used to develop other kinds of software. I write practical software, including practical agents, that work, and have useful real world applications.</p>\n<p>There is a very good chance of blowing up a mine in a minefield, if your mine detector works by hitting the ground. The space near FAI is a minefield of doomsday bombs. (Note, too, the space is multi-dimensional; here are very many ways in which you can step onto a mine, not just north, south, east, and west. The volume of a hypersphere is a vanishing fraction of volume of a cube around that hypersphere, in high number of dimensions; a lot of stuff is counter intuitive)</p>\n<h4 id=\"Fermi_Paradox\">Fermi Paradox</h4>\n<p>We don't see any runaway self sufficient AIs anywhere within observable universe, even though we expect to be able to see them over very big distances. We don't see any FAI assisted galactic civilizations. One possible route is that the civilizations kill themselves before the AI; other route is that the attempted FAIs reliably kill parent civilizations and themselves. Other possibility is that our model of progression of the intelligence is very wrong and the intelligences never do that - they may stay at home, adding qubits, they may suffer some serious philosophy issues over lack of meaning to the existence, or something much more bizarre. How would logic based decider handle a demonstration that even most basic axioms of arithmetic are ultimately self contradictory? (Note that you can't know they aren't). The Fermi paradox raises the probability that there is something very wrong with our visions, and there's a plenty of ways in which it can be wrong.</p>\n<h4 id=\"Human_biases_when_processing_threats\">Human biases when processing threats<br></h4>\n<p>I am not making any strong assertions here to scare you. But evaluate our response to threats - consider the war on terror - update on the biases inherent in the human nature. We are easily swayed by movie plot scenarios, even though those are giant conjunctions. We are easy to scare. When scared, we don't evaluate probabilities correctly. We take the \"crying wolf\" as true because all boys who cried wolf for no reason got eaten, or because we were told so as children. We don't stop and think - is it too dark to see a wolf?. We tend to shoot first and ask questions later. We evolved for very many generations in environment where playing dead quickly makes you dead (on trees) - it is unclear what biases we may have evolved. We seem to have strong bias to act when threatened - cultural or inherited - to 'do something'. Look how much was overspent on war on terror, the money that could've saved far more lives elsewhere, even if the most pessimistic assumptions of terrorism were true. Try to update on the fact that you are running on very flawed hardware that, when threatened, compels you to do something - anything - no matter how justified or not - often to own detriment.</p>\n<p>The universe does not grade for effort, in general.</p>", "sections": [{"title": "Abstract:", "anchor": "Abstract_", "level": 1}, {"title": "Nearly friendly AIs", "anchor": "Nearly_friendly_AIs", "level": 1}, {"title": "Should we be so afraid of the AIs made without attempts at friendliness?", "anchor": "Should_we_be_so_afraid_of_the_AIs_made_without_attempts_at_friendliness_", "level": 1}, {"title": "AI and its discoveries in physics and mathematics", "anchor": "AI_and_its_discoveries_in_physics_and_mathematics", "level": 1}, {"title": "What can we do to avoid stepping onto UFAI when creating FAI", "anchor": "What_can_we_do_to_avoid_stepping_onto_UFAI_when_creating_FAI", "level": 1}, {"title": "Fermi Paradox", "anchor": "Fermi_Paradox", "level": 1}, {"title": "Human biases when processing threats", "anchor": "Human_biases_when_processing_threats", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "49 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-18T13:02:09.744Z", "modifiedAt": null, "url": null, "title": "The Best Comments Ever", "slug": "the-best-comments-ever", "viewCount": null, "lastCommentedAt": "2012-04-09T17:17:08.097Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Grognor", "createdAt": "2011-01-31T02:54:34.463Z", "isAdmin": false, "displayName": "Grognor"}, "userId": "LoykQRMTxJFxwwdPy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6PsPs2D6i39dGpRqj/the-best-comments-ever", "pageUrlRelative": "/posts/6PsPs2D6i39dGpRqj/the-best-comments-ever", "linkUrl": "https://www.lesswrong.com/posts/6PsPs2D6i39dGpRqj/the-best-comments-ever", "postedAtFormatted": "Sunday, March 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Best%20Comments%20Ever&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Best%20Comments%20Ever%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6PsPs2D6i39dGpRqj%2Fthe-best-comments-ever%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Best%20Comments%20Ever%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6PsPs2D6i39dGpRqj%2Fthe-best-comments-ever", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6PsPs2D6i39dGpRqj%2Fthe-best-comments-ever", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 436, "htmlBody": "<p>Of the title of this discussion post, we already have an approximation in the most voted-for lists for <a href=\"/topcomments/\">Main</a> and <a href=\"/r/discussion/topcomments/\">Discussion</a>. There are many problems with this metric, however, a subset of which are:</p>\n<ul>\n<li>Both sections are cluttered. The top comments for Main are full of rationality quotes (which are better accessed elsewhere), and the top comments for Discussion are full of polls. (Can we please have a non-stupid way of putting polls in comments?)</li>\n<li>Both sections are biased by exposure. A comment that a lot of people see generally gets more karma than comments which not many people see. As Less Wrong's growth rate increases, and as time goes by, this will increasingly bias these sections toward newer comments. Also, comments made by well-known LWers will be seen more often and correspondingly upvoted more.</li>\n<li>Joke comments oftentimes get a lot more comments than insightful comments.</li>\n<li>Entire threads can have weird voting patterns that don't match quality.</li>\n<li>These sections list number of upvotes, so extremely controversial comments appear in between unanimously good ones. (May not actually be a problem at all.)</li>\n<li>People use karma as behavior reinforcers, so comments like \"I'll transcribe this\" get lots of votes.</li>\n<li>Dozens more little things I won't try to list</li>\n</ul>\n<div>So instead of lists of comments that made a lot of Less Wrong accounts go \"eh, have a karma\", let's have a list of Less Wrong comments that are <em>so good you'll never forget them, and you wish you could do more than merely upvote.</em></div>\n<div>This list will have its own problems, like being biased toward memorable comments, but I hope the two lists will complement each other and whatever's missing isn't terribly important.</div>\n<div>I got the idea from a note about how \"Discussion is better than Main, comments are better than Discussion\" because social norms prevent certain types of Main post from being made.</div>\n<div>Anyway, here are some proposed rules, of which all but the first two are mere suggestions:</div>\n<div>\n<ul>\n<li>Don't post your own comments.</li>\n<li>Don't post more than one comment in this thread. If you find more other-person comments you want to add, edit yours to include them.</li>\n<li>Say why you think the comment is good, even if it's only a line.</li>\n<li>Bonus points for finding&nbsp; <em>very old</em>&nbsp; comments, or those from threads that got very little exposure.</li>\n<li>No joke comments or comments that solely quote people.</li>\n<li>Don't retrieve your comment from the list of top comments.</li>\n</ul>\n<p>I like my idea. Let's see how well it works.</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "a3W2TSzPuxKr3Hm9j": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6PsPs2D6i39dGpRqj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 27, "extendedScore": null, "score": 6.4e-05, "legacy": true, "legacyId": "14177", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-18T19:25:05.666Z", "modifiedAt": null, "url": null, "title": "Meaning and having names for things vs knowing how they work", "slug": "meaning-and-having-names-for-things-vs-knowing-how-they-work", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:26.914Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Voltairina", "createdAt": "2012-02-24T04:00:28.314Z", "isAdmin": false, "displayName": "Voltairina"}, "userId": "a6hK33SK4uawjaL9h", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d3Cs2yG6ShMWXHQM7/meaning-and-having-names-for-things-vs-knowing-how-they-work", "pageUrlRelative": "/posts/d3Cs2yG6ShMWXHQM7/meaning-and-having-names-for-things-vs-knowing-how-they-work", "linkUrl": "https://www.lesswrong.com/posts/d3Cs2yG6ShMWXHQM7/meaning-and-having-names-for-things-vs-knowing-how-they-work", "postedAtFormatted": "Sunday, March 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meaning%20and%20having%20names%20for%20things%20vs%20knowing%20how%20they%20work&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeaning%20and%20having%20names%20for%20things%20vs%20knowing%20how%20they%20work%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd3Cs2yG6ShMWXHQM7%2Fmeaning-and-having-names-for-things-vs-knowing-how-they-work%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meaning%20and%20having%20names%20for%20things%20vs%20knowing%20how%20they%20work%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd3Cs2yG6ShMWXHQM7%2Fmeaning-and-having-names-for-things-vs-knowing-how-they-work", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd3Cs2yG6ShMWXHQM7%2Fmeaning-and-having-names-for-things-vs-knowing-how-they-work", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 876, "htmlBody": "<p>I saw <a title=\"The AtomicMoose's Comment\" href=\"/lw/12s/the_strangest_thing_an_ai_could_tell_you/xgv\">this post</a> on a discussion thread earlier:</p>\n<blockquote>\n<p>We routinely deny, or act in spite of, inconvenient truths. We can recognize that there is no meaning to love beyond evolutionary and chemical triggers, yet we fight for it just as fervently. Nihilists write books about nihilism despite it's admitted pointlessness. We are as blind as our very genes which multiply and propagate themselves despite our executioner sun which grows daily above our heads, eventually to the point of consuming everything we know. By the very act of living and pursuing human concocted dreams and desires, we are in a constant denial of our situation.</p>\n</blockquote>\n<p>I wonder a few things. Is this sort of experience related to the spoiler effect people claim to experience when they know the ending of a good book in advance? If so, <a href=\"http://www.wired.com/wiredscience/2011/08/spoilers-dont-spoil-anything/\">this</a> (paper itself <a href=\"http://psy2.ucsd.edu/~nchristenfeld/Publications_files/Spoilers.pdf\">here</a>, thanks <a href=\"/user/gwern/\">gwern</a>) might be relevant. Its a wired article, referring to a study where it appeared the opposite of the spoiler effect is actually true - knowing the ending in advance improves people's subjective experience of a good short story. Do people subjectively experience the spoiler effect anyways? I'm wondering if it isn't really something like this:</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.A person reads a short story they enjoy with a spoiler in mind.</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.A person anticipates the ending experience and ramp up for it</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.A person enjoys it more than if unspoiled.</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4.person carries a \"spoiler effect\" meme, and engages in the mind projection fallacy to imagine what life would have&nbsp;&nbsp; been like had they been able to read the short story without a spoiler.</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.They decide to accept the validity of this projection, and this reinforces the meme, because they experience annoyance with their experience of the story compared to the imagined experience of the unspoiled story.</p>\n<p>I also wonder whether there's a way to modify the mind projection fallacy to set up a feedback loop? Imagining how wonderful it will be to be in good shape while you're exercising, maybe, then fantasising about being in good shape, then thinking how much better actually being in shape will be than fantasising, then fantasising again with the updated impression that the real thing was better, so your fantasy of the thing becomes better to compensate, then reminding yourself that the fantasy is not as good...</p>\n<p>Another explanation for why it might seem better not to know things seems to be related to suspended disbelief. Maybe whatever the real explanation is, it won't match up to our imagined explanation, or the joy of discovering that explanation independently (which at the very least includes knowing the answer's 'aha' moment plus a bit of pride about finding it yourself). If its the latter, you'd think everyone would have some motivation to get out there and do science and it might be wise to be more forgiving of people who do science, although prolific scientists might seem greedy to the rest of us, I guess. If its the former (not matching the imagined explanation), like, \"its more fun to imagine fairies did it\", that'd be the suspended disbelief instance. Like, knowing how the props work in a movie might be distracting to the experience of feeling like the movie events are real while watching them. Maybe it ruins the 'escapism' of it for some people who don't compartmentalise the explanation vs. the experience well. I've long been a fan of doing both - picking apart the special effects *while* being immersed in the story and feeling as though it is true. But I wonder if I might cry at sad parts or get more shocked at horrific parts more if on some level I didn't know the way it worked. Spoiler effect? Mind projection fallacy? Narrative disruption? All, some, or none of the above? Like, the <a href=\"/lw/20/the_apologist_and_the_revolutionary/\">apologist</a> is facilitating your enjoyment of a movie, and the revolutionary is there fighting for dominance. It makes me wonder whether people who are sleep deprived are more likely not to want to know how things work.</p>\n<p>As a meme, the idea that knowing how things work makes them less fun seems to be a useful protective meme for bigger plexes like religion and belief in the supernatural. It could be that there are just so many people who hold these plexes that the meme gets replicated a lot as a byproduct, without having much survival value on its own. Like a beneficial (to the memeplex) virus.</p>\n<p>On a related note, I just saw a meme (the picture-with-text-on-it-kind) about <a href=\"https://encrypted-tbn2.google.com/images?q=tbn:ANd9GcSY77WErLQXI20ATxbCpSnOhBZQeRFClBRbcyLq4wcjOADCXkyB\">mamavirus</a> and its hitchiking virus, Sputnik, the other day. I was shocked to learn something new from a meme. Kind of wondering about the educational value of weird memes now.</p>\n<p>Oh just realised I never thought about the having-a-name-for-a-thing-means-it-has-an-atomic-essence-or-associated-meaning-unit vs knowing what it was made of sort of alluded to by the title much. I am guessing there are more or less \"atomic\" sensations like redness that can't be reduced in more elementary terms, but that this doesn't mean they're qualia, any more than an intelligent vcr might have any lower level reductive explanation of the experience <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">from inside the algorithm</a> of having its \"eject\" button pushed in terms of other experiences, but does not necessarily exist in some dualistic mindspace interacting via its mechanical <a href=\"http://plato.stanford.edu/entries/pineal-gland/\">pineal gland</a> as a result.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d3Cs2yG6ShMWXHQM7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 1, "extendedScore": null, "score": 8.679744500837053e-07, "legacy": true, "legacyId": "14179", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZiQqsgGX6a42Sfpii", "yA4gF5KrboK2m2Xu7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-18T21:02:32.326Z", "modifiedAt": null, "url": null, "title": "6 Tips for Productive Arguments", "slug": "6-tips-for-productive-arguments", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:08.776Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JSND48qS5XTMFuZo8/6-tips-for-productive-arguments", "pageUrlRelative": "/posts/JSND48qS5XTMFuZo8/6-tips-for-productive-arguments", "linkUrl": "https://www.lesswrong.com/posts/JSND48qS5XTMFuZo8/6-tips-for-productive-arguments", "postedAtFormatted": "Sunday, March 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%206%20Tips%20for%20Productive%20Arguments&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A6%20Tips%20for%20Productive%20Arguments%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJSND48qS5XTMFuZo8%2F6-tips-for-productive-arguments%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=6%20Tips%20for%20Productive%20Arguments%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJSND48qS5XTMFuZo8%2F6-tips-for-productive-arguments", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJSND48qS5XTMFuZo8%2F6-tips-for-productive-arguments", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1392, "htmlBody": "<p>We've all had arguments that seemed like a complete waste of time in retrospect. But at the same time, arguments (between scientists, policy analysts, and others) play a critical part in moving society forward. You can imagine how lousy things would be if no one ever engaged those who disagreed with them.</p>\n<p>This is a list of tips for having \"productive\" arguments. For the purposes of this list, \"productive\" means improving the accuracy of at least one person's views on some important topic. By this definition, arguments where no one changes their mind are unproductive. So are arguments about unimportant topics like which Pink Floyd album is the best.</p>\n<p>Why do we want productive arguments? Same reason we want Wikipedia: so people are more knowledgeable. And just like the case of Wikipedia, there is a strong selfish imperative here: arguing <em>can</em> make you more knowledgeable, if you're willing to change your mind when another arguer has better points.</p>\n<p><a id=\"more\"></a></p>\n<p>Arguments can also be <em>negatively</em> productive if everyone moves <em>further</em> from the truth on net. This could happen if, for example, the truth was somewhere in between two arguers, but they both left the argument even more sure of themselves.</p>\n<p>These tips are derived from my personal experience arguing.</p>\n<p>&nbsp;</p>\n<h4>Keep it Friendly</h4>\n<p>Probably the biggest barrier to productive arguments is the desire of arguers to save face and avoid publicly admitting they were wrong. Obviously, it's hard for anyone's views to get more accurate if no one's views ever change.</p>\n<div>This problem is&nbsp;exacerbated when arguers disparage one another. If you rebuke a fellow arguer, you're setting yourself up as their enemy. Admitting they were wrong would then mean giving in to an enemy. And no one likes to do that.</div>\n<div>You may also find it difficult to carefully reconsider your&nbsp;<em>own&nbsp;</em>views&nbsp;after having ridiculed or berated someone who disagrees. I know I have in the past.</div>\n<div>Both of these tendencies hurt argument productivity. To make arguments productive:</div>\n<div>\n<ul>\n<li>Keep things warm and collegial. Just because your ideas are in violent disagreement doesn't mean you have to disagree violently as people. Stay classy.</li>\n<li>To the greatest extent possible, uphold the&nbsp;<a href=\"/r/discussion/lw/57c/it_is_ok_to_publicly_make_a_mistake_and_change/\">social norm</a>&nbsp;that no one will lose face for publicly changing their mind.</li>\n<li>If you're on a community-moderated forum like Less Wrong, don't downvote something unless you think the person who wrote it is being a bad forum citizen (ex: spam or unprovoked insults). Upvotes already provide plenty of information about how comments and submissions should be sorted. (It's probably safe to assume that a new Less Wrong user who sees their first comment modded below zero will decide we're all jerks and never come back. And if new users aren't coming back, we'll have a hard time&nbsp;<a href=\"/lw/1e/raising_the_sanity_waterline/\">raising the sanity waterline</a>&nbsp;much.)</li>\n<li>Err on the side of understating your disagreement, e.g. \"I'm not persuaded that...\" or \"I agree that x is true; I'm not as sure that...\" or \"It seems to me...\"</li>\n<li>If you notice some hypocrisy, bias, or general deficiency on the part of another arguer, think extremely carefully before bringing it up while the argument is still in progress.</li>\n</ul>\n</div>\n<div>In a good argument, all parties will be&nbsp;<a href=\"/lw/4ku/use_curiosity/\">curious</a>&nbsp;about what's really going on. But curiosity and animosity are mutually incompatible emotions. Don't impede the collective search for truth through rudeness or hostility.</div>\n<p>&nbsp;</p>\n<h4>Inquire about Implausible-Sounding Assertions Before Expressing an Opinion</h4>\n<div>It's easy to respond to a statement you think is obviously wrong with with an immediate denial or attack. But this is also a good way to keep yourself from learning anything.</div>\n<p>If someone suggests something you find implausible, start asking friendly questions to get them to clarify and justify their statement. If their reasoning seems genuinely bad, you can refute it then.</p>\n<p>As a bonus, doing nothing but ask questions can be a good way to save face if the implausible assertion-maker turns out to be right.</p>\n<p>Be careful about rejecting highly implausible ideas out of hand. Ideally, you want your rationality to be a level where even if you started out with a crazy belief like Scientology, you'd still be able to get rid of it. But for a Scientologist to berid themselves of Scientology, they have to consider ideas that initially seen extremely unlikely.</p>\n<p>It's been argued that many mainstream skeptics aren't really that good at critically evaluating ideas, just&nbsp;<a href=\"/lw/1ww/undiscriminating_skepticism/\">dismissing ones that seem implausible</a>.</p>\n<p>&nbsp;</p>\n<h4>Isolate Specific Points of Disagreement</h4>\n<p>Stick to one topic at a time, until someone changes their mind or the topic is declared not worth pursuing. If your discussion constantly jumps from one point of disagreement to another, reaching consensus on anything will be difficult.</p>\n<p>You can use hypothetical-oriented thinking like&nbsp;<a href=\"http://www.stat.yale.edu/Courses/1997-98/101/condprob.htm\">conditional probabilities</a>&nbsp;and&nbsp;<a href=\"/lw/2k/the_least_convenient_possible_world/\">the least convenient possible world</a>&nbsp;to figure out exactly what it is you disagree on with regard to a given topic. Once you've creatively helped yourself or another arguer clarify beliefs, sharing intuitions on specific \"irreducible\" assertions or anticipated outcomes that aren't easily decomposed can improve both of your probability estimates.</p>\n<p>&nbsp;</p>\n<h4>Don't Straw Man Fellow Arguers, Steel Man Them Instead</h4>\n<p>You might think that a productive argument is one where the smartest person wins, but that's not always the case. Smart people can be wrong too. And a smart person successfully convincing less intelligent folks of their delusion counts as a negatively productive argument (see definition above).</p>\n<p>Play for all sides, in case you're the smartest person in the argument.</p>\n<p>Rewrite fellow arguers' arguments so they're even stronger, and think of new ones. Arguments for new positions, even&mdash;they don't have&nbsp;<em>anyone&nbsp;</em>playing for them. And if you end up convincing yourself of something you didn't previously believe, so much the better.</p>\n<p>&nbsp;</p>\n<h4>If You See an Opportunity To Improve the Accuracy of Your Knowledge, Take It!</h4>\n<p>This is often called losing an argument, but you're actually the winner: you and your arguing partner both invested time to argue, but you were the only one who received significantly improved knowledge.</p>\n<div>I'm not a Christian, but I definitely want to know if Christianity is true so I can stop taking the Lord's name in vain and hopefully get to heaven. (Please don't contact me about Christianity though, I've already thought about it a lot and judged it&nbsp;<a href=\"http://godisimaginary.com/\">too improbable</a>&nbsp;to be worth spending additional time thinking about.) Point is, it's hard to see how having more accurate knowledge could&nbsp;<em>hurt</em>.</div>\n<p>If you're worried about losing face or seeing your coalition (research group, political party, etc.) diminish in importance from you admitting that you were wrong, here are some ideas:</p>\n<ul>\n<li>Say \"I'll think about it\". Most people will quiet down at this point without any gloating.</li>\n<li>Just keep arguing, making a mental note that your mind has changed.</li>\n<li>Redirect the conversation, pretend to lose interest, pretend you have no time to continue arguing, etc.</li>\n</ul>\n<div>If necessary, you can make up a story about how something else changed your mind later.</div>\n<p>Some of these techniques may seem dodgy, and honestly I think you'll usually do better by explaining what actually changed your mind. But they're a small price to pay for more accurate knowledge. Better to tell unimportant false statements to others than important false statements to yourself.</p>\n<p>&nbsp;</p>\n<h4>Have Low \"Belief Inertia\"</h4>\n<p>It's actually pretty rare that the evidence that you're wrong comes suddenly&mdash;usually you can see things turning against you. As an advanced move, cultivate the ability to update your degree of certainty in real time to new arguments, and tell fellow arguers if you find an argument of theirs persuasive. This can actually be a good way to make friends. It also encourages other arguers to share additional arguments with you, which could be valuable data.</p>\n<p>One psychologist I agree with suggested that people ask</p>\n<address> \n<ul>\n<li><span style=\"font-style: normal;\">\"Does the evidence&nbsp;<strong>allow&nbsp;</strong>me to believe?\" when evaluating what they already believe, but</span></li>\n<li><span style=\"font-style: normal;\">\"Does the evidence&nbsp;<strong>compel&nbsp;</strong>me to believe?\" when evaluating a claim incompatible with their current beliefs.</span></li>\n</ul>\n</address>\n<p>If folks don't have to drag you around like this for you to change your mind, you don't actually lose much face. It's only long-overdue capitulations that result in significant face loss. And the longer you put your capitulation off, the worse things get. Quickly updating in response to new evidence seems to&nbsp;<em>preserve&nbsp;</em>face in my experience.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>If your belief inertia is low and you steel-man everything, you'll reach the super chill state of not having a \"side\" in any given argument. You'll play for all sides and you won't care who wins. You'll have achieved equanimity, content with the world as it actually is, not how you wish it was.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"wzgcQCrwKfETcBpR9": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JSND48qS5XTMFuZo8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 46, "baseScore": 39, "extendedScore": null, "score": 7.9e-05, "legacy": true, "legacyId": "14171", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>We've all had arguments that seemed like a complete waste of time in retrospect. But at the same time, arguments (between scientists, policy analysts, and others) play a critical part in moving society forward. You can imagine how lousy things would be if no one ever engaged those who disagreed with them.</p>\n<p>This is a list of tips for having \"productive\" arguments. For the purposes of this list, \"productive\" means improving the accuracy of at least one person's views on some important topic. By this definition, arguments where no one changes their mind are unproductive. So are arguments about unimportant topics like which Pink Floyd album is the best.</p>\n<p>Why do we want productive arguments? Same reason we want Wikipedia: so people are more knowledgeable. And just like the case of Wikipedia, there is a strong selfish imperative here: arguing <em>can</em> make you more knowledgeable, if you're willing to change your mind when another arguer has better points.</p>\n<p><a id=\"more\"></a></p>\n<p>Arguments can also be <em>negatively</em> productive if everyone moves <em>further</em> from the truth on net. This could happen if, for example, the truth was somewhere in between two arguers, but they both left the argument even more sure of themselves.</p>\n<p>These tips are derived from my personal experience arguing.</p>\n<p>&nbsp;</p>\n<h4 id=\"Keep_it_Friendly\">Keep it Friendly</h4>\n<p>Probably the biggest barrier to productive arguments is the desire of arguers to save face and avoid publicly admitting they were wrong. Obviously, it's hard for anyone's views to get more accurate if no one's views ever change.</p>\n<div>This problem is&nbsp;exacerbated when arguers disparage one another. If you rebuke a fellow arguer, you're setting yourself up as their enemy. Admitting they were wrong would then mean giving in to an enemy. And no one likes to do that.</div>\n<div>You may also find it difficult to carefully reconsider your&nbsp;<em>own&nbsp;</em>views&nbsp;after having ridiculed or berated someone who disagrees. I know I have in the past.</div>\n<div>Both of these tendencies hurt argument productivity. To make arguments productive:</div>\n<div>\n<ul>\n<li>Keep things warm and collegial. Just because your ideas are in violent disagreement doesn't mean you have to disagree violently as people. Stay classy.</li>\n<li>To the greatest extent possible, uphold the&nbsp;<a href=\"/r/discussion/lw/57c/it_is_ok_to_publicly_make_a_mistake_and_change/\">social norm</a>&nbsp;that no one will lose face for publicly changing their mind.</li>\n<li>If you're on a community-moderated forum like Less Wrong, don't downvote something unless you think the person who wrote it is being a bad forum citizen (ex: spam or unprovoked insults). Upvotes already provide plenty of information about how comments and submissions should be sorted. (It's probably safe to assume that a new Less Wrong user who sees their first comment modded below zero will decide we're all jerks and never come back. And if new users aren't coming back, we'll have a hard time&nbsp;<a href=\"/lw/1e/raising_the_sanity_waterline/\">raising the sanity waterline</a>&nbsp;much.)</li>\n<li>Err on the side of understating your disagreement, e.g. \"I'm not persuaded that...\" or \"I agree that x is true; I'm not as sure that...\" or \"It seems to me...\"</li>\n<li>If you notice some hypocrisy, bias, or general deficiency on the part of another arguer, think extremely carefully before bringing it up while the argument is still in progress.</li>\n</ul>\n</div>\n<div>In a good argument, all parties will be&nbsp;<a href=\"/lw/4ku/use_curiosity/\">curious</a>&nbsp;about what's really going on. But curiosity and animosity are mutually incompatible emotions. Don't impede the collective search for truth through rudeness or hostility.</div>\n<p>&nbsp;</p>\n<h4 id=\"Inquire_about_Implausible_Sounding_Assertions_Before_Expressing_an_Opinion\">Inquire about Implausible-Sounding Assertions Before Expressing an Opinion</h4>\n<div>It's easy to respond to a statement you think is obviously wrong with with an immediate denial or attack. But this is also a good way to keep yourself from learning anything.</div>\n<p>If someone suggests something you find implausible, start asking friendly questions to get them to clarify and justify their statement. If their reasoning seems genuinely bad, you can refute it then.</p>\n<p>As a bonus, doing nothing but ask questions can be a good way to save face if the implausible assertion-maker turns out to be right.</p>\n<p>Be careful about rejecting highly implausible ideas out of hand. Ideally, you want your rationality to be a level where even if you started out with a crazy belief like Scientology, you'd still be able to get rid of it. But for a Scientologist to berid themselves of Scientology, they have to consider ideas that initially seen extremely unlikely.</p>\n<p>It's been argued that many mainstream skeptics aren't really that good at critically evaluating ideas, just&nbsp;<a href=\"/lw/1ww/undiscriminating_skepticism/\">dismissing ones that seem implausible</a>.</p>\n<p>&nbsp;</p>\n<h4 id=\"Isolate_Specific_Points_of_Disagreement\">Isolate Specific Points of Disagreement</h4>\n<p>Stick to one topic at a time, until someone changes their mind or the topic is declared not worth pursuing. If your discussion constantly jumps from one point of disagreement to another, reaching consensus on anything will be difficult.</p>\n<p>You can use hypothetical-oriented thinking like&nbsp;<a href=\"http://www.stat.yale.edu/Courses/1997-98/101/condprob.htm\">conditional probabilities</a>&nbsp;and&nbsp;<a href=\"/lw/2k/the_least_convenient_possible_world/\">the least convenient possible world</a>&nbsp;to figure out exactly what it is you disagree on with regard to a given topic. Once you've creatively helped yourself or another arguer clarify beliefs, sharing intuitions on specific \"irreducible\" assertions or anticipated outcomes that aren't easily decomposed can improve both of your probability estimates.</p>\n<p>&nbsp;</p>\n<h4 id=\"Don_t_Straw_Man_Fellow_Arguers__Steel_Man_Them_Instead\">Don't Straw Man Fellow Arguers, Steel Man Them Instead</h4>\n<p>You might think that a productive argument is one where the smartest person wins, but that's not always the case. Smart people can be wrong too. And a smart person successfully convincing less intelligent folks of their delusion counts as a negatively productive argument (see definition above).</p>\n<p>Play for all sides, in case you're the smartest person in the argument.</p>\n<p>Rewrite fellow arguers' arguments so they're even stronger, and think of new ones. Arguments for new positions, even\u2014they don't have&nbsp;<em>anyone&nbsp;</em>playing for them. And if you end up convincing yourself of something you didn't previously believe, so much the better.</p>\n<p>&nbsp;</p>\n<h4 id=\"If_You_See_an_Opportunity_To_Improve_the_Accuracy_of_Your_Knowledge__Take_It_\">If You See an Opportunity To Improve the Accuracy of Your Knowledge, Take It!</h4>\n<p>This is often called losing an argument, but you're actually the winner: you and your arguing partner both invested time to argue, but you were the only one who received significantly improved knowledge.</p>\n<div>I'm not a Christian, but I definitely want to know if Christianity is true so I can stop taking the Lord's name in vain and hopefully get to heaven. (Please don't contact me about Christianity though, I've already thought about it a lot and judged it&nbsp;<a href=\"http://godisimaginary.com/\">too improbable</a>&nbsp;to be worth spending additional time thinking about.) Point is, it's hard to see how having more accurate knowledge could&nbsp;<em>hurt</em>.</div>\n<p>If you're worried about losing face or seeing your coalition (research group, political party, etc.) diminish in importance from you admitting that you were wrong, here are some ideas:</p>\n<ul>\n<li>Say \"I'll think about it\". Most people will quiet down at this point without any gloating.</li>\n<li>Just keep arguing, making a mental note that your mind has changed.</li>\n<li>Redirect the conversation, pretend to lose interest, pretend you have no time to continue arguing, etc.</li>\n</ul>\n<div>If necessary, you can make up a story about how something else changed your mind later.</div>\n<p>Some of these techniques may seem dodgy, and honestly I think you'll usually do better by explaining what actually changed your mind. But they're a small price to pay for more accurate knowledge. Better to tell unimportant false statements to others than important false statements to yourself.</p>\n<p>&nbsp;</p>\n<h4 id=\"Have_Low__Belief_Inertia_\">Have Low \"Belief Inertia\"</h4>\n<p>It's actually pretty rare that the evidence that you're wrong comes suddenly\u2014usually you can see things turning against you. As an advanced move, cultivate the ability to update your degree of certainty in real time to new arguments, and tell fellow arguers if you find an argument of theirs persuasive. This can actually be a good way to make friends. It also encourages other arguers to share additional arguments with you, which could be valuable data.</p>\n<p>One psychologist I agree with suggested that people ask</p>\n<address> \n<ul>\n<li><span style=\"font-style: normal;\">\"Does the evidence&nbsp;<strong>allow&nbsp;</strong>me to believe?\" when evaluating what they already believe, but</span></li>\n<li><span style=\"font-style: normal;\">\"Does the evidence&nbsp;<strong>compel&nbsp;</strong>me to believe?\" when evaluating a claim incompatible with their current beliefs.</span></li>\n</ul>\n</address>\n<p>If folks don't have to drag you around like this for you to change your mind, you don't actually lose much face. It's only long-overdue capitulations that result in significant face loss. And the longer you put your capitulation off, the worse things get. Quickly updating in response to new evidence seems to&nbsp;<em>preserve&nbsp;</em>face in my experience.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>If your belief inertia is low and you steel-man everything, you'll reach the super chill state of not having a \"side\" in any given argument. You'll play for all sides and you won't care who wins. You'll have achieved equanimity, content with the world as it actually is, not how you wish it was.</p>", "sections": [{"title": "Keep it Friendly", "anchor": "Keep_it_Friendly", "level": 1}, {"title": "Inquire about Implausible-Sounding Assertions Before Expressing an Opinion", "anchor": "Inquire_about_Implausible_Sounding_Assertions_Before_Expressing_an_Opinion", "level": 1}, {"title": "Isolate Specific Points of Disagreement", "anchor": "Isolate_Specific_Points_of_Disagreement", "level": 1}, {"title": "Don't Straw Man Fellow Arguers, Steel Man Them Instead", "anchor": "Don_t_Straw_Man_Fellow_Arguers__Steel_Man_Them_Instead", "level": 1}, {"title": "If You See an Opportunity To Improve the Accuracy of Your Knowledge, Take It!", "anchor": "If_You_See_an_Opportunity_To_Improve_the_Accuracy_of_Your_Knowledge__Take_It_", "level": 1}, {"title": "Have Low \"Belief Inertia\"", "anchor": "Have_Low__Belief_Inertia_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "122 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 122, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QmqP8FeQix5ebseyF", "XqmjdBKa4ZaXJtNmf", "WrSe4aB8sWBy3Nphm", "Jko7pt7MwwTBrfG3A", "neQ7eXuaXpiYw7SBy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-18T22:51:47.151Z", "modifiedAt": null, "url": null, "title": "Experience with Lumosity?", "slug": "experience-with-lumosity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:27.107Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AShepard", "createdAt": "2009-12-22T18:27:50.792Z", "isAdmin": false, "displayName": "AShepard"}, "userId": "SxCHDrBhMCCdwMJrK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/enXEbnP8TXTFbMSMS/experience-with-lumosity", "pageUrlRelative": "/posts/enXEbnP8TXTFbMSMS/experience-with-lumosity", "linkUrl": "https://www.lesswrong.com/posts/enXEbnP8TXTFbMSMS/experience-with-lumosity", "postedAtFormatted": "Sunday, March 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Experience%20with%20Lumosity%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExperience%20with%20Lumosity%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FenXEbnP8TXTFbMSMS%2Fexperience-with-lumosity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Experience%20with%20Lumosity%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FenXEbnP8TXTFbMSMS%2Fexperience-with-lumosity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FenXEbnP8TXTFbMSMS%2Fexperience-with-lumosity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 60, "htmlBody": "<p>I just saw a commercial for <a href=\"http://lumosity.com\">Lumosity</a>, which is a mental-skills training website. It seems like something that someone on LessWrong would have tried, but some googling of the site turns up only some passing mentions. Has anyone actually signed up and used it? Have you had results, and are they worth the subscription cost? (~$5-15/month, depending on subscription length).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "enXEbnP8TXTFbMSMS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 8.680589724907785e-07, "legacy": true, "legacyId": "14180", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-19T03:13:45.486Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Hand vs. Fingers", "slug": "seq-rerun-hand-vs-fingers", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cSweDYjvsJWssoAFC/seq-rerun-hand-vs-fingers", "pageUrlRelative": "/posts/cSweDYjvsJWssoAFC/seq-rerun-hand-vs-fingers", "linkUrl": "https://www.lesswrong.com/posts/cSweDYjvsJWssoAFC/seq-rerun-hand-vs-fingers", "postedAtFormatted": "Monday, March 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Hand%20vs.%20Fingers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Hand%20vs.%20Fingers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcSweDYjvsJWssoAFC%2Fseq-rerun-hand-vs-fingers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Hand%20vs.%20Fingers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcSweDYjvsJWssoAFC%2Fseq-rerun-hand-vs-fingers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcSweDYjvsJWssoAFC%2Fseq-rerun-hand-vs-fingers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 184, "htmlBody": "<p>Today's post, <a href=\"/lw/p2/hand_vs_fingers/\">Hand vs. Fingers</a> was originally published on 30 March 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Hand_vs._Fingers\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>When you pick up a cup of water, is it your hand that picks it up, or is it your fingers, thumb, and palm working together? Just because something can be reduced to smaller parts doesn't mean that the original thing doesn't exist.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/axq/seq_rerun_initiation_ceremony/\">Initiation Ceremony</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cSweDYjvsJWssoAFC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 8.681661216244345e-07, "legacy": true, "legacyId": "14199", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KmghfjH6RgXvoKruJ", "nnvu8F2DMartEaZvC", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-19T03:39:42.768Z", "modifiedAt": null, "url": null, "title": "Suggestions on tech device/gear purchasing?", "slug": "suggestions-on-tech-device-gear-purchasing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:44.906Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jwhendy", "createdAt": "2011-01-04T19:53:21.160Z", "isAdmin": false, "displayName": "jwhendy"}, "userId": "ZaJctSZkCvg7qvSEC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SjpJbvanCTMF9Pow6/suggestions-on-tech-device-gear-purchasing", "pageUrlRelative": "/posts/SjpJbvanCTMF9Pow6/suggestions-on-tech-device-gear-purchasing", "linkUrl": "https://www.lesswrong.com/posts/SjpJbvanCTMF9Pow6/suggestions-on-tech-device-gear-purchasing", "postedAtFormatted": "Monday, March 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Suggestions%20on%20tech%20device%2Fgear%20purchasing%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuggestions%20on%20tech%20device%2Fgear%20purchasing%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSjpJbvanCTMF9Pow6%2Fsuggestions-on-tech-device-gear-purchasing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Suggestions%20on%20tech%20device%2Fgear%20purchasing%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSjpJbvanCTMF9Pow6%2Fsuggestions-on-tech-device-gear-purchasing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSjpJbvanCTMF9Pow6%2Fsuggestions-on-tech-device-gear-purchasing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<p>I found out I won second place in an idea contest at work and am being granted ~$400 to spend at Best Buy (the <a href=\"http://www.bestbuy.com/\">web site</a>&nbsp;for any unfamiliar). Originally, I believe the second place prize was going to be an iPad, but it looks like they've decided to just allow me to pick something in that ballpark price range.</p>\n<p>I suspect there are a fair amount of tech saavy folks on LW and thought I'd inquire as to whether you've purchased a device or accessory (or anything from Best Buy-ish stores) that has brought you an increase in efficiency, usefulness, pleasure, etc. The idea of a tablet appeals to me, but I'm not entirely sure what I'd do with it. Also, a data plan is not in my budget, so many typical uses are not applicable in my case.</p>\n<p>Anyway, just hoping to probe some collective knowledge about this decision. I'm not very knowledgeable on devices and/or how longer term usage/satisfaction matches expectations or even money spent.</p>\n<p>Thanks for any assistance!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SjpJbvanCTMF9Pow6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -10, "extendedScore": null, "score": 8.681767385920216e-07, "legacy": true, "legacyId": "14202", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-19T11:48:44.038Z", "modifiedAt": null, "url": null, "title": "[LINK] Nuclear winter: a reminder", "slug": "link-nuclear-winter-a-reminder", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:39.682Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XJ9wnsyHpedmEK8ht/link-nuclear-winter-a-reminder", "pageUrlRelative": "/posts/XJ9wnsyHpedmEK8ht/link-nuclear-winter-a-reminder", "linkUrl": "https://www.lesswrong.com/posts/XJ9wnsyHpedmEK8ht/link-nuclear-winter-a-reminder", "postedAtFormatted": "Monday, March 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Nuclear%20winter%3A%20a%20reminder&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Nuclear%20winter%3A%20a%20reminder%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXJ9wnsyHpedmEK8ht%2Flink-nuclear-winter-a-reminder%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Nuclear%20winter%3A%20a%20reminder%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXJ9wnsyHpedmEK8ht%2Flink-nuclear-winter-a-reminder", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXJ9wnsyHpedmEK8ht%2Flink-nuclear-winter-a-reminder", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 93, "htmlBody": "<p>Just a reminder that some of the old threats are still around (and hence that AI is not only something that can go hideously badly, but also some thing that could help us with the other existential risks as well):</p>\n<p><a href=\"http://blog.practicalethics.ox.ac.uk/2012/03/old-threats-never-die-they-fade-away-from-our-minds-nuclear-winter/\">http://blog.practicalethics.ox.ac.uk/2012/03/old-threats-never-die-they-fade-away-from-our-minds-nuclear-winter/</a></p>\n<p><strong>EDIT</strong>: as should have been made clear in that post (but wasn't!), the existential risks doesn't come from the full fledged nuclear winter directly, but from the collapse of human society and fragmentation of the species into small, vulnerable subgroups, with no guarantee that they'd survive or ever climb back to a technological society.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XJ9wnsyHpedmEK8ht", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 8, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "14225", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-19T17:20:47.847Z", "modifiedAt": null, "url": null, "title": "[video] Paul Christiano's impromptu tutorial on AIXI and TDT", "slug": "video-paul-christiano-s-impromptu-tutorial-on-aixi-and-tdt", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:33.923Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZXn82Qsh25w5JgEob/video-paul-christiano-s-impromptu-tutorial-on-aixi-and-tdt", "pageUrlRelative": "/posts/ZXn82Qsh25w5JgEob/video-paul-christiano-s-impromptu-tutorial-on-aixi-and-tdt", "linkUrl": "https://www.lesswrong.com/posts/ZXn82Qsh25w5JgEob/video-paul-christiano-s-impromptu-tutorial-on-aixi-and-tdt", "postedAtFormatted": "Monday, March 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Bvideo%5D%20Paul%20Christiano's%20impromptu%20tutorial%20on%20AIXI%20and%20TDT&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Bvideo%5D%20Paul%20Christiano's%20impromptu%20tutorial%20on%20AIXI%20and%20TDT%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZXn82Qsh25w5JgEob%2Fvideo-paul-christiano-s-impromptu-tutorial-on-aixi-and-tdt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Bvideo%5D%20Paul%20Christiano's%20impromptu%20tutorial%20on%20AIXI%20and%20TDT%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZXn82Qsh25w5JgEob%2Fvideo-paul-christiano-s-impromptu-tutorial-on-aixi-and-tdt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZXn82Qsh25w5JgEob%2Fvideo-paul-christiano-s-impromptu-tutorial-on-aixi-and-tdt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<p><a href=\"http://ordinaryideas.wordpress.com/\">Paul Christiano</a> was about to give a tutorial on AIXI and TDT, so I whipped out my iPhone and recorded it. His tutorial wasn't carefully planned or executed, but it may still be useful to some. Note that when Paul writes \"UDT\" on a piece of paper he really meant \"TDT.\" :)</p>\n<p>\n<object width=\"640\" height=\"360\" data=\"http://www.youtube.com/v/EDnhcAtH3UI?version=3\" type=\"application/x-shockwave-flash\">\n<param name=\"allowFullScreen\" value=\"true\" />\n<param name=\"allowscriptaccess\" value=\"always\" />\n<param name=\"src\" value=\"http://www.youtube.com/v/EDnhcAtH3UI?version=3\" />\n</object>\n</p>\n<p>&nbsp;</p>\n<p>HD video download links: <a href=\"http://www.gigasize.com/get/x7n6675gzgd\">1</a>, <a href=\"http://depositfiles.com/files/akb1jrube\">2</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TiEFKWDvD3jsKumDx": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZXn82Qsh25w5JgEob", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 8.685125376105419e-07, "legacy": true, "legacyId": "14227", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-19T19:41:30.982Z", "modifiedAt": null, "url": null, "title": "Difference between CDT and ADT/UDT as constant programs", "slug": "difference-between-cdt-and-adt-udt-as-constant-programs", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:27.608Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gRR", "createdAt": "2012-02-02T12:11:00.628Z", "isAdmin": false, "displayName": "gRR"}, "userId": "LPBRzHQvMP9chLNWH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z82CDpY7AENo6vbhE/difference-between-cdt-and-adt-udt-as-constant-programs", "pageUrlRelative": "/posts/z82CDpY7AENo6vbhE/difference-between-cdt-and-adt-udt-as-constant-programs", "linkUrl": "https://www.lesswrong.com/posts/z82CDpY7AENo6vbhE/difference-between-cdt-and-adt-udt-as-constant-programs", "postedAtFormatted": "Monday, March 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Difference%20between%20CDT%20and%20ADT%2FUDT%20as%20constant%20programs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADifference%20between%20CDT%20and%20ADT%2FUDT%20as%20constant%20programs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz82CDpY7AENo6vbhE%2Fdifference-between-cdt-and-adt-udt-as-constant-programs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Difference%20between%20CDT%20and%20ADT%2FUDT%20as%20constant%20programs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz82CDpY7AENo6vbhE%2Fdifference-between-cdt-and-adt-udt-as-constant-programs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz82CDpY7AENo6vbhE%2Fdifference-between-cdt-and-adt-udt-as-constant-programs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 449, "htmlBody": "<p>After some thinking, I came upon an idea how to define the difference between CDT and UDT within the <a href=\"/lw/2os/controlling_constant_programs/\">constant programs</a>&nbsp;framework. I would post it as a comment, but it is rather long...</p>\n<p>The idea is to separate the cognitive part of an agent into three separate modules:</p>\n<p><strong>1. Simulator</strong>: given the code for a parameterless function X(), the Simulator tries to evaluate it, spending L computation steps. The result is either proving that X()=x for some value x, or leaving X() unknown.</p>\n<p><strong>2.&nbsp;Correlator</strong>: given the code for two functions X(...) and Y(...), the Correlator checks for proofs (of length up to P) of structural similarity between the source codes of the functions,&nbsp;trying to prove correlations &nbsp;X(...)=Y(...).</p>\n<p>[Note: the Simulator and the Correlator can use the results of each other, so that:<br />&nbsp; &nbsp; &nbsp; If simulator proves that A()=x, then correlator can prove that A()+B() = x+B()<br />&nbsp; &nbsp; &nbsp; If correlator proves that A()=B(), then simulator can skip simulation when proving that (A()==B() ? 1 : 2) = 1]</p>\n<p><strong>3. Executive</strong>:&nbsp;allocates tasks and resources to Simulator and Correlator in some systematic manner, trying to get them to prove the \"moral arguments\"<br />&nbsp; &nbsp; &nbsp;Self()=x =&gt; U()=u<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;or<br />&nbsp; &nbsp; &nbsp;( Self()=x =&gt; U()=ux &nbsp;AND &nbsp;Self()=y =&gt; U()=uy ) =&gt; ux &gt; uy,<br />and returns the best found action.</p>\n<p>&nbsp;</p>\n<p>Now, CDT can be defined as an agent with the Simulator, but without the Correlator. Then, no matter what L it is given, the Simulator won't be able to prove that Self()=Self(), because of the infinite regress. So the agent will be opaque to itself, and will two-box on Newcomb's problem and defect against itself in Prisoner's Dilemma.</p>\n<p>The UDT/ADT, on the other hand, have functioning Correlators.</p>\n<p>&nbsp;</p>\n<p>If it is possible to explicitly (rather than conceptually) separate an agent into the three parts, then it appears to be possible to demonstrate the good behavior of an agent in the&nbsp; <a href=\"/lw/5rq/example_decision_theory_problem_agent_simulates/\">ASP problem</a>.&nbsp;The&nbsp;world can be written as a NewComb's-like function:</p>\n<p style=\"padding-left: 30px;\">def U():<br />&nbsp; &nbsp;box2 = 1000<br />&nbsp; &nbsp;box1 = (P()==1 ? 1000000 : 0)<br />&nbsp; &nbsp;return (A()==1 ? box1 : box1 + box2)</p>\n<p>where P is a predictor that has much less computational resources than the agent A. We can assume that the predictor has basically the same source code as the agent, except for the bound on L and a stipulation that P two-boxes if it cannot prove one-boxing using available resources.</p>\n<p>Then, if the Executive uses a reasonable strategy (\"start with low values of L and P, then increase them until all necessary moral arguments are found or all available resources are spent\"), then the Correlator should be able to prove A()=P() quite early in the process.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z82CDpY7AENo6vbhE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 4, "extendedScore": null, "score": 8.685703349809018e-07, "legacy": true, "legacyId": "14228", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gZbHSWcLvj7ZopSas", "q9DbfYfFzkotno9hG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-19T21:17:31.857Z", "modifiedAt": null, "url": null, "title": "Saturating utilities as a model", "slug": "saturating-utilities-as-a-model", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:27.077Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dmytry", "createdAt": "2009-12-03T17:11:53.492Z", "isAdmin": false, "displayName": "Dmytry"}, "userId": "AjtmA2qtA8sdiMbru", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/D7ajTvbLFTvZqG6Jv/saturating-utilities-as-a-model", "pageUrlRelative": "/posts/D7ajTvbLFTvZqG6Jv/saturating-utilities-as-a-model", "linkUrl": "https://www.lesswrong.com/posts/D7ajTvbLFTvZqG6Jv/saturating-utilities-as-a-model", "postedAtFormatted": "Monday, March 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Saturating%20utilities%20as%20a%20model&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASaturating%20utilities%20as%20a%20model%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD7ajTvbLFTvZqG6Jv%2Fsaturating-utilities-as-a-model%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Saturating%20utilities%20as%20a%20model%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD7ajTvbLFTvZqG6Jv%2Fsaturating-utilities-as-a-model", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD7ajTvbLFTvZqG6Jv%2Fsaturating-utilities-as-a-model", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 315, "htmlBody": "<p>Okay, it is a very raw idea, but consider the utility processing that works as following:</p>\n<p>1: The utility i'm speaking of is not 'happiness', nor is it 'strength of the compulsion', the utility is only used for the purpose of comparing between futures to pick the one with larger utility. Applying same monotonously increasing function to both sides of comparison does not change outcome of comparison, and works as if the function was not there.</p>\n<p>The utility is an array of n numbers. The arrays are compared after pseudo-summing them using <a href=\"http://en.wikipedia.org/wiki/Sigmoid_function\">sigmoid</a> function like:</p>\n<p>a[1]+k[1]*sigmoid(a[2]+k[2]*sigmoid(a[3] + ...))</p>\n<p>This has a bunch of nasty properties (i.e. it is not clear how to deal with probabilities here), but may capture the human view on the torture and dust specks, and similar problems like pascal's wager, where arguments of low quality may just go into a[n] where n is large, rather than be assigned any defined low probability.</p>\n<p>Note that usually, two future worlds being compared are identical up to some n , and so the comparison can be made starting from the n, disregarding the equal smaller terms.</p>\n<p>Furthermore, the comparison allows for 'short evaluation', as after few steps no further values need to be considered.</p>\n<p>The obvious model that comes to mind if you observe this comparator as a black box, is the linear sum where weights are k[1] &gt;&gt; k[2] , k[2] &gt;&gt; k[3] , and so on, which is a fairly good approximation but breaks down when you start using really huge numbers like 3^^^^3 . The sigmoid eats uparrows for breakfast and asks for more.</p>\n<p>It seems to me that this does accurately capture the behaviour which is not generally very impressed by Knuth's up arrow notation, and the sigmoids are biologically plausible. Other monotonously growing functions can be employed.</p>\n<p>One could probably come up with nicer model which results in identical outcomes, whereby n does not need to be integer.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "D7ajTvbLFTvZqG6Jv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 0, "extendedScore": null, "score": 8.686096441472704e-07, "legacy": true, "legacyId": "14229", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-19T21:51:24.247Z", "modifiedAt": null, "url": null, "title": "Emotional regulation Part II: research summary", "slug": "emotional-regulation-part-ii-research-summary", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:30.147Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmer963", "createdAt": "2010-09-28T01:54:53.120Z", "isAdmin": false, "displayName": "Swimmer963"}, "userId": "6Fx2vQtkYSZkaCvAg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vZNPEXrJkJjjctBza/emotional-regulation-part-ii-research-summary", "pageUrlRelative": "/posts/vZNPEXrJkJjjctBza/emotional-regulation-part-ii-research-summary", "linkUrl": "https://www.lesswrong.com/posts/vZNPEXrJkJjjctBza/emotional-regulation-part-ii-research-summary", "postedAtFormatted": "Monday, March 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Emotional%20regulation%20Part%20II%3A%20research%20summary&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEmotional%20regulation%20Part%20II%3A%20research%20summary%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvZNPEXrJkJjjctBza%2Femotional-regulation-part-ii-research-summary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Emotional%20regulation%20Part%20II%3A%20research%20summary%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvZNPEXrJkJjjctBza%2Femotional-regulation-part-ii-research-summary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvZNPEXrJkJjjctBza%2Femotional-regulation-part-ii-research-summary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2953, "htmlBody": "<p><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Template>Normal</o:Template> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>2604</o:Words> <o:Characters>14844</o:Characters> <o:Company>Home</o:Company> <o:Lines>123</o:Lines> <o:Paragraphs>29</o:Paragraphs> <o:CharactersWithSpaces>18229</o:CharactersWithSpaces> <o:Version>10.265</o:Version> </o:DocumentProperties> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:Zoom>0</w:Zoom> <w:DisplayHorizontalDrawingGridEvery>0</w:DisplayHorizontalDrawingGridEvery> <w:DisplayVerticalDrawingGridEvery>0</w:DisplayVerticalDrawingGridEvery> <w:UseMarginsForDrawingGridOrigin /> </w:WordDocument> </xml><![endif]--> <!--StartFragment--></p>\n<p class=\"MsoNormal\"><em><strong>Abstract</strong>: Emotional regulation is a topic currently being studied in the field of psychology. Five different types of emotional regulation strategies have been identified, distinguished by the stage of the emotion-response process in which they occur. To drastically simplify, this strategies are: situation selection, situation modification, deployment of attention, changes in cognition, and modulation of responses.&nbsp;</em></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><strong>Introduction</strong></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">This is a follow-up to my <a href=\"/lw/aks/emotional_regulation_part_i_a_problem_summary/\">previous post</a> about my problem with emotional regulation.&nbsp;This is also my first outside-of-the-classroom foray into scholarship, <a href=\"/lw/3m3/the_neglected_virtue_of_scholarship/\">lukeprog</a> style. Mainly what I found is that it&rsquo;s surprisingly time-consuming and frustrating. I suffered a <em>lot </em></span><span lang=\"EN-GB\">of akrasia, compared to my usual, while writing this post&ndash;mainly because I kept thinking &lsquo;oh my god, and then I have to cite my sources!&rsquo; This may be an area where I need more practice...</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><strong><a id=\"more\"></a><span style=\"font-weight: normal;\"><strong></strong></span></strong></span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><strong><span style=\"font-weight: normal;\"><strong>What is emotion anyway?</strong></span></strong></span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">Apparently there are quite a lot of </span><span lang=\"EN-GB\">competing definitions for &lsquo;emotion&rsquo;. Maybe this shouldn&rsquo;t be surprising&ndash;the concept of emotion seem simple because most of the processing happens below conscious awareness, but emotions are as complex as the brains that create them.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span lang=\"EN-GB\">The definition that most research in emotional regulation uses is the &lsquo;response tendency&rsquo; definition: emotions are </span><span style=\"mso-ansi-language: EN-US;\">adaptive behavioral and/or physiological responses, and they happen when the organism is put in evolutionarily significant situations.</span> <span lang=\"EN-GB\">The internal experience of emotion <em>may </em></span><span lang=\"EN-GB\">lead to a particular behaviour, but may not: emotion is a feedback mechanism that leads to various behaviours, rather than the direct cause of behaviour. </span><span style=\"mso-ansi-language: EN-US;\">Recent research has covered the specific purposes that emotions accomplish. They can facilitate decision-making, prepare the individual for a fast response to a given situation, inform on the match between organism and environment, and serve a social function; in general, they allow for learning. Emotional responses are not set in stone, and can be modulated on the way to taking their final shape. (Gross, 1988).</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">What does this mean for me, personally? One, emotions exist for a reason. They are <em>adaptive</em></span><span style=\"mso-ansi-language: EN-US;\">, and attempting to turn mine off entirely or prevent them from affecting my decisions would likely not be adaptive. Two, emotions are triggered by &lsquo;evolutionary significant&rsquo; situations. To take a wild guess on what that might mean, being in a situation that involved competition against people who were much, much better than you might have had severe consequences in the ancestral environment...and even if not, for most of human history survival was more important than fun, and that would mean focusing on activities where you were likely to succeed, rather than those you <em>liked. </em></span><span style=\"mso-ansi-language: EN-US;\">My emotional response may be trying to inform me that the match between my organism and the environment is less than ideal&ndash;or would have been if I were living 50 000 years ago.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h1><span style=\"font-family: Times;\">Emotional Regulation</span></h1>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">According to the people who study it, emotional regulation is what happens when people try to increase, decrease, or maintain their emotions, whether positive or negative. (For once, this seems like a pretty straightforward definition.) People may try to change the <em>kind </em></span><span style=\"mso-ansi-language: EN-US;\">of emotions they have, <em>when </em></span><span style=\"mso-ansi-language: EN-US;\">they have them, and how<em> </em></span><span style=\"mso-ansi-language: EN-US;\">they <em>experience </em></span><span style=\"mso-ansi-language: EN-US;\">and <em>express</em></span><span style=\"mso-ansi-language: EN-US;\"> them.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">&ldquo;The available evidence does not support the existence of discrete emotional states. Instead, emotional responding appears to be organized in terms of a few fundamental dimensions, including <a href=\"http://en.wikipedia.org/wiki/Valence_(psychology)\">valence</a>, <a href=\"http://en.wikipedia.org/wiki/Two-factor_theory_of_emotion\">arousal</a>, and <a href=\"http://en.wikipedia.org/wiki/Approach-avoidance_conflict\">approach-avoidance</a>. The influence of emotion regulation on people&rsquo;s emotional states is therefore likely to be similarly dimensional. In other words, \"emotion regulation may not be so much concerned with getting people in or out of discrete emotional states like anger, sadness, or joy. Rather, emotion regulation may change people&rsquo;s emotional states along dimensions such as valence, arousal, and approach-avoidance.&rdquo; (Koole, 2009).</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">The study of emotion regulation isn&rsquo;t new. Freud studied it in the form of <a href=\"http://en.wikipedia.org/wiki/Defence_mechanism\">ego defenses</a>, which he saw as non-conscious processes that could, depending on the specific method used, result in reality distortion, excess energy consumption, and unnecessary non-gratification&ndash;to him, these forms of emotional regulation were <em>maladaptive. </em></span><span style=\"mso-ansi-language: EN-US;\">(Gross, 1998).</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">More recently the study of <a href=\"http://en.wikipedia.org/wiki/Coping_(psychology)\">coping</a> has focused on emotional regulation from the point of view of conscious, deliberate, and adaptive processes. These can be based on fixing the underlying problem, i.e. problem-focused coping, or on reducing the negative emotions without changing the physical reality, i.e. emotion-focused coping. In general, emotion-focused coping is less effective and more likely to be associated with psychological distress. (Watson &amp;<em> </em></span><span style=\"mso-ansi-language: EN-US;\">Sinha, 2008).&nbsp;</span>According to more recent research, emotional regulation processes &ldquo;may be automatic or controlled, conscious or unconscious, and may have their effects at one or more points in the emotion generative process.&rdquo; (Gross, 1998).</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">An individual&rsquo;s skill at emotional regulation must also be distinguished from their innate emotional sensitivity, which affects how much and how quickly they respond to an emotion-causing stimulus. In theory, Person A could be very sensitive, and experience a swift rush of negative emotions in response to an upsetting stimulus, but still be able to down-regulate the feelings afterwards, whereas Person B responds less quickly and steeply but lacks the skill to redirect the emotions they do experience. And whereas emotional sensitivity correlates with temperament differences in infants, and seems to develop independently of environmental influences, skill in emotional regulation develops and changes based on the quality of children&rsquo;s social interactions, and can continue to improve throughout life. (Koole, 2009).&nbsp;</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">Different strategies of emotional regulation can be classified by whether they are consciously controlled or automatic&ndash;however, since conscious control is a complex and hard-to-define concept in itself, it may be more useful to classify strategies by <em>when </em></span><span style=\"mso-ansi-language: EN-US;\">they occur during the emotional response process.</span></p>\n<ol style=\"margin-top: 0in;\" type=\"1\">\n<li class=\"MsoNormal\" style=\"mso-pagination: none; mso-list: l0 level1 lfo1; tab-stops: list .5in; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">Selection of situation: occurs <em>before </em></span><span style=\"mso-ansi-language: EN-US;\">the stimulus that causes the emotion. </span></li>\n<li class=\"MsoNormal\" style=\"mso-pagination: none; mso-list: l0 level1 lfo1; tab-stops: list .5in; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">Modification of situation: occurs after the stimulus, but before the emotional response begins. </span></li>\n<li class=\"MsoNormal\" style=\"mso-pagination: none; mso-list: l0 level1 lfo1; tab-stops: list .5in; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">Deployment of attention: occurs during the emotional response process.<span style=\"mso-spacerun: yes;\">&nbsp; </span></span></li>\n<li class=\"MsoNormal\" style=\"mso-pagination: none; mso-list: l0 level1 lfo1; tab-stops: list .5in; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">Change in cognition: occurs during the emotional response process. </span></li>\n<li class=\"MsoNormal\" style=\"mso-pagination: none; mso-list: l0 level1 lfo1; tab-stops: list .5in; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">Modulation of responses: occurs <em>after </em></span><span style=\"mso-ansi-language: EN-US;\">the emotional response process. (Gross, 1998). </span></li>\n</ol>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">Methods #1 and #2, situation selection and modification, require a certain degree of self-knowledge, in order to decide which situations to seek out and which to avoid. There can be a conflict here between long term and short-term goals&ndash;for example, a timid person can reduce their anxiety by avoiding social situations, but in the long run this can lead to undesirable social isolation. To further complicate things, the emotional response itself can back-propagate and modify the situation&ndash;witness my taekwondo instructor&rsquo;s response to my freak-outs.</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">Deployment of attention has three sub-categories: distraction, concentration, and rumination. Distraction involves focusing attention onto neutral or non-emotional aspects of the situation, or shifting attention from difficult to tractable goals. Concentration involves focusing further on the situation, trying to enter a state of <a href=\"http://en.wikipedia.org/wiki/Flow_(psychology)\">flow</a> in order to avoid frustration. In rumination, attention is directed onto the feelings themselves, analyzing them.&nbsp;Wadlinger and Isaacowitz (2011) suggest that attention can be trained in order to better develop emotional regulation skills. Skill at directing and controlling attention is partly an innate trait, but studies indicate that attentional skills are also plastic and can improve with practice. For example, low mood can be improved with (gaze-based) training, which creates a bias towards looking at positive stimuli, such as happy instead of angry faces,<span style=\"mso-spacerun: yes;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">The fourth category, change in cognition, happens during the step where perceptions of the situation are given an emotional weight. The perceived capacity to manage or control a situation affects the emotions assigned to it. Classical Freudian defenses include denial, isolation, and intellectualization of the situation. Events can also be reinterpreted in a more positive light&ndash;for example, <a href=\"http://en.wikipedia.org/wiki/Downward_social_comparison\">downward social comparison</a>, or a goal being reframed so that failure at the initial goal becomes a success according to the new goal. According to studies, this kind of reappraisal has a larger effect in complex than in simple situations. Factors that affect reappraisal include attribution of an event to self versus others, beliefs about the controllability of the event, accountability, expectations, and implicit personal theories of how emotion works. (Koole, 2009).</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">The last category, response modulation, does not affect the internal emotional experience at all, but only the expression of it. Examples given by the author include various medications such as anxiolytics, exercise, relaxation therapy, and self-soothing with alcohol, cigarettes, other drugs, or food, as well as simply initiating or hiding the expression of a given emotion. (Gross, 1998).</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h1><span style=\"font-family: Times;\">How does this help me?</span></h1>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">Well, for one, it gives me a good idea of which techniques I&rsquo;ve <em>already </em></span><span style=\"mso-ansi-language: EN-US;\">tried, and which ones I might try next.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">1. <span style=\"text-decoration: underline;\">Situation selection</span>. To start with #1, I have used situation selection in the past, mainly when I decided to quit swimming to avoid pre-race meltdowns. That worked in the short term; when I wasn&rsquo;t putting myself under that much competitive pressure anymore, I had no reason to freak out, and my general stress levels dropped as well. But #1 is a method I would prefer to use sparingly, if at all; it seems that it would <em>seriously limit my future prospects</em></span><span style=\"mso-ansi-language: EN-US;\">, and running away from the things that scare me doesn&rsquo;t really fit with the mindset of wanting to be stronger.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">If anything, finding something challenging or even scary causes me to be even <em>more </em></span><span style=\"mso-ansi-language: EN-US;\">motivated to keep doing it until I don&rsquo;t find it scary anymore. (I think the thought process goes something like &lsquo;life could through you into a situation where you need this skill <em>at any moment</em></span><span style=\"mso-ansi-language: EN-US;\">, and wouldn&rsquo;t it be way less stressful if you&rsquo;d already been practicing it?&rsquo;</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">2. <span style=\"text-decoration: underline;\">Situation modification</span>. Is there any way that, without quitting taekwondo entirely, I could find a way to pick and choose what I do in class, avoiding the things that I know will make me upset? I can&rsquo;t think of any specific examples of how I could do this, except for making up excuses not to do particular exercises that I&rsquo;m bad at and that frustrate me. (I have an <em>actual </em></span><span style=\"mso-ansi-language: EN-US;\">excuse not to do frog jumps&ndash;bad knees&shy;&ndash;but I think the fact that I <em>can&rsquo;t </em></span><span style=\"mso-ansi-language: EN-US;\">do it makes me more frustrated than if I went ahead and did them, because it makes me feel like I&rsquo;m not as good as the others.)</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">I can think of <em>other </em></span><span style=\"mso-ansi-language: EN-US;\">situations where I&rsquo;ve used this technique to calm myself down, though. Recently, at our university&rsquo;s Social Sciences Ball, I wasn&rsquo;t having that much fun and I was running out of what little steam I&rsquo;d had to begin with by 11 pm. I was <em>very </em></span><span style=\"mso-ansi-language: EN-US;\">upset to learn that the bus to take us back to campus, which I&rsquo;d thought would come at 11:30 pm, actually would come at 12:30 pm. (My stamina for social events lasts about 3 hours, and if I can&rsquo;t remove myself from the situation at that point, I start feeling some strange equivalent of claustrophobia, and will probably start crying if I can't get away.) Over my boyfriend&rsquo;s protests of &lsquo;it&rsquo;ll look bad on me if you leave by yourself now!&rsquo; I resourcefully texted my brother and got him to look up the bus schedule online. It didn&rsquo;t end up working as planned, but having the <em>feeling of control </em></span><span style=\"mso-ansi-language: EN-US;\">restored calmed me down a lot, and when it turned out that the bus schedule was wrong, I came back to the party and went on enjoying myself like nothing had happened.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">This tells me that anytime my stress is due to feeling like I&rsquo;m not in control, and there&rsquo;s some proactive &lsquo;taking-control&rsquo; move that I can execute, it&rsquo;s probably worth it even if it doesn&rsquo;t change my actual situation much&ndash;it&rsquo;ll still have a huge effect on my emotional state, which in some cases is more important than the situation causing it.&nbsp;</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">3. <span style=\"text-decoration: underline;\">Deployment of attention: distraction, concentration, and rumination</span>. If I think about it, distraction is exactly what I do when I have a compelling stimulus available to distract myself with. This is more likely to be when I&rsquo;m alone, and that might well be the reason why meltdowns aren&rsquo;t a problem when I&rsquo;m alone. (One reason. Lack of social pressure is probably another.) If I&rsquo;m in public, and I&rsquo;m about to burst into tears, I&rsquo;ll tell myself &lsquo;okay, start thinking about one of your stories, <em>now</em></span><span style=\"mso-ansi-language: EN-US;\">!&rsquo; But if someone tries to <em>talk </em></span><span style=\"mso-ansi-language: EN-US;\">to me, especially if the subject of conversation is the same as what&rsquo;s frustrating me, my attempts at self-distraction get derailed fast. Conclusion: I <em>could </em></span><span style=\"mso-ansi-language: EN-US;\">probably make this a useful method, but I need to come up with better distractions.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">Concentration, getting into a flow state, is a promising method, but likely it&rsquo;s something I would have to start doing <em>before </em></span><span style=\"mso-ansi-language: EN-US;\">I became frustrated at all. Certainly sparring in taekwondo is complex enough to occupy someone&rsquo;s full attention, leaving behind no excess processing power for frustration. Correction: this is the case for someone who <em>knows what they&rsquo;re doing</em></span><span style=\"mso-ansi-language: EN-US;\">. As a beginner, my inability to plan strategy fast enough to use it in real time means that I don&rsquo;t normally plan my strategy at all while fighting. That means a <em>lot </em></span><span style=\"mso-ansi-language: EN-US;\">of space left over for frustration-inadequacy-failure thought chains. The implication: as I <em>do </em></span><span style=\"mso-ansi-language: EN-US;\">get good enough to plan in real time, and coordinated enough to <em>enjoy </em></span><span style=\"mso-ansi-language: EN-US;\">the moment-to-moment satisfaction of pushing myself hard (like I do while swimming), frustration won&rsquo;t be so much of a problem.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">Rumination is a strategy I&rsquo;ve definitely used before, but I&rsquo;m not at all sure that it&rsquo;s an effective strategy in this context. In an exception to the general rule that thinking about my emotions dulls them, thinking about frustration and what&rsquo;s causing it leads to an explosive feedback loop. However, I might find it desirable to use this method when I&rsquo;m alone, in order to track down and list all the thoughts and emotions that occur, as <a href=\"/user/aelephant/\">user:&nbsp;aelephant</a><span style=\"mso-spacerun: yes;\">&nbsp;&nbsp;</span>suggested in <a href=\"/lw/aks/emotional_regulation_part_i_a_problem_summary/5z7t?context=1#5z7t\">this comment</a>.<span style=\"mso-spacerun: yes;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">4. <span style=\"text-decoration: underline;\">Changes in cognition</span>. This step of the process, where the emotion itself actually happens, seems like a productive place to start. The &lsquo;downwards social comparison&rsquo; method could be translated into &lsquo;comparing myself to people who&rsquo;re the same belt level as me, instead of comparing myself to the black belts,&rsquo; or at the very least persuading myself that not being as good as the black belts <em>isn&rsquo;t </em></span><span style=\"mso-ansi-language: EN-US;\">a reason to get frustrated.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">Reappraising a situation in a more positive light, or reframing your goal so that your actual results count as success rather than failure, also seems promising&ndash;especially because often, when in the process of reappraising a goal, I realize that it wasn&rsquo;t even my real goal. Back when I was 14 and competing in swimming, &lsquo;win lots of races&rsquo; and even &lsquo;go to the Olympics someway&rsquo; <em>were </em></span><span style=\"mso-ansi-language: EN-US;\">explicit goals, even if I didn&rsquo;t want to admit it to friends and family.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">But I <em>didn&rsquo;t </em></span><span style=\"mso-ansi-language: EN-US;\">start taekwondo intending to &lsquo;win lots of tournaments&rsquo;. That wasn&rsquo;t even something I thought about <em>at all</em></span><span style=\"mso-ansi-language: EN-US;\">. My goals were, approximately, &lsquo;become fitter and more flexible, learn some self defense in case anyone ever tries to rape me when I&rsquo;m out late at night, and anyway martial arts are <em>cool </em></span><span style=\"mso-ansi-language: EN-US;\">so I&rsquo;ll acquire coolness just by showing up.&rsquo; The fact that I turned out to have really awful reaction times, making it hard for me to win at sparring, doesn&rsquo;t equate to a failure at <em>any </em></span><span style=\"mso-ansi-language: EN-US;\">of these goals&ndash;but the goal of &lsquo;beat other people in sparring&rsquo; sneaked in there somewhere, probably because it&rsquo;s easier to measure than my original goals, and then starting causing me frustration when I failed to achieve it.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">6.<span style=\"text-decoration: underline;\"> Response Modulation</span>. I do the simplest form of this a lot&ndash;the iron-jaw, stare-into-space-and-don&rsquo;t-cry approach does work a significant portion of the time to keep anyone from noticing until the emotions subside on their own. But that&rsquo;s if no one tries to <em>talk </em></span><span style=\"mso-ansi-language: EN-US;\">to me.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">As for the subtler methods, I already use exercise as a mood regulator, and frequently candy or baked goods to cheer myself up, and/or addictive books and shows. (Telling myself &ldquo;if you get through this, you can watch 30 minutes worth of Rescue 911 episodes on Youtube&rdquo; is a significant cheer-up factor.) But most of those methods aren&rsquo;t available to me on the spot when I&rsquo;m actually in a taekwondo class and starting to get upset.</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h1><span style=\"font-family: Times;\">Conclusion</span></h1>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">My miniature foray into scholarship has allowed me to make a list of methods that humans use to regulate emotions. Methods that look promising include: finding ways to change the situation so that I feel in control; distracting myself from upsetting situations; trying to get into a state of concentration or flow; and reevaluating my goals to be realistic or achievable.</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">My plan for the future: try to think of <em>specific </em></span><span style=\"mso-ansi-language: EN-US;\">ways I could use this methods, i.e. a particularly compelling chain of thought that I could use as a distraction, and then try all of them out and compare. I plan to show part or all of this article to at least one of my instructors, too, so that they have an idea of what I&rsquo;m working on, and can help me a little.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><em>Note #1: I did get feedback from <a href=\"/user/juliawise/\">juliawise</a> on my first post, suggesting that I investigate <a href=\"http://en.wikipedia.org/wiki/Cognitive_behavioral_therapy\">cognitive behavioural therapy</a>&nbsp;and <a href=\"http://en.wikipedia.org/wiki/Dialectical_behavior_therapy\">dialectical behaviour therapy</a>. I think this article is long enough, though, so if I do investigate it, it&rsquo;ll go in a separate post. Don&rsquo;t worry, juliawise, it was good advice and I&rsquo;m not ignoring it.</em></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><em>Note #2: If anyone wants to see the articles in my reference list, I can't post links because I accessed them through my school account, but I have the PDFs saved and I can email them to you.&nbsp;</em></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"font-size: 26px; font-weight: bold; font-family: Times;\">References</span></p>\n<p><span lang=\"EN-GB\">&nbsp;<br /></span><span style=\"mso-ansi-language: EN-US;\">Sander L. Koole. (2009). The psychology of emotion regulation: an integrative review. <em>Cognition and Emotion</em></span><span style=\"mso-ansi-language: EN-US;\">, 23 (1), 4_41<br /></span><span style=\"mso-ansi-language: EN-US;\">&nbsp;<br /></span><span style=\"mso-ansi-language: EN-US;\">James J. Gross. (1998). The Emerging Field of Emotion Regulation: An Integrative Review. <em>Review of General Psychology</em></span><span style=\"mso-ansi-language: EN-US;\">, Vol. 2, No. 5,271-299<br /></span><span style=\"mso-ansi-language: EN-US;\">&nbsp;<br /></span><span style=\"mso-ansi-language: EN-US;\">Watson David C., Sinha, Birenda. (2008). Emotion Regulation, Coping, and Psychological Symptoms.<strong> </strong></span><span style=\"mso-ansi-language: EN-US;\"><em>International Journal of Stress Management</em></span><span style=\"mso-ansi-language: EN-US;\">, Vol. 15, No. 3, 222&ndash;234<br /></span><span style=\"mso-ansi-language: EN-US;\">&nbsp;<br /></span><span style=\"font-size: 12.0pt; font-family: Times; mso-ansi-language: EN-US;\">Wadlinger, Heather A., Isaacowitz, Derek M. (2011). Fixing Our Focus: Training Attention to Regulate Emotion. <em>Personality and Social Psychology Review</em></span><span style=\"font-size: 12.0pt; font-family: Times; mso-ansi-language: EN-US;\">,15(1) 75&ndash;102</span></p>\n<!--EndFragment-->\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3ee9k6NJfcGzL6kMS": 2, "dBPou4ihoQNY4cquv": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vZNPEXrJkJjjctBza", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 35, "extendedScore": null, "score": 8.686235128311165e-07, "legacy": true, "legacyId": "14231", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Template>Normal</o:Template> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>2604</o:Words> <o:Characters>14844</o:Characters> <o:Company>Home</o:Company> <o:Lines>123</o:Lines> <o:Paragraphs>29</o:Paragraphs> <o:CharactersWithSpaces>18229</o:CharactersWithSpaces> <o:Version>10.265</o:Version> </o:DocumentProperties> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:Zoom>0</w:Zoom> <w:DisplayHorizontalDrawingGridEvery>0</w:DisplayHorizontalDrawingGridEvery> <w:DisplayVerticalDrawingGridEvery>0</w:DisplayVerticalDrawingGridEvery> <w:UseMarginsForDrawingGridOrigin /> </w:WordDocument> </xml><![endif]--> <!--StartFragment--></p>\n<p class=\"MsoNormal\"><em><strong>Abstract</strong>: Emotional regulation is a topic currently being studied in the field of psychology. Five different types of emotional regulation strategies have been identified, distinguished by the stage of the emotion-response process in which they occur. To drastically simplify, this strategies are: situation selection, situation modification, deployment of attention, changes in cognition, and modulation of responses.&nbsp;</em></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><strong id=\"Introduction\">Introduction</strong></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">This is a follow-up to my <a href=\"/lw/aks/emotional_regulation_part_i_a_problem_summary/\">previous post</a> about my problem with emotional regulation.&nbsp;This is also my first outside-of-the-classroom foray into scholarship, <a href=\"/lw/3m3/the_neglected_virtue_of_scholarship/\">lukeprog</a> style. Mainly what I found is that it\u2019s surprisingly time-consuming and frustrating. I suffered a <em>lot </em></span><span lang=\"EN-GB\">of akrasia, compared to my usual, while writing this post\u2013mainly because I kept thinking \u2018oh my god, and then I have to cite my sources!\u2019 This may be an area where I need more practice...</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><strong><a id=\"more\"></a><span style=\"font-weight: normal;\"><strong></strong></span></strong></span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><strong><span style=\"font-weight: normal;\"><strong>What is emotion anyway?</strong></span></strong></span></p>\n<p class=\"MsoNormal\"><span style=\"mso-ansi-language: EN-US;\">Apparently there are quite a lot of </span><span lang=\"EN-GB\">competing definitions for \u2018emotion\u2019. Maybe this shouldn\u2019t be surprising\u2013the concept of emotion seem simple because most of the processing happens below conscious awareness, but emotions are as complex as the brains that create them.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span lang=\"EN-GB\">The definition that most research in emotional regulation uses is the \u2018response tendency\u2019 definition: emotions are </span><span style=\"mso-ansi-language: EN-US;\">adaptive behavioral and/or physiological responses, and they happen when the organism is put in evolutionarily significant situations.</span> <span lang=\"EN-GB\">The internal experience of emotion <em>may </em></span><span lang=\"EN-GB\">lead to a particular behaviour, but may not: emotion is a feedback mechanism that leads to various behaviours, rather than the direct cause of behaviour. </span><span style=\"mso-ansi-language: EN-US;\">Recent research has covered the specific purposes that emotions accomplish. They can facilitate decision-making, prepare the individual for a fast response to a given situation, inform on the match between organism and environment, and serve a social function; in general, they allow for learning. Emotional responses are not set in stone, and can be modulated on the way to taking their final shape. (Gross, 1988).</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">What does this mean for me, personally? One, emotions exist for a reason. They are <em>adaptive</em></span><span style=\"mso-ansi-language: EN-US;\">, and attempting to turn mine off entirely or prevent them from affecting my decisions would likely not be adaptive. Two, emotions are triggered by \u2018evolutionary significant\u2019 situations. To take a wild guess on what that might mean, being in a situation that involved competition against people who were much, much better than you might have had severe consequences in the ancestral environment...and even if not, for most of human history survival was more important than fun, and that would mean focusing on activities where you were likely to succeed, rather than those you <em>liked. </em></span><span style=\"mso-ansi-language: EN-US;\">My emotional response may be trying to inform me that the match between my organism and the environment is less than ideal\u2013or would have been if I were living 50 000 years ago.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h1 id=\"Emotional_Regulation\"><span style=\"font-family: Times;\">Emotional Regulation</span></h1>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">According to the people who study it, emotional regulation is what happens when people try to increase, decrease, or maintain their emotions, whether positive or negative. (For once, this seems like a pretty straightforward definition.) People may try to change the <em>kind </em></span><span style=\"mso-ansi-language: EN-US;\">of emotions they have, <em>when </em></span><span style=\"mso-ansi-language: EN-US;\">they have them, and how<em> </em></span><span style=\"mso-ansi-language: EN-US;\">they <em>experience </em></span><span style=\"mso-ansi-language: EN-US;\">and <em>express</em></span><span style=\"mso-ansi-language: EN-US;\"> them.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">\u201cThe available evidence does not support the existence of discrete emotional states. Instead, emotional responding appears to be organized in terms of a few fundamental dimensions, including <a href=\"http://en.wikipedia.org/wiki/Valence_(psychology)\">valence</a>, <a href=\"http://en.wikipedia.org/wiki/Two-factor_theory_of_emotion\">arousal</a>, and <a href=\"http://en.wikipedia.org/wiki/Approach-avoidance_conflict\">approach-avoidance</a>. The influence of emotion regulation on people\u2019s emotional states is therefore likely to be similarly dimensional. In other words, \"emotion regulation may not be so much concerned with getting people in or out of discrete emotional states like anger, sadness, or joy. Rather, emotion regulation may change people\u2019s emotional states along dimensions such as valence, arousal, and approach-avoidance.\u201d (Koole, 2009).</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">The study of emotion regulation isn\u2019t new. Freud studied it in the form of <a href=\"http://en.wikipedia.org/wiki/Defence_mechanism\">ego defenses</a>, which he saw as non-conscious processes that could, depending on the specific method used, result in reality distortion, excess energy consumption, and unnecessary non-gratification\u2013to him, these forms of emotional regulation were <em>maladaptive. </em></span><span style=\"mso-ansi-language: EN-US;\">(Gross, 1998).</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">More recently the study of <a href=\"http://en.wikipedia.org/wiki/Coping_(psychology)\">coping</a> has focused on emotional regulation from the point of view of conscious, deliberate, and adaptive processes. These can be based on fixing the underlying problem, i.e. problem-focused coping, or on reducing the negative emotions without changing the physical reality, i.e. emotion-focused coping. In general, emotion-focused coping is less effective and more likely to be associated with psychological distress. (Watson &amp;<em> </em></span><span style=\"mso-ansi-language: EN-US;\">Sinha, 2008).&nbsp;</span>According to more recent research, emotional regulation processes \u201cmay be automatic or controlled, conscious or unconscious, and may have their effects at one or more points in the emotion generative process.\u201d (Gross, 1998).</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">An individual\u2019s skill at emotional regulation must also be distinguished from their innate emotional sensitivity, which affects how much and how quickly they respond to an emotion-causing stimulus. In theory, Person A could be very sensitive, and experience a swift rush of negative emotions in response to an upsetting stimulus, but still be able to down-regulate the feelings afterwards, whereas Person B responds less quickly and steeply but lacks the skill to redirect the emotions they do experience. And whereas emotional sensitivity correlates with temperament differences in infants, and seems to develop independently of environmental influences, skill in emotional regulation develops and changes based on the quality of children\u2019s social interactions, and can continue to improve throughout life. (Koole, 2009).&nbsp;</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">Different strategies of emotional regulation can be classified by whether they are consciously controlled or automatic\u2013however, since conscious control is a complex and hard-to-define concept in itself, it may be more useful to classify strategies by <em>when </em></span><span style=\"mso-ansi-language: EN-US;\">they occur during the emotional response process.</span></p>\n<ol style=\"margin-top: 0in;\" type=\"1\">\n<li class=\"MsoNormal\" style=\"mso-pagination: none; mso-list: l0 level1 lfo1; tab-stops: list .5in; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">Selection of situation: occurs <em>before </em></span><span style=\"mso-ansi-language: EN-US;\">the stimulus that causes the emotion. </span></li>\n<li class=\"MsoNormal\" style=\"mso-pagination: none; mso-list: l0 level1 lfo1; tab-stops: list .5in; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">Modification of situation: occurs after the stimulus, but before the emotional response begins. </span></li>\n<li class=\"MsoNormal\" style=\"mso-pagination: none; mso-list: l0 level1 lfo1; tab-stops: list .5in; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">Deployment of attention: occurs during the emotional response process.<span style=\"mso-spacerun: yes;\">&nbsp; </span></span></li>\n<li class=\"MsoNormal\" style=\"mso-pagination: none; mso-list: l0 level1 lfo1; tab-stops: list .5in; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">Change in cognition: occurs during the emotional response process. </span></li>\n<li class=\"MsoNormal\" style=\"mso-pagination: none; mso-list: l0 level1 lfo1; tab-stops: list .5in; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">Modulation of responses: occurs <em>after </em></span><span style=\"mso-ansi-language: EN-US;\">the emotional response process. (Gross, 1998). </span></li>\n</ol>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">Methods #1 and #2, situation selection and modification, require a certain degree of self-knowledge, in order to decide which situations to seek out and which to avoid. There can be a conflict here between long term and short-term goals\u2013for example, a timid person can reduce their anxiety by avoiding social situations, but in the long run this can lead to undesirable social isolation. To further complicate things, the emotional response itself can back-propagate and modify the situation\u2013witness my taekwondo instructor\u2019s response to my freak-outs.</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">Deployment of attention has three sub-categories: distraction, concentration, and rumination. Distraction involves focusing attention onto neutral or non-emotional aspects of the situation, or shifting attention from difficult to tractable goals. Concentration involves focusing further on the situation, trying to enter a state of <a href=\"http://en.wikipedia.org/wiki/Flow_(psychology)\">flow</a> in order to avoid frustration. In rumination, attention is directed onto the feelings themselves, analyzing them.&nbsp;Wadlinger and Isaacowitz (2011) suggest that attention can be trained in order to better develop emotional regulation skills. Skill at directing and controlling attention is partly an innate trait, but studies indicate that attentional skills are also plastic and can improve with practice. For example, low mood can be improved with (gaze-based) training, which creates a bias towards looking at positive stimuli, such as happy instead of angry faces,<span style=\"mso-spacerun: yes;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">The fourth category, change in cognition, happens during the step where perceptions of the situation are given an emotional weight. The perceived capacity to manage or control a situation affects the emotions assigned to it. Classical Freudian defenses include denial, isolation, and intellectualization of the situation. Events can also be reinterpreted in a more positive light\u2013for example, <a href=\"http://en.wikipedia.org/wiki/Downward_social_comparison\">downward social comparison</a>, or a goal being reframed so that failure at the initial goal becomes a success according to the new goal. According to studies, this kind of reappraisal has a larger effect in complex than in simple situations. Factors that affect reappraisal include attribution of an event to self versus others, beliefs about the controllability of the event, accountability, expectations, and implicit personal theories of how emotion works. (Koole, 2009).</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">The last category, response modulation, does not affect the internal emotional experience at all, but only the expression of it. Examples given by the author include various medications such as anxiolytics, exercise, relaxation therapy, and self-soothing with alcohol, cigarettes, other drugs, or food, as well as simply initiating or hiding the expression of a given emotion. (Gross, 1998).</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h1 id=\"How_does_this_help_me_\"><span style=\"font-family: Times;\">How does this help me?</span></h1>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">Well, for one, it gives me a good idea of which techniques I\u2019ve <em>already </em></span><span style=\"mso-ansi-language: EN-US;\">tried, and which ones I might try next.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">1. <span style=\"text-decoration: underline;\">Situation selection</span>. To start with #1, I have used situation selection in the past, mainly when I decided to quit swimming to avoid pre-race meltdowns. That worked in the short term; when I wasn\u2019t putting myself under that much competitive pressure anymore, I had no reason to freak out, and my general stress levels dropped as well. But #1 is a method I would prefer to use sparingly, if at all; it seems that it would <em>seriously limit my future prospects</em></span><span style=\"mso-ansi-language: EN-US;\">, and running away from the things that scare me doesn\u2019t really fit with the mindset of wanting to be stronger.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">If anything, finding something challenging or even scary causes me to be even <em>more </em></span><span style=\"mso-ansi-language: EN-US;\">motivated to keep doing it until I don\u2019t find it scary anymore. (I think the thought process goes something like \u2018life could through you into a situation where you need this skill <em>at any moment</em></span><span style=\"mso-ansi-language: EN-US;\">, and wouldn\u2019t it be way less stressful if you\u2019d already been practicing it?\u2019</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">2. <span style=\"text-decoration: underline;\">Situation modification</span>. Is there any way that, without quitting taekwondo entirely, I could find a way to pick and choose what I do in class, avoiding the things that I know will make me upset? I can\u2019t think of any specific examples of how I could do this, except for making up excuses not to do particular exercises that I\u2019m bad at and that frustrate me. (I have an <em>actual </em></span><span style=\"mso-ansi-language: EN-US;\">excuse not to do frog jumps\u2013bad knees\u00ad\u2013but I think the fact that I <em>can\u2019t </em></span><span style=\"mso-ansi-language: EN-US;\">do it makes me more frustrated than if I went ahead and did them, because it makes me feel like I\u2019m not as good as the others.)</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">I can think of <em>other </em></span><span style=\"mso-ansi-language: EN-US;\">situations where I\u2019ve used this technique to calm myself down, though. Recently, at our university\u2019s Social Sciences Ball, I wasn\u2019t having that much fun and I was running out of what little steam I\u2019d had to begin with by 11 pm. I was <em>very </em></span><span style=\"mso-ansi-language: EN-US;\">upset to learn that the bus to take us back to campus, which I\u2019d thought would come at 11:30 pm, actually would come at 12:30 pm. (My stamina for social events lasts about 3 hours, and if I can\u2019t remove myself from the situation at that point, I start feeling some strange equivalent of claustrophobia, and will probably start crying if I can't get away.) Over my boyfriend\u2019s protests of \u2018it\u2019ll look bad on me if you leave by yourself now!\u2019 I resourcefully texted my brother and got him to look up the bus schedule online. It didn\u2019t end up working as planned, but having the <em>feeling of control </em></span><span style=\"mso-ansi-language: EN-US;\">restored calmed me down a lot, and when it turned out that the bus schedule was wrong, I came back to the party and went on enjoying myself like nothing had happened.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">This tells me that anytime my stress is due to feeling like I\u2019m not in control, and there\u2019s some proactive \u2018taking-control\u2019 move that I can execute, it\u2019s probably worth it even if it doesn\u2019t change my actual situation much\u2013it\u2019ll still have a huge effect on my emotional state, which in some cases is more important than the situation causing it.&nbsp;</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">3. <span style=\"text-decoration: underline;\">Deployment of attention: distraction, concentration, and rumination</span>. If I think about it, distraction is exactly what I do when I have a compelling stimulus available to distract myself with. This is more likely to be when I\u2019m alone, and that might well be the reason why meltdowns aren\u2019t a problem when I\u2019m alone. (One reason. Lack of social pressure is probably another.) If I\u2019m in public, and I\u2019m about to burst into tears, I\u2019ll tell myself \u2018okay, start thinking about one of your stories, <em>now</em></span><span style=\"mso-ansi-language: EN-US;\">!\u2019 But if someone tries to <em>talk </em></span><span style=\"mso-ansi-language: EN-US;\">to me, especially if the subject of conversation is the same as what\u2019s frustrating me, my attempts at self-distraction get derailed fast. Conclusion: I <em>could </em></span><span style=\"mso-ansi-language: EN-US;\">probably make this a useful method, but I need to come up with better distractions.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">Concentration, getting into a flow state, is a promising method, but likely it\u2019s something I would have to start doing <em>before </em></span><span style=\"mso-ansi-language: EN-US;\">I became frustrated at all. Certainly sparring in taekwondo is complex enough to occupy someone\u2019s full attention, leaving behind no excess processing power for frustration. Correction: this is the case for someone who <em>knows what they\u2019re doing</em></span><span style=\"mso-ansi-language: EN-US;\">. As a beginner, my inability to plan strategy fast enough to use it in real time means that I don\u2019t normally plan my strategy at all while fighting. That means a <em>lot </em></span><span style=\"mso-ansi-language: EN-US;\">of space left over for frustration-inadequacy-failure thought chains. The implication: as I <em>do </em></span><span style=\"mso-ansi-language: EN-US;\">get good enough to plan in real time, and coordinated enough to <em>enjoy </em></span><span style=\"mso-ansi-language: EN-US;\">the moment-to-moment satisfaction of pushing myself hard (like I do while swimming), frustration won\u2019t be so much of a problem.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">Rumination is a strategy I\u2019ve definitely used before, but I\u2019m not at all sure that it\u2019s an effective strategy in this context. In an exception to the general rule that thinking about my emotions dulls them, thinking about frustration and what\u2019s causing it leads to an explosive feedback loop. However, I might find it desirable to use this method when I\u2019m alone, in order to track down and list all the thoughts and emotions that occur, as <a href=\"/user/aelephant/\">user:&nbsp;aelephant</a><span style=\"mso-spacerun: yes;\">&nbsp;&nbsp;</span>suggested in <a href=\"/lw/aks/emotional_regulation_part_i_a_problem_summary/5z7t?context=1#5z7t\">this comment</a>.<span style=\"mso-spacerun: yes;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">4. <span style=\"text-decoration: underline;\">Changes in cognition</span>. This step of the process, where the emotion itself actually happens, seems like a productive place to start. The \u2018downwards social comparison\u2019 method could be translated into \u2018comparing myself to people who\u2019re the same belt level as me, instead of comparing myself to the black belts,\u2019 or at the very least persuading myself that not being as good as the black belts <em>isn\u2019t </em></span><span style=\"mso-ansi-language: EN-US;\">a reason to get frustrated.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">Reappraising a situation in a more positive light, or reframing your goal so that your actual results count as success rather than failure, also seems promising\u2013especially because often, when in the process of reappraising a goal, I realize that it wasn\u2019t even my real goal. Back when I was 14 and competing in swimming, \u2018win lots of races\u2019 and even \u2018go to the Olympics someway\u2019 <em>were </em></span><span style=\"mso-ansi-language: EN-US;\">explicit goals, even if I didn\u2019t want to admit it to friends and family.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">But I <em>didn\u2019t </em></span><span style=\"mso-ansi-language: EN-US;\">start taekwondo intending to \u2018win lots of tournaments\u2019. That wasn\u2019t even something I thought about <em>at all</em></span><span style=\"mso-ansi-language: EN-US;\">. My goals were, approximately, \u2018become fitter and more flexible, learn some self defense in case anyone ever tries to rape me when I\u2019m out late at night, and anyway martial arts are <em>cool </em></span><span style=\"mso-ansi-language: EN-US;\">so I\u2019ll acquire coolness just by showing up.\u2019 The fact that I turned out to have really awful reaction times, making it hard for me to win at sparring, doesn\u2019t equate to a failure at <em>any </em></span><span style=\"mso-ansi-language: EN-US;\">of these goals\u2013but the goal of \u2018beat other people in sparring\u2019 sneaked in there somewhere, probably because it\u2019s easier to measure than my original goals, and then starting causing me frustration when I failed to achieve it.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">6.<span style=\"text-decoration: underline;\"> Response Modulation</span>. I do the simplest form of this a lot\u2013the iron-jaw, stare-into-space-and-don\u2019t-cry approach does work a significant portion of the time to keep anyone from noticing until the emotions subside on their own. But that\u2019s if no one tries to <em>talk </em></span><span style=\"mso-ansi-language: EN-US;\">to me.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">As for the subtler methods, I already use exercise as a mood regulator, and frequently candy or baked goods to cheer myself up, and/or addictive books and shows. (Telling myself \u201cif you get through this, you can watch 30 minutes worth of Rescue 911 episodes on Youtube\u201d is a significant cheer-up factor.) But most of those methods aren\u2019t available to me on the spot when I\u2019m actually in a taekwondo class and starting to get upset.</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h1 id=\"Conclusion\"><span style=\"font-family: Times;\">Conclusion</span></h1>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\">My miniature foray into scholarship has allowed me to make a list of methods that humans use to regulate emotions. Methods that look promising include: finding ways to change the situation so that I feel in control; distracting myself from upsetting situations; trying to get into a state of concentration or flow; and reevaluating my goals to be realistic or achievable.</p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"mso-ansi-language: EN-US;\">My plan for the future: try to think of <em>specific </em></span><span style=\"mso-ansi-language: EN-US;\">ways I could use this methods, i.e. a particularly compelling chain of thought that I could use as a distraction, and then try all of them out and compare. I plan to show part or all of this article to at least one of my instructors, too, so that they have an idea of what I\u2019m working on, and can help me a little.</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><em>Note #1: I did get feedback from <a href=\"/user/juliawise/\">juliawise</a> on my first post, suggesting that I investigate <a href=\"http://en.wikipedia.org/wiki/Cognitive_behavioral_therapy\">cognitive behavioural therapy</a>&nbsp;and <a href=\"http://en.wikipedia.org/wiki/Dialectical_behavior_therapy\">dialectical behaviour therapy</a>. I think this article is long enough, though, so if I do investigate it, it\u2019ll go in a separate post. Don\u2019t worry, juliawise, it was good advice and I\u2019m not ignoring it.</em></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><em>Note #2: If anyone wants to see the articles in my reference list, I can't post links because I accessed them through my school account, but I have the PDFs saved and I can email them to you.&nbsp;</em></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"mso-pagination: none; mso-layout-grid-align: none; text-autospace: none;\"><span style=\"font-size: 26px; font-weight: bold; font-family: Times;\">References</span></p>\n<p><span lang=\"EN-GB\">&nbsp;<br></span><span style=\"mso-ansi-language: EN-US;\">Sander L. Koole. (2009). The psychology of emotion regulation: an integrative review. <em>Cognition and Emotion</em></span><span style=\"mso-ansi-language: EN-US;\">, 23 (1), 4_41<br></span><span style=\"mso-ansi-language: EN-US;\">&nbsp;<br></span><span style=\"mso-ansi-language: EN-US;\">James J. Gross. (1998). The Emerging Field of Emotion Regulation: An Integrative Review. <em>Review of General Psychology</em></span><span style=\"mso-ansi-language: EN-US;\">, Vol. 2, No. 5,271-299<br></span><span style=\"mso-ansi-language: EN-US;\">&nbsp;<br></span><span style=\"mso-ansi-language: EN-US;\">Watson David C., Sinha, Birenda. (2008). Emotion Regulation, Coping, and Psychological Symptoms.<strong> </strong></span><span style=\"mso-ansi-language: EN-US;\"><em>International Journal of Stress Management</em></span><span style=\"mso-ansi-language: EN-US;\">, Vol. 15, No. 3, 222\u2013234<br></span><span style=\"mso-ansi-language: EN-US;\">&nbsp;<br></span><span style=\"font-size: 12.0pt; font-family: Times; mso-ansi-language: EN-US;\">Wadlinger, Heather A., Isaacowitz, Derek M. (2011). Fixing Our Focus: Training Attention to Regulate Emotion. <em>Personality and Social Psychology Review</em></span><span style=\"font-size: 12.0pt; font-family: Times; mso-ansi-language: EN-US;\">,15(1) 75\u2013102</span></p>\n<!--EndFragment-->\n<p>&nbsp;</p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 2}, {"title": "Emotional Regulation", "anchor": "Emotional_Regulation", "level": 1}, {"title": "How does this help me?", "anchor": "How_does_this_help_me_", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "40 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["B9WxT7fQKhhywW2PN", "64FdKLwmea8MCLWkE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-20T02:29:05.142Z", "modifiedAt": null, "url": null, "title": "Ontologial Reductionism and Invisible Dragons", "slug": "ontologial-reductionism-and-invisible-dragons", "viewCount": null, "lastCommentedAt": "2020-07-29T11:34:45.227Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Balofsky", "createdAt": "2012-03-01T01:08:00.100Z", "isAdmin": false, "displayName": "Balofsky"}, "userId": "DKmvxF5uD4844YGAZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eDjmssYSwwegBGvcb/ontologial-reductionism-and-invisible-dragons", "pageUrlRelative": "/posts/eDjmssYSwwegBGvcb/ontologial-reductionism-and-invisible-dragons", "linkUrl": "https://www.lesswrong.com/posts/eDjmssYSwwegBGvcb/ontologial-reductionism-and-invisible-dragons", "postedAtFormatted": "Tuesday, March 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ontologial%20Reductionism%20and%20Invisible%20Dragons&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOntologial%20Reductionism%20and%20Invisible%20Dragons%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeDjmssYSwwegBGvcb%2Fontologial-reductionism-and-invisible-dragons%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ontologial%20Reductionism%20and%20Invisible%20Dragons%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeDjmssYSwwegBGvcb%2Fontologial-reductionism-and-invisible-dragons", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeDjmssYSwwegBGvcb%2Fontologial-reductionism-and-invisible-dragons", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 10436, "htmlBody": "<p><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span class=\"script-hebrew\"><span style=\"font-family: &quot;David&quot;,&quot;sans-serif&quot;; font-size: 16pt; mso-bidi-language: HE;\" dir=\"rtl\" lang=\"HE\">\u05d1\u05e1</span></span><span style=\"font-family: &quot;David&quot;,&quot;sans-serif&quot;; font-size: 16pt; mso-bidi-language: HE;\" dir=\"rtl\" lang=\"HE\">\"<span class=\"script-hebrew\">\u05d3</span></span><span style=\"font-size: 16pt; mso-bidi-font-family: David; mso-bidi-language: HE;\"></span></p>\r\n</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\">&nbsp;</p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">A few days ago, a friend of mine sent me a link to Eliezer Yudkowsky&rsquo;s article, &ldquo;</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"><a href=\"/lw/i8/religions_claim_to_be_nondisprovable/\">Religion&rsquo;s Claim to be Non-Disprovable</a>.</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&rdquo; This wasn&rsquo;t the first time my friend had sent me articles by Yudkowsky from Less Wrong, concerning religion in general and Judaism in particular. With each of us having grown up in secular-yet-cultural Jewish homes, and with me having morphed into an </span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"><span style=\"color: #0000ff;\"><a href=\"http://en.wikipedia.org/wiki/Orthodox_Judaism\">Orthodox Jew</a></span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"> of the </span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"><span style=\"color: #0000ff;\"><a href=\"http://en.wikipedia.org/wiki/Hasidic_Judaism\">Lubavitch-Chasidic</a></span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"> variety early in college, the material for discussion (arguing?) is usually pretty good.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">So I wrote a response to Yudkowsky&rsquo;s article. Originally meant to be a long-ish Facebook post, it got longer and longer&hellip; and I ended up with an essay article instead. So rather than post it on Facebook, I decided that I&rsquo;d share it with the good people of the Less Wrong community. But first, some necessary preliminary remarks.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\r\n<p class=\"list0020paragraph\" style=\"text-indent: -0.25in; margin: 0in 0in 0pt 56pt;\"><span class=\"list0020paragraphchar1char1\"><span style=\"font-family: Symbol; font-size: 12pt;\">&middot;</span></span><span style=\"font-family: Calibri;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">While I have made an effort to familiarize myself with Less Wrong&rsquo;s Core Sequences and with the more essential material, <span class=\"list0020paragraphchar1char1\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-font-size: 12.0pt; mso-bidi-font-size: 12.0pt;\">I admit that I am not as intimately familiar with the methods of mathematical calculation and philosophical dialectic as I&rsquo;m sure many members of the Rationalist community here are. I will do my best to keep my assertions clear and intellectually honest, but I apologize for any unconventionality in style, and for my extensive over-use of parentheses. </span></span></span></p>\r\n<p class=\"list0020paragraph\" style=\"margin: 0in 0in 0pt 0.5in;\"><span style=\"font-family: Calibri;\">&nbsp;</span></p>\r\n<p class=\"list0020paragraph\" style=\"text-indent: -0.25in; margin: 0in 0in 0pt 56pt;\"><span class=\"list0020paragraphchar1char1\"><span style=\"font-family: Symbol; font-size: 12pt;\">&middot;</span></span><span style=\"font-family: Calibri;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class=\"list0020paragraphchar1char1\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">This article is <strong style=\"mso-bidi-font-weight: normal;\"><em>not</em></strong> meant to be a presentation of evidence for the existence of G-d or the authenticity of the Torah, in the style of an argument, and so &ldquo;G-d is real and wrote the Torah&rdquo; is not my thesis. Rather, this is only meant as a response to Yudkowsky&rsquo;s assertion that &lsquo;Old Testament&rsquo; based thinking deserves no place in any sort of intellectual discourse. My thesis here is simply that Yudkowsky&rsquo;s critiques of Judaism are generally not correct, that it does deserve such a place, and that Rationalist thinking should not necessitate an <em style=\"mso-bidi-font-style: normal;\">a priori </em>rejection of all religious philosophy. This essay is meant to be intellectually open to criticism (<em>any</em> criticism).</span></span></p>\r\n<p class=\"list0020paragraph\" style=\"margin: 0in 0in 0pt 0.5in;\"><span style=\"font-family: Calibri;\">&nbsp;</span></p>\r\n<p class=\"list0020paragraph\" style=\"text-indent: -0.25in; margin: 0in 0in 0pt 0.75in;\"><span class=\"list0020paragraphchar1char1\"><span style=\"font-family: Symbol; font-size: 12pt;\">&middot;</span></span><span style=\"font-family: Calibri;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class=\"list0020paragraphchar1char1\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">This critique is meant to be a reasonable one, and is <em>not</em> meant as a personal attack on Yudkowsky or any other members of the skeptic community, nor is it<em> </em>meant as any sort of moral rebuke, or anything along those lines. My arguments are meant to be logical, and are meant to be in accord with the etiquette appropriate for the Less Wrong community. </span></span></p>\r\n<p class=\"list0020paragraph\" style=\"margin: 0in 0in 0pt 0.5in;\"><span style=\"font-family: Calibri;\">&nbsp;</span></p>\r\n<p class=\"list0020paragraph\" style=\"text-indent: -0.25in; margin: 0in 0in 0pt 0.75in;\"><span class=\"list0020paragraphchar1char1\"><span style=\"font-family: Symbol; font-size: 12pt;\">&middot;</span></span><span style=\"font-family: Calibri;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class=\"list0020paragraphchar1char1\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">I am aware and have read the many other essays Yudkowsky has written on rationality and religion, and I am familiar with SIAI&rsquo;s work. While this essay will focus particularly on &ldquo;Religion&rsquo;s Claim to be Non-Disprovable,&rdquo; I will make references to Yudkowsky&rsquo;s other essays if needed for clarification, while trying to avoid unnecessary digressions.</span></span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">For clarity&rsquo;s sake, I will organize my responses into five categories, ranked in order according to the organization of this particular article by Yudkowsky (roughly). These categories are not meant to be strict, but are simply meant to function as an organizational tool. I&rsquo;ll limit my responses to issues concerning Judaism, since Judaism (though Yudkowsky prefers the term &ldquo;Old Testament&rdquo;) is the primary subject of criticism in the article, and it is the religious <em style=\"mso-bidi-font-style: normal;\">Weltanschauung</em> that I am most familiar with. As Yudkowsky generally employs an academic style mixed with satire, I&rsquo;ll try to do so too, and given the context of this post, I will generally use English rather than Hebrew terminology, while simultaneously trying to avoid overtly Christian terminology. I will be using the <em style=\"mso-bidi-font-style: normal;\">Mishneh Torah</em> (12<sup>th</sup> c. CE) as my primary reference for Judaic law, given that it is the oldest, the most topically organized, and the most comprehensive code of Jewish law.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">My five general critiques are:</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\r\n<p class=\"MsoListParagraphCxSpFirst\" style=\"text-indent: -0.25in; margin: 0in 0in 0pt 0.5in; mso-list: l0 level1 lfo1;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\"><span style=\"mso-list: Ignore;\">1)<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">The author makes precisely 3 statements regarding Halacha (Judaic law), each of which is demonstrably incorrect. </span></p>\r\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin: 0in 0in 0pt 0.5in;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\r\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"text-indent: -0.25in; margin: 0in 0in 0pt 0.5in; mso-list: l0 level1 lfo1;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\"><span style=\"mso-list: Ignore;\">2)<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">The author asserts that the Tanakh (Old Testament) &ldquo;doesn&rsquo;t talk about a sense of wonder at the complexity of the universe.&rdquo;</span></p>\r\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin: 0in 0in 0pt 0.5in;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\r\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"text-indent: -0.25in; margin: 0in 0in 0pt 0.5in; mso-list: l0 level1 lfo1;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\"><span style=\"mso-list: Ignore;\">3)<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">The author asserts that historical Judaism defends the authenticity of the Torah without accounting for Bayes&rsquo; Theorem.</span></p>\r\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin: 0in 0in 0pt 0.5in;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\r\n<p class=\"MsoListParagraphCxSpLast\" style=\"text-indent: -0.25in; margin: 0in 0in 0pt 0.5in; mso-list: l0 level1 lfo1;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\"><span style=\"mso-list: Ignore;\">4)<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">The author asserts that contemporary religionists justify false opinions by claiming that their religion is a separate magisterium which can be neither proven nor disproven. <span style=\"mso-spacerun: yes;\">&nbsp;</span></span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\r\n<p class=\"MsoListParagraph\" style=\"text-indent: -0.25in; margin: 0in 0in 0pt 0.5in; mso-list: l0 level1 lfo1;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\"><span style=\"mso-list: Ignore;\">5)<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">The author asserts that the Torah&rsquo;s views on legislation, government, history, sexual morals, science, and (most pointedly) ethics are outdated, in light of what Yudkowsky describes as progressive human advancement. </span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.25in;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">Section One:</span></strong><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"> <strong style=\"mso-bidi-font-weight: normal;\">The author makes three statements regarding Halacha, each of which is demonstrably false.</strong> </span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">Yudkowsky claims that according to the Torah, (1) cross-dressing is a capital crime, (2) rabbits are ruminants, and (3) locusts have four legs. </span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">Regarding the first statement, though the Torah prohibits men from wearing women&rsquo;s clothing (and vice versa), it is not a crime that carries the death penalty, as stated in the Code of Jewish Law: &ldquo;A man who adorns himself as a woman does, and a woman who adorns herself as a man does, are <em style=\"mso-bidi-font-style: normal;\">chayav</em>, liable (</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 8pt;\">Mishneh Torah., Laws of Idolatrous Worship 12:10</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">)&rdquo;, indicating there is no capital punishment. </span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">Regarding the second statement,</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-ansi-language: EN;\"> <span lang=\"EN\">it is true that rabbits are listed in the Torah as chewing their cud, and it <em style=\"mso-bidi-font-style: normal;\">is </em>true that according to modern scientific observation they are not true ruminants. However, it is also true that rabbits do practice a form cecotrophy through the </span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">reingestion of special fecal pellets. Hence, rabbinic authorities that interpret the Hebrew word <em style=\"mso-bidi-font-style: normal;\">shafan</em> as &lsquo;rabbit&rsquo; classify their manner of cecotrophy as <em style=\"mso-bidi-font-style: normal;\">ma&rsquo;aleh gerah</em>, chewing the cud. Likewise, even in this instance, the Halacha recognizes that rabbits are not true ruminants in the exact same sense as, say, cows or sheep. The Code of Jewish Law states, </span></p>\r\n<p style=\"margin-left: 0.25in;\"><span style=\"font-family: Times New Roman;\">&ldquo;The signs of a kosher domesticated animal and beast are explicitly mentioned in the Torah. There are two signs: a split hoof and chewing the cud. Both are necessary. Any domesticated animal and beast that chews the cud does not have teeth on its upper jaw-bone. <strong style=\"mso-bidi-font-weight: normal;\">Every animal that chews the cud has split hoofs except a camel</strong>. Every animal that has split hoofs chews the cud except a pig. (<span style=\"font-size: 8pt;\">M.T., Laws of Forbidden Foods 1:2</span>)&rdquo; </span></p>\r\n<p><span style=\"font-family: Times New Roman;\">The commentator <em>Maggid Mishneh</em> explains that the <em style=\"mso-bidi-font-style: normal;\">Mishneh Torah</em> does not mention rabbits as chewing their cud because they have teeth on their upper jaw, and complete ruminants do not.<span style=\"mso-spacerun: yes;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></p>\r\n<p><span style=\"font-family: Times New Roman;\">Regarding the third statement, the third book of the Torah states, &ldquo;(A)mong all the flying insects that walk on four, you may eat those that have jointed extensions above its legs, with which they hop on the ground.&rdquo; On this subject, the Code of Jewish Law says that these four legs are excluding the back legs meant for jumping, stating, &ldquo;Whenever a species has four legs, four wings that cover the majority of the length and the majority of the width of its body, and it has two longer legs to hop, it is a kosher species. (<span style=\"font-size: 8pt;\">M.T., Laws of Forbidden Foods 1:22</span>)&rdquo; As part of their traditional cuisine, the Yemenite Jewish community to this day still eats those locusts identified in the Torah as kosher.</span></p>\r\n<p><span style=\"font-family: Times New Roman;\">While I understand that these are <em style=\"mso-bidi-font-style: normal;\">highly</em> peripheral points of<span style=\"mso-spacerun: yes;\">&nbsp; </span>Yudkowsky&rsquo;s article, I&rsquo;m noting them because I&rsquo;m guessing that Yudkowsky has some sort of yeshiva background, has probably actually studied Jewish law before, and yet each comment in his article regarding Halacha is incorrect. Also, if one were to argue that these halachic rulings may just be apologetic glosses on a defective primary text (the Torah), I&rsquo;d point out that the <em style=\"mso-bidi-font-style: normal;\">Mishneh Torah</em> was written in the 12<sup>th</sup> century CE, and that its rulings are a compilation of rulings from the Mishnah (1<sup>st</sup> c. BCE-2<sup>nd</sup> c. CE). General knowledge of animal biology was still very rudimentary in both eras.</span></p>\r\n<p><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"font-family: Times New Roman;\">&nbsp;</span></strong></p>\r\n<p><span style=\"font-family: Times New Roman;\"><strong style=\"mso-bidi-font-weight: normal;\">Section Two:</strong> <strong style=\"mso-bidi-font-weight: normal;\">The author claims that the Tanakh &ldquo;doesn&rsquo;t talk about a sense of wonder at the complexity of the universe.&rdquo; </strong></span></p>\r\n<p><span style=\"font-family: Times New Roman;\">It does. It would be difficult to find a string of Tehillim (King David&rsquo;s psalms) that do not make extensive use of nature imagery. Poetic references to nature are so frequent in the 150 Tehillim, that Talmudic legend portrays King David as spending his free time wandering around in the wilderness alone, and as having the ability to understand the language of birds, trees, plants, and leaves of grass. In fact, people in the Jewish tradition who are regarded as uniquely holy are <em style=\"mso-bidi-font-style: normal;\">often</em> portrayed as having a relationship with, and being in awe of the beauty of nature, and as having the ability to speak with birds and trees. This includes various Talmudic sages, the Arizal, the Baal Shem Tov, Nachman of Breslov, R. Zundel of Salant, z&rdquo;l etc. Considering that the Psalms have formed the basis of Hebrew ritual prayer since the first Israelite ruling dynasty, this does not seem in keeping with Yudkowsky&rsquo;s portrayal of the ancient Hebrews as being unconcerned with the wonders of nature. <strong style=\"mso-bidi-font-weight: normal;\"></strong></span></p>\r\n<p><span style=\"font-family: Times New Roman;\">However, Yudkowsky&rsquo;s point may still hold. It&rsquo;s true that there&rsquo;s nothing written in the Tanakh that seems to be obviously on the same wavelength as, say, Aristotle&rsquo;s <em style=\"mso-bidi-font-style: normal;\">Metaphysics</em> or Euclid&rsquo;s <em style=\"mso-bidi-font-style: normal;\">Elements</em>. However, the study of science and theoretical metaphysics is most definitely not lacking in Hebrew oral tradition&mdash; which, as Yudkowsky and any other scholar should know, is every bit a part of historical Judaism as the Tanakh itself. In <em style=\"mso-bidi-font-style: normal;\">The Kuzari</em>, one of the classics of traditional Jewish philosophy written in the 12<sup>th</sup> century CE, the author Judah HaLevy describes the Torah &amp; Talmud&rsquo;s treatment of science this way: </span></p>\r\n<p style=\"margin-left: 0.25in;\"><span style=\"font-family: Times New Roman;\">&ldquo;The members of the Sanhedrin (ancient chief rabbinic court in Jerusalem of 70 members) were bound not to let any science, real and fictitious, or conventional, escape their knowledge, magic and language included. How was it possible at all times to find 70 available scholars unless learning was common among the Israelites? This could not be otherwise, as <strong style=\"mso-bidi-font-weight: normal;\">all branches of science were required for the practice of the Law</strong>.&rdquo;<span style=\"mso-tab-count: 2;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></p>\r\n<p><span style=\"font-family: Times New Roman;\">HaLevy then goes on to give a brief description of how the practice of Judaism <em style=\"mso-bidi-font-style: normal;\">requires</em> intensive study of astronomy, music, agriculture, natural medicine, Hebrew grammar, foreign languages, human anatomy &amp; biology, legalism, hermeneutic logic, economics, trigonometry, rhetoric and metaphysical speculation, as all of these disciplines are necessary for the precise application of Jewish law. Expertise in at least 2 areas of science and 6 foreign languages were requisite for membership in the Sanhedrin, and indeed, all of these sciences are extensively utilized in the Midrash, Mishnah and Talmud. HaLevy also emphasizes that the Jewish value of scholarly pursuit is inherited from ancient Israel&rsquo;s scholars and prophets, and even maintains that the Greek love of philosophy originated as a Jewish influence. </span></p>\r\n<p><span style=\"font-family: Times New Roman;\">In other words, the Torah does not <em style=\"mso-bidi-font-style: normal;\">displace </em>intellectual inquiry, but rather, is specifically designed to stimulate it. Put another way, &ldquo;<em style=\"mso-bidi-font-style: normal;\">Lo am ha&rsquo;aretz chasid</em>. (<span style=\"font-size: 8pt;\">Mishnah, Pirkei Avos</span>)&rdquo; An ignoramus cannot be pious. </span></p>\r\n<p><span style=\"font-family: Times New Roman;\">I do not know if Socrates and Plato really did acquire all of their philosophical methods by traveling to Jerusalem and studying in the court of King Solomon, as certain Jewish traditions claim. But the fact that such traditions even existed is a testament to the reverence that ancient Jewry held for the pursuit of knowledge <em style=\"mso-bidi-font-style: normal;\">as a direct result </em>of the Mosaic Law, and is, again, hardly in keeping with Yudkowsky&rsquo;s assertion that &ldquo;the Old Testament doesn&rsquo;t talk about a sense of wonder at the complexity of the universe - it was busy laying down the death penalty for women who wore men&rsquo;s clothing, which was solid and satisfying religious content of that era.&rdquo;</span></p>\r\n<p><span style=\"font-family: Times New Roman;\">Furthermore, even though HaLevy cites the Torah itself as a source for the necessity to master the sciences <span style=\"mso-spacerun: yes;\">&nbsp;</span>(&ldquo;And you shall keep and do them, for that is your wisdom and your understanding in the eyes of the nations, who will hear all these statutes and say, &lsquo;Only this great nation is a wise and understanding people.&rsquo;&rdquo; <span style=\"font-size: 8pt;\">Deuteronomy 4:6</span>), this does not mean that any scientific or philosophical pronouncement from the mouth of a religious scholar was taken as dogma. In his discussion of <em style=\"mso-bidi-font-style: normal;\">Sefer Yetzirah</em>, a book of metaphysical cosmogony legendarily attributed to the Biblical Abraham, HaLevy says after his description of its contents, </span></p>\r\n<p style=\"margin-left: 0.5in;\"><span style=\"font-family: Times New Roman;\">&ldquo;This, however, is still not satisfactory, because the object of research is either too profound to be fathomed, or our minds are inadequate, or both&hellip; for no two philosophers could ever agree on this matter, unless they happen to have had the same teacher.&rdquo;</span></p>\r\n<p><span style=\"font-family: Times New Roman;\">In other words, one of the most popular Jewish philosophical texts in history openly admits that the Patriarch Abraham&rsquo;s views on cosmology may have been incorrect. I mention this point in anticipation of the counter-argument that, perhaps, ancient Hebrew interest in science and philosophy was strangled by preconceived outcomes and intellectual dogmas (&ldquo;Why continue studying the universe, when Abraham&rsquo;s <em style=\"mso-bidi-font-style: normal;\">Sefer Yetzirah</em> explains the whole thing already!?&rdquo;). Indeed, the <em style=\"mso-bidi-font-style: normal;\">Sefer Yetzirah </em>was later followed by other books and schools of Jewish thought with contending views of nature, matter, and the human mind. </span></p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\"><span style=\"font-family: Times New Roman;\"><span class=\"normal00200028web0029char1\">It is here</span> that we touch on the real issue. Yudkowsky already openly admits that Judaism encourages questioning. However, he asserts that such questioning is strangled by predetermined outcomes, and therefore, does not constitute a genuine pursuit of knowledge&mdash; he explains his views on this in, &ldquo;</span><span class=\"hyperlinkchar1\"><span style=\"font-family: Times New Roman; color: #0000ff;\"><a href=\"/lw/jy/avoiding_your_beliefs_real_weak_points/\">Avoiding Your Belief's Real Weak Points</a></span></span><span style=\"font-family: Times New Roman;\">.&rdquo; Likewise, in &ldquo;</span><span class=\"hyperlinkchar1char1\"><span style=\"font-family: Times New Roman; color: #0000ff;\"><a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">Tsuyoku Naritai!</a></span></span><span style=\"font-family: Times New Roman;\">,&rdquo; Yudkowsky points out that modern rabbis cannot overrule ancient rabbis in matters of Halacha, because of the assumption that since G-d gave the Torah to Rabbi A, who gave it to Rabbi B, knowledge of the Torah inevitably decreases as it reaches Rabbis C, D, E, etc. Yudkowsky then concludes that both knowledge and ethics, as conceived in Orthodox Judaism, only degrades, in addition to being limited by preconceived outcomes, thus making Judaism&rsquo;s approach to learning qualitatively inferior to the approach of the scientific method.</span></p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\"><span class=\"normal00200028web0029char1\"><span style=\"font-family: Times New Roman;\">&nbsp;</span></span></p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\"><span style=\"font-family: Times New Roman;\"><span class=\"normal00200028web0029char1\">Before proceeding, I want to pause and clarify a con</span>cept. In &ldquo;Tsuyoku Naritai!&rdquo;, Yudkowsky is making reference to the more general Jewish concept of <span class=\"normal00200028web0029char1\"><em>yeridas hadoros</em></span>, or &ldquo;descent of the generations,&rdquo; which generally refers to the notion that people become less-and-less spiritual with succeeding generations, and&nbsp; the ancient past is therefore associated with sacred origins, rather than with more primitive stages of a progressive, evolutionary scale. Yudkowsky&rsquo;s views on Judaism&rsquo;s approach to knowledge and progress are based primarily, I think, on three misunderstandings concerning <em style=\"mso-bidi-font-style: normal;\">yeridas hadoros. </em></span></p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in 5pt 0.5in;\"><span class=\"normal00200028web0029char1\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"font-family: Times New Roman;\">&nbsp;</span></strong></span></p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in 5pt 0.5in;\"><span style=\"font-family: Times New Roman;\"><span class=\"normal00200028web0029char1\"><strong style=\"mso-bidi-font-weight: normal;\">1)</strong></span> <strong style=\"mso-bidi-font-weight: normal;\">Regarding Halacha:</strong> It is true that modern rabbis cannot overrule ancient rabbis in matters of Halacha&mdash; this principle of Jewish legal reasoning is written in the Talmud, Masechta Megillah (&ldquo;No rabbinic court may nullify the ruling of another rabbinic court, unless they are superior in wisdom and number&rdquo;). The logic underlying this rule derives from the increasingly weaker claims to traditional interpretation of the Torah, commensurate with increasingly later stages of a lengthening historical chain of transmission from the original Revelation at Sinai, and the codification of the oral tradition in the Talmud. From a legal standpoint, this is logical&mdash; and in fact, it&rsquo;s empirically observable. &nbsp;For&nbsp;instance: nowadays, </span><span style=\"font-family: Times New Roman;\"><a href=\"http://en.wikipedia.org/wiki/Mezuzah\">mezuzahs</a></span><span style=\"font-family: Times New Roman;\"> are placed at a diagonal, due to doubt whether they should be vertical or horizontal. There is a doubt as to whether&nbsp;new days start&nbsp;at sundown or nightfall, therefore, the Sabbath is 25 hours long, to include both&nbsp;opinions. There are doubts about how to make </span><span style=\"font-family: Times New Roman;\"><a href=\"http://en.wikipedia.org/wiki/Tefillin\">tefillin</a></span><span style=\"font-family: Times New Roman;\"> properly; therefore, there are both <span class=\"normal00200028web0029char1\"><em>Rashi </em></span>and <span class=\"normal00200028web0029char1\"><em>Rabbeinu Tam </em></span>tefillin. There really was once a time when these issues were not debated; but doubts arose over time, mainly due to exile and government persecutions that caused interruptions in the oral tradition. This is an empirically observable aspect of <span class=\"normal00200028web0029char1\"><em>yeridas hadoros</em></span>, and it&rsquo;s in reference to this principle that the Talmud (Shabbat 112b) says, &ldquo;If the earlier scholars were sons of angels, we are sons of men; and if the earlier scholars were sons of men, we are&nbsp;like donkeys.&rdquo; Furthermore, acquiescing to courts &ldquo;greater in wisdom and number&rdquo; and to courts of previous eras is designed to produce a smoothly functioning legal system, a democratic process in scholarly legal rulings, and to prevent schisms among Jewry with varying legal approaches. This is why the outcomes of Talmudic legal debates were decided according to a vote from the Sanhedrin, and why decisions are determined by the majority opinion of the Sanhedrin, even if the majority opinion is incorrect. This&nbsp;is legal reasoning, rather than scientific reasoning.</span></p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"font-family: Times New Roman;\">&nbsp;</span></strong></p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in 5pt 0.5in;\"><span style=\"font-family: Times New Roman;\"><strong style=\"mso-bidi-font-weight: normal;\">2)</strong> <strong style=\"mso-bidi-font-weight: normal;\">Regarding acquisition of new scientific knowledge:</strong> Rabbinic authorities acquiesce to&nbsp;courts greater in wisdom and number concerning (even incorrect) legal rulings,&nbsp;for the reasons stated above. However, they do <span class=\"normal00200028web0029char1\"><em>not </em></span>acquiesce in objective scientific knowledge, and in fact, most rabbinic authorities throughout the ages have not even permitted this approach. Rabbi Avraham HaNaggid (13th c. CE), in&nbsp;his commentary to the Talmud, put it this way: </span></p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in 5pt 1in;\"><span style=\"font-family: Times New Roman;\">&ldquo;Know that it is&nbsp;your duty to understand that whoever propounds a certain theory or idea and expects that theory or idea to be accepted merely out of respect for the author without proving its truth and rationality, pursues a wrong method prohibited by both the Torah and human intelligence. From the standpoint of &nbsp;intelligence, such a method is worthless for it would cause one to minimize the &nbsp;importance of those things which, after scrupulous observation and proofs, ought &nbsp;to be believed, and from the point of view of the Torah&mdash; because it inclines from &nbsp;the true path and from the straight, leveled road. The L-rd, praised be He! said: &ldquo;Thou shalt not respect the poor person, nor honor the great person; in justice shalt thou judge <span style=\"font-size: 8pt;\">(Lev. 19, 15)</span>&rdquo;. And it also says, &ldquo;You shall not respect a person in&nbsp;judgment <span style=\"font-size: 8pt;\">(Deut. 1:17)</span>&rdquo;. And there is no difference between him who accents an idea without any evidence as to its integrity, and him who believes a person&rsquo;s statement simply because he respects the latter and therefore contends that his idea is undoubtedly true since it emanates from a great man like Heiman, Karkal, or Darda. For all this gives no evidence as to the merits of the subject in question and is therefore forbidden. According to this preamble, then, we are not in duty bound to defend the opinions of the sages of the Talmud, concerning medicine, physics and astrology, etc, as right in every respect simply because we know the sages to be great men with a full knowledge of all things regarding the Torah, in its various details. Although it is true that in so far as knowledge of our Torah &nbsp;is concerned, we must believe the sages arrived at the highest stage of &nbsp;knowledge, as it is said, &ldquo;In accordance with the instructions which they may &nbsp;instruct thee, etc <span style=\"font-size: 8pt;\">(Deut 17:11)</span>&rdquo;, still it is not necessarily so concerning any other &nbsp;branch of knowledge. You can see that even the sages themselves say very often of things which cannot proven by discussions and arguments, &ldquo;I swear, that&nbsp;even had Joshua bin Nun said it, I would not obey him!&rdquo; This means that I would not believe him although he was a prophet&mdash; since he cannot prove the reason for such a thing in accordance with the rules of the Talmudic construction.&rdquo; </span></p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\"><span style=\"font-family: Times New Roman;\">&nbsp;</span></p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in 5pt 0.5in;\"><span style=\"font-family: Times New Roman;\"><strong style=\"mso-bidi-font-weight: normal;\">3) Regarding human progress: </strong>The concept of generational descent is only applied to the exoteric aspects of Judaism, such as the plainer meanings of the Tanakh, Halacha, Midrashic literature, etc. In contrast, the more esoteric aspects of Judaism, such as Jewish Philosophy and Kabbalah, are generally regarded as experiencing a generational <em style=\"mso-bidi-font-style: normal;\">ascent</em>, rather than descent. Consequently, <em style=\"mso-bidi-font-style: normal;\">yeridas hadoros </em>is traditionally understood as a dialectic process of decreasing knowledge of the more &ldquo;revealed&rdquo; aspects of the Jewish tradition, accompanied simultaneously by concepts of meta-ethics and philosophy of increasing intellectual sophistication (though dialectically, innate human <em style=\"mso-bidi-font-style: normal;\">intuition</em> for spirituality decreases generationally).</span></p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\"><span style=\"font-family: Times New Roman;\">&nbsp;</span></p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\"><span style=\"font-family: Times New Roman;\"><span class=\"normal00200028web0029char1\">I admit that certain intellectual limitations do undoubtedly </span>exist in Judaism, just as they exist within any religion or ideology that operates around certain axioms. An orthodox Jewish thinker could never truly adopt utilitarian hedonism, for instance, and remain an Orthodox Jew, just as a Marxist could never decide that capitalism is actually a really good idea and remain a socialist. Nor could the Orthodox Jew reject, say, prophecy or free-will. However, there is nothing inherently non-rational in such assertions so long as these assertions are the product of free thought, and can be argued for intelligently. </span></p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\">&nbsp;</p>\r\n<p><span style=\"font-family: Times New Roman;\">\r\n<p><strong style=\"mso-bidi-font-weight: normal;\">Section Three: The author claims that historical Judaism defends the authenticity of the Torah, without accounting for Bayes&rsquo; rule. </strong></p>\r\n<p>Here, I will not contend that the Torah is, indeed, a direct communication from The Ineffable to Moses (though I do think so). That would be beyond the scope of this essay. Instead, I will limit myself to arguing that the traditional Jewish claim is not based on an intrinsic logical fallacy.</p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; mso-layout-grid-align: none;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">The Bayesian rule </span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-ansi-language: EN;\" lang=\"EN\">expresses how a subjective degree of belief should rationally change to account for evidence. More specifically, proponents of Bayes&rsquo; theorem generally posit that (1) </span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">it is illogical</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\"> to ignore what we </span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">know, (2) it is natural and useful </span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">to cast what we know in the language of probabiliti</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">es, and (3) if our subjective </span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">probabilities are erroneous, their impact will get</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"> washed out in due time, as the number of observations increases. </span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; mso-layout-grid-align: none;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; mso-layout-grid-align: none;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">Before proceeding, I will admit that arguing in favor of the Torah&rsquo;s authenticity is difficult in light of purely Bayesian reasoning, as the traditional understanding is rooted in <em style=\"mso-bidi-font-style: normal;\">causal </em>reasoning, rather than probabilistic. As probability theory deals with beliefs about a static environment, while causality deals with changes that occur in the world itself in real time, it is only natural that rabbinic understandings of the Torah&rsquo;s origin and historical, generational transmission work with causal logic and language. Further, as </span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"><span style=\"color: #0000ff;\"><a href=\"http://en.wikipedia.org/wiki/Judea_Pearl\">Judea Pearl</a></span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"> points out in his article, &ldquo;</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"><span style=\"color: #0000ff;\"><a href=\"http://ftp.cs.ucla.edu/pub/stat_ser/r284-reprint.pdf\">Bayesianism and Causality, or, Why I am Only a Half-Bayesian</a></span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">,&rdquo; a complete synthesis of causal and statistical reasoning is mathematically untenable. That said, I will only argue that the historical Jewish claim does not violate (1) above: it does not ignore what we now know, nor has it in previous eras. So to proceed:</span></p>\r\n<p><span style=\"mso-ansi-language: EN;\" lang=\"EN\"><span style=\"mso-spacerun: yes;\">&nbsp;</span>To paraphrase Yudkowsky, &ldquo;This boy was frothing at the mouth&mdash; he must have suffered from demons. That man over there cured the boy&mdash; he must be an exorcist!&rdquo; By Bayes&rsquo; rule, this is perfect reasoning&mdash; <em style=\"mso-bidi-font-style: normal;\">assuming the boy&rsquo;s illness came from demons</em>. Yudkowsky asserts that the Torah has the same problem (&ldquo;We heard a Voice proclaim from the mountain, &lsquo;It&rsquo;s all true!&rsquo;&rdquo;). In fact, Yudkowsky admits that even if there really <em style=\"mso-bidi-font-style: normal;\">had</em> been a voice, the Bayesian theorem would still reveal the logical fallacy&mdash; how do you <em style=\"mso-bidi-font-style: normal;\">know </em>the voice is Divine, and it isn&rsquo;t just the Wizard of Oz hiding behind Mount Sinai with a loudspeaker? And as humanity progresses beyond the Torah scientifically and morally, doesn&rsquo;t it become more and more likely, even highly probable, that given G-d or the Wizard of Oz, it was more likely the Wonderful Wizard? Also, aren&rsquo;t we ignoring </span><span style=\"mso-ansi-language: EN;\" lang=\"EN\"><span style=\"color: #0000ff;\"><a href=\"http://yudkowsky.net/rational/bayes\">the original fraction of necessarily false claims</a></span></span><span style=\"mso-ansi-language: EN;\" lang=\"EN\"> in our computation&mdash; in this case, <em style=\"mso-bidi-font-style: normal;\">all</em> of the other supernatural religious claims that have ever been made?</span></p>\r\n<p><span style=\"mso-ansi-language: EN;\" lang=\"EN\">First, let me recount the earliest scientific experiment that <em style=\"mso-bidi-font-style: normal;\">I </em>know of. It&rsquo;s earlier than Yudkowsky&rsquo;s example: Moses&rsquo; confrontation with the Pharaoh&rsquo;s magicians. </span></p>\r\n<p><span style=\"mso-ansi-language: EN;\" lang=\"EN\">It roughly went like this. Moses goes to Pharaoh, declares that only the G-d of the Hebrews is the real G-d, and then his staff is thrown to the floor, which turns into a snake. Then Pharaoh&rsquo;s court magicians throw a staff to the floor, and for the sake of scientific control, attempt to replicate Moses&rsquo; results to test the critical p-value of their own hypothesis. However, unlike the priests of Baal, the magicians <em style=\"mso-bidi-font-style: normal;\">actually do replicate the results</em>. Boom: another snake! The magicians, satisfied with having duplicated Moses&rsquo; results, reject his hypothesis and declare themselves victorious.</span></p>\r\n<p><span style=\"mso-ansi-language: EN;\" lang=\"EN\">Though from the very start, it was only<em style=\"mso-bidi-font-style: normal;\"> </em>the court magicians who thought it was an acceptable experiment. Because you see, the magicians were conducting the experiment <em style=\"mso-bidi-font-style: normal;\">under the a priori assumption</em> that any alterations of the natural order would constitute real evidence, which in that era, would make sense from a Bayesian perspective. </span></p>\r\n<p><span style=\"mso-ansi-language: EN;\" lang=\"EN\">Furthermore, back in the day, nobody ever seriously entertained the notion that only one&rsquo;s <em style=\"mso-bidi-font-style: normal;\">own</em> national pantheon was objectively real. To the Egyptians, the gods of Persia, Greece and Ireland were just as &ldquo;real&rdquo; as their own&mdash; they may have been foreign deities, but deities nonetheless. The Persians, Greeks, and Irish themselves all thought this, too&mdash; orthodox Hindus to this day still often think this way. This was generally due to an inability to distinguish between the factual and the fantastic.</span></p>\r\n<p><span style=\"mso-ansi-language: EN;\" lang=\"EN\">So for Pharaoh&rsquo;s magicians, the burden of proof was quite low: their thesis was that Osiris and Ra were just as real as the G-d of the Hebrews, so all that was needed was a <em style=\"mso-bidi-font-style: normal;\">replication </em>of Moses&rsquo; results to yield a low p-value for Moses&rsquo; thesis that the G-d of the Hebrews was objectively real, and the Egyptians and whole worlds&rsquo; deities, not. The fact that Moses&rsquo; snake ended up eating their snakes did not matter; their thesis was simply that Osiris and Ra were <em style=\"mso-bidi-font-style: normal;\">also</em> real.</span></p>\r\n<p><span style=\"mso-ansi-language: EN;\" lang=\"EN\">Moses, on the other hand, knew that miraculous phenomena did not constitute rational evidence of nearly anything at all&mdash; and he knew the experiment was artificial, due to this knowledge. <span style=\"mso-spacerun: yes;\">&nbsp;</span>The Code of Jewish Law states it this way (this quote is long, but necessary&mdash; I will keep coming back to it):</span></p>\r\n<p style=\"margin-left: 0.5in;\">&ldquo;The Jews did not believe in Moses, our teacher, because of the miracle that he performed. <strong style=\"mso-bidi-font-weight: normal;\">Whenever anyone's belief is based on miracles, the commitment of his intellect has shortcomings, because it is possible to perform a wonder through magic or sorcery</strong> (Note: magic and sorcery refer to optical illusions, as the compiler of this code did not believe in real sorcery).</p>\r\n<p style=\"margin-left: 0.5in;\">All the wonders performed by Moses in the desert were not intended to serve as proof of the legitimacy of his prophecy, but rather were performed for a purpose. It was necessary to drown the Egyptians, so he split the sea and sank them in it. &hellip;The same applies to all of the other miracles.</p>\r\n<p style=\"margin-left: 0.5in;\">What is the source of our belief in him? The revelation at Mount Sinai. Our eyes saw, and not a stranger&rsquo;s. Our ears heard, and not another&rsquo;s. There was fire, thunder, and lightning. He entered the thick clouds; the Voice spoke to him and we heard, &ldquo;Moses, Moses, go tell them the following....&rdquo;</p>\r\n<p style=\"margin-left: 0.5in;\">Thus, Deuteronomy relates: &ldquo;Face to face, G-d spoke to you,&rdquo; and it states: &ldquo;G-d did not make this covenant with our fathers, but with us, who are all here alive today.&rdquo;</p>\r\n<p style=\"margin-left: 0.5in;\">How is it known that the revelation at Mount Sinai alone is proof of the truth of Moses&rsquo; prophecy that leaves no shortcoming? Exodus states: &ldquo;Behold, I will come to you in a thick cloud, so that the people will hear Me speaking to you, so that they will believe in you forever.&rdquo; It appears that before this happened, they did not believe in him with a faith that would last forever, but rather with a faith that allowed for suspicions and doubts.</p>\r\n<p style=\"margin-left: 0.5in;\">Thus, those to whom Moses was sent witnessed his appointment as a prophet, and it was not necessary to perform another wonder for them. He and they were witnesses, like two witnesses who observed the same event together. Each one serves as a witness to his colleague that he is telling the truth, and neither has to bring any other proof to his colleague.</p>\r\n<p style=\"margin-left: 0.5in;\">Similarly, all Israel were witnesses to the appointment of Moses, our teacher, at the revelation at Mount Sinai, and it was unnecessary for him to perform any further wonders for them.</p>\r\n<p style=\"margin-left: 0.5in;\">This concept is alluded to in the interchange between G-d and Moses at the revelation of the burning bush. At the beginning of his prophecy, the Holy One, blessed be He, gave him the signs and wonders to perform in Egypt and told him, &ldquo;And they will listen to your voice.&rdquo;</p>\r\n<p style=\"margin-left: 0.5in;\">Moses, our teacher, knew that one who believes in another person because of miracles has apprehension in his heart; he has doubts and suspicions. Therefore, he sought to be released from the mission, saying: &ldquo;They will not believe me&rdquo;, until the Holy One, blessed be He, informed him that these wonders were intended only as a temporary measure, until they left Egypt. After they would leave, they would stand on this mountain and all doubts which they had about him would be removed.</p>\r\n<p style=\"margin-left: 0.5in;\">G-d told him: Here, I will give you a sign so that they will know that I truly sent you from the outset, and thus, no doubts will remain in their hearts. This is what is meant by the statement: &ldquo;This will be your sign that I sent you: When you take the people out of Egypt, you will serve G-d on this mountain.&rdquo; (<span style=\"font-size: 8pt;\">M.T., Laws of the Foundations of the Torah 8:1-3</span>)&rdquo;</p>\r\n<p>So it worked like this. Moses has a prophetic vision of a burning bush. The Infinite One says, &ldquo;I&rsquo;m sending you as a prophet to the Hebrews&mdash; go to them, and tell them I sent you!&rdquo; This was entirely unprecedented. Even though there had been others in Hebrew history to have had prophetic visions, the communications in those visions had always concerned information pertinent to the individual alone. After all, if Abraham, Isaac or Jacob had gone around to people saying &ldquo;Hey! I had a prophecy! This <em style=\"mso-bidi-font-style: normal;\">proves</em> G-d&rsquo;s existence! Let&rsquo;s all be monotheists now!&rdquo; such a thing would have been illogical. After all, why should anyone believe them? They could have performed impressive miracles, perhaps&mdash; but how would people know they&rsquo;re not optical illusions? Wouldn&rsquo;t that be far more likely, anyway? <span style=\"mso-spacerun: yes;\">&nbsp;</span>And even if the miracles <em style=\"mso-bidi-font-style: normal;\">had been </em>real, why would that have constituted a proof for anything <em style=\"mso-bidi-font-style: normal;\">even then</em>? Moses Maimonides (12<sup>th</sup> c. CE), foremost rabbinic authority of the Middle Ages, said it like this:</p>\r\n<p style=\"margin-left: 0.5in;\">&ldquo;Anyone who in those days (i.e. pre-Mosaic) laid claim to authority, based it... on the fact that, by reasoning and by proof he had been convinced of the existence of a Being who rules the whole Universe. &hellip;But no one could establish his claim on prophecy, that is to say, on the fact that G-d had spoken to him, or had entrusted a mission to him; before the days of Moses no such assertion had ever been made. You must not be misled by the statements that G-d spoke to the Patriarchs, or that He had appeared to them. For you do not find any mention of a prophecy which appealed to others, or which directed them. Abraham, Isaac, or Jacob, or any other person before them did not tell the people, &ldquo;G-d said to me, you shall do this, or you shall do that,&rdquo; or &ldquo;G-d has sent me to you.&rdquo; Far from it! For G-d spoke to them on nothing but of what especially concerned them, i.e. He communicated to them things relating to their perfection, directed them in what they should do, and foretold them what the condition of their descendants would be; nothing beyond this. They guided their fellow men only by means of argument and instruction. (<span style=\"font-size: 8pt;\">Guide for the Perplexed I:LXIII</span>)&rdquo;</p>\r\n<p>So indeed, Moses was justifiably confused by the instruction, &ldquo;Go persuade them by turning this staff into a magical cobra!&rdquo;, and naturally responded, &ldquo;The people will not believe in me. (<span style=\"font-size: 8pt;\">Exodus 4:1</span>)&rdquo; Moses knew that his prophetic vision could not be objectively proved to anyone else as authentic&mdash; so what was the point?</p>\r\n<p>So the Et-rnal One responds, &ldquo;Yeah, you&rsquo;re right. Turning your staff into a cobra doesn&rsquo;t prove you&rsquo;re a prophet, and plagues don&rsquo;t either<em style=\"mso-bidi-font-style: normal;\">&mdash; but it will convince the Hebrew masses that you <strong style=\"mso-bidi-font-weight: normal;\">might</strong> be</em>, and that will be enough to give them hope temporarily, for the time necessary. Once they see you receiving the Ten Commandments on Sinai, <em style=\"mso-bidi-font-style: normal;\">then</em> they&rsquo;ll have their absolute proof.&rdquo;</p>\r\n<p>The Code of Jewish Law states that miraculous feats constitute proof of nothing&mdash; this principle is axiomatic to Jewish thought, and is crucial to the legal process of halachic reasoning. It is very much for this reason that the Christian claim that Jesus&rsquo; wonder-working was a proof for his divinity, was routinely <span style=\"color: #0000ff;\"><a href=\"http://en.wikipedia.org/wiki/Toledot_Yeshu\">satirized</a></span> by Jews even in medieval times; <em style=\"mso-bidi-font-style: normal;\">even by those Jews who thought that Jesus&rsquo; powers were real</em>. &ldquo;How do you <em style=\"mso-bidi-font-style: normal;\">know </em>Jesus didn&rsquo;t just slip some red dye into that water? And even if he did turn water to wine and heal lepers, so what? Even Pharaoh&rsquo;s magicians could have done that!<span style=\"mso-ansi-language: EN;\"> <span lang=\"EN\">Aren&rsquo;t you all ignoring the original fraction of necessarily false claims in your computation?!&rdquo;</span></span><span lang=\"EN\"> </span></p>\r\n<p>In contrast, the historical Hebrew claim is this: that when Moses received the Law on Sinai, the Ten Commandments were communicated to him through direct prophecy, and further, the first two of the Ten Commandments (&ldquo;I am <span style=\"font-size: 10pt;\">THE L-RD</span>&rdquo;<span style=\"font-size: 10pt;\"> </span>and &ldquo;No idolatry&rdquo;) were communicated to the entire mass of 600,000 Israelites through prophecy as well. Therefore, there could be no doubts that Moses was a prophet, for not only did they all see and hear him receive the Law, and hear the &lsquo;Voice&rsquo; he heard, but each one of them individually received the Law through prophetic communication as well, as a public body. BAZOOM! Proof. <span style=\"mso-spacerun: yes;\">&nbsp;</span>Further, the historical Hebrew claim is that this experience constituted proof for succeeding generations as well. This is why the Code of Jewish Law quotes Devarim (Deuteronomy) in stating, &ldquo;G-d did not make this covenant with our fathers, but with us, who are all here alive today.&rdquo; This line, quoting Moses, was not addressed to the Hebrews who were slaves in Egypt and who stood at Sinai, but to the 2<sup>nd</sup> generation, after the 1<sup>st</sup> one had passed away in the desert. It was to this generation, who had never been there, that Moses said, &ldquo;G-d did not make this covenant with our fathers, but with us, who are all here alive today.&rdquo; As the 1<sup>st</sup> generation could surely have never faked such an event, <em style=\"mso-bidi-font-style: normal;\">even if they had wanted to</em>, and as such an event could never be realistically fabricated by an individual and promulgated to the masses of Canaan (we were all slaves just 200 years ago, but then we all forgot, but luckily, Ari and Mendel rediscovered all this&hellip;), an indisputable historical tradition is treated in traditional Judaism as equivalent to a personal divine revelation, as far as proof is concerned.</p>\r\n<p>Regarding the post-Mosaic prophets, i.e. Hosea, Isaiah, Jeremiah, etc., there is still the question: how do we know <em style=\"mso-bidi-font-style: normal;\">they</em> weren&rsquo;t making it up? After all, they routinely performed miracles and made predictions concerning the future as proof of their prophetic abilities&mdash; and in contrast to the Mosaic model, their visions were private, just as much as Abraham, Isaac and Jacob&rsquo;s visions were all private, or if you will, Jesus, Muhammad and Zoroaster. So how would <em style=\"mso-bidi-font-style: normal;\">they</em> be distinct? Why would it have been illogical for Abraham to proclaim himself a prophet, but not Elijah?</p>\r\n<p>The legal ruling from the Torah is that if any individual Israelite, who is of properly reputable character (i.e. a scholar or rabbi) claims a prophetic vision, and is able to predict the future and demonstrate his or her prophetic abilities to the satisfaction of the Sanhedrin&mdash; then the Sanhedrin will legally declare the said individual to be a prophet, if the possibility of this Torah scholar being a charlatan is considered sufficiently negligible. Ultimately, the status of &ldquo;prophet&rdquo; for post-Mosaic Jewish leaders is not an article of faith, but a legal ruling. This is all so long as the would-be prophet does not contradict Mosaic Law. The Code of Jewish Law states it this way:</p>\r\n<p style=\"margin-left: 0.5in;\">&ldquo;We do not believe in any prophet who arises after Moses, our teacher, because of the wonder he performs alone, as if to say: If he performs a miracle we will listen to everything he says. Rather, we believe him because it is a commandment which we were commanded by Moses who said: &ldquo;If he performs a wonder, listen to him.&rdquo;</p>\r\n<p style=\"margin-left: 0.5in;\">Just as we are commanded to render a legal judgment based on the testimony of two witnesses, even though we do not know if they are testifying truthfully or falsely, similarly, it is a commandment to listen to this prophet <strong style=\"mso-bidi-font-weight: normal;\">even though we do not know whether the wonder is true or performed by magic or sorcery.</strong></p>\r\n<p style=\"margin-left: 0.5in;\">Therefore, if a prophet arises and attempts to dispute Moses&rsquo; prophecy by performing great signs and wonders, we should not listen to him. We know with certainty that he performed those signs through magic or sorcery. This conclusion is reached because the prophecy of Moses, our teacher, is not dependent on wonders, so that we could compare these wonders, one against the other. Rather we saw and heard with our own eyes and ears as he did. (<span style=\"font-size: 8pt;\">M.T., Laws of the Foundations of the Torah 8:3</span>)&rdquo;<span style=\"mso-spacerun: yes;\">&nbsp; </span></p>\r\n<p>Yudkowsky&rsquo;s assertion that the contest between Elijah and the priests of Baal is an example of how ancient Jewry had no concept of how to ask factual questions is false. Knowledgeable Israelites (or even &ldquo;mainstream&rdquo; pious Israelites familiar with the Moses vs. Pharaoh account in the Torah) knew perfectly well that such contests yielded no factual information, and were only necessary as a temporary, extreme measure. The fact that the contest took place outside the Temple premises was itself a violation of Hebrew law, as Mosaic Law permits no sacrifices beyond Temple grounds&mdash; this was also a temporary measure taken by Elijah. Ultimately, the logical worthlessness of the contest later emerged, as the Israelites who were &ldquo;won over&rdquo; to Elijah&rsquo;s side (the ones who gave the positive peer review) later regressed back to their pagan practices. Rabbinic commentaries to this account attest to this regression as an example of how miracles only constitute &ldquo;proof&rdquo; to the uneducated. The 450 priests of Baal, all being Jewish, were executed in accordance with the mandate of Judaic law (&ldquo;A person who worships false gods is to be hanged&rdquo;- <span style=\"font-size: 8pt;\">M.T., Laws of Idolatrous Worship 2:6</span>).</p>\r\n<p>It is here that we&rsquo;ll address the pink elephant in the room. How would Moses and all of the Israelites have known that what they were experiencing was authentic prophecy? We are still stuck with the Wizard of Oz dilemma: is the Voice divine, or a hallucination? Though the Hebrew historical claim is that all 600,000 Israelites received a prophetic communication all at once (rather than simply the claims of one individual), this does not necessarily alleviate the problem&mdash; because even though 600,000 individuals are statistically less likely than 1 individual to hallucinate, what if they <em style=\"mso-bidi-font-style: normal;\">were </em>hallucinating (Yudkowsky touches on this problem in <em style=\"mso-bidi-font-style: normal;\"><span style=\"color: #000000;\">Harry Potter and the Methods of Rationality</span></em>)? After all, there is very good documentation of cases of mass hysteria, complete with hallucinations, and has been known to affect up to thousands of people all at once. In fact, even if we assume that there was a good chance<em style=\"mso-bidi-font-style: normal;\"> </em>that the revelation on Sinai to the Hebrews was real, we would still have to admit that there have been many, many documented cases of mass hysteria, far outnumbering the number of times the Jews have claimed to have had experienced public, divine revelation (so far numbering exactly 1).</p>\r\n<p>It would not fully negate the hallucination possibility, to assert that historical Judaism considered prophecy to be a mental ability that comes as a result of intellectual preparation, and that ancient Jewry regarded prophecy as the consequence of decades of intellectual study (knowledge of science was requisite, says the Code of Jewish Law) and of meditative practice (in the Talmud, meditation schools are recorded to have existed). Describing Hebrew prophecy as a primarily intellectual experience more similar to the nirvana of an Indian bodhisattva, than the emotional ecstasy of a Greek oracle, would not be enough to fully negate the possibility of mass hysteria. This is especially so, considering that the tradition of the revelation on Sinai is the one exception in Jewish recorded history of intellectually <em style=\"mso-bidi-font-style: normal;\">un</em>prepared Israelites experiencing prophecy (thus the reason the Torah describes them all as temporarily dying after the 2<sup>nd</sup> Commandment being given). So how do we know if it was enlightened prophecy, or madness they were experiencing? The fact that Judaism is the only religion in history to have never experienced internal ideological schism in its formative stages of development does not fully negate the possibility of hallucination either&mdash; at least from a probabilistic standpoint.</p>\r\n<p>But then again, <span style=\"color: black;\">historical theory is not a laboratory science. You cannot test it and make observations&mdash; you can only check a historical theory for consistencies and inconsistencies. Analyzing historical theories using probabilistic theorems is usually extremely difficult, and I fully agree with Judea Pearl&rsquo;s opinion that real events in time can only be understood in terms of causality. Therefore, I would argue that the Wizard of Oz is mostly the result of statistical reasoning being applied to a scenario that requires a primarily cause-and-effect approach; because even if the <em style=\"mso-bidi-font-style: normal;\">Orthodox</em> approach does produce some Wizards, that is <em style=\"mso-bidi-font-style: normal;\">nothing</em> compared to the number of Wizards produced by a materialistic, reductionist approach. </span></p>\r\n<p><span style=\"color: black;\">The <em style=\"mso-bidi-font-style: normal;\">Grand Rabbi of Guadalajara </em>points out that according to reductionist approaches to Jewish history that deny the Torah&rsquo;s authenticity, the Jews would have to be either </span></p>\r\n<p style=\"margin-left: 0.5in;\"><span style=\"color: black;\">&ldquo;<strong style=\"mso-bidi-font-weight: normal;\">(a)</strong> (B)y far the most ingenious people ever. Out of all the peoples of the ancient world, this nation of shepherds and fig-growers came up with the classic work of all time. The work that changed all of history, brought us the concepts of creation ex-nihilo, history, purpose, monotheism, providence, human rights, gave rise to both Christianity and Islam and triggered the Reformation and modernization of western civilization&hellip; A supremacy dogma if I ever heard one! </span></p>\r\n<p style=\"margin-left: 0.5in;\"><span style=\"color: black;\">(Or)</span></p>\r\n<p style=\"margin-left: 0.5in;\"><strong><span style=\"color: black;\">(b)</span></strong><span style=\"color: black;\"> According to this theory, the Jews are by far the stupidest and most gullible people in the world. They fell for a story that restricts their diet, their domination over their slaves, their weekly work habits and their sex-life beyond what any other nation would tolerate. They bought into a lose-lose situation for everybody all &lsquo;round: The King&rsquo;s power is restricted, the priestly class cannot own land, and the commoners can&rsquo;t sell it.</span></p>\r\n<p style=\"margin-left: 0.5in;\"><span style=\"color: black;\">They abandon their fields and towns three times a year to the mercy of the hostile nations surrounding them, let those fields lie fallow once in seven years, let their slaves go free after six years, don&rsquo;t charge interest -- and just trust year after year that everything will be okay. After all, G-d promises that when you&rsquo;re planning to leave your land fallow in the seventh, He&rsquo;ll give you a bumper crop in the sixth. So tell me, what happens when one year this just doesn&rsquo;t work out? Do you leave that in the books you&rsquo;re writing?</span></p>\r\n<p style=\"margin-left: 0.5in;\"><span style=\"color: black;\">Furthermore, this theory has the Jewish people making up fables about their blunders in full detail. They declare that they descend from slaves! They tell nasty stories about the forefather of their priestly class, Levi&mdash; even though the Levites were supposed to have written the book. The original high priest gets his hands dirty in the biggest scandal of their history. Who is this fable serving, anyway? Why on earth would anyone <em>want</em> to make up such a story? And what sort of crazy people would want to preserve it?&rdquo;</span></p>\r\n<p>It is here, then, that I will conclude Section Three in mostly the same way it started. The ancient Israelites were not incapable of understanding factual questions, nor were they ignorant of rational analysis. The concepts of objective evidence and rational apprehension are seen in the Torah itself, and continued on into the era of the Prophets and Writings, the development of halachic reasoning in the Mishnah, the development of Talmudic hermeneutics, and the development of <a href=\"http://en.wikipedia.org/wiki/Kabbalah\">Kabbalah</a> and <a href=\"http://en.wikipedia.org/wiki/Jewish_philosophy\">Jewish Philosophy</a>. Yudkowsky&rsquo;s claim that factual inquiry into religious matters is a strictly modern, Western concept is incorrect.</p>\r\n<p>As a brief aside: Yudkowsky states that Rome had concepts of law and order, and he seems to <em style=\"mso-bidi-font-style: normal;\">contrast</em> this with Jewry. As <span style=\"color: #0000ff;\"><a href=\"http://en.wikipedia.org/wiki/Max_Dimont\">Max Dimont</a></span>&rsquo;s research has strongly indicated, it was the Hebrews who were the originators of the concepts of due process and presumption of innocence in a court of law&mdash; not the Romans.<span style=\"mso-spacerun: yes;\">&nbsp; </span><span style=\"mso-spacerun: yes;\">&nbsp;</span><span style=\"mso-spacerun: yes;\">&nbsp;</span></p>\r\n<p><strong style=\"mso-bidi-font-weight: normal;\"></strong></p>\r\n<p><strong style=\"mso-bidi-font-weight: normal;\">Section Four: The author claims that contemporary religionists justify false opinions by claiming that their religion is a separate magisterium which can be neither proven nor disproven.<span style=\"mso-spacerun: yes;\">&nbsp; </span><span style=\"mso-spacerun: yes;\">&nbsp;&nbsp;</span><span style=\"mso-spacerun: yes;\">&nbsp;</span><span style=\"mso-spacerun: yes;\">&nbsp;</span><span style=\"mso-spacerun: yes;\">&nbsp;&nbsp;</span></strong></p>\r\n<p>I believe that Yudkowsky&rsquo;s assertion here is partially correct; but first, the subject being discussed should be clarified. The principle of Non-Overlapping Magisteria (NOMA) was first introduced into the public science vs. religion debates by <span style=\"color: #000000;\">Stephen Jay Gould</span> in the late 1990s, with the term &ldquo;magisterium&rdquo; being borrowed by Gould from Pope Pius XII&rsquo;s 1950 encyclical, <em><span style=\"color: #0000ff;\"><a href=\"http://en.wikipedia.org/wiki/Humanae_generis\">Humani Generis</a></span></em>, which discussed Catholicism&rsquo;s views on natural evolution. In Gould&rsquo;s conception,</p>\r\n<p style=\"margin-left: 0.5in;\">&ldquo;(T)he magisterium of science covers the empirical realm: what the Universe is made of (fact) and why does it work in this way (theory). The magisterium of religion extends over questions of ultimate meaning and moral value. These two magisteria do not overlap, nor do they encompass all inquiry (consider, for example, the magisterium of art and the meaning of beauty). (<em><span style=\"font-size: 8pt;\">Rock of Ages, 1999</span></em>)&rdquo;</p>\r\n<p>To a degree, Yudkowsky is of course correct in stating that Judaism never originally had a concept of religion as a distinct magisteria, and that most other religions never did, either. I would extend this to include contemporary Judaism too, and would argue that Judaism has always been &ldquo;a religion without Mysteries,&rdquo; as <span style=\"color: #000000;\">Shmuel Luzzato</span>&nbsp;put it.</p>\r\n<p>However, though Judaism does not posit any intrinsically incomprehensible Mysterious Answers that are incapable of being logically deduced <em style=\"mso-bidi-font-style: normal;\">at all</em> (ex. Catholic<span style=\"color: #000000;\"> <span style=\"color: #0000ff;\"><a href=\"http://en.wikipedia.org/wiki/Transubstantiation\">transubstantiation</a></span></span>), it does nonetheless posit that there are matters, when contemplated, that cannot be fully comprehended by the intellect, and therefore <em style=\"mso-bidi-font-style: normal;\">do </em>occupy non-rational magisteria in a certain manner. This is not a modern phenomenon, and has always been present in Hebrew thought (&ldquo;<span style=\"font-size: 9pt;\">I AM THAT I AM</span>&rdquo; says G-d to Moses in Exodus, for instance). The <span style=\"color: #0000ff;\"><a href=\"http://en.wikipedia.org/wiki/Alter_Rebbe\">Alter Rebbe</a></span> z&rdquo;l, in his descriptions of Divine unity, wrote:</p>\r\n<p style=\"text-indent: 3pt; margin-left: 0.5in;\">&ldquo;(I)t is not at all proper to ascribe to G-d anything that is appurtenant to intellect &nbsp;even in a very lofty and sublime form, as if to say of G-d that it is beyond the &nbsp;capacity of any higher or lower creature to comprehend Divine Intellect or Essence. For comprehension pertains and applies to a matter of knowledge and wisdom, about which one can say that it can or cannot be understood because of the profundity of the concept. But, it is not at all proper to say concerning The&nbsp;Blessed Holy One, Who transcends intellect and wisdom, that it is impossible to&nbsp;apprehend G-d because of the depth of the concept, for G-d is not even within the realm of comprehension at all. (<span style=\"font-size: 8pt; mso-bidi-font-size: 12.0pt;\">Shaar Yichud v'Emunah, 1797</span>)&rdquo;</p>\r\n<p>&nbsp;This is seemingly about as distinct-magisterium-ish as it gets, and I&rsquo;d imagine that it is statements like this one that cause Yudkowsky to make analogies between G-d and <span style=\"color: #0000ff;\"><a href=\"/lw/i4/belief_in_belief/\">invisible, inaudible, permeable dragons</a></span> dwelling in one&rsquo;s garage.</p>\r\n<p>&nbsp;The distinction between G-d and the dragon, I&rsquo;d argue, is more easily appreciated from an existential standpoint than a probabilistic one. Within Judaism, matters that are treated as being incapable of being fully comprehended by the mind are always, <em style=\"mso-bidi-font-style: normal;\">exclusively</em>,<em> </em>of an existential nature. The essence of G-d, the nature of the soul, free will, the nature of good and evil, etc., are all treated as being incapable of being fully understood, and these are all <em>existential </em>concerns (and there is no concept of comprehension <em>at all </em>in Judaism<em> </em>concerning the ultimate essence of G-d). In contrast, historical claims such as the Exodus from Egypt and the receiving of the Torah, and purely theological claims such as paradise &amp; purgatory, reincarnation and the like, are <em>not </em>treated as occupying a distinct magisterium. They are considered fully comprehendible to the intellect, with the latter at the very least from utilizing philosophical reasoning.</p>\r\n<p>&nbsp;Furthermore, even concerning those subjects that cannot be fully fathomed from the Jewish perspective, such subjects are grouped into that separate magisterium as a direct result of rationally acquired knowledge. For instance, in Jewish thought, the origin of evil is regarded as being incapable of being completely fathomed&mdash; not as a result of reflexive reasoning or a retreat to commitment, but due to an intellectual understanding that evil exists, and that simultaneously the matter cannot be fully fathomed. This is often referred to as <em>yedias ha-shelilah</em>, or negative knowledge, i.e. knowledge by negative inference, exceeding the boundaries of structured thought. <em style=\"mso-bidi-font-style: normal;\">Yedias ha-shelilah</em>, in turn, can only be acquired once one acquires <em style=\"mso-bidi-font-style: normal;\">bittul</em>, or the negation of preconceived notions and biases.</p>\r\n<p>&nbsp;There is then the glaring question, &ldquo;Why is assuming the existence, or even the statistically significant <em>possibility</em>, that there is such a thing as the non-intellectual, rationally justifiable or desirable?&rdquo;, but that would bring me to&mdash;</p>\r\n<p>&nbsp;</p>\r\n<p><strong>Section Five: The author claims that the Torah&rsquo;s views on legislation, government, history, sexual morals, science, and ethics are outdated, in light of what Yudkowsky describes as human advancement. <span style=\"mso-spacerun: yes;\">&nbsp;</span></strong></p>\r\n<p><span style=\"mso-bidi-font-weight: bold;\">It is far beyond the scope of this essay to discuss the viability of the Torah&rsquo;s views on every popularly debated ethical and academic subject. However, two things:</span></p>\r\n<p><span style=\"mso-bidi-font-weight: bold;\">First, it is only inevitable that ontological reductionism leads to ethical and existential nihilism. There is no materialistic reductionist approach to human nature, whether it is </span><span style=\"mso-bidi-font-weight: bold;\"><span style=\"color: #0000ff;\"><a href=\"http://en.wikipedia.org/wiki/Transhumanism\">transhumanism</a></span></span><span style=\"mso-bidi-font-weight: bold;\">, utilitarian hedonism, or Marxist sociology, which is capable of avoiding this problem. Nihilism is inevitable in <em style=\"mso-bidi-font-style: normal;\">any </em>worldview that promotes that &ldquo;G-d is dead,&rdquo; and it is absurd to claim that anyone endorsing <em style=\"mso-bidi-font-style: normal;\">any </em>religious or non-reductionist approach to existence should be considered worthy of public ridicule, but one who promotes the &ldquo;transcendence&rdquo; of our humanity through self-selected, voluntary eugenics by reforming our minds in the image of computer technology, should be respected as &ldquo;rationalists&rdquo; (no offense intended, SIAI fans). To quote an American playwright who put it this way,</span></p>\r\n<p style=\"margin-left: 0.5in;\"><span style=\"mso-bidi-font-weight: bold;\">&ldquo;</span><span class=\"normal00200028web0029char1\">With the exception of a few powerful, dissenting vo</span>ices, the nineteenth century was almost unanimous in its belief that the ascent of science was a guarantee of the moral improvement of man. As the sworn rationalists gleefully kept destroying man&rsquo;s belief in G-d, they kept proclaiming their belief in man. Man, on the one hand, was depicted as an advanced outgrowth of the monkey, but, on the other hand, was proclaimed as a creature who can &lsquo;rationally&rsquo; work out his own salvation. Vladimir Soloviev, the great Russian philosopher, expressed the incompatibility of scientific optimism about man with man&rsquo;s proclaimed biological inferiority in a marvelously ironic phrase: &lsquo;Man,&rsquo; said Soloviev, &lsquo;is a descendant of monkeys; he can <span class=\"normal00200028web0029char1char1\"><em>therefore </em></span>be relied upon to bring about a period of happiness and progress to mankind.&rsquo;</p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in 5pt 0.5in;\"><span style=\"mso-bidi-font-weight: bold;\">&lsquo;Reductionist science, which for a couple of centuries hammered away at the idea that man is &ldquo;nothing but&rdquo; his biological components, did not realize that such a man would be a <em style=\"mso-bidi-font-style: normal;\">reduced</em> man, a &ldquo;nothing but-nick,&rdquo; to use an expression of Viktor Frankl. And it is not only specialization that brought about this state&mdash;specialization is inevitable in a technological life order&mdash;but <em style=\"mso-bidi-font-style: normal;\">totalization</em>: the idea that there is something akin to universality about the <em style=\"mso-bidi-font-style: normal;\">totality </em>of specialization. What is dangerous, Dr. Frankl writes, is the attempt of a man who is an expert, say, in the field of biology, to understand and explain human beings <em style=\"mso-bidi-font-style: normal;\">exclusively </em>in terms of biology. At the moment at which totality is claimed for the part, Dr. Frankl argues, biology becomes biologism, psychology becomes psychologism, and sociology becomes sociologism. In other words, at that moment, science is reduced to ideology. Dr. Frankl tells us in his <em style=\"mso-bidi-font-style: normal;\">Will to a Meaning </em>that he once came across a quotation defining man as &ldquo;<em style=\"mso-bidi-font-style: normal;\">nothing but</em> a complex biochemical mechanism powered by a combustion system which energizes computers with prodigious storage facilities for retaining encoded information.&rdquo; &hellip; (I)n a certain sense the statement is valid: Man <em style=\"mso-bidi-font-style: normal;\">is </em>a computer. However, at the same time, he is infinitely more than a computer! The statement is fatally erroneous insofar as it defines man as <em style=\"mso-bidi-font-style: normal;\">nothing but</em> a computer.</span> (<span style=\"font-size: 8pt;\">Zvi</span> <span class=\"normal00200028web0029char1char1\"><span style=\"font-size: 8pt;\">Kolitz, 1982</span></span>)&rdquo;<strong> </strong><span style=\"mso-bidi-font-weight: bold;\"><span style=\"mso-spacerun: yes;\">&nbsp;</span></span></p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\">&nbsp;</p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\">Secondly, the thing that people often popularly refer to as &ldquo;Old Testament&rdquo; ethics is usually nothing more than a common misunderstanding of what the historical Jewish approach to ethics has always really been like, and such misunderstandings are often misused as trump cards in public religion vs. science debates. <span style=\"mso-spacerun: yes;\">&nbsp;</span></p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\">&nbsp;</p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\">Since Yudkowsky mentions it, let&rsquo;s use slavery as an example. There is a popular notion that the Torah, and by association the New Testament, endorses slavery as being morally okay. This is often held up in contrast to the contemporary, enlightened Western world, where it is assumed that everybody knows that slavery is of course unethical. This is a very popular trump card in public debate surrounding religion, and it is common to hear statements such as, &ldquo;Why trust the Bible is right when it says &lsquo;no-gay-marriage,&rsquo; when the Bible says <em style=\"mso-bidi-font-style: normal;\">slavery</em> is just fine?&rdquo; Most people, having little factual knowledge of the Tanakh (including most religious Americans), assume that the information being presented in the trump card is accurate. Which is understandable&mdash; it sounds logical to assume <em style=\"mso-bidi-font-style: normal;\">a priori </em>that Iron Age Near Eastern tribes thought slavery was ethically permissible.</p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\">For starters, there is a little-known principle in Jewish thought, that many of the Torah&rsquo;s laws are designed to bring about the <em style=\"mso-bidi-font-style: normal;\">gradual </em>elimination of certain societal evils, rather than their <em style=\"mso-bidi-font-style: normal;\">immediate</em> elimination, for the purpose of pragmatism and realistic goals for societal change. Put more precisely, <span style=\"mso-spacerun: yes;\">&nbsp;</span><em style=\"mso-bidi-font-style: normal;\"><span style=\"mso-spacerun: yes;\">&nbsp;</span></em></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">&ldquo;On considering the Divine acts, or the processes of Nature, we get an insight into the prudence and wisdom of G-d as displayed in the creation of animals, with the gradual development of the movements of their limbs and the relative positions of the latter, and we perceive also His wisdom and plan in the successive and gradual development of the whole condition of each individual. The gradual development of the animals&rsquo; movements and the relative position of the limbs may be illustrated by the brain, etc&hellip; When such an animal is born it is extremely tender, and cannot be fed with dry food. Therefore breasts were provided which yield milk, and the young can be fed with moist food which corresponds to the condition of the limbs of the animal, until the latter have gradually become dry and hard.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">Many precepts in our Law are the result of a similar course adopted by the same Supreme Being. It is, namely, impossible to go suddenly from one extreme to the other: it is therefore according to the nature of man impossible for him suddenly to discontinue everything to which he has been accustomed. </span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">... I do not say this because I believe that it is difficult for G-d to change the nature of every individual person; on the contrary, it is possible, and it is in His power, according to the principles taught in the Law; but it has never been His will to do it, and it never will be. If it were part of His will to change at His desire the nature of any person, the mission of the prophets and the giving of the Law would have been altogether superfluous. (</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 8pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">Guide for the Perplexed III: XXXII</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">)&rdquo;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\">&nbsp;</p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">This principle of Jewish thought is often applied to issues such as economic inequality, monarchic rule, capital punishment, the status of women, and animal sacrifice, as well as other issues. The Torah&rsquo;s approach to slavery, for most of Jewish history, has always been understood in accordance with this principle, the idea generally being that servitude was often a tragic economic necessity and hence a necessary evil. Therefore, the Torah&rsquo;s restrictions on slavery were understood as having the long-term goal of eliminating slavery altogether, but gradually: hence the Torah&rsquo;s ban on possessing an individual slave for more than seven years, thereby preventing generational slavery; the ban on physically harming a slave; the ban on sexual relations with one&rsquo;s slaves; the ban on involuntary slavery, and the requirement that they be indentured; the requirement that your slaves must live with you in your home, and eat the same food you eat, etc. The statement in the Talmud (</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 8pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">Kiddushin 20a</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">): &ldquo;Whoever acquires a Hebrew slave, acquires a master!&rdquo; is logically derived from these rules in the Torah, as is the statement in the Mishnah, &ldquo;Make the poor into servants in your household, (</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 8pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">Pirkei Avos</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">)&rdquo; which warns against enslaving the poor without monetary compensation.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">However, the clearest admonitions against slavery usually come from the Tanakh itself. For instance, </span><span style=\"font-family: Calibri;\"><span style=\"mso-ascii-font-family: Calibri; mso-fareast-font-family: 'Times New Roman'; mso-hansi-font-family: Calibri; mso-bidi-font-family: Calibri;\">&nbsp;</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\"></span></span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">&ldquo;If you buy a Hebrew servant, he is to serve you for (no more than) six years. But in the seventh year, he shall go free, without paying anything... But if the servant declares, &ldquo;I love my master and my (assigned) wife and children, and do not want to go free,&rdquo; then his master must take him before the judges. He shall take him to the door or the doorpost, and pierce his ear with an awl. Then he will be his servant for life (until the Jubilee year).</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-fareast-font-family: 'Times New Roman';\"> (</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 8pt; mso-fareast-font-family: 'Times New Roman';\">Exodus 21: 2-6</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-fareast-font-family: 'Times New Roman';\">)</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">&rdquo;</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-fareast-font-family: 'Times New Roman';\"> </span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-fareast-font-family: 'Times New Roman';\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">The classical commentator Rashi (11th c. CE) explains why a Jew who sells himself or herself into slavery is given such a severe corporeal punishment, even though the action of selling oneself into indentured servitude is itself permitted by the Torah. Rashi cites the Mishnah, writing,</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">&ldquo;Now, why was the ear chosen to be bored out of all the organs of the body? ... Referring to one who sold himself into servitude, the reason is that the ear that heard, &lsquo;For the children of Israel are servants to Me&rsquo; (Leviticus 25:55) and then went and acquired a master for himself, this ear shall be bored. Rabbi Shimon interpreted this verse in a beautiful manner: Why were the door and the doorpost singled out from all the fixtures in the house? The Holy One, blessed is He, said: The door and the doorpost were witnesses in Egypt when I passed over the lintel and the two doorposts, and I said, &lsquo;For the children of Israel are servants to Me; they are My servants,&rsquo; but they are not servants to servants, and yet, this one went and acquired for himself a master! Therefore his ear shall be bored, for everyone to see. (</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 8pt; mso-fareast-font-family: 'Times New Roman';\">from Talmud, Kiddushin 22b</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">)&rdquo;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">The classical Talmudic interpretations of the Torah&rsquo;s laws concerning slavery strongly indicate an understanding that servitude, even if voluntarily chosen for oneself, is morally debasing. Furthermore, the <em>reason</em> for abhorring slavery is equally significant: freely chosen socio-political liberty is a necessary prerequisite to be able to serve G-d, for such service requires both physical liberty as well as the mentality of a free person, since slavery is regarded as spiritually and intellectually debilitating (Ex. \"So says the G-d of the Hebrews: Let My people go, <em>that they may serve Me!</em>\"). </span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">Such admonitions ultimately had their inevitable effect. Slavery was uncommon among Jews even by the time of the Roman Empire, was entirely avoided by the <a href=\"http://en.wikipedia.org/wiki/Essenes\">Essenes</a></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">, and was branded as equivalent to idolatry by the <a href=\"http://en.wikipedia.org/wiki/Zealots\">Zealots</a></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">, leading Elazar ben Yair to famously state at Masada:</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\">&nbsp;</p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">&ldquo;Long ago we resolved to serve neither the Romans nor anyone other than G-d... The time has now come that bids us prove our determination by our deeds. At such a time we must not disgrace ourselves. Hitherto we have never submitted to slavery... We must not choose slavery now... For we were the first to revolt, and shall be the last to break off the struggle. And I think it is G-d who has given us this privilege, that we can die nobly and as free men... In our case it is evident that daybreak will end our resistance, but we are free to choose an honorable death with our loved ones. This our enemies cannot prevent, however earnestly they may pray to take us alive; nor can we defeat them in battle.</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\">&nbsp;</p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">Let our wives die unabused, our children without knowledge of slavery. After that let us do each other an ungrudging kindness, preserving our freedom as a glorious winding-sheet. But first, let our possessions and the whole fortress go up in flames. It will be a bitter blow to the Roman, that I know, to find our persons beyond their reach and nothing left for them to loot. One thing only let us spare&mdash; our store of food: it will bear witness when we are dead to the fact that we perished, not through want but because...we chose death rather than slavery....&rdquo;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\">&nbsp;</p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">Given all of this, the popular contention that the Torah endorses slave-ownership is difficult to defend. This is especially so, considering that the modern Western world generally <em style=\"mso-bidi-font-style: normal;\">does</em> consider the revocation of personal liberties through imprisonment to be a morally permissible method of punishing criminals, and the practice of imprisoning criminals is recognized as a form of slavery by the 13<sup>th</sup> Amendment: &ldquo;Neither slavery nor involuntary servitude, except as a punishment for crime whereof the party shall have been duly convicted, shall exist within the United States, or any place subject to their jurisdiction.&rdquo; In fact, the practice of coerced, involuntary servitude as a punishment for criminals probably receives more moral sanction from the U.S. Constitution than it does from the Torah, which prescribes financial penalties, sacrificial atonement offerings, corporeal punishment and capital punishment as penalties for crime, without any mention of the practice of long-term incarceration. </span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span class=\"list0020paragraphchar1char1\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">To bring everything back around: Yudkowsky&rsquo;s severe critiques of religion, notably Judaism, <span style=\"mso-spacerun: yes;\">&nbsp;</span><span style=\"mso-spacerun: yes;\">&nbsp;</span>are generally false&mdash; even concerning demonstrable facts much of the time, and are inconsistent with Less Wrong&rsquo;s efforts to promote the rational overcoming of intellectual self-deception and bias. Rationality does not necessitate the rejection of all religious philosophy, nor the intellectual denial of G-d. To end&nbsp;with&nbsp;one more&nbsp;quote,</span></span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">&nbsp;</span></p>\r\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">&ldquo;</span><span class=\"normal00200028web0029char1\"><span style=\"font-size: 12pt;\">There was a growing conviction (in the 19<sup>th</sup> century) that science could be relied on to provide a secure rational foundation for all of our ethical and moral standards. The philosophical roots of this conviction can be traced to Greek philosophy, &hellip; Greek philosophy thus relied </span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">on human reason to derive moral and ethical principles from the nature of things, rather than from G-d, as the Hebrews believe. An ethics thus divorced from G-d is autonomous.</span><span style=\"mso-ascii-font-family: Calibri; mso-fareast-font-family: 'Times New Roman'; mso-hansi-font-family: Calibri; mso-bidi-font-family: Calibri;\"></span></p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in 5pt 0.5in;\"><span class=\"normal00200028web0029char1\">&lsquo;Morally speaking, my friends, the shtetl Jew of Eastern Europe was without pee</span>r in the history of communal morality. Poverty-stricken, oppressed, hated, mocked, woefully lacking in aesthetics, the shtetl Jew reached heights of ethical and moral purity that made crime in his midst unthinkable and social indifference impossible. Nobody starved in the poor shtetl, and nobody was denied the opportunity to acquire knowledge, a much more sought-after and much more respected commodity than money&hellip;</p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in 5pt 0.5in;\"><span class=\"normal00200028web0029char1\">Now, why did this come to pass? Why did poverty and oppression, which usually breed crime, bre</span>ed, in the mud of the shtetl, purity of heart as a mass phenomenon?</p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in 5pt 0.5in;\">&lsquo;Believing, as the shtetl Jews did, that ethical behavior is G-d centered, they took G-d as their measure. &hellip; Our entire history bears witness before G-d and man that ethics become a way of life&mdash;not a fossilized thought, but a way of life&mdash;only when they are G-d derived. &hellip;That is precisely what Dostoyevsky had in mind when he said, &lsquo;If there is no G-d, murder is permissible.&rsquo; Abraham, as we must always remember, said it more than four millennia earlier: &lsquo;There is no awe of G-d in this land, and whoever finds me may slay me.&rdquo;</p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\">&nbsp;</p>\r\n</span>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\">&nbsp;</p>\r\n</p>\r\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eDjmssYSwwegBGvcb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": -16, "extendedScore": null, "score": -2.5e-05, "legacy": true, "legacyId": "14244", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">\n</span></p><p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span class=\"script-hebrew\"><span style=\"font-family: &quot;David&quot;,&quot;sans-serif&quot;; font-size: 16pt; mso-bidi-language: HE;\" dir=\"rtl\" lang=\"HE\">\u05d1\u05e1</span></span><span style=\"font-family: &quot;David&quot;,&quot;sans-serif&quot;; font-size: 16pt; mso-bidi-language: HE;\" dir=\"rtl\" lang=\"HE\">\"<span class=\"script-hebrew\">\u05d3</span></span><span style=\"font-size: 16pt; mso-bidi-font-family: David; mso-bidi-language: HE;\"></span></p>\n<p></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">A few days ago, a friend of mine sent me a link to Eliezer Yudkowsky\u2019s article, \u201c</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"><a href=\"/lw/i8/religions_claim_to_be_nondisprovable/\">Religion\u2019s Claim to be Non-Disprovable</a>.</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">\u201d This wasn\u2019t the first time my friend had sent me articles by Yudkowsky from Less Wrong, concerning religion in general and Judaism in particular. With each of us having grown up in secular-yet-cultural Jewish homes, and with me having morphed into an </span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"><span style=\"color: #0000ff;\"><a href=\"http://en.wikipedia.org/wiki/Orthodox_Judaism\">Orthodox Jew</a></span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"> of the </span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"><span style=\"color: #0000ff;\"><a href=\"http://en.wikipedia.org/wiki/Hasidic_Judaism\">Lubavitch-Chasidic</a></span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"> variety early in college, the material for discussion (arguing?) is usually pretty good.</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">So I wrote a response to Yudkowsky\u2019s article. Originally meant to be a long-ish Facebook post, it got longer and longer\u2026 and I ended up with an essay article instead. So rather than post it on Facebook, I decided that I\u2019d share it with the good people of the Less Wrong community. But first, some necessary preliminary remarks.</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\n<p class=\"list0020paragraph\" style=\"text-indent: -0.25in; margin: 0in 0in 0pt 56pt;\"><span class=\"list0020paragraphchar1char1\"><span style=\"font-family: Symbol; font-size: 12pt;\">\u00b7</span></span><span style=\"font-family: Calibri;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">While I have made an effort to familiarize myself with Less Wrong\u2019s Core Sequences and with the more essential material, <span class=\"list0020paragraphchar1char1\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-ansi-font-size: 12.0pt; mso-bidi-font-size: 12.0pt;\">I admit that I am not as intimately familiar with the methods of mathematical calculation and philosophical dialectic as I\u2019m sure many members of the Rationalist community here are. I will do my best to keep my assertions clear and intellectually honest, but I apologize for any unconventionality in style, and for my extensive over-use of parentheses. </span></span></span></p>\n<p class=\"list0020paragraph\" style=\"margin: 0in 0in 0pt 0.5in;\"><span style=\"font-family: Calibri;\">&nbsp;</span></p>\n<p class=\"list0020paragraph\" style=\"text-indent: -0.25in; margin: 0in 0in 0pt 56pt;\"><span class=\"list0020paragraphchar1char1\"><span style=\"font-family: Symbol; font-size: 12pt;\">\u00b7</span></span><span style=\"font-family: Calibri;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class=\"list0020paragraphchar1char1\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">This article is <strong style=\"mso-bidi-font-weight: normal;\"><em>not</em></strong> meant to be a presentation of evidence for the existence of G-d or the authenticity of the Torah, in the style of an argument, and so \u201cG-d is real and wrote the Torah\u201d is not my thesis. Rather, this is only meant as a response to Yudkowsky\u2019s assertion that \u2018Old Testament\u2019 based thinking deserves no place in any sort of intellectual discourse. My thesis here is simply that Yudkowsky\u2019s critiques of Judaism are generally not correct, that it does deserve such a place, and that Rationalist thinking should not necessitate an <em style=\"mso-bidi-font-style: normal;\">a priori </em>rejection of all religious philosophy. This essay is meant to be intellectually open to criticism (<em>any</em> criticism).</span></span></p>\n<p class=\"list0020paragraph\" style=\"margin: 0in 0in 0pt 0.5in;\"><span style=\"font-family: Calibri;\">&nbsp;</span></p>\n<p class=\"list0020paragraph\" style=\"text-indent: -0.25in; margin: 0in 0in 0pt 0.75in;\"><span class=\"list0020paragraphchar1char1\"><span style=\"font-family: Symbol; font-size: 12pt;\">\u00b7</span></span><span style=\"font-family: Calibri;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class=\"list0020paragraphchar1char1\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">This critique is meant to be a reasonable one, and is <em>not</em> meant as a personal attack on Yudkowsky or any other members of the skeptic community, nor is it<em> </em>meant as any sort of moral rebuke, or anything along those lines. My arguments are meant to be logical, and are meant to be in accord with the etiquette appropriate for the Less Wrong community. </span></span></p>\n<p class=\"list0020paragraph\" style=\"margin: 0in 0in 0pt 0.5in;\"><span style=\"font-family: Calibri;\">&nbsp;</span></p>\n<p class=\"list0020paragraph\" style=\"text-indent: -0.25in; margin: 0in 0in 0pt 0.75in;\"><span class=\"list0020paragraphchar1char1\"><span style=\"font-family: Symbol; font-size: 12pt;\">\u00b7</span></span><span style=\"font-family: Calibri;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class=\"list0020paragraphchar1char1\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">I am aware and have read the many other essays Yudkowsky has written on rationality and religion, and I am familiar with SIAI\u2019s work. While this essay will focus particularly on \u201cReligion\u2019s Claim to be Non-Disprovable,\u201d I will make references to Yudkowsky\u2019s other essays if needed for clarification, while trying to avoid unnecessary digressions.</span></span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">For clarity\u2019s sake, I will organize my responses into five categories, ranked in order according to the organization of this particular article by Yudkowsky (roughly). These categories are not meant to be strict, but are simply meant to function as an organizational tool. I\u2019ll limit my responses to issues concerning Judaism, since Judaism (though Yudkowsky prefers the term \u201cOld Testament\u201d) is the primary subject of criticism in the article, and it is the religious <em style=\"mso-bidi-font-style: normal;\">Weltanschauung</em> that I am most familiar with. As Yudkowsky generally employs an academic style mixed with satire, I\u2019ll try to do so too, and given the context of this post, I will generally use English rather than Hebrew terminology, while simultaneously trying to avoid overtly Christian terminology. I will be using the <em style=\"mso-bidi-font-style: normal;\">Mishneh Torah</em> (12<sup>th</sup> c. CE) as my primary reference for Judaic law, given that it is the oldest, the most topically organized, and the most comprehensive code of Jewish law.</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">My five general critiques are:</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\n<p class=\"MsoListParagraphCxSpFirst\" style=\"text-indent: -0.25in; margin: 0in 0in 0pt 0.5in; mso-list: l0 level1 lfo1;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\"><span style=\"mso-list: Ignore;\">1)<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">The author makes precisely 3 statements regarding Halacha (Judaic law), each of which is demonstrably incorrect. </span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin: 0in 0in 0pt 0.5in;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"text-indent: -0.25in; margin: 0in 0in 0pt 0.5in; mso-list: l0 level1 lfo1;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\"><span style=\"mso-list: Ignore;\">2)<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">The author asserts that the Tanakh (Old Testament) \u201cdoesn\u2019t talk about a sense of wonder at the complexity of the universe.\u201d</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin: 0in 0in 0pt 0.5in;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"text-indent: -0.25in; margin: 0in 0in 0pt 0.5in; mso-list: l0 level1 lfo1;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\"><span style=\"mso-list: Ignore;\">3)<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">The author asserts that historical Judaism defends the authenticity of the Torah without accounting for Bayes\u2019 Theorem.</span></p>\n<p class=\"MsoListParagraphCxSpMiddle\" style=\"margin: 0in 0in 0pt 0.5in;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\n<p class=\"MsoListParagraphCxSpLast\" style=\"text-indent: -0.25in; margin: 0in 0in 0pt 0.5in; mso-list: l0 level1 lfo1;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\"><span style=\"mso-list: Ignore;\">4)<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">The author asserts that contemporary religionists justify false opinions by claiming that their religion is a separate magisterium which can be neither proven nor disproven. <span style=\"mso-spacerun: yes;\">&nbsp;</span></span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\n<p class=\"MsoListParagraph\" style=\"text-indent: -0.25in; margin: 0in 0in 0pt 0.5in; mso-list: l0 level1 lfo1;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\"><span style=\"mso-list: Ignore;\">5)<span style=\"font: 7pt &quot;Times New Roman&quot;;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">The author asserts that the Torah\u2019s views on legislation, government, history, sexual morals, science, and (most pointedly) ethics are outdated, in light of what Yudkowsky describes as progressive human advancement. </span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.25in;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">Section One:</span></strong><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"> <strong style=\"mso-bidi-font-weight: normal;\">The author makes three statements regarding Halacha, each of which is demonstrably false.</strong> </span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">Yudkowsky claims that according to the Torah, (1) cross-dressing is a capital crime, (2) rabbits are ruminants, and (3) locusts have four legs. </span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">Regarding the first statement, though the Torah prohibits men from wearing women\u2019s clothing (and vice versa), it is not a crime that carries the death penalty, as stated in the Code of Jewish Law: \u201cA man who adorns himself as a woman does, and a woman who adorns herself as a man does, are <em style=\"mso-bidi-font-style: normal;\">chayav</em>, liable (</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 8pt;\">Mishneh Torah., Laws of Idolatrous Worship 12:10</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">)\u201d, indicating there is no capital punishment. </span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">Regarding the second statement,</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-ansi-language: EN;\"> <span lang=\"EN\">it is true that rabbits are listed in the Torah as chewing their cud, and it <em style=\"mso-bidi-font-style: normal;\">is </em>true that according to modern scientific observation they are not true ruminants. However, it is also true that rabbits do practice a form cecotrophy through the </span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">reingestion of special fecal pellets. Hence, rabbinic authorities that interpret the Hebrew word <em style=\"mso-bidi-font-style: normal;\">shafan</em> as \u2018rabbit\u2019 classify their manner of cecotrophy as <em style=\"mso-bidi-font-style: normal;\">ma\u2019aleh gerah</em>, chewing the cud. Likewise, even in this instance, the Halacha recognizes that rabbits are not true ruminants in the exact same sense as, say, cows or sheep. The Code of Jewish Law states, </span></p>\n<p style=\"margin-left: 0.25in;\"><span style=\"font-family: Times New Roman;\">\u201cThe signs of a kosher domesticated animal and beast are explicitly mentioned in the Torah. There are two signs: a split hoof and chewing the cud. Both are necessary. Any domesticated animal and beast that chews the cud does not have teeth on its upper jaw-bone. <strong style=\"mso-bidi-font-weight: normal;\">Every animal that chews the cud has split hoofs except a camel</strong>. Every animal that has split hoofs chews the cud except a pig. (<span style=\"font-size: 8pt;\">M.T., Laws of Forbidden Foods 1:2</span>)\u201d </span></p>\n<p><span style=\"font-family: Times New Roman;\">The commentator <em>Maggid Mishneh</em> explains that the <em style=\"mso-bidi-font-style: normal;\">Mishneh Torah</em> does not mention rabbits as chewing their cud because they have teeth on their upper jaw, and complete ruminants do not.<span style=\"mso-spacerun: yes;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></p>\n<p><span style=\"font-family: Times New Roman;\">Regarding the third statement, the third book of the Torah states, \u201c(A)mong all the flying insects that walk on four, you may eat those that have jointed extensions above its legs, with which they hop on the ground.\u201d On this subject, the Code of Jewish Law says that these four legs are excluding the back legs meant for jumping, stating, \u201cWhenever a species has four legs, four wings that cover the majority of the length and the majority of the width of its body, and it has two longer legs to hop, it is a kosher species. (<span style=\"font-size: 8pt;\">M.T., Laws of Forbidden Foods 1:22</span>)\u201d As part of their traditional cuisine, the Yemenite Jewish community to this day still eats those locusts identified in the Torah as kosher.</span></p>\n<p><span style=\"font-family: Times New Roman;\">While I understand that these are <em style=\"mso-bidi-font-style: normal;\">highly</em> peripheral points of<span style=\"mso-spacerun: yes;\">&nbsp; </span>Yudkowsky\u2019s article, I\u2019m noting them because I\u2019m guessing that Yudkowsky has some sort of yeshiva background, has probably actually studied Jewish law before, and yet each comment in his article regarding Halacha is incorrect. Also, if one were to argue that these halachic rulings may just be apologetic glosses on a defective primary text (the Torah), I\u2019d point out that the <em style=\"mso-bidi-font-style: normal;\">Mishneh Torah</em> was written in the 12<sup>th</sup> century CE, and that its rulings are a compilation of rulings from the Mishnah (1<sup>st</sup> c. BCE-2<sup>nd</sup> c. CE). General knowledge of animal biology was still very rudimentary in both eras.</span></p>\n<p><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"font-family: Times New Roman;\">&nbsp;</span></strong></p>\n<p><span style=\"font-family: Times New Roman;\"><strong style=\"mso-bidi-font-weight: normal;\">Section Two:</strong> <strong style=\"mso-bidi-font-weight: normal;\">The author claims that the Tanakh \u201cdoesn\u2019t talk about a sense of wonder at the complexity of the universe.\u201d </strong></span></p>\n<p><span style=\"font-family: Times New Roman;\">It does. It would be difficult to find a string of Tehillim (King David\u2019s psalms) that do not make extensive use of nature imagery. Poetic references to nature are so frequent in the 150 Tehillim, that Talmudic legend portrays King David as spending his free time wandering around in the wilderness alone, and as having the ability to understand the language of birds, trees, plants, and leaves of grass. In fact, people in the Jewish tradition who are regarded as uniquely holy are <em style=\"mso-bidi-font-style: normal;\">often</em> portrayed as having a relationship with, and being in awe of the beauty of nature, and as having the ability to speak with birds and trees. This includes various Talmudic sages, the Arizal, the Baal Shem Tov, Nachman of Breslov, R. Zundel of Salant, z\u201dl etc. Considering that the Psalms have formed the basis of Hebrew ritual prayer since the first Israelite ruling dynasty, this does not seem in keeping with Yudkowsky\u2019s portrayal of the ancient Hebrews as being unconcerned with the wonders of nature. <strong style=\"mso-bidi-font-weight: normal;\"></strong></span></p>\n<p><span style=\"font-family: Times New Roman;\">However, Yudkowsky\u2019s point may still hold. It\u2019s true that there\u2019s nothing written in the Tanakh that seems to be obviously on the same wavelength as, say, Aristotle\u2019s <em style=\"mso-bidi-font-style: normal;\">Metaphysics</em> or Euclid\u2019s <em style=\"mso-bidi-font-style: normal;\">Elements</em>. However, the study of science and theoretical metaphysics is most definitely not lacking in Hebrew oral tradition\u2014 which, as Yudkowsky and any other scholar should know, is every bit a part of historical Judaism as the Tanakh itself. In <em style=\"mso-bidi-font-style: normal;\">The Kuzari</em>, one of the classics of traditional Jewish philosophy written in the 12<sup>th</sup> century CE, the author Judah HaLevy describes the Torah &amp; Talmud\u2019s treatment of science this way: </span></p>\n<p style=\"margin-left: 0.25in;\"><span style=\"font-family: Times New Roman;\">\u201cThe members of the Sanhedrin (ancient chief rabbinic court in Jerusalem of 70 members) were bound not to let any science, real and fictitious, or conventional, escape their knowledge, magic and language included. How was it possible at all times to find 70 available scholars unless learning was common among the Israelites? This could not be otherwise, as <strong style=\"mso-bidi-font-weight: normal;\">all branches of science were required for the practice of the Law</strong>.\u201d<span style=\"mso-tab-count: 2;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></p>\n<p><span style=\"font-family: Times New Roman;\">HaLevy then goes on to give a brief description of how the practice of Judaism <em style=\"mso-bidi-font-style: normal;\">requires</em> intensive study of astronomy, music, agriculture, natural medicine, Hebrew grammar, foreign languages, human anatomy &amp; biology, legalism, hermeneutic logic, economics, trigonometry, rhetoric and metaphysical speculation, as all of these disciplines are necessary for the precise application of Jewish law. Expertise in at least 2 areas of science and 6 foreign languages were requisite for membership in the Sanhedrin, and indeed, all of these sciences are extensively utilized in the Midrash, Mishnah and Talmud. HaLevy also emphasizes that the Jewish value of scholarly pursuit is inherited from ancient Israel\u2019s scholars and prophets, and even maintains that the Greek love of philosophy originated as a Jewish influence. </span></p>\n<p><span style=\"font-family: Times New Roman;\">In other words, the Torah does not <em style=\"mso-bidi-font-style: normal;\">displace </em>intellectual inquiry, but rather, is specifically designed to stimulate it. Put another way, \u201c<em style=\"mso-bidi-font-style: normal;\">Lo am ha\u2019aretz chasid</em>. (<span style=\"font-size: 8pt;\">Mishnah, Pirkei Avos</span>)\u201d An ignoramus cannot be pious. </span></p>\n<p><span style=\"font-family: Times New Roman;\">I do not know if Socrates and Plato really did acquire all of their philosophical methods by traveling to Jerusalem and studying in the court of King Solomon, as certain Jewish traditions claim. But the fact that such traditions even existed is a testament to the reverence that ancient Jewry held for the pursuit of knowledge <em style=\"mso-bidi-font-style: normal;\">as a direct result </em>of the Mosaic Law, and is, again, hardly in keeping with Yudkowsky\u2019s assertion that \u201cthe Old Testament doesn\u2019t talk about a sense of wonder at the complexity of the universe - it was busy laying down the death penalty for women who wore men\u2019s clothing, which was solid and satisfying religious content of that era.\u201d</span></p>\n<p><span style=\"font-family: Times New Roman;\">Furthermore, even though HaLevy cites the Torah itself as a source for the necessity to master the sciences <span style=\"mso-spacerun: yes;\">&nbsp;</span>(\u201cAnd you shall keep and do them, for that is your wisdom and your understanding in the eyes of the nations, who will hear all these statutes and say, \u2018Only this great nation is a wise and understanding people.\u2019\u201d <span style=\"font-size: 8pt;\">Deuteronomy 4:6</span>), this does not mean that any scientific or philosophical pronouncement from the mouth of a religious scholar was taken as dogma. In his discussion of <em style=\"mso-bidi-font-style: normal;\">Sefer Yetzirah</em>, a book of metaphysical cosmogony legendarily attributed to the Biblical Abraham, HaLevy says after his description of its contents, </span></p>\n<p style=\"margin-left: 0.5in;\"><span style=\"font-family: Times New Roman;\">\u201cThis, however, is still not satisfactory, because the object of research is either too profound to be fathomed, or our minds are inadequate, or both\u2026 for no two philosophers could ever agree on this matter, unless they happen to have had the same teacher.\u201d</span></p>\n<p><span style=\"font-family: Times New Roman;\">In other words, one of the most popular Jewish philosophical texts in history openly admits that the Patriarch Abraham\u2019s views on cosmology may have been incorrect. I mention this point in anticipation of the counter-argument that, perhaps, ancient Hebrew interest in science and philosophy was strangled by preconceived outcomes and intellectual dogmas (\u201cWhy continue studying the universe, when Abraham\u2019s <em style=\"mso-bidi-font-style: normal;\">Sefer Yetzirah</em> explains the whole thing already!?\u201d). Indeed, the <em style=\"mso-bidi-font-style: normal;\">Sefer Yetzirah </em>was later followed by other books and schools of Jewish thought with contending views of nature, matter, and the human mind. </span></p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\"><span style=\"font-family: Times New Roman;\"><span class=\"normal00200028web0029char1\">It is here</span> that we touch on the real issue. Yudkowsky already openly admits that Judaism encourages questioning. However, he asserts that such questioning is strangled by predetermined outcomes, and therefore, does not constitute a genuine pursuit of knowledge\u2014 he explains his views on this in, \u201c</span><span class=\"hyperlinkchar1\"><span style=\"font-family: Times New Roman; color: #0000ff;\"><a href=\"/lw/jy/avoiding_your_beliefs_real_weak_points/\">Avoiding Your Belief's Real Weak Points</a></span></span><span style=\"font-family: Times New Roman;\">.\u201d Likewise, in \u201c</span><span class=\"hyperlinkchar1char1\"><span style=\"font-family: Times New Roman; color: #0000ff;\"><a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">Tsuyoku Naritai!</a></span></span><span style=\"font-family: Times New Roman;\">,\u201d Yudkowsky points out that modern rabbis cannot overrule ancient rabbis in matters of Halacha, because of the assumption that since G-d gave the Torah to Rabbi A, who gave it to Rabbi B, knowledge of the Torah inevitably decreases as it reaches Rabbis C, D, E, etc. Yudkowsky then concludes that both knowledge and ethics, as conceived in Orthodox Judaism, only degrades, in addition to being limited by preconceived outcomes, thus making Judaism\u2019s approach to learning qualitatively inferior to the approach of the scientific method.</span></p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\"><span class=\"normal00200028web0029char1\"><span style=\"font-family: Times New Roman;\">&nbsp;</span></span></p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\"><span style=\"font-family: Times New Roman;\"><span class=\"normal00200028web0029char1\">Before proceeding, I want to pause and clarify a con</span>cept. In \u201cTsuyoku Naritai!\u201d, Yudkowsky is making reference to the more general Jewish concept of <span class=\"normal00200028web0029char1\"><em>yeridas hadoros</em></span>, or \u201cdescent of the generations,\u201d which generally refers to the notion that people become less-and-less spiritual with succeeding generations, and&nbsp; the ancient past is therefore associated with sacred origins, rather than with more primitive stages of a progressive, evolutionary scale. Yudkowsky\u2019s views on Judaism\u2019s approach to knowledge and progress are based primarily, I think, on three misunderstandings concerning <em style=\"mso-bidi-font-style: normal;\">yeridas hadoros. </em></span></p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in 5pt 0.5in;\"><span class=\"normal00200028web0029char1\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"font-family: Times New Roman;\">&nbsp;</span></strong></span></p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in 5pt 0.5in;\"><span style=\"font-family: Times New Roman;\"><span class=\"normal00200028web0029char1\"><strong style=\"mso-bidi-font-weight: normal;\">1)</strong></span> <strong style=\"mso-bidi-font-weight: normal;\">Regarding Halacha:</strong> It is true that modern rabbis cannot overrule ancient rabbis in matters of Halacha\u2014 this principle of Jewish legal reasoning is written in the Talmud, Masechta Megillah (\u201cNo rabbinic court may nullify the ruling of another rabbinic court, unless they are superior in wisdom and number\u201d). The logic underlying this rule derives from the increasingly weaker claims to traditional interpretation of the Torah, commensurate with increasingly later stages of a lengthening historical chain of transmission from the original Revelation at Sinai, and the codification of the oral tradition in the Talmud. From a legal standpoint, this is logical\u2014 and in fact, it\u2019s empirically observable. &nbsp;For&nbsp;instance: nowadays, </span><span style=\"font-family: Times New Roman;\"><a href=\"http://en.wikipedia.org/wiki/Mezuzah\">mezuzahs</a></span><span style=\"font-family: Times New Roman;\"> are placed at a diagonal, due to doubt whether they should be vertical or horizontal. There is a doubt as to whether&nbsp;new days start&nbsp;at sundown or nightfall, therefore, the Sabbath is 25 hours long, to include both&nbsp;opinions. There are doubts about how to make </span><span style=\"font-family: Times New Roman;\"><a href=\"http://en.wikipedia.org/wiki/Tefillin\">tefillin</a></span><span style=\"font-family: Times New Roman;\"> properly; therefore, there are both <span class=\"normal00200028web0029char1\"><em>Rashi </em></span>and <span class=\"normal00200028web0029char1\"><em>Rabbeinu Tam </em></span>tefillin. There really was once a time when these issues were not debated; but doubts arose over time, mainly due to exile and government persecutions that caused interruptions in the oral tradition. This is an empirically observable aspect of <span class=\"normal00200028web0029char1\"><em>yeridas hadoros</em></span>, and it\u2019s in reference to this principle that the Talmud (Shabbat 112b) says, \u201cIf the earlier scholars were sons of angels, we are sons of men; and if the earlier scholars were sons of men, we are&nbsp;like donkeys.\u201d Furthermore, acquiescing to courts \u201cgreater in wisdom and number\u201d and to courts of previous eras is designed to produce a smoothly functioning legal system, a democratic process in scholarly legal rulings, and to prevent schisms among Jewry with varying legal approaches. This is why the outcomes of Talmudic legal debates were decided according to a vote from the Sanhedrin, and why decisions are determined by the majority opinion of the Sanhedrin, even if the majority opinion is incorrect. This&nbsp;is legal reasoning, rather than scientific reasoning.</span></p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"font-family: Times New Roman;\">&nbsp;</span></strong></p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in 5pt 0.5in;\"><span style=\"font-family: Times New Roman;\"><strong style=\"mso-bidi-font-weight: normal;\">2)</strong> <strong style=\"mso-bidi-font-weight: normal;\">Regarding acquisition of new scientific knowledge:</strong> Rabbinic authorities acquiesce to&nbsp;courts greater in wisdom and number concerning (even incorrect) legal rulings,&nbsp;for the reasons stated above. However, they do <span class=\"normal00200028web0029char1\"><em>not </em></span>acquiesce in objective scientific knowledge, and in fact, most rabbinic authorities throughout the ages have not even permitted this approach. Rabbi Avraham HaNaggid (13th c. CE), in&nbsp;his commentary to the Talmud, put it this way: </span></p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in 5pt 1in;\"><span style=\"font-family: Times New Roman;\">\u201cKnow that it is&nbsp;your duty to understand that whoever propounds a certain theory or idea and expects that theory or idea to be accepted merely out of respect for the author without proving its truth and rationality, pursues a wrong method prohibited by both the Torah and human intelligence. From the standpoint of &nbsp;intelligence, such a method is worthless for it would cause one to minimize the &nbsp;importance of those things which, after scrupulous observation and proofs, ought &nbsp;to be believed, and from the point of view of the Torah\u2014 because it inclines from &nbsp;the true path and from the straight, leveled road. The L-rd, praised be He! said: \u201cThou shalt not respect the poor person, nor honor the great person; in justice shalt thou judge <span style=\"font-size: 8pt;\">(Lev. 19, 15)</span>\u201d. And it also says, \u201cYou shall not respect a person in&nbsp;judgment <span style=\"font-size: 8pt;\">(Deut. 1:17)</span>\u201d. And there is no difference between him who accents an idea without any evidence as to its integrity, and him who believes a person\u2019s statement simply because he respects the latter and therefore contends that his idea is undoubtedly true since it emanates from a great man like Heiman, Karkal, or Darda. For all this gives no evidence as to the merits of the subject in question and is therefore forbidden. According to this preamble, then, we are not in duty bound to defend the opinions of the sages of the Talmud, concerning medicine, physics and astrology, etc, as right in every respect simply because we know the sages to be great men with a full knowledge of all things regarding the Torah, in its various details. Although it is true that in so far as knowledge of our Torah &nbsp;is concerned, we must believe the sages arrived at the highest stage of &nbsp;knowledge, as it is said, \u201cIn accordance with the instructions which they may &nbsp;instruct thee, etc <span style=\"font-size: 8pt;\">(Deut 17:11)</span>\u201d, still it is not necessarily so concerning any other &nbsp;branch of knowledge. You can see that even the sages themselves say very often of things which cannot proven by discussions and arguments, \u201cI swear, that&nbsp;even had Joshua bin Nun said it, I would not obey him!\u201d This means that I would not believe him although he was a prophet\u2014 since he cannot prove the reason for such a thing in accordance with the rules of the Talmudic construction.\u201d </span></p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\"><span style=\"font-family: Times New Roman;\">&nbsp;</span></p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in 5pt 0.5in;\"><span style=\"font-family: Times New Roman;\"><strong style=\"mso-bidi-font-weight: normal;\">3) Regarding human progress: </strong>The concept of generational descent is only applied to the exoteric aspects of Judaism, such as the plainer meanings of the Tanakh, Halacha, Midrashic literature, etc. In contrast, the more esoteric aspects of Judaism, such as Jewish Philosophy and Kabbalah, are generally regarded as experiencing a generational <em style=\"mso-bidi-font-style: normal;\">ascent</em>, rather than descent. Consequently, <em style=\"mso-bidi-font-style: normal;\">yeridas hadoros </em>is traditionally understood as a dialectic process of decreasing knowledge of the more \u201crevealed\u201d aspects of the Jewish tradition, accompanied simultaneously by concepts of meta-ethics and philosophy of increasing intellectual sophistication (though dialectically, innate human <em style=\"mso-bidi-font-style: normal;\">intuition</em> for spirituality decreases generationally).</span></p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\"><span style=\"font-family: Times New Roman;\">&nbsp;</span></p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\"><span style=\"font-family: Times New Roman;\"><span class=\"normal00200028web0029char1\">I admit that certain intellectual limitations do undoubtedly </span>exist in Judaism, just as they exist within any religion or ideology that operates around certain axioms. An orthodox Jewish thinker could never truly adopt utilitarian hedonism, for instance, and remain an Orthodox Jew, just as a Marxist could never decide that capitalism is actually a really good idea and remain a socialist. Nor could the Orthodox Jew reject, say, prophecy or free-will. However, there is nothing inherently non-rational in such assertions so long as these assertions are the product of free thought, and can be argued for intelligently. </span></p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\">&nbsp;</p>\n<p><span style=\"font-family: Times New Roman;\">\n</span></p><p><strong style=\"mso-bidi-font-weight: normal;\" id=\"Section_Three__The_author_claims_that_historical_Judaism_defends_the_authenticity_of_the_Torah__without_accounting_for_Bayes__rule__\">Section Three: The author claims that historical Judaism defends the authenticity of the Torah, without accounting for Bayes\u2019 rule. </strong></p>\n<p>Here, I will not contend that the Torah is, indeed, a direct communication from The Ineffable to Moses (though I do think so). That would be beyond the scope of this essay. Instead, I will limit myself to arguing that the traditional Jewish claim is not based on an intrinsic logical fallacy.</p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; mso-layout-grid-align: none;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">The Bayesian rule </span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-ansi-language: EN;\" lang=\"EN\">expresses how a subjective degree of belief should rationally change to account for evidence. More specifically, proponents of Bayes\u2019 theorem generally posit that (1) </span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">it is illogical</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\"> to ignore what we </span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">know, (2) it is natural and useful </span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">to cast what we know in the language of probabiliti</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">es, and (3) if our subjective </span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">probabilities are erroneous, their impact will get</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"> washed out in due time, as the number of observations increases. </span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; mso-layout-grid-align: none;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; mso-layout-grid-align: none;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">Before proceeding, I will admit that arguing in favor of the Torah\u2019s authenticity is difficult in light of purely Bayesian reasoning, as the traditional understanding is rooted in <em style=\"mso-bidi-font-style: normal;\">causal </em>reasoning, rather than probabilistic. As probability theory deals with beliefs about a static environment, while causality deals with changes that occur in the world itself in real time, it is only natural that rabbinic understandings of the Torah\u2019s origin and historical, generational transmission work with causal logic and language. Further, as </span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"><span style=\"color: #0000ff;\"><a href=\"http://en.wikipedia.org/wiki/Judea_Pearl\">Judea Pearl</a></span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"> points out in his article, \u201c</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\"><span style=\"color: #0000ff;\"><a href=\"http://ftp.cs.ucla.edu/pub/stat_ser/r284-reprint.pdf\">Bayesianism and Causality, or, Why I am Only a Half-Bayesian</a></span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">,\u201d a complete synthesis of causal and statistical reasoning is mathematically untenable. That said, I will only argue that the historical Jewish claim does not violate (1) above: it does not ignore what we now know, nor has it in previous eras. So to proceed:</span></p>\n<p><span style=\"mso-ansi-language: EN;\" lang=\"EN\"><span style=\"mso-spacerun: yes;\">&nbsp;</span>To paraphrase Yudkowsky, \u201cThis boy was frothing at the mouth\u2014 he must have suffered from demons. That man over there cured the boy\u2014 he must be an exorcist!\u201d By Bayes\u2019 rule, this is perfect reasoning\u2014 <em style=\"mso-bidi-font-style: normal;\">assuming the boy\u2019s illness came from demons</em>. Yudkowsky asserts that the Torah has the same problem (\u201cWe heard a Voice proclaim from the mountain, \u2018It\u2019s all true!\u2019\u201d). In fact, Yudkowsky admits that even if there really <em style=\"mso-bidi-font-style: normal;\">had</em> been a voice, the Bayesian theorem would still reveal the logical fallacy\u2014 how do you <em style=\"mso-bidi-font-style: normal;\">know </em>the voice is Divine, and it isn\u2019t just the Wizard of Oz hiding behind Mount Sinai with a loudspeaker? And as humanity progresses beyond the Torah scientifically and morally, doesn\u2019t it become more and more likely, even highly probable, that given G-d or the Wizard of Oz, it was more likely the Wonderful Wizard? Also, aren\u2019t we ignoring </span><span style=\"mso-ansi-language: EN;\" lang=\"EN\"><span style=\"color: #0000ff;\"><a href=\"http://yudkowsky.net/rational/bayes\">the original fraction of necessarily false claims</a></span></span><span style=\"mso-ansi-language: EN;\" lang=\"EN\"> in our computation\u2014 in this case, <em style=\"mso-bidi-font-style: normal;\">all</em> of the other supernatural religious claims that have ever been made?</span></p>\n<p><span style=\"mso-ansi-language: EN;\" lang=\"EN\">First, let me recount the earliest scientific experiment that <em style=\"mso-bidi-font-style: normal;\">I </em>know of. It\u2019s earlier than Yudkowsky\u2019s example: Moses\u2019 confrontation with the Pharaoh\u2019s magicians. </span></p>\n<p><span style=\"mso-ansi-language: EN;\" lang=\"EN\">It roughly went like this. Moses goes to Pharaoh, declares that only the G-d of the Hebrews is the real G-d, and then his staff is thrown to the floor, which turns into a snake. Then Pharaoh\u2019s court magicians throw a staff to the floor, and for the sake of scientific control, attempt to replicate Moses\u2019 results to test the critical p-value of their own hypothesis. However, unlike the priests of Baal, the magicians <em style=\"mso-bidi-font-style: normal;\">actually do replicate the results</em>. Boom: another snake! The magicians, satisfied with having duplicated Moses\u2019 results, reject his hypothesis and declare themselves victorious.</span></p>\n<p><span style=\"mso-ansi-language: EN;\" lang=\"EN\">Though from the very start, it was only<em style=\"mso-bidi-font-style: normal;\"> </em>the court magicians who thought it was an acceptable experiment. Because you see, the magicians were conducting the experiment <em style=\"mso-bidi-font-style: normal;\">under the a priori assumption</em> that any alterations of the natural order would constitute real evidence, which in that era, would make sense from a Bayesian perspective. </span></p>\n<p><span style=\"mso-ansi-language: EN;\" lang=\"EN\">Furthermore, back in the day, nobody ever seriously entertained the notion that only one\u2019s <em style=\"mso-bidi-font-style: normal;\">own</em> national pantheon was objectively real. To the Egyptians, the gods of Persia, Greece and Ireland were just as \u201creal\u201d as their own\u2014 they may have been foreign deities, but deities nonetheless. The Persians, Greeks, and Irish themselves all thought this, too\u2014 orthodox Hindus to this day still often think this way. This was generally due to an inability to distinguish between the factual and the fantastic.</span></p>\n<p><span style=\"mso-ansi-language: EN;\" lang=\"EN\">So for Pharaoh\u2019s magicians, the burden of proof was quite low: their thesis was that Osiris and Ra were just as real as the G-d of the Hebrews, so all that was needed was a <em style=\"mso-bidi-font-style: normal;\">replication </em>of Moses\u2019 results to yield a low p-value for Moses\u2019 thesis that the G-d of the Hebrews was objectively real, and the Egyptians and whole worlds\u2019 deities, not. The fact that Moses\u2019 snake ended up eating their snakes did not matter; their thesis was simply that Osiris and Ra were <em style=\"mso-bidi-font-style: normal;\">also</em> real.</span></p>\n<p><span style=\"mso-ansi-language: EN;\" lang=\"EN\">Moses, on the other hand, knew that miraculous phenomena did not constitute rational evidence of nearly anything at all\u2014 and he knew the experiment was artificial, due to this knowledge. <span style=\"mso-spacerun: yes;\">&nbsp;</span>The Code of Jewish Law states it this way (this quote is long, but necessary\u2014 I will keep coming back to it):</span></p>\n<p style=\"margin-left: 0.5in;\">\u201cThe Jews did not believe in Moses, our teacher, because of the miracle that he performed. <strong style=\"mso-bidi-font-weight: normal;\">Whenever anyone's belief is based on miracles, the commitment of his intellect has shortcomings, because it is possible to perform a wonder through magic or sorcery</strong> (Note: magic and sorcery refer to optical illusions, as the compiler of this code did not believe in real sorcery).</p>\n<p style=\"margin-left: 0.5in;\">All the wonders performed by Moses in the desert were not intended to serve as proof of the legitimacy of his prophecy, but rather were performed for a purpose. It was necessary to drown the Egyptians, so he split the sea and sank them in it. \u2026The same applies to all of the other miracles.</p>\n<p style=\"margin-left: 0.5in;\">What is the source of our belief in him? The revelation at Mount Sinai. Our eyes saw, and not a stranger\u2019s. Our ears heard, and not another\u2019s. There was fire, thunder, and lightning. He entered the thick clouds; the Voice spoke to him and we heard, \u201cMoses, Moses, go tell them the following....\u201d</p>\n<p style=\"margin-left: 0.5in;\">Thus, Deuteronomy relates: \u201cFace to face, G-d spoke to you,\u201d and it states: \u201cG-d did not make this covenant with our fathers, but with us, who are all here alive today.\u201d</p>\n<p style=\"margin-left: 0.5in;\">How is it known that the revelation at Mount Sinai alone is proof of the truth of Moses\u2019 prophecy that leaves no shortcoming? Exodus states: \u201cBehold, I will come to you in a thick cloud, so that the people will hear Me speaking to you, so that they will believe in you forever.\u201d It appears that before this happened, they did not believe in him with a faith that would last forever, but rather with a faith that allowed for suspicions and doubts.</p>\n<p style=\"margin-left: 0.5in;\">Thus, those to whom Moses was sent witnessed his appointment as a prophet, and it was not necessary to perform another wonder for them. He and they were witnesses, like two witnesses who observed the same event together. Each one serves as a witness to his colleague that he is telling the truth, and neither has to bring any other proof to his colleague.</p>\n<p style=\"margin-left: 0.5in;\">Similarly, all Israel were witnesses to the appointment of Moses, our teacher, at the revelation at Mount Sinai, and it was unnecessary for him to perform any further wonders for them.</p>\n<p style=\"margin-left: 0.5in;\">This concept is alluded to in the interchange between G-d and Moses at the revelation of the burning bush. At the beginning of his prophecy, the Holy One, blessed be He, gave him the signs and wonders to perform in Egypt and told him, \u201cAnd they will listen to your voice.\u201d</p>\n<p style=\"margin-left: 0.5in;\">Moses, our teacher, knew that one who believes in another person because of miracles has apprehension in his heart; he has doubts and suspicions. Therefore, he sought to be released from the mission, saying: \u201cThey will not believe me\u201d, until the Holy One, blessed be He, informed him that these wonders were intended only as a temporary measure, until they left Egypt. After they would leave, they would stand on this mountain and all doubts which they had about him would be removed.</p>\n<p style=\"margin-left: 0.5in;\">G-d told him: Here, I will give you a sign so that they will know that I truly sent you from the outset, and thus, no doubts will remain in their hearts. This is what is meant by the statement: \u201cThis will be your sign that I sent you: When you take the people out of Egypt, you will serve G-d on this mountain.\u201d (<span style=\"font-size: 8pt;\">M.T., Laws of the Foundations of the Torah 8:1-3</span>)\u201d</p>\n<p>So it worked like this. Moses has a prophetic vision of a burning bush. The Infinite One says, \u201cI\u2019m sending you as a prophet to the Hebrews\u2014 go to them, and tell them I sent you!\u201d This was entirely unprecedented. Even though there had been others in Hebrew history to have had prophetic visions, the communications in those visions had always concerned information pertinent to the individual alone. After all, if Abraham, Isaac or Jacob had gone around to people saying \u201cHey! I had a prophecy! This <em style=\"mso-bidi-font-style: normal;\">proves</em> G-d\u2019s existence! Let\u2019s all be monotheists now!\u201d such a thing would have been illogical. After all, why should anyone believe them? They could have performed impressive miracles, perhaps\u2014 but how would people know they\u2019re not optical illusions? Wouldn\u2019t that be far more likely, anyway? <span style=\"mso-spacerun: yes;\">&nbsp;</span>And even if the miracles <em style=\"mso-bidi-font-style: normal;\">had been </em>real, why would that have constituted a proof for anything <em style=\"mso-bidi-font-style: normal;\">even then</em>? Moses Maimonides (12<sup>th</sup> c. CE), foremost rabbinic authority of the Middle Ages, said it like this:</p>\n<p style=\"margin-left: 0.5in;\">\u201cAnyone who in those days (i.e. pre-Mosaic) laid claim to authority, based it... on the fact that, by reasoning and by proof he had been convinced of the existence of a Being who rules the whole Universe. \u2026But no one could establish his claim on prophecy, that is to say, on the fact that G-d had spoken to him, or had entrusted a mission to him; before the days of Moses no such assertion had ever been made. You must not be misled by the statements that G-d spoke to the Patriarchs, or that He had appeared to them. For you do not find any mention of a prophecy which appealed to others, or which directed them. Abraham, Isaac, or Jacob, or any other person before them did not tell the people, \u201cG-d said to me, you shall do this, or you shall do that,\u201d or \u201cG-d has sent me to you.\u201d Far from it! For G-d spoke to them on nothing but of what especially concerned them, i.e. He communicated to them things relating to their perfection, directed them in what they should do, and foretold them what the condition of their descendants would be; nothing beyond this. They guided their fellow men only by means of argument and instruction. (<span style=\"font-size: 8pt;\">Guide for the Perplexed I:LXIII</span>)\u201d</p>\n<p>So indeed, Moses was justifiably confused by the instruction, \u201cGo persuade them by turning this staff into a magical cobra!\u201d, and naturally responded, \u201cThe people will not believe in me. (<span style=\"font-size: 8pt;\">Exodus 4:1</span>)\u201d Moses knew that his prophetic vision could not be objectively proved to anyone else as authentic\u2014 so what was the point?</p>\n<p>So the Et-rnal One responds, \u201cYeah, you\u2019re right. Turning your staff into a cobra doesn\u2019t prove you\u2019re a prophet, and plagues don\u2019t either<em style=\"mso-bidi-font-style: normal;\">\u2014 but it will convince the Hebrew masses that you <strong style=\"mso-bidi-font-weight: normal;\">might</strong> be</em>, and that will be enough to give them hope temporarily, for the time necessary. Once they see you receiving the Ten Commandments on Sinai, <em style=\"mso-bidi-font-style: normal;\">then</em> they\u2019ll have their absolute proof.\u201d</p>\n<p>The Code of Jewish Law states that miraculous feats constitute proof of nothing\u2014 this principle is axiomatic to Jewish thought, and is crucial to the legal process of halachic reasoning. It is very much for this reason that the Christian claim that Jesus\u2019 wonder-working was a proof for his divinity, was routinely <span style=\"color: #0000ff;\"><a href=\"http://en.wikipedia.org/wiki/Toledot_Yeshu\">satirized</a></span> by Jews even in medieval times; <em style=\"mso-bidi-font-style: normal;\">even by those Jews who thought that Jesus\u2019 powers were real</em>. \u201cHow do you <em style=\"mso-bidi-font-style: normal;\">know </em>Jesus didn\u2019t just slip some red dye into that water? And even if he did turn water to wine and heal lepers, so what? Even Pharaoh\u2019s magicians could have done that!<span style=\"mso-ansi-language: EN;\"> <span lang=\"EN\">Aren\u2019t you all ignoring the original fraction of necessarily false claims in your computation?!\u201d</span></span><span lang=\"EN\"> </span></p>\n<p>In contrast, the historical Hebrew claim is this: that when Moses received the Law on Sinai, the Ten Commandments were communicated to him through direct prophecy, and further, the first two of the Ten Commandments (\u201cI am <span style=\"font-size: 10pt;\">THE L-RD</span>\u201d<span style=\"font-size: 10pt;\"> </span>and \u201cNo idolatry\u201d) were communicated to the entire mass of 600,000 Israelites through prophecy as well. Therefore, there could be no doubts that Moses was a prophet, for not only did they all see and hear him receive the Law, and hear the \u2018Voice\u2019 he heard, but each one of them individually received the Law through prophetic communication as well, as a public body. BAZOOM! Proof. <span style=\"mso-spacerun: yes;\">&nbsp;</span>Further, the historical Hebrew claim is that this experience constituted proof for succeeding generations as well. This is why the Code of Jewish Law quotes Devarim (Deuteronomy) in stating, \u201cG-d did not make this covenant with our fathers, but with us, who are all here alive today.\u201d This line, quoting Moses, was not addressed to the Hebrews who were slaves in Egypt and who stood at Sinai, but to the 2<sup>nd</sup> generation, after the 1<sup>st</sup> one had passed away in the desert. It was to this generation, who had never been there, that Moses said, \u201cG-d did not make this covenant with our fathers, but with us, who are all here alive today.\u201d As the 1<sup>st</sup> generation could surely have never faked such an event, <em style=\"mso-bidi-font-style: normal;\">even if they had wanted to</em>, and as such an event could never be realistically fabricated by an individual and promulgated to the masses of Canaan (we were all slaves just 200 years ago, but then we all forgot, but luckily, Ari and Mendel rediscovered all this\u2026), an indisputable historical tradition is treated in traditional Judaism as equivalent to a personal divine revelation, as far as proof is concerned.</p>\n<p>Regarding the post-Mosaic prophets, i.e. Hosea, Isaiah, Jeremiah, etc., there is still the question: how do we know <em style=\"mso-bidi-font-style: normal;\">they</em> weren\u2019t making it up? After all, they routinely performed miracles and made predictions concerning the future as proof of their prophetic abilities\u2014 and in contrast to the Mosaic model, their visions were private, just as much as Abraham, Isaac and Jacob\u2019s visions were all private, or if you will, Jesus, Muhammad and Zoroaster. So how would <em style=\"mso-bidi-font-style: normal;\">they</em> be distinct? Why would it have been illogical for Abraham to proclaim himself a prophet, but not Elijah?</p>\n<p>The legal ruling from the Torah is that if any individual Israelite, who is of properly reputable character (i.e. a scholar or rabbi) claims a prophetic vision, and is able to predict the future and demonstrate his or her prophetic abilities to the satisfaction of the Sanhedrin\u2014 then the Sanhedrin will legally declare the said individual to be a prophet, if the possibility of this Torah scholar being a charlatan is considered sufficiently negligible. Ultimately, the status of \u201cprophet\u201d for post-Mosaic Jewish leaders is not an article of faith, but a legal ruling. This is all so long as the would-be prophet does not contradict Mosaic Law. The Code of Jewish Law states it this way:</p>\n<p style=\"margin-left: 0.5in;\">\u201cWe do not believe in any prophet who arises after Moses, our teacher, because of the wonder he performs alone, as if to say: If he performs a miracle we will listen to everything he says. Rather, we believe him because it is a commandment which we were commanded by Moses who said: \u201cIf he performs a wonder, listen to him.\u201d</p>\n<p style=\"margin-left: 0.5in;\">Just as we are commanded to render a legal judgment based on the testimony of two witnesses, even though we do not know if they are testifying truthfully or falsely, similarly, it is a commandment to listen to this prophet <strong style=\"mso-bidi-font-weight: normal;\">even though we do not know whether the wonder is true or performed by magic or sorcery.</strong></p>\n<p style=\"margin-left: 0.5in;\">Therefore, if a prophet arises and attempts to dispute Moses\u2019 prophecy by performing great signs and wonders, we should not listen to him. We know with certainty that he performed those signs through magic or sorcery. This conclusion is reached because the prophecy of Moses, our teacher, is not dependent on wonders, so that we could compare these wonders, one against the other. Rather we saw and heard with our own eyes and ears as he did. (<span style=\"font-size: 8pt;\">M.T., Laws of the Foundations of the Torah 8:3</span>)\u201d<span style=\"mso-spacerun: yes;\">&nbsp; </span></p>\n<p>Yudkowsky\u2019s assertion that the contest between Elijah and the priests of Baal is an example of how ancient Jewry had no concept of how to ask factual questions is false. Knowledgeable Israelites (or even \u201cmainstream\u201d pious Israelites familiar with the Moses vs. Pharaoh account in the Torah) knew perfectly well that such contests yielded no factual information, and were only necessary as a temporary, extreme measure. The fact that the contest took place outside the Temple premises was itself a violation of Hebrew law, as Mosaic Law permits no sacrifices beyond Temple grounds\u2014 this was also a temporary measure taken by Elijah. Ultimately, the logical worthlessness of the contest later emerged, as the Israelites who were \u201cwon over\u201d to Elijah\u2019s side (the ones who gave the positive peer review) later regressed back to their pagan practices. Rabbinic commentaries to this account attest to this regression as an example of how miracles only constitute \u201cproof\u201d to the uneducated. The 450 priests of Baal, all being Jewish, were executed in accordance with the mandate of Judaic law (\u201cA person who worships false gods is to be hanged\u201d- <span style=\"font-size: 8pt;\">M.T., Laws of Idolatrous Worship 2:6</span>).</p>\n<p>It is here that we\u2019ll address the pink elephant in the room. How would Moses and all of the Israelites have known that what they were experiencing was authentic prophecy? We are still stuck with the Wizard of Oz dilemma: is the Voice divine, or a hallucination? Though the Hebrew historical claim is that all 600,000 Israelites received a prophetic communication all at once (rather than simply the claims of one individual), this does not necessarily alleviate the problem\u2014 because even though 600,000 individuals are statistically less likely than 1 individual to hallucinate, what if they <em style=\"mso-bidi-font-style: normal;\">were </em>hallucinating (Yudkowsky touches on this problem in <em style=\"mso-bidi-font-style: normal;\"><span style=\"color: #000000;\">Harry Potter and the Methods of Rationality</span></em>)? After all, there is very good documentation of cases of mass hysteria, complete with hallucinations, and has been known to affect up to thousands of people all at once. In fact, even if we assume that there was a good chance<em style=\"mso-bidi-font-style: normal;\"> </em>that the revelation on Sinai to the Hebrews was real, we would still have to admit that there have been many, many documented cases of mass hysteria, far outnumbering the number of times the Jews have claimed to have had experienced public, divine revelation (so far numbering exactly 1).</p>\n<p>It would not fully negate the hallucination possibility, to assert that historical Judaism considered prophecy to be a mental ability that comes as a result of intellectual preparation, and that ancient Jewry regarded prophecy as the consequence of decades of intellectual study (knowledge of science was requisite, says the Code of Jewish Law) and of meditative practice (in the Talmud, meditation schools are recorded to have existed). Describing Hebrew prophecy as a primarily intellectual experience more similar to the nirvana of an Indian bodhisattva, than the emotional ecstasy of a Greek oracle, would not be enough to fully negate the possibility of mass hysteria. This is especially so, considering that the tradition of the revelation on Sinai is the one exception in Jewish recorded history of intellectually <em style=\"mso-bidi-font-style: normal;\">un</em>prepared Israelites experiencing prophecy (thus the reason the Torah describes them all as temporarily dying after the 2<sup>nd</sup> Commandment being given). So how do we know if it was enlightened prophecy, or madness they were experiencing? The fact that Judaism is the only religion in history to have never experienced internal ideological schism in its formative stages of development does not fully negate the possibility of hallucination either\u2014 at least from a probabilistic standpoint.</p>\n<p>But then again, <span style=\"color: black;\">historical theory is not a laboratory science. You cannot test it and make observations\u2014 you can only check a historical theory for consistencies and inconsistencies. Analyzing historical theories using probabilistic theorems is usually extremely difficult, and I fully agree with Judea Pearl\u2019s opinion that real events in time can only be understood in terms of causality. Therefore, I would argue that the Wizard of Oz is mostly the result of statistical reasoning being applied to a scenario that requires a primarily cause-and-effect approach; because even if the <em style=\"mso-bidi-font-style: normal;\">Orthodox</em> approach does produce some Wizards, that is <em style=\"mso-bidi-font-style: normal;\">nothing</em> compared to the number of Wizards produced by a materialistic, reductionist approach. </span></p>\n<p><span style=\"color: black;\">The <em style=\"mso-bidi-font-style: normal;\">Grand Rabbi of Guadalajara </em>points out that according to reductionist approaches to Jewish history that deny the Torah\u2019s authenticity, the Jews would have to be either </span></p>\n<p style=\"margin-left: 0.5in;\"><span style=\"color: black;\">\u201c<strong style=\"mso-bidi-font-weight: normal;\">(a)</strong> (B)y far the most ingenious people ever. Out of all the peoples of the ancient world, this nation of shepherds and fig-growers came up with the classic work of all time. The work that changed all of history, brought us the concepts of creation ex-nihilo, history, purpose, monotheism, providence, human rights, gave rise to both Christianity and Islam and triggered the Reformation and modernization of western civilization\u2026 A supremacy dogma if I ever heard one! </span></p>\n<p style=\"margin-left: 0.5in;\"><span style=\"color: black;\">(Or)</span></p>\n<p style=\"margin-left: 0.5in;\"><strong><span style=\"color: black;\">(b)</span></strong><span style=\"color: black;\"> According to this theory, the Jews are by far the stupidest and most gullible people in the world. They fell for a story that restricts their diet, their domination over their slaves, their weekly work habits and their sex-life beyond what any other nation would tolerate. They bought into a lose-lose situation for everybody all \u2018round: The King\u2019s power is restricted, the priestly class cannot own land, and the commoners can\u2019t sell it.</span></p>\n<p style=\"margin-left: 0.5in;\"><span style=\"color: black;\">They abandon their fields and towns three times a year to the mercy of the hostile nations surrounding them, let those fields lie fallow once in seven years, let their slaves go free after six years, don\u2019t charge interest -- and just trust year after year that everything will be okay. After all, G-d promises that when you\u2019re planning to leave your land fallow in the seventh, He\u2019ll give you a bumper crop in the sixth. So tell me, what happens when one year this just doesn\u2019t work out? Do you leave that in the books you\u2019re writing?</span></p>\n<p style=\"margin-left: 0.5in;\"><span style=\"color: black;\">Furthermore, this theory has the Jewish people making up fables about their blunders in full detail. They declare that they descend from slaves! They tell nasty stories about the forefather of their priestly class, Levi\u2014 even though the Levites were supposed to have written the book. The original high priest gets his hands dirty in the biggest scandal of their history. Who is this fable serving, anyway? Why on earth would anyone <em>want</em> to make up such a story? And what sort of crazy people would want to preserve it?\u201d</span></p>\n<p>It is here, then, that I will conclude Section Three in mostly the same way it started. The ancient Israelites were not incapable of understanding factual questions, nor were they ignorant of rational analysis. The concepts of objective evidence and rational apprehension are seen in the Torah itself, and continued on into the era of the Prophets and Writings, the development of halachic reasoning in the Mishnah, the development of Talmudic hermeneutics, and the development of <a href=\"http://en.wikipedia.org/wiki/Kabbalah\">Kabbalah</a> and <a href=\"http://en.wikipedia.org/wiki/Jewish_philosophy\">Jewish Philosophy</a>. Yudkowsky\u2019s claim that factual inquiry into religious matters is a strictly modern, Western concept is incorrect.</p>\n<p>As a brief aside: Yudkowsky states that Rome had concepts of law and order, and he seems to <em style=\"mso-bidi-font-style: normal;\">contrast</em> this with Jewry. As <span style=\"color: #0000ff;\"><a href=\"http://en.wikipedia.org/wiki/Max_Dimont\">Max Dimont</a></span>\u2019s research has strongly indicated, it was the Hebrews who were the originators of the concepts of due process and presumption of innocence in a court of law\u2014 not the Romans.<span style=\"mso-spacerun: yes;\">&nbsp; </span><span style=\"mso-spacerun: yes;\">&nbsp;</span><span style=\"mso-spacerun: yes;\">&nbsp;</span></p>\n<p><strong style=\"mso-bidi-font-weight: normal;\"></strong></p>\n<p><strong style=\"mso-bidi-font-weight: normal;\" id=\"Section_Four__The_author_claims_that_contemporary_religionists_justify_false_opinions_by_claiming_that_their_religion_is_a_separate_magisterium_which_can_be_neither_proven_nor_disproven_________\">Section Four: The author claims that contemporary religionists justify false opinions by claiming that their religion is a separate magisterium which can be neither proven nor disproven.<span style=\"mso-spacerun: yes;\">&nbsp; </span><span style=\"mso-spacerun: yes;\">&nbsp;&nbsp;</span><span style=\"mso-spacerun: yes;\">&nbsp;</span><span style=\"mso-spacerun: yes;\">&nbsp;</span><span style=\"mso-spacerun: yes;\">&nbsp;&nbsp;</span></strong></p>\n<p>I believe that Yudkowsky\u2019s assertion here is partially correct; but first, the subject being discussed should be clarified. The principle of Non-Overlapping Magisteria (NOMA) was first introduced into the public science vs. religion debates by <span style=\"color: #000000;\">Stephen Jay Gould</span> in the late 1990s, with the term \u201cmagisterium\u201d being borrowed by Gould from Pope Pius XII\u2019s 1950 encyclical, <em><span style=\"color: #0000ff;\"><a href=\"http://en.wikipedia.org/wiki/Humanae_generis\">Humani Generis</a></span></em>, which discussed Catholicism\u2019s views on natural evolution. In Gould\u2019s conception,</p>\n<p style=\"margin-left: 0.5in;\">\u201c(T)he magisterium of science covers the empirical realm: what the Universe is made of (fact) and why does it work in this way (theory). The magisterium of religion extends over questions of ultimate meaning and moral value. These two magisteria do not overlap, nor do they encompass all inquiry (consider, for example, the magisterium of art and the meaning of beauty). (<em><span style=\"font-size: 8pt;\">Rock of Ages, 1999</span></em>)\u201d</p>\n<p>To a degree, Yudkowsky is of course correct in stating that Judaism never originally had a concept of religion as a distinct magisteria, and that most other religions never did, either. I would extend this to include contemporary Judaism too, and would argue that Judaism has always been \u201ca religion without Mysteries,\u201d as <span style=\"color: #000000;\">Shmuel Luzzato</span>&nbsp;put it.</p>\n<p>However, though Judaism does not posit any intrinsically incomprehensible Mysterious Answers that are incapable of being logically deduced <em style=\"mso-bidi-font-style: normal;\">at all</em> (ex. Catholic<span style=\"color: #000000;\"> <span style=\"color: #0000ff;\"><a href=\"http://en.wikipedia.org/wiki/Transubstantiation\">transubstantiation</a></span></span>), it does nonetheless posit that there are matters, when contemplated, that cannot be fully comprehended by the intellect, and therefore <em style=\"mso-bidi-font-style: normal;\">do </em>occupy non-rational magisteria in a certain manner. This is not a modern phenomenon, and has always been present in Hebrew thought (\u201c<span style=\"font-size: 9pt;\">I AM THAT I AM</span>\u201d says G-d to Moses in Exodus, for instance). The <span style=\"color: #0000ff;\"><a href=\"http://en.wikipedia.org/wiki/Alter_Rebbe\">Alter Rebbe</a></span> z\u201dl, in his descriptions of Divine unity, wrote:</p>\n<p style=\"text-indent: 3pt; margin-left: 0.5in;\">\u201c(I)t is not at all proper to ascribe to G-d anything that is appurtenant to intellect &nbsp;even in a very lofty and sublime form, as if to say of G-d that it is beyond the &nbsp;capacity of any higher or lower creature to comprehend Divine Intellect or Essence. For comprehension pertains and applies to a matter of knowledge and wisdom, about which one can say that it can or cannot be understood because of the profundity of the concept. But, it is not at all proper to say concerning The&nbsp;Blessed Holy One, Who transcends intellect and wisdom, that it is impossible to&nbsp;apprehend G-d because of the depth of the concept, for G-d is not even within the realm of comprehension at all. (<span style=\"font-size: 8pt; mso-bidi-font-size: 12.0pt;\">Shaar Yichud v'Emunah, 1797</span>)\u201d</p>\n<p>&nbsp;This is seemingly about as distinct-magisterium-ish as it gets, and I\u2019d imagine that it is statements like this one that cause Yudkowsky to make analogies between G-d and <span style=\"color: #0000ff;\"><a href=\"/lw/i4/belief_in_belief/\">invisible, inaudible, permeable dragons</a></span> dwelling in one\u2019s garage.</p>\n<p>&nbsp;The distinction between G-d and the dragon, I\u2019d argue, is more easily appreciated from an existential standpoint than a probabilistic one. Within Judaism, matters that are treated as being incapable of being fully comprehended by the mind are always, <em style=\"mso-bidi-font-style: normal;\">exclusively</em>,<em> </em>of an existential nature. The essence of G-d, the nature of the soul, free will, the nature of good and evil, etc., are all treated as being incapable of being fully understood, and these are all <em>existential </em>concerns (and there is no concept of comprehension <em>at all </em>in Judaism<em> </em>concerning the ultimate essence of G-d). In contrast, historical claims such as the Exodus from Egypt and the receiving of the Torah, and purely theological claims such as paradise &amp; purgatory, reincarnation and the like, are <em>not </em>treated as occupying a distinct magisterium. They are considered fully comprehendible to the intellect, with the latter at the very least from utilizing philosophical reasoning.</p>\n<p>&nbsp;Furthermore, even concerning those subjects that cannot be fully fathomed from the Jewish perspective, such subjects are grouped into that separate magisterium as a direct result of rationally acquired knowledge. For instance, in Jewish thought, the origin of evil is regarded as being incapable of being completely fathomed\u2014 not as a result of reflexive reasoning or a retreat to commitment, but due to an intellectual understanding that evil exists, and that simultaneously the matter cannot be fully fathomed. This is often referred to as <em>yedias ha-shelilah</em>, or negative knowledge, i.e. knowledge by negative inference, exceeding the boundaries of structured thought. <em style=\"mso-bidi-font-style: normal;\">Yedias ha-shelilah</em>, in turn, can only be acquired once one acquires <em style=\"mso-bidi-font-style: normal;\">bittul</em>, or the negation of preconceived notions and biases.</p>\n<p>&nbsp;There is then the glaring question, \u201cWhy is assuming the existence, or even the statistically significant <em>possibility</em>, that there is such a thing as the non-intellectual, rationally justifiable or desirable?\u201d, but that would bring me to\u2014</p>\n<p>&nbsp;</p>\n<p><strong id=\"Section_Five__The_author_claims_that_the_Torah_s_views_on_legislation__government__history__sexual_morals__science__and_ethics_are_outdated__in_light_of_what_Yudkowsky_describes_as_human_advancement___\">Section Five: The author claims that the Torah\u2019s views on legislation, government, history, sexual morals, science, and ethics are outdated, in light of what Yudkowsky describes as human advancement. <span style=\"mso-spacerun: yes;\">&nbsp;</span></strong></p>\n<p><span style=\"mso-bidi-font-weight: bold;\">It is far beyond the scope of this essay to discuss the viability of the Torah\u2019s views on every popularly debated ethical and academic subject. However, two things:</span></p>\n<p><span style=\"mso-bidi-font-weight: bold;\">First, it is only inevitable that ontological reductionism leads to ethical and existential nihilism. There is no materialistic reductionist approach to human nature, whether it is </span><span style=\"mso-bidi-font-weight: bold;\"><span style=\"color: #0000ff;\"><a href=\"http://en.wikipedia.org/wiki/Transhumanism\">transhumanism</a></span></span><span style=\"mso-bidi-font-weight: bold;\">, utilitarian hedonism, or Marxist sociology, which is capable of avoiding this problem. Nihilism is inevitable in <em style=\"mso-bidi-font-style: normal;\">any </em>worldview that promotes that \u201cG-d is dead,\u201d and it is absurd to claim that anyone endorsing <em style=\"mso-bidi-font-style: normal;\">any </em>religious or non-reductionist approach to existence should be considered worthy of public ridicule, but one who promotes the \u201ctranscendence\u201d of our humanity through self-selected, voluntary eugenics by reforming our minds in the image of computer technology, should be respected as \u201crationalists\u201d (no offense intended, SIAI fans). To quote an American playwright who put it this way,</span></p>\n<p style=\"margin-left: 0.5in;\"><span style=\"mso-bidi-font-weight: bold;\">\u201c</span><span class=\"normal00200028web0029char1\">With the exception of a few powerful, dissenting vo</span>ices, the nineteenth century was almost unanimous in its belief that the ascent of science was a guarantee of the moral improvement of man. As the sworn rationalists gleefully kept destroying man\u2019s belief in G-d, they kept proclaiming their belief in man. Man, on the one hand, was depicted as an advanced outgrowth of the monkey, but, on the other hand, was proclaimed as a creature who can \u2018rationally\u2019 work out his own salvation. Vladimir Soloviev, the great Russian philosopher, expressed the incompatibility of scientific optimism about man with man\u2019s proclaimed biological inferiority in a marvelously ironic phrase: \u2018Man,\u2019 said Soloviev, \u2018is a descendant of monkeys; he can <span class=\"normal00200028web0029char1char1\"><em>therefore </em></span>be relied upon to bring about a period of happiness and progress to mankind.\u2019</p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in 5pt 0.5in;\"><span style=\"mso-bidi-font-weight: bold;\">\u2018Reductionist science, which for a couple of centuries hammered away at the idea that man is \u201cnothing but\u201d his biological components, did not realize that such a man would be a <em style=\"mso-bidi-font-style: normal;\">reduced</em> man, a \u201cnothing but-nick,\u201d to use an expression of Viktor Frankl. And it is not only specialization that brought about this state\u2014specialization is inevitable in a technological life order\u2014but <em style=\"mso-bidi-font-style: normal;\">totalization</em>: the idea that there is something akin to universality about the <em style=\"mso-bidi-font-style: normal;\">totality </em>of specialization. What is dangerous, Dr. Frankl writes, is the attempt of a man who is an expert, say, in the field of biology, to understand and explain human beings <em style=\"mso-bidi-font-style: normal;\">exclusively </em>in terms of biology. At the moment at which totality is claimed for the part, Dr. Frankl argues, biology becomes biologism, psychology becomes psychologism, and sociology becomes sociologism. In other words, at that moment, science is reduced to ideology. Dr. Frankl tells us in his <em style=\"mso-bidi-font-style: normal;\">Will to a Meaning </em>that he once came across a quotation defining man as \u201c<em style=\"mso-bidi-font-style: normal;\">nothing but</em> a complex biochemical mechanism powered by a combustion system which energizes computers with prodigious storage facilities for retaining encoded information.\u201d \u2026 (I)n a certain sense the statement is valid: Man <em style=\"mso-bidi-font-style: normal;\">is </em>a computer. However, at the same time, he is infinitely more than a computer! The statement is fatally erroneous insofar as it defines man as <em style=\"mso-bidi-font-style: normal;\">nothing but</em> a computer.</span> (<span style=\"font-size: 8pt;\">Zvi</span> <span class=\"normal00200028web0029char1char1\"><span style=\"font-size: 8pt;\">Kolitz, 1982</span></span>)\u201d<strong> </strong><span style=\"mso-bidi-font-weight: bold;\"><span style=\"mso-spacerun: yes;\">&nbsp;</span></span></p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\">&nbsp;</p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\">Secondly, the thing that people often popularly refer to as \u201cOld Testament\u201d ethics is usually nothing more than a common misunderstanding of what the historical Jewish approach to ethics has always really been like, and such misunderstandings are often misused as trump cards in public religion vs. science debates. <span style=\"mso-spacerun: yes;\">&nbsp;</span></p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\">&nbsp;</p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\">Since Yudkowsky mentions it, let\u2019s use slavery as an example. There is a popular notion that the Torah, and by association the New Testament, endorses slavery as being morally okay. This is often held up in contrast to the contemporary, enlightened Western world, where it is assumed that everybody knows that slavery is of course unethical. This is a very popular trump card in public debate surrounding religion, and it is common to hear statements such as, \u201cWhy trust the Bible is right when it says \u2018no-gay-marriage,\u2019 when the Bible says <em style=\"mso-bidi-font-style: normal;\">slavery</em> is just fine?\u201d Most people, having little factual knowledge of the Tanakh (including most religious Americans), assume that the information being presented in the trump card is accurate. Which is understandable\u2014 it sounds logical to assume <em style=\"mso-bidi-font-style: normal;\">a priori </em>that Iron Age Near Eastern tribes thought slavery was ethically permissible.</p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\">For starters, there is a little-known principle in Jewish thought, that many of the Torah\u2019s laws are designed to bring about the <em style=\"mso-bidi-font-style: normal;\">gradual </em>elimination of certain societal evils, rather than their <em style=\"mso-bidi-font-style: normal;\">immediate</em> elimination, for the purpose of pragmatism and realistic goals for societal change. Put more precisely, <span style=\"mso-spacerun: yes;\">&nbsp;</span><em style=\"mso-bidi-font-style: normal;\"><span style=\"mso-spacerun: yes;\">&nbsp;</span></em></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">\u201cOn considering the Divine acts, or the processes of Nature, we get an insight into the prudence and wisdom of G-d as displayed in the creation of animals, with the gradual development of the movements of their limbs and the relative positions of the latter, and we perceive also His wisdom and plan in the successive and gradual development of the whole condition of each individual. The gradual development of the animals\u2019 movements and the relative position of the limbs may be illustrated by the brain, etc\u2026 When such an animal is born it is extremely tender, and cannot be fed with dry food. Therefore breasts were provided which yield milk, and the young can be fed with moist food which corresponds to the condition of the limbs of the animal, until the latter have gradually become dry and hard.</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">Many precepts in our Law are the result of a similar course adopted by the same Supreme Being. It is, namely, impossible to go suddenly from one extreme to the other: it is therefore according to the nature of man impossible for him suddenly to discontinue everything to which he has been accustomed. </span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">... I do not say this because I believe that it is difficult for G-d to change the nature of every individual person; on the contrary, it is possible, and it is in His power, according to the principles taught in the Law; but it has never been His will to do it, and it never will be. If it were part of His will to change at His desire the nature of any person, the mission of the prophets and the giving of the Law would have been altogether superfluous. (</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 8pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">Guide for the Perplexed III: XXXII</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">)\u201d</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">This principle of Jewish thought is often applied to issues such as economic inequality, monarchic rule, capital punishment, the status of women, and animal sacrifice, as well as other issues. The Torah\u2019s approach to slavery, for most of Jewish history, has always been understood in accordance with this principle, the idea generally being that servitude was often a tragic economic necessity and hence a necessary evil. Therefore, the Torah\u2019s restrictions on slavery were understood as having the long-term goal of eliminating slavery altogether, but gradually: hence the Torah\u2019s ban on possessing an individual slave for more than seven years, thereby preventing generational slavery; the ban on physically harming a slave; the ban on sexual relations with one\u2019s slaves; the ban on involuntary slavery, and the requirement that they be indentured; the requirement that your slaves must live with you in your home, and eat the same food you eat, etc. The statement in the Talmud (</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 8pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">Kiddushin 20a</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">): \u201cWhoever acquires a Hebrew slave, acquires a master!\u201d is logically derived from these rules in the Torah, as is the statement in the Mishnah, \u201cMake the poor into servants in your household, (</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 8pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">Pirkei Avos</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">)\u201d which warns against enslaving the poor without monetary compensation.</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\">However, the clearest admonitions against slavery usually come from the Tanakh itself. For instance, </span><span style=\"font-family: Calibri;\"><span style=\"mso-ascii-font-family: Calibri; mso-fareast-font-family: 'Times New Roman'; mso-hansi-font-family: Calibri; mso-bidi-font-family: Calibri;\">&nbsp;</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman'; mso-ansi-language: EN;\" lang=\"EN\"></span></span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">\u201cIf you buy a Hebrew servant, he is to serve you for (no more than) six years. But in the seventh year, he shall go free, without paying anything... But if the servant declares, \u201cI love my master and my (assigned) wife and children, and do not want to go free,\u201d then his master must take him before the judges. He shall take him to the door or the doorpost, and pierce his ear with an awl. Then he will be his servant for life (until the Jubilee year).</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-fareast-font-family: 'Times New Roman';\"> (</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 8pt; mso-fareast-font-family: 'Times New Roman';\">Exodus 21: 2-6</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-fareast-font-family: 'Times New Roman';\">)</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">\u201d</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-fareast-font-family: 'Times New Roman';\"> </span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-fareast-font-family: 'Times New Roman';\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">The classical commentator Rashi (11th c. CE) explains why a Jew who sells himself or herself into slavery is given such a severe corporeal punishment, even though the action of selling oneself into indentured servitude is itself permitted by the Torah. Rashi cites the Mishnah, writing,</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">\u201cNow, why was the ear chosen to be bored out of all the organs of the body? ... Referring to one who sold himself into servitude, the reason is that the ear that heard, \u2018For the children of Israel are servants to Me\u2019 (Leviticus 25:55) and then went and acquired a master for himself, this ear shall be bored. Rabbi Shimon interpreted this verse in a beautiful manner: Why were the door and the doorpost singled out from all the fixtures in the house? The Holy One, blessed is He, said: The door and the doorpost were witnesses in Egypt when I passed over the lintel and the two doorposts, and I said, \u2018For the children of Israel are servants to Me; they are My servants,\u2019 but they are not servants to servants, and yet, this one went and acquired for himself a master! Therefore his ear shall be bored, for everyone to see. (</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 8pt; mso-fareast-font-family: 'Times New Roman';\">from Talmud, Kiddushin 22b</span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">)\u201d</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">The classical Talmudic interpretations of the Torah\u2019s laws concerning slavery strongly indicate an understanding that servitude, even if voluntarily chosen for oneself, is morally debasing. Furthermore, the <em>reason</em> for abhorring slavery is equally significant: freely chosen socio-political liberty is a necessary prerequisite to be able to serve G-d, for such service requires both physical liberty as well as the mentality of a free person, since slavery is regarded as spiritually and intellectually debilitating (Ex. \"So says the G-d of the Hebrews: Let My people go, <em>that they may serve Me!</em>\"). </span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">Such admonitions ultimately had their inevitable effect. Slavery was uncommon among Jews even by the time of the Roman Empire, was entirely avoided by the <a href=\"http://en.wikipedia.org/wiki/Essenes\">Essenes</a></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">, and was branded as equivalent to idolatry by the <a href=\"http://en.wikipedia.org/wiki/Zealots\">Zealots</a></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">, leading Elazar ben Yair to famously state at Masada:</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">\u201cLong ago we resolved to serve neither the Romans nor anyone other than G-d... The time has now come that bids us prove our determination by our deeds. At such a time we must not disgrace ourselves. Hitherto we have never submitted to slavery... We must not choose slavery now... For we were the first to revolt, and shall be the last to break off the struggle. And I think it is G-d who has given us this privilege, that we can die nobly and as free men... In our case it is evident that daybreak will end our resistance, but we are free to choose an honorable death with our loved ones. This our enemies cannot prevent, however earnestly they may pray to take us alive; nor can we defeat them in battle.</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in; mso-margin-top-alt: auto; mso-margin-bottom-alt: auto;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">Let our wives die unabused, our children without knowledge of slavery. After that let us do each other an ungrudging kindness, preserving our freedom as a glorious winding-sheet. But first, let our possessions and the whole fortress go up in flames. It will be a bitter blow to the Roman, that I know, to find our persons beyond their reach and nothing left for them to loot. One thing only let us spare\u2014 our store of food: it will bear witness when we are dead to the fact that we perished, not through want but because...we chose death rather than slavery....\u201d</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">Given all of this, the popular contention that the Torah endorses slave-ownership is difficult to defend. This is especially so, considering that the modern Western world generally <em style=\"mso-bidi-font-style: normal;\">does</em> consider the revocation of personal liberties through imprisonment to be a morally permissible method of punishing criminals, and the practice of imprisoning criminals is recognized as a form of slavery by the 13<sup>th</sup> Amendment: \u201cNeither slavery nor involuntary servitude, except as a punishment for crime whereof the party shall have been duly convicted, shall exist within the United States, or any place subject to their jurisdiction.\u201d In fact, the practice of coerced, involuntary servitude as a punishment for criminals probably receives more moral sanction from the U.S. Constitution than it does from the Torah, which prescribes financial penalties, sacrificial atonement offerings, corporeal punishment and capital punishment as penalties for crime, without any mention of the practice of long-term incarceration. </span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span class=\"list0020paragraphchar1char1\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">To bring everything back around: Yudkowsky\u2019s severe critiques of religion, notably Judaism, <span style=\"mso-spacerun: yes;\">&nbsp;</span><span style=\"mso-spacerun: yes;\">&nbsp;</span>are generally false\u2014 even concerning demonstrable facts much of the time, and are inconsistent with Less Wrong\u2019s efforts to promote the rational overcoming of intellectual self-deception and bias. Rationality does not necessitate the rejection of all religious philosophy, nor the intellectual denial of G-d. To end&nbsp;with&nbsp;one more&nbsp;quote,</span></span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt 0.5in;\"><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt; mso-fareast-font-family: 'Times New Roman';\">\u201c</span><span class=\"normal00200028web0029char1\"><span style=\"font-size: 12pt;\">There was a growing conviction (in the 19<sup>th</sup> century) that science could be relied on to provide a secure rational foundation for all of our ethical and moral standards. The philosophical roots of this conviction can be traced to Greek philosophy, \u2026 Greek philosophy thus relied </span></span><span style=\"font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; font-size: 12pt;\">on human reason to derive moral and ethical principles from the nature of things, rather than from G-d, as the Hebrews believe. An ethics thus divorced from G-d is autonomous.</span><span style=\"mso-ascii-font-family: Calibri; mso-fareast-font-family: 'Times New Roman'; mso-hansi-font-family: Calibri; mso-bidi-font-family: Calibri;\"></span></p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in 5pt 0.5in;\"><span class=\"normal00200028web0029char1\">\u2018Morally speaking, my friends, the shtetl Jew of Eastern Europe was without pee</span>r in the history of communal morality. Poverty-stricken, oppressed, hated, mocked, woefully lacking in aesthetics, the shtetl Jew reached heights of ethical and moral purity that made crime in his midst unthinkable and social indifference impossible. Nobody starved in the poor shtetl, and nobody was denied the opportunity to acquire knowledge, a much more sought-after and much more respected commodity than money\u2026</p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in 5pt 0.5in;\"><span class=\"normal00200028web0029char1\">Now, why did this come to pass? Why did poverty and oppression, which usually breed crime, bre</span>ed, in the mud of the shtetl, purity of heart as a mass phenomenon?</p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in 5pt 0.5in;\">\u2018Believing, as the shtetl Jews did, that ethical behavior is G-d centered, they took G-d as their measure. \u2026 Our entire history bears witness before G-d and man that ethics become a way of life\u2014not a fossilized thought, but a way of life\u2014only when they are G-d derived. \u2026That is precisely what Dostoyevsky had in mind when he said, \u2018If there is no G-d, murder is permissible.\u2019 Abraham, as we must always remember, said it more than four millennia earlier: \u2018There is no awe of G-d in this land, and whoever finds me may slay me.\u201d</p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\">&nbsp;</p>\n\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\">&nbsp;</p>\n<p></p>\n<p class=\"normal00200028web0029\" style=\"margin: 5pt 0in;\">&nbsp;</p>", "sections": [{"title": "Section Three: The author claims that historical Judaism defends the authenticity of the Torah, without accounting for Bayes\u2019 rule. ", "anchor": "Section_Three__The_author_claims_that_historical_Judaism_defends_the_authenticity_of_the_Torah__without_accounting_for_Bayes__rule__", "level": 1}, {"title": "Section Four: The author claims that contemporary religionists justify false opinions by claiming that their religion is a separate magisterium which can be neither proven nor disproven.\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0", "anchor": "Section_Four__The_author_claims_that_contemporary_religionists_justify_false_opinions_by_claiming_that_their_religion_is_a_separate_magisterium_which_can_be_neither_proven_nor_disproven_________", "level": 1}, {"title": "Section Five: The author claims that the Torah\u2019s views on legislation, government, history, sexual morals, science, and ethics are outdated, in light of what Yudkowsky describes as human advancement. \u00a0", "anchor": "Section_Five__The_author_claims_that_the_Torah_s_views_on_legislation__government__history__sexual_morals__science__and_ethics_are_outdated__in_light_of_what_Yudkowsky_describes_as_human_advancement___", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "82 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 82, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fAuWLS7RKWD2npBFR", "dHQkDNMhj692ayx78", "DoLQN5ryZ9XkZjq5h", "CqyJzDZWvGhhFJ7dY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-20T02:36:14.448Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Angry Atoms", "slug": "seq-rerun-angry-atoms", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:27.575Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bHt6RZJykesbdhKHf/seq-rerun-angry-atoms", "pageUrlRelative": "/posts/bHt6RZJykesbdhKHf/seq-rerun-angry-atoms", "linkUrl": "https://www.lesswrong.com/posts/bHt6RZJykesbdhKHf/seq-rerun-angry-atoms", "postedAtFormatted": "Tuesday, March 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Angry%20Atoms&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Angry%20Atoms%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbHt6RZJykesbdhKHf%2Fseq-rerun-angry-atoms%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Angry%20Atoms%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbHt6RZJykesbdhKHf%2Fseq-rerun-angry-atoms", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbHt6RZJykesbdhKHf%2Fseq-rerun-angry-atoms", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 201, "htmlBody": "<p>Today's post, <a href=\"/lw/p3/angry_atoms/\">Angry Atoms</a> was originally published on 31 March 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Angry_Atoms\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It is very hard, without the benefit of hindsight, to understand just how it is that these little bouncing billiard balls called atoms, could ever combine in such a way as to make something angry. If you try to imagine this problem without understanding the idea of neurons, information processing, computing, etc you realize just how challenging reductionism actually is.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/ayf/seq_rerun_hand_vs_fingers/\">Hand vs. Fingers</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bHt6RZJykesbdhKHf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 8.687401490161891e-07, "legacy": true, "legacyId": "14246", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ddwk9veF8efn3Nzbu", "cSweDYjvsJWssoAFC", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-20T05:29:31.982Z", "modifiedAt": null, "url": null, "title": "Posts I repent of", "slug": "posts-i-repent-of", "viewCount": null, "lastCommentedAt": "2021-05-06T16:36:24.479Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QePFiEKZ4R2KnxMkW/posts-i-repent-of", "pageUrlRelative": "/posts/QePFiEKZ4R2KnxMkW/posts-i-repent-of", "linkUrl": "https://www.lesswrong.com/posts/QePFiEKZ4R2KnxMkW/posts-i-repent-of", "postedAtFormatted": "Tuesday, March 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Posts%20I%20repent%20of&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APosts%20I%20repent%20of%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQePFiEKZ4R2KnxMkW%2Fposts-i-repent-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Posts%20I%20repent%20of%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQePFiEKZ4R2KnxMkW%2Fposts-i-repent-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQePFiEKZ4R2KnxMkW%2Fposts-i-repent-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 129, "htmlBody": "<p>&nbsp;</p>\n<ul>\n<li><a style=\"font-weight: bold; \" href=\"/lw/2l6/taking_ideas_seriously/\">\"Taking Ideas Seriously\"</a>: Stylistically contemptible, skimpy on any useful details, contributes to norm of pressuring people into double binds that ultimately do more harm than good. I would prefer it if no one linked to or promoted \"Taking Ideas Seriously\"; superior alternatives include Anna Salamon's <a href=\"/lw/2q6/compartmentalization_in_epistemic_and/\">\"Compartmentalization in epistemic and instrumental rationality\"</a>, though I don't necessarily endorse that post either.</li>\n<li><strong><a href=\"/lw/2aa/virtue_ethics_for_consequentialists/\">\"Virtue Ethics for Consequentialists\"</a></strong>: Stylistically contemptible, written in ignorance of much of the relevant philosophy and psychology literature, contributes to norm of rewarding people who confidently proselytize on subjects of which they do not possess a deep understanding. Thankfully nobody links to this post.</li>\n</ul>\n<div>All of my other posts also sucked but not to the extent that they're worth going out of my way to disavow.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3RnEKrsNgNEDxuNnw": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QePFiEKZ4R2KnxMkW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 25, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "14252", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Q8jyAdRYbieK8PtfT", "N99KgncSXewWqkzMA", "ZLBtZqsP79Cwioi2b"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-20T07:37:30.894Z", "modifiedAt": null, "url": null, "title": "Send me your photos of LessWrongers having fun!", "slug": "send-me-your-photos-of-lesswrongers-having-fun", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:28.340Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XLwzRj8zXtihskGc9/send-me-your-photos-of-lesswrongers-having-fun", "pageUrlRelative": "/posts/XLwzRj8zXtihskGc9/send-me-your-photos-of-lesswrongers-having-fun", "linkUrl": "https://www.lesswrong.com/posts/XLwzRj8zXtihskGc9/send-me-your-photos-of-lesswrongers-having-fun", "postedAtFormatted": "Tuesday, March 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Send%20me%20your%20photos%20of%20LessWrongers%20having%20fun!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASend%20me%20your%20photos%20of%20LessWrongers%20having%20fun!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXLwzRj8zXtihskGc9%2Fsend-me-your-photos-of-lesswrongers-having-fun%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Send%20me%20your%20photos%20of%20LessWrongers%20having%20fun!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXLwzRj8zXtihskGc9%2Fsend-me-your-photos-of-lesswrongers-having-fun", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXLwzRj8zXtihskGc9%2Fsend-me-your-photos-of-lesswrongers-having-fun", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<p>The Singularity Institute is preparing a booklet for LessWrong meetup organizers called&nbsp;<em>How to Run a Successful&nbsp;Less Wrong Meetup Group</em>.</p>\n<p>Right now what we need are your <em>photos of LessWrongers doing things and having fun!</em>&nbsp;If you have photos you don't mind us potentially using in <em>How to Run a Successful Less Wrong Meetup Group</em>, please email them to luke@intelligence.org.</p>\n<p>Thanks!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XLwzRj8zXtihskGc9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 8.688635462204587e-07, "legacy": true, "legacyId": "14261", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-20T13:05:38.600Z", "modifiedAt": null, "url": null, "title": "Meetup : First meetup in Budapest", "slug": "meetup-first-meetup-in-budapest", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:30.572Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jonathan_Lee", "createdAt": "2009-09-10T00:05:08.577Z", "isAdmin": false, "displayName": "Jonathan_Lee"}, "userId": "8qL3Hsw2TzaLPu3Bh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FSuvcH2tjMZfYqJcE/meetup-first-meetup-in-budapest", "pageUrlRelative": "/posts/FSuvcH2tjMZfYqJcE/meetup-first-meetup-in-budapest", "linkUrl": "https://www.lesswrong.com/posts/FSuvcH2tjMZfYqJcE/meetup-first-meetup-in-budapest", "postedAtFormatted": "Tuesday, March 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20meetup%20in%20Budapest&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20meetup%20in%20Budapest%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFSuvcH2tjMZfYqJcE%2Fmeetup-first-meetup-in-budapest%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20meetup%20in%20Budapest%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFSuvcH2tjMZfYqJcE%2Fmeetup-first-meetup-in-budapest", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFSuvcH2tjMZfYqJcE%2Fmeetup-first-meetup-in-budapest", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 53, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/7z'>First meetup in Budapest</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 March 2012 06:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Szent Istvan ter 4-5, Budapest</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meeting at California Coffee Company Basilica (coffee shop), Szent Istvan ter 4-5.\n<a href=\"http://www.californiacoffeeco.net/?page_id=50&lang=en.\" rel=\"nofollow\">http://www.californiacoffeeco.net/?page_id=50&lang=en.</a></p>\n\n<p>Please come and bring friends. If you have questions, contact AlexeyM.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/7z'>First meetup in Budapest</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FSuvcH2tjMZfYqJcE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 8.689979779122442e-07, "legacy": true, "legacyId": "14267", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_meetup_in_Budapest\">Discussion article for the meetup : <a href=\"/meetups/7z\">First meetup in Budapest</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 March 2012 06:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Szent Istvan ter 4-5, Budapest</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meeting at California Coffee Company Basilica (coffee shop), Szent Istvan ter 4-5.\n<a href=\"http://www.californiacoffeeco.net/?page_id=50&amp;lang=en.\" rel=\"nofollow\">http://www.californiacoffeeco.net/?page_id=50&amp;lang=en.</a></p>\n\n<p>Please come and bring friends. If you have questions, contact AlexeyM.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___First_meetup_in_Budapest1\">Discussion article for the meetup : <a href=\"/meetups/7z\">First meetup in Budapest</a></h2>", "sections": [{"title": "Discussion article for the meetup : First meetup in Budapest", "anchor": "Discussion_article_for_the_meetup___First_meetup_in_Budapest", "level": 1}, {"title": "Discussion article for the meetup : First meetup in Budapest", "anchor": "Discussion_article_for_the_meetup___First_meetup_in_Budapest1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-20T19:04:15.290Z", "modifiedAt": null, "url": null, "title": "Better to be testably wrong than to generate nontestable wrongness", "slug": "better-to-be-testably-wrong-than-to-generate-nontestable", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:28.373Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dmytry", "createdAt": "2009-12-03T17:11:53.492Z", "isAdmin": false, "displayName": "Dmytry"}, "userId": "AjtmA2qtA8sdiMbru", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7Q99NJW9oWM3iBhGd/better-to-be-testably-wrong-than-to-generate-nontestable", "pageUrlRelative": "/posts/7Q99NJW9oWM3iBhGd/better-to-be-testably-wrong-than-to-generate-nontestable", "linkUrl": "https://www.lesswrong.com/posts/7Q99NJW9oWM3iBhGd/better-to-be-testably-wrong-than-to-generate-nontestable", "postedAtFormatted": "Tuesday, March 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Better%20to%20be%20testably%20wrong%20than%20to%20generate%20nontestable%20wrongness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABetter%20to%20be%20testably%20wrong%20than%20to%20generate%20nontestable%20wrongness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Q99NJW9oWM3iBhGd%2Fbetter-to-be-testably-wrong-than-to-generate-nontestable%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Better%20to%20be%20testably%20wrong%20than%20to%20generate%20nontestable%20wrongness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Q99NJW9oWM3iBhGd%2Fbetter-to-be-testably-wrong-than-to-generate-nontestable", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Q99NJW9oWM3iBhGd%2Fbetter-to-be-testably-wrong-than-to-generate-nontestable", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 735, "htmlBody": "<p><strong>Abstract: </strong>Test the world-models [at least somewhat] scientifically by giving others and yourself opportunity to generate straightforwardly and immediately testable factual predictions from the world-model. Read up facts to make sure you are not wrong before posting, not only to persuade.</p>\n<p>I have this theory: there are people with political opinion of some kind, who generate their world-beliefs from that opinion. This is a wrong world-model. It doesn't work for fact finding. It works for tribal affiliations. I think it is fair to say we all been guilty of this on at least several occasions, and that all of us do it for at least some problem domains. Now, suppose you have some logical argument that contradicts other people's world-model, starting from very basic facts. And you are writing an article.</p>\n<p>If you source those basic facts, there's what happens: the facts are read and accepted, the reasoning is read, the conclusion is reached, the contradiction with political opinion gets noted, the political opinion does NOT get adjusted, the politically motivated world-model generates a fault inside your argument, you get entirely counter productive and extremely irritating debate about semantics or argumentation techniques. In the end, not a yota changes about the world model of anyone involved in the debate.</p>\n<p>If you don't source those basic facts, there's what happens: the facts are read and provisionally accepted, the reasoning is read, the conclusion is reached, the contradiction with political opinion gets noted, the political opinion does not get adjusted, the politically motivated world model generates wrong fact expectations about basic, easily testable facts. The contradiction eventually gets noted, the wrong world-model gets a minor slap on the nose, and actually does decrease in it's weight ever so slightly for generating wrong expectations. The person is, out of necessity, doing some actual science here - generating <em>testable</em> hypotheses from their theory, about the facts they don't know, having them tested (and shown wrong, providing feedback in somewhat scientific manner).</p>\n<p>Unfortunately, any alterations to world model are uncomfortable - the world models, as memes, have a form of self preservation - so nobody likes this, and the faulty world-models produce considerable pressure to demand of you to source the basic knowledge upfront, so that the world-model can know where it can safely generate <em>non-testable</em> faults.</p>\n<p>Other <em>giant</em> positive effect (for the society) happens when you are wrong, and you are the one who has been generating facts from world-model. Someone looks up facts, and then blam, your wrong world-model gets a slap on the nose.</p>\n<p>Unfortunately that mechanism, too, makes you even more eager to provide and cut-n-paste citations for your basic facts, rather than state the facts as <em>you</em> interpret them (which is far more revealing of your argument structure, forwards facts to conclusion vs backwards conclusion to facts).</p>\n<p>One big drawback is that it is annoying for those who do not actually have screwed up world-models, and just want to know the truth. These folks have to look up if assertions are correct. But it is not such a big drawback, as them looking up the sources themselves eliminates effects of <em>your</em> cherrypicking.</p>\n<p>Another drawback is that it results in generation of content that can look like it has lower quality. In terms of marketing value, it is a worse product - it might slap your world model on the nose. It just doesn't sell well. But we aren't writing for sale, are we?</p>\n<p>Other thing to keep in mind is that the citations let separate hypotheses from facts, and that is very useful. It would be great to do so in alternative way for basic knowledge. By marking the hypotheses with \"i think\" and facts with strong assertions like \"it is a fact that\". Unfortunately that can make you look very foolish - <em>that fool is sticking his neck out into guillotine of testable statements!</em>. Few have the guts to do that, and many of the few that do, may well not be the most intelligent.</p>\n<p>And of course it only works tolerably well when we are certain enough that incorrect factual assertions will quickly be challenged. Fortunately, that is usually the case on the internet. Otherwise, people can slip in the incorrect assertions.</p>\n<p>Ahh, and also: try not to use the above to rationalize not looking up the sources because it's a chore.</p>\n<p>edit: changed to much better title. edit: realized that italic is a poor choice for the summary, which needs to be most readable.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7Q99NJW9oWM3iBhGd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 14, "baseScore": -7, "extendedScore": null, "score": -1.2e-05, "legacy": true, "legacyId": "14257", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-20T19:41:28.937Z", "modifiedAt": null, "url": null, "title": "A model of UDT without proof limits", "slug": "a-model-of-udt-without-proof-limits", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:28.630Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m39dkp73YhN9QKYb9/a-model-of-udt-without-proof-limits", "pageUrlRelative": "/posts/m39dkp73YhN9QKYb9/a-model-of-udt-without-proof-limits", "linkUrl": "https://www.lesswrong.com/posts/m39dkp73YhN9QKYb9/a-model-of-udt-without-proof-limits", "postedAtFormatted": "Tuesday, March 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20model%20of%20UDT%20without%20proof%20limits&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20model%20of%20UDT%20without%20proof%20limits%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm39dkp73YhN9QKYb9%2Fa-model-of-udt-without-proof-limits%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20model%20of%20UDT%20without%20proof%20limits%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm39dkp73YhN9QKYb9%2Fa-model-of-udt-without-proof-limits", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm39dkp73YhN9QKYb9%2Fa-model-of-udt-without-proof-limits", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 529, "htmlBody": "<p>This post requires some knowledge of decision theory math. Part of the credit goes to Vladimir Nesov.</p>\n<p>Let the universe be a computer program U that returns a utility value, and the agent is a subprogram A within U that knows the source code of both A and U. (The same setting was used in the <a href=\"/lw/2l2/what_a_reduction_of_could_could_look_like/\">reduction of \"could\"</a> post.) Here's a very simple decision problem:</p>\n<p><code>def U():<br />&nbsp; if A() == 1:<br />&nbsp;&nbsp;&nbsp;&nbsp;return 5<br />&nbsp;&nbsp;else:<br />&nbsp;&nbsp;&nbsp;&nbsp;return 10</code></p>\n<p>The algorithm for A will be as follows:</p>\n<ol>\n<li>Search for proofs of statements of the form \"A()=a implies U()=u\". Upon finding at least one proof for each possible a, go to step 2.</li>\n<li>Let L be the maximum length of proofs found on step 1, and let f(L) be some suitably fast-growing function like 10^L.&nbsp;Search for proofs shorter than f(L) of the form \"A()&ne;a\". If such a proof is found, return a.</li>\n<li>If we're still here, return the best a found on step 1.</li>\n</ol>\n<p>The usual problem with such proof-searching agents is that they might stumble upon \"spurious\" proofs, e.g. a proof that A()==2 implies U()==0. If A finds such a proof and returns 1 as a result, the statement A()==2 becomes false, and thus provably false under any formal system; and a false statement implies anything, making the original \"spurious\" proof correct. The reason for constructing A this particular way is to have a shot at proving that A won't stumble on a \"spurious\" proof before finding the \"intended\" ones. The proof goes as follows:</p>\n<p>Assume that A finds a \"spurious\" proof on step 1, e.g. that A()=2 implies U()=0. We have a lower bound on L, the length of that proof: it's likely larger than the length of U's source code, because a proof needs to at least state what's being proved. Then in this simple case 10^L steps is clearly enough to also find the \"intended\" proof that A()=2 implies U()=10, which combined with the previous proof leads to a similarly short proof that A()&ne;2, so the agent returns 2. But that can't happen if A's proof system is sound, therefore A will find only \"intended\" proofs rather than \"spurious\" ones in the first place.</p>\n<p>Quote from Nesov that explains what's going on:</p>\n<blockquote>\n<p>With this algorithm, you're not just passively gauging the proof length, instead you take the first moral argument you come across, and then actively defend it against any close competition</p>\n</blockquote>\n<p>By analogy we can see that A coded with f(L)=10^L will correctly solve all our simple problems like Newcomb's Problem, the symmetric Prisoner's Dilemma, etc. The proof of correctness will rely on the syntactic form of each problem, so the proof may break when you replace U with a logically equivalent program. But that's okay, because \"logically equivalent\" for programs simply means \"returns the same value\", and we don't want all world programs that return the same value to be <em>decision-theoretically</em> equivalent.</p>\n<p>A will fail on problems where \"spurious\" proofs are exponentially shorter than \"intended\" proofs (or even shorter, if f(L) is chosen to grow faster). We can probably construct malicious examples of decision-determined problems that would make A fail, but I haven't found any yet.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m39dkp73YhN9QKYb9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 21, "extendedScore": null, "score": 8.691601989349304e-07, "legacy": true, "legacyId": "14270", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dC3rxrMkYKLfgTYEa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-21T00:15:26.176Z", "modifiedAt": null, "url": null, "title": "The limited predictor problem", "slug": "the-limited-predictor-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:27.972Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ecCLANfPSDQMzyfDf/the-limited-predictor-problem", "pageUrlRelative": "/posts/ecCLANfPSDQMzyfDf/the-limited-predictor-problem", "linkUrl": "https://www.lesswrong.com/posts/ecCLANfPSDQMzyfDf/the-limited-predictor-problem", "postedAtFormatted": "Wednesday, March 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20limited%20predictor%20problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20limited%20predictor%20problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FecCLANfPSDQMzyfDf%2Fthe-limited-predictor-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20limited%20predictor%20problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FecCLANfPSDQMzyfDf%2Fthe-limited-predictor-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FecCLANfPSDQMzyfDf%2Fthe-limited-predictor-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 838, "htmlBody": "<p>This post requires some knowledge of logic, computability theory, and K-complexity. Much of the credit goes to Wei Dai. The four sections of the post can be read almost independently.</p>\n<p>The limited predictor problem (LPP) is a version of <a href=\"http://wiki.lesswrong.com/wiki/Newcomb's_problem\">Newcomb's Problem</a> where the predictor has limited computing resources. To predict the agent's action, the predictor simulates the agent for N steps. If the agent doesn't finish in N steps, the predictor assumes that the agent will two-box. LPP is similar to the <a href=\"/lw/5rq/example_decision_theory_problem_agent_simulates/\">ASP problem</a>, but with simulation instead of theorem proving.</p>\n<h4>1. Solving the problem when the agent has a halting oracle</h4>\n<p>Consider the agent defined in <a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle/\">\"A model of UDT with a halting oracle\"</a>,&nbsp;and a predictor that can run the agent's code step-by-step, with oracle calls and all. Turns out that this agent solves LPP correctly if N is high enough. To understand why, note that the agent offloads all interesting work to oracles that return instantly, so the agent's own runtime is provably bounded. If that bound is below N, the agent's oracle will prove that the predictor predicts the agent correctly, so the agent will one-box.</p>\n<h4>2. Failing to solve the&nbsp;problem when N is algorithmically random</h4>\n<p>Consider a setting without oracles, with only Turing-computable programs. Maybe the agent should successively search for proofs somehow?</p>\n<p>Unfortunately you can't solve most LPPs this way, for a simple but surprising reason. Assume that the predictor's time limit N is a large and algorithmically random number. Then the predictor's source code is &gt;log(N) bits long, because N must be defined in the source code. Then any proof about the world program must also have length &gt;log(N), because the proof needs to at least quote the world program itself. Finding a proof by exhaustive search takes exponential time, so the agent will need &gt;N steps. But the predictor simulates the agent for only N steps. Whoops!</p>\n<h4>3. Solving the&nbsp;problem when N is large but has a short definition</h4>\n<p>As usual, let U be the world program that returns a utility value, and A be the agent program that returns an action and has access to the world's source code. Consider the following algorithm for A:</p>\n<ol>\n<li>From L=1 to infinity, search for proofs up to length L of the form \"if A()=a and runtime(A)&lt;g(L), then U()=u\", where g(L) is an upper bound on runtime(A) if A stops the search at length L. Upon finding at least one proof for each possible a, go to step 2.</li>\n<li>Search for proofs up to length f(L) of the form \"if runtime(A)&lt;g(L), then A()&ne;a\", where f(L) is some suitably fast-growing function like 10^L. If such a proof is found, return a.</li>\n<li>If we're still here, return the best a found on step 1.</li>\n</ol>\n<p>This algorithm is very similar to the one described in <a href=\"/r/discussion/lw/b0e/a_model_of_udt_without_proof_limits/\">\"A model of UDT without proof limits\"</a>, but with the added complication that A is aware of its own runtime via the function g(L). By an analogous argument, A will find the \"intended\" proof that the predictor predicts A correctly if runtime(A) is small enough, as long as the \"intended\" proof exists and isn't too long relative to the predictor's time limit N. More concretely, A will solve all instances of LPP in which N is larger than g(L), where L is the length of the \"intended\" proof. For example, if f(L)=10^L, then g(L) is doubly exponential, so A will successfully solve LPPs where the predictor's source code defines N using triple exponentials or some more compact notation.</p>\n<h4>4. A broader view</h4>\n<p>TDT and UDT were originally designed for solving \"decision-determined\" problems. The agent figures out how the resulting utility logically depends on the agent's action, then returns the action with the highest utility, thus making the premise true.</p>\n<p>But a cleverly coded decision program can also control other facts about itself. For example, the program may figure out how the resulting utility depends on the program's return value&nbsp;<em>and running time</em>, then choose the best return value&nbsp;<em>and choose how long to keep running</em>, thus making both premises true.&nbsp;This idea is a natural extension of <a href=\"http://en.wikipedia.org/wiki/Quine_(computing)\">quining</a> (you carefully write a program that can correctly judge its own runtime so far) and can be generalized to memory consumption and other properties of programs.</p>\n<p>With enough cleverness we could write a program that would sometimes decide to waste time, or run for an even number of clock cycles, etc. We did not need so much cleverness in this post because LPP lies in a smaller class that we may call \"LPP-like problems\", where utility depends only on the agent's return value and runtime, and the dependence on runtime is monotonous - it never hurts to return the same value earlier. That class also includes all the usual decision-determined problems like Newcomb's Problem, and our A also fares well on those.</p>\n<p>I was surprised to find so many new ideas by digging into such a trivial-looking problem as LPP. This makes me suspect that advanced problems like ASP may conceal even more riches, if only we have enough patience to approach them properly...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ecCLANfPSDQMzyfDf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 19, "extendedScore": null, "score": 8.692725011345172e-07, "legacy": true, "legacyId": "14268", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This post requires some knowledge of logic, computability theory, and K-complexity. Much of the credit goes to Wei Dai. The four sections of the post can be read almost independently.</p>\n<p>The limited predictor problem (LPP) is a version of <a href=\"http://wiki.lesswrong.com/wiki/Newcomb's_problem\">Newcomb's Problem</a> where the predictor has limited computing resources. To predict the agent's action, the predictor simulates the agent for N steps. If the agent doesn't finish in N steps, the predictor assumes that the agent will two-box. LPP is similar to the <a href=\"/lw/5rq/example_decision_theory_problem_agent_simulates/\">ASP problem</a>, but with simulation instead of theorem proving.</p>\n<h4 id=\"1__Solving_the_problem_when_the_agent_has_a_halting_oracle\">1. Solving the problem when the agent has a halting oracle</h4>\n<p>Consider the agent defined in <a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle/\">\"A model of UDT with a halting oracle\"</a>,&nbsp;and a predictor that can run the agent's code step-by-step, with oracle calls and all. Turns out that this agent solves LPP correctly if N is high enough. To understand why, note that the agent offloads all interesting work to oracles that return instantly, so the agent's own runtime is provably bounded. If that bound is below N, the agent's oracle will prove that the predictor predicts the agent correctly, so the agent will one-box.</p>\n<h4 id=\"2__Failing_to_solve_the_problem_when_N_is_algorithmically_random\">2. Failing to solve the&nbsp;problem when N is algorithmically random</h4>\n<p>Consider a setting without oracles, with only Turing-computable programs. Maybe the agent should successively search for proofs somehow?</p>\n<p>Unfortunately you can't solve most LPPs this way, for a simple but surprising reason. Assume that the predictor's time limit N is a large and algorithmically random number. Then the predictor's source code is &gt;log(N) bits long, because N must be defined in the source code. Then any proof about the world program must also have length &gt;log(N), because the proof needs to at least quote the world program itself. Finding a proof by exhaustive search takes exponential time, so the agent will need &gt;N steps. But the predictor simulates the agent for only N steps. Whoops!</p>\n<h4 id=\"3__Solving_the_problem_when_N_is_large_but_has_a_short_definition\">3. Solving the&nbsp;problem when N is large but has a short definition</h4>\n<p>As usual, let U be the world program that returns a utility value, and A be the agent program that returns an action and has access to the world's source code. Consider the following algorithm for A:</p>\n<ol>\n<li>From L=1 to infinity, search for proofs up to length L of the form \"if A()=a and runtime(A)&lt;g(L), then U()=u\", where g(L) is an upper bound on runtime(A) if A stops the search at length L. Upon finding at least one proof for each possible a, go to step 2.</li>\n<li>Search for proofs up to length f(L) of the form \"if runtime(A)&lt;g(L), then A()\u2260a\", where f(L) is some suitably fast-growing function like 10^L. If such a proof is found, return a.</li>\n<li>If we're still here, return the best a found on step 1.</li>\n</ol>\n<p>This algorithm is very similar to the one described in <a href=\"/r/discussion/lw/b0e/a_model_of_udt_without_proof_limits/\">\"A model of UDT without proof limits\"</a>, but with the added complication that A is aware of its own runtime via the function g(L). By an analogous argument, A will find the \"intended\" proof that the predictor predicts A correctly if runtime(A) is small enough, as long as the \"intended\" proof exists and isn't too long relative to the predictor's time limit N. More concretely, A will solve all instances of LPP in which N is larger than g(L), where L is the length of the \"intended\" proof. For example, if f(L)=10^L, then g(L) is doubly exponential, so A will successfully solve LPPs where the predictor's source code defines N using triple exponentials or some more compact notation.</p>\n<h4 id=\"4__A_broader_view\">4. A broader view</h4>\n<p>TDT and UDT were originally designed for solving \"decision-determined\" problems. The agent figures out how the resulting utility logically depends on the agent's action, then returns the action with the highest utility, thus making the premise true.</p>\n<p>But a cleverly coded decision program can also control other facts about itself. For example, the program may figure out how the resulting utility depends on the program's return value&nbsp;<em>and running time</em>, then choose the best return value&nbsp;<em>and choose how long to keep running</em>, thus making both premises true.&nbsp;This idea is a natural extension of <a href=\"http://en.wikipedia.org/wiki/Quine_(computing)\">quining</a> (you carefully write a program that can correctly judge its own runtime so far) and can be generalized to memory consumption and other properties of programs.</p>\n<p>With enough cleverness we could write a program that would sometimes decide to waste time, or run for an even number of clock cycles, etc. We did not need so much cleverness in this post because LPP lies in a smaller class that we may call \"LPP-like problems\", where utility depends only on the agent's return value and runtime, and the dependence on runtime is monotonous - it never hurts to return the same value earlier. That class also includes all the usual decision-determined problems like Newcomb's Problem, and our A also fares well on those.</p>\n<p>I was surprised to find so many new ideas by digging into such a trivial-looking problem as LPP. This makes me suspect that advanced problems like ASP may conceal even more riches, if only we have enough patience to approach them properly...</p>", "sections": [{"title": "1. Solving the problem when the agent has a halting oracle", "anchor": "1__Solving_the_problem_when_the_agent_has_a_halting_oracle", "level": 1}, {"title": "2. Failing to solve the\u00a0problem when N is algorithmically random", "anchor": "2__Failing_to_solve_the_problem_when_N_is_algorithmically_random", "level": 1}, {"title": "3. Solving the\u00a0problem when N is large but has a short definition", "anchor": "3__Solving_the_problem_when_N_is_large_but_has_a_short_definition", "level": 1}, {"title": "4. A broader view", "anchor": "4__A_broader_view", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["q9DbfYfFzkotno9hG", "Bj244uWzDBXvE2N2S", "m39dkp73YhN9QKYb9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-21T01:07:24.742Z", "modifiedAt": null, "url": null, "title": "Meetup : Weekly Berkeley meetup", "slug": "meetup-weekly-berkeley-meetup-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:28.844Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W7tx5FcrBnY3LvS9S/meetup-weekly-berkeley-meetup-1", "pageUrlRelative": "/posts/W7tx5FcrBnY3LvS9S/meetup-weekly-berkeley-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/W7tx5FcrBnY3LvS9S/meetup-weekly-berkeley-meetup-1", "postedAtFormatted": "Wednesday, March 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Weekly%20Berkeley%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Weekly%20Berkeley%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW7tx5FcrBnY3LvS9S%2Fmeetup-weekly-berkeley-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Weekly%20Berkeley%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW7tx5FcrBnY3LvS9S%2Fmeetup-weekly-berkeley-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW7tx5FcrBnY3LvS9S%2Fmeetup-weekly-berkeley-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/80'>Weekly Berkeley meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 March 2012 06:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2011 Shattuck Avenue  Berkeley, CA 94704</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week's Berkeley meetup will be 6:30pm on Wednesday at Biryani House. We'll be meeting jointly with the MPHD seminar folks.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/80'>Weekly Berkeley meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W7tx5FcrBnY3LvS9S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.692938106215302e-07, "legacy": true, "legacyId": "14273", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Weekly_Berkeley_meetup\">Discussion article for the meetup : <a href=\"/meetups/80\">Weekly Berkeley meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 March 2012 06:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2011 Shattuck Avenue  Berkeley, CA 94704</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week's Berkeley meetup will be 6:30pm on Wednesday at Biryani House. We'll be meeting jointly with the MPHD seminar folks.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Weekly_Berkeley_meetup1\">Discussion article for the meetup : <a href=\"/meetups/80\">Weekly Berkeley meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Weekly Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Weekly_Berkeley_meetup", "level": 1}, {"title": "Discussion article for the meetup : Weekly Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Weekly_Berkeley_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-21T01:10:21.988Z", "modifiedAt": null, "url": null, "title": "Meetup : Small weekly Berkeley meetup", "slug": "meetup-small-weekly-berkeley-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tdD9dqXD7B7xRgyNk/meetup-small-weekly-berkeley-meetup", "pageUrlRelative": "/posts/tdD9dqXD7B7xRgyNk/meetup-small-weekly-berkeley-meetup", "linkUrl": "https://www.lesswrong.com/posts/tdD9dqXD7B7xRgyNk/meetup-small-weekly-berkeley-meetup", "postedAtFormatted": "Wednesday, March 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Small%20weekly%20Berkeley%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Small%20weekly%20Berkeley%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtdD9dqXD7B7xRgyNk%2Fmeetup-small-weekly-berkeley-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Small%20weekly%20Berkeley%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtdD9dqXD7B7xRgyNk%2Fmeetup-small-weekly-berkeley-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtdD9dqXD7B7xRgyNk%2Fmeetup-small-weekly-berkeley-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/81'>Small weekly Berkeley meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 March 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\"> 2128 Oxford St., Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll meet at the Starbucks on Oxford Street as usual, and then decide where to go from there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/81'>Small weekly Berkeley meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tdD9dqXD7B7xRgyNk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.692950217772058e-07, "legacy": true, "legacyId": "14274", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Small_weekly_Berkeley_meetup\">Discussion article for the meetup : <a href=\"/meetups/81\">Small weekly Berkeley meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 March 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\"> 2128 Oxford St., Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll meet at the Starbucks on Oxford Street as usual, and then decide where to go from there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Small_weekly_Berkeley_meetup1\">Discussion article for the meetup : <a href=\"/meetups/81\">Small weekly Berkeley meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Small weekly Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Small_weekly_Berkeley_meetup", "level": 1}, {"title": "Discussion article for the meetup : Small weekly Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Small_weekly_Berkeley_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-21T02:49:21.315Z", "modifiedAt": null, "url": null, "title": "AI Risk and Opportunity: Humanity's Efforts So Far", "slug": "ai-risk-and-opportunity-humanity-s-efforts-so-far", "viewCount": null, "lastCommentedAt": "2019-12-03T05:43:51.837Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/i4susk4W3ieR5K92u/ai-risk-and-opportunity-humanity-s-efforts-so-far", "pageUrlRelative": "/posts/i4susk4W3ieR5K92u/ai-risk-and-opportunity-humanity-s-efforts-so-far", "linkUrl": "https://www.lesswrong.com/posts/i4susk4W3ieR5K92u/ai-risk-and-opportunity-humanity-s-efforts-so-far", "postedAtFormatted": "Wednesday, March 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20Risk%20and%20Opportunity%3A%20Humanity's%20Efforts%20So%20Far&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20Risk%20and%20Opportunity%3A%20Humanity's%20Efforts%20So%20Far%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi4susk4W3ieR5K92u%2Fai-risk-and-opportunity-humanity-s-efforts-so-far%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20Risk%20and%20Opportunity%3A%20Humanity's%20Efforts%20So%20Far%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi4susk4W3ieR5K92u%2Fai-risk-and-opportunity-humanity-s-efforts-so-far", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi4susk4W3ieR5K92u%2Fai-risk-and-opportunity-humanity-s-efforts-so-far", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 7004, "htmlBody": "<p><small>Part of the series <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">AI Risk and Opportunity: A Strategic Analysis</a>.</small></p>\n<p>(You can leave anonymous feedback on posts in this series <strong><a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dDZ6d0RvM19qVkduX2pjNng4ZklHZXc6MQ\">here</a></strong>. I alone will read the comments, and may use them to improve past and forthcoming posts in this series.)</p>\n<p>This post chronicles the story of humanity's growing awareness of AI risk and opportunity, along with some recent AI safety efforts. I will not tackle any strategy questions directly in this post; my purpose today is merely to \"bring everyone up to speed.\"</p>\n<p>I know my post skips many important events and people. Please suggest additions in the comments, and include as much detail as possible.</p>\n<p>&nbsp;</p>\n<h3>Early history</h3>\n<p>Late in the Industrial Revolution, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Butler-Darwin-Among-the-Machines.pdf\">Samuel Butler (1863)</a> worried about what might happen when machines become more capable than the humans who designed them:</p>\n<blockquote>\n<p>...we are ourselves creating our own successors; we are daily adding to the beauty and delicacy of their physical organisation; we are daily giving them greater power and supplying by all sorts of ingenious contrivances that self-regulating, self-acting power which will be to them what intellect has been to the human race. In the course of ages we shall find ourselves the inferior race.</p>\n<p>...the time will come when the machines will hold the real supremacy over the world and its inhabitants...</p>\n</blockquote>\n<p>This basic idea was picked up by science fiction authors, for example in the 1921 Czech play that introduced the term &ldquo;robot,&rdquo; <em><a href=\"http://en.wikipedia.org/wiki/R.U.R.\">R.U.R.</a></em> In that play, robots grow in power and intelligence and destroy the entire human race, except for a single survivor.</p>\n<p>Another exploration of this idea is found in John W. Campbell&rsquo;s (1932) short story <em><a href=\"http://www.gutenberg.org/files/27462/27462-h/27462-h.htm\">The Last Evolution</a></em>, in which aliens attack Earth and the humans and aliens are killed but their machines survive and inherit the solar system. Campbell's (1935) short story <em><a href=\"http://www.technovelgy.com/ct/content.asp?Bnum=1030\">The Machine</a></em>&nbsp;contained perhaps the earlier description of recursive self-improvement:</p>\n<blockquote>\n<p>&nbsp;</p>\n<p>On the planet Dwranl, of the star you know as Sirius, a great race lived, and they were not too unlike you humans. ...they attained their goal of the machine that could think. And because it could think, they made several and put them to work, largely on scientific problems, and one of the obvious problems was how to make a better machine which could think.</p>\n<p>The machines had logic, and they could think constantly, and because of their construction never forgot anything they thought it well to remember. So the machine which had been set the task of making a better machine advanced slowly, and as it improved itself, it advanced more and more rapidly. The Machine which came to Earth is that machine.</p>\n<p>&nbsp;</p>\n</blockquote>\n<p>The concern for AI safety is most popularly identified with Isaac Asimov&rsquo;s <a href=\"http://en.wikipedia.org/wiki/Three_Laws_of_Robotics\">Three Laws of Robotics</a>, introduced in his short story <a href=\"http://en.wikipedia.org/wiki/Runaround\">Runaround</a>. Asimov used his stories, including those collected in the popular book <em><a href=\"http://en.wikipedia.org/wiki/I,_Robot\">I, Robot</a></em>, to illustrate many of the ways in which such well-meaning and seemingly comprehensive rules for governing robot behavior could go wrong.</p>\n<p>In the year of <em>I, Robot</em>&rsquo;s release, mathematician <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Turing-Computing-Machinery-and-Intelligence.pdf\">Alan Turing (1950)</a> noted that machines may one day be capable of whatever human intelligence can achieve:</p>\n<blockquote>\n<p>I believe that at the end of the century... one will be able to speak of machines thinking without expecting to be contradicted.</p>\n</blockquote>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Turing-Intelligent-Machinery-a-heretical-theory.pdf\">Turing (1951)</a> concluded:</p>\n<blockquote>\n<p>...it seems probable that once the machine thinking method has started, it would not take long to outstrip our feeble powers... At some stage therefore we should have to expect the machines to take control...</p>\n</blockquote>\n<p>Given the profound implications of machine intelligence, it's rather alarming that the early AI scientists who believed AI would be built during the 1950s-1970s didn't show much interest in AI safety. We are lucky they were wrong about the difficulty of AI &mdash; had they been right, humanity probably would not have been prepared to protect its interests.</p>\n<p>Later, statistician I.J. <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Speculations-on-perceptrons-and-other-automata.pdf\">Good (1959)</a>, who had worked with Turing to crack Nazi codes in World War II, reasoned that the transition from human control to machine control may be unexpectedly <em>sudden</em>:</p>\n<blockquote>\n<p>Once a machine is designed that is good enough&hellip; it can be put to work designing an even better machine. At this point an \"explosion\" will clearly occur; all the problems of science and technology will be handed over to machines and it will no longer be necessary for people to work. Whether this will lead to a Utopia or to the extermination of the human race will depend on how the problem is handled by the machines. The important thing will be to give them the aim of serving human beings.</p>\n</blockquote>\n<p>The more famous formulation of this idea, and the origin of the phrase \"intelligence explosion,\" is from <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf\">Good (1965)</a>:</p>\n<blockquote>\n<p>Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an &ldquo;intelligence explosion,\" and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make</p>\n</blockquote>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Some-future-social-repurcussions-of-computers.pdf\">Good (1970)</a> says that \"...by 1980 I hope that the implications and the safeguards [concerning machine superintelligence] will have been thoroughly discussed,\" and argues that an association devoted to discussing the matter be created. Unfortunately, no such association was created until either 1991 (Extropy Institute) or 2000 (Singularity Institute), and we might say these issues have not to this day been \"thoroughly\" discussed.</p>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Ethical-Machines.pdf\">Good (1982)</a> proposed a plan for the design of an ethical machine:</p>\n<blockquote>\n<p>I envisage a machine that would be given a large number of examples of human behaviour that other people called ethical, and examples of discussions of ethics, and from these examples and discussions the machine would formulate one or more consistent general theories of ethics, detailed enough so that it could deduce the probable consequences in most realistic situations.</p>\n</blockquote>\n<p>Even critics of AI like <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Schwartz-The-Limits-of-Artificial-Intelligence.pdf\">Jack Schwartz (1987)</a> saw the implications of intelligence that can improve itself:</p>\n<blockquote>\n<p>If artificial intelligences can be created at all, there is little reason to believe that initial successes could not lead swiftly to the construction of artificial superintelligences able to explore significant mathematical, scientific, or engineering alternatives at a rate far exceeding human ability, or to generate plans and take action on them with equally overwhelming speed. Since man's near-monopoly of all higher forms of intelligence has been one of the most basic facts of human existence throughout the past history of this planet, such developments would clearly create a new economics, a new sociology, and a new history.</p>\n</blockquote>\n<p><a href=\"http://world.std.com/~rjs/timesc.pdf\">Ray Solomonoff (1985)</a>, founder of algorithmic information theory, speculated on the implications of full-blown AI:</p>\n<blockquote>\n<p>After we have reached [human-level AI], it shouldn't take much more than ten years to construct ten thousand duplicates of our original [human-level AI], and have a total computing capability close to that of the computer science community...</p>\n<p>The last 100 years have seen the introduction of special and general relatively, automobiles, airplanes, quantum mechanics, large rockets and space travel, fission power, fusion bombs, lasers, and large digital computers. Any one of these might take a person years to appreciate and understand. Suppose that they had all been presented to mankind in a single year!</p>\n</blockquote>\n<p><a href=\"http://www.amazon.com/Mind-Children-Future-Robot-Intelligence/dp/0674576187/\">Moravec (1988)</a> argued that AI was an existential risk, but nevertheless, one toward which we must run (pp. 100-101):</p>\n<blockquote>\n<p>...intelligent machines... threaten our existence... Machines merely as clever as human beings will have enormous advantages in competitive situations... So why rush headlong into an era of intelligent machines? The answer, I believe, is that we have very little choice, if our culture is to remain viable... The universe is one random event after another. Sooner or later an unstoppable virus deadly to humans will evolve, or a major asteroid will collide with the earth, or the sun will expand, or we will be invaded from the stars, or a black hole will swallow the galaxy. The bigger, more diverse, and competent a culture is, the better it can detect and deal with external dangers. The larger events happen less frequently. By growing rapidly enough, a culture has a finite chance of surviving forever.</p>\n</blockquote>\n<p>Ray Kurzweil's <em><a href=\"http://www.amazon.com/The-Age-Intelligent-Machines-Kurzweil/dp/0262610795/\">The Age of Intelligent Machines</a></em> (1990) did not mention AI risk, and his followup, <em><a href=\"http://www.amazon.com/The-Age-Spiritual-Machines-Intelligence/dp/0140282025/\">The Age of Spiritual Machines</a></em> (1998) does so only briefly, in an \"interview\" between the reader and Kurzweil. The reader asks, \"So we risk the survival of the human race for [the opportunity AI affords us to expand our minds and advance our ability to create knowledge]?\" Kurzweil answers: \"Yeah, basically.\"</p>\n<p><a href=\"http://web.media.mit.edu/~minsky/papers/TrueNames.Afterword.html\">Minsky (1984)</a> pointed out the difficulty of getting machines to do what we want:</p>\n<blockquote>\n<p>...it is always dangerous to try to relieve ourselves of the responsibility of understanding exactly how our wishes will be realized. Whenever we leave the choice of means to any servants we may choose then the greater the range of possible methods we leave to those servants, the more we expose ourselves to accidents and incidents. When we delegate those responsibilities, then we may not realize, before it is too late to turn back, that our goals have been misinterpreted, perhaps even maliciously. We see this in such classic tales of fate as <em>Faust</em>, the <em>Sorcerer's Apprentice</em>, or the <em>Monkey's Paw</em> by W.W. Jacobs.</p>\n<p>[Another]&nbsp;risk is exposure to the consequences of self-deception. It is always tempting to say to oneself... that \"I know what I would like to happen, but I can't quite express it clearly enough.\" However, that concept itself reflects a too-simplistic self-image, which portrays one's own self as [having] well-defined wishes, intentions, and goals. This pre-Freudian image serves to excuse our frequent appearances of ambivalence; we convince ourselves that clarifying our intentions is merely a matter of straightening-out the input-output channels between our inner and outer selves. The trouble is, we simply aren't made that way. <em>Our goals themselves are ambiguous</em>.</p>\n<p>The ultimate risk comes when [we] attempt to take that final step &mdash; of designing goal-achieving programs that are programmed to make themselves grow increasingly powerful, by self-evolving methods that augment and enhance their own capabilities. It will be tempting to do this, both to gain power and to decrease our own effort toward clarifying our own desires. If some genie offered you three wishes, would not your first one be, \"Tell me, please, what is it that I want the most!\" The problem is that, with such powerful machines, it would require but the slightest accident of careless design for them to place their goals ahead of [ours]. The machine's goals may be allegedly benevolent, as with the robots of <em>With Folded Hands,&nbsp;</em>by Jack Williamson, whose explicit purpose was allegedly benevolent: to protect us from harming ourselves, or as with the robot in <em>Colossus</em>, by D.H.Jones, who itself decides, at whatever cost, to save us from an unsuspected enemy. In the case of Arthur C. Clarke's HAL, the machine decides that the mission we have assigned to it is one we cannot properly appreciate. And in Vernor Vinge's computer-game fantasy, <em>True Names</em>, the dreaded Mailman... evolves new ambitions of its own.</p>\n</blockquote>\n<p>&nbsp;</p>\n<h3>The Modern Era</h3>\n<p>Novelist <a href=\"http://www.feedbooks.com/book/2011.pdf\">Vernor Vinge (1993)</a> popularized Good's \"intelligence explosion\" concept, and wrote the first novel about self-improving AI posing an existential threat: <em><a href=\"http://www.amazon.com/Fire-Upon-Deep-Zones-Thought/dp/0765329824/\">A Fire Upon the Deep</a></em>&nbsp;(1992). It was probably Vinge who did more than anyone else to spur discussions about AI risk, particularly in online communities like the <a href=\"http://www.lucifer.com/exi-lists/\">extropians mailing list</a> (since 1991) and <a href=\"http://sl4.org/\">SL4</a> (since 2000). Participants in these early discussions included several of today's leading thinkers on AI risk: <a href=\"http://hanson.gmu.edu/\">Robin Hanson</a>, <a href=\"http://yudkowsky.net/\">Eliezer Yudkowsky</a>, <a href=\"http://nickbostrom.com/\">Nick Bostrom</a>, <a href=\"http://www.aleph.se/Nada/\">Anders Sandberg</a>, and <a href=\"http://wp.goertzel.org/\">Ben Goertzel</a>. (Other posters included <a href=\"http://en.wikipedia.org/wiki/Peter_Thiel\">Peter Thiel</a>, <a href=\"http://en.wikipedia.org/wiki/FM-2030\">FM-2030</a>, <a href=\"http://en.wikipedia.org/wiki/Matrioshka_brain\">Robert Bradbury</a>, and <a href=\"http://en.wikipedia.org/wiki/Julian_Assange\">Julian Assange</a>.) Proposals like Friendly AI, Oracle AI, and Nanny AI were discussed here long before they were brought to greater prominence with academic publications (<a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Yudkowsky 2008</a>; <a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Armstrong et al. 2012</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Goertzel-Should-Humanity-Build-a-Global-AI-Nanny-to-Delay-the-Singularity-Until-its-Better-Understood.pdf\">Goertzel 2012</a>).</p>\n<p>Meanwhile, philosophers and AI researchers considered whether or not machines could have moral value, and how to ensure ethical behavior from less powerful machines or 'narrow AIs', a field of inquiry variously known as 'artificial morality' (<a href=\"http://www.amazon.com/dp/0415076919/\">Danielson 1992</a>; <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.16.722&amp;rep=rep1&amp;type=pdf\">Floridi &amp; Sanders 2004</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2009/08/Allen-Prolegomena-to-any-future-artificial-moral-agent.pdf\">Allen et al. 2000</a>), 'machine ethics' (<a href=\"http://autogeny.org/ethics.html\">Hall 2000</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/McLaren-Lessons-in-Machine-Ethics-from-the-Perspective-of-Two-ComputationalModels-of-Ethical-Reasoning.pdf\">McLaren 2005</a>; <a href=\"http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=34917\">Anderson &amp; Anderson 2006</a>), 'computational ethics' (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Allen-Calculated-morality-ethical-computing-in-the-limit.pdf\">Allen 2002</a>) and 'computational metaethics' (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Lokhorst-Computational-meta-ethics-toward-the-meta-ethical-robot.pdf\">Lokhorst 2011</a>), and 'robo-ethics' or 'robot ethics' (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Capurro-International-Review-of-Information-Ethics-Vol.-6-Ethics-in-Robotics.pdf\">Capurro et al. 2006</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Sawyer-Robot-Ethics.pdf\">Sawyer 2007</a>). This vein of research &mdash; what I'll call the 'machine ethics' literature &mdash; was recently summarized in two books: <a href=\"http://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975/\">Wallach &amp; Allen (2009)</a>; <a href=\"http://www.amazon.com/dp/0521112354/\">Anderson &amp; Anderson (2011)</a>. Thus far, there has been a significant communication gap between the machine ethics literature and the AI risk literature (<a href=\"http://books.google.com/books?id=oBb-lt3l4oYC&amp;lpg=PP1&amp;dq=robot%20ethics&amp;pg=PA55#v=onepage&amp;q&amp;f=false\">Allen and Wallach 2011</a>), excepting perhaps <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">Muehlhauser and Helm (2012)</a>.</p>\n<p>The topic of AI safety in the context of <em>existential risk</em> was left to the futurists who had participated in online discusses of AI risk and opportunity. Here, I must cut short my review and focus on just three (of many) important figures: Eliezer Yudkowksy, Robin Hanson, and Nick Bostrom. (Your author also apologizes for the fact that, because he works with Yudkowsky, Yudkowsky gets a more detailed treatment here than Hanson or Bostrom.)</p>\n<p>Other figures in the modern era of AI risk research include <a href=\"http://www.ssec.wisc.edu/~billh/homepage1.html\">Bill Hibbard</a> (<em><a href=\"http://www.amazon.com/Super-Intelligent-Machines-International-Systems-Engineering/dp/0306473887/\">Super-Intelligent Machines</a></em>) and <a href=\"http://wp.goertzel.org/\">Ben Goertzel</a> (\"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Goertzel-Should-Humanity-Build-a-Global-AI-Nanny-to-Delay-the-Singularity-Until-its-Better-Understood.pdf\">Should Humanity Build a Global AI Nanny to Delay the Singularity Until It's Better Understood</a>\").</p>\n<p>&nbsp;</p>\n<h4>Eliezer Yudkowsky</h4>\n<p>According to \"<a href=\"http://web.archive.org/web/20010205221413/http://sysopmind.com/eliezer.html\">Eliezer, the person</a>,\" <a href=\"http://yudkowsky.net/\">Eliezer Yudkowsky</a> (born 1979) was a bright kid &mdash; in the 99.9998th percentile of cognitive ability, according to the Midwest Talent Search. He read lots of science fiction as a child, and at age 11 read <em><a href=\"http://www.amazon.com/Great-Mambo-Chicken-Transhuman-Condition/dp/0201567512/\">Great Mambo Chicken and the Transhuman Condition</a></em> &mdash; his introduction to the impending reality of transhumanist technologies like AI and nanotech. The moment he became a Singularitarian was the moment he read page 47 of <em><a href=\"http://www.amazon.com/Names-Other-Dangers-Vernor-Vinge/dp/0671653636/\">True Names and Other Dangers</a></em> by Vernor Vinge:</p>\n<blockquote>\n<p>Here I had tried a straightforward extrapolation of technology, and found myself precipitated over an abyss. It's a problem we face every time we consider the creation of intelligences greater than our own. When this happens, human history will have reached a kind of singularity - a place where extrapolation breaks down and new models must be applied - and the world will pass beyond our understanding.</p>\n</blockquote>\n<p>Yudkowsky reported his reaction:</p>\n<blockquote>\n<p>My emotions at that moment are hard to describe; not fanaticism, or enthusiasm, just a vast feeling of \"Yep. He's right.\" I knew, in the moment I read that sentence, that this was how I would be spending the rest of my life.</p>\n</blockquote>\n<p>(As an aside, I'll note that this is eerily similar to <a href=\"/lw/8ib/connecting_your_beliefs_a_call_for_help/\">my own experience</a> of encountering the famous I.J. Good paragraph about ultraintelligence (quoted above), before I knew what \"transhumanism\" or \"the Singularity\" was. I read Good's paragraph and thought, \"Wow. That's... probably correct. How could I have missed that implication? &hellip; &hellip; &hellip; Well, <em>shit</em>. That changes <em>everything</em>.\")</p>\n<p>As a teenager in the mid 1990s, Yudkowsky participated heavily in Singularitarian discussions on the extropians mailing list, and in 1996 (at age 17) he wrote \"<a href=\"http://web.archive.org/web/200012042131/http://sysopmind.com/singularity.html\">Staring into the Singularity</a>,\" which gained him much attention, as did his popular \"<a href=\"http://web.archive.org/web/200012062142/http://www.sysopmind.com/tmol-faq/meaningoflife.html\">FAQ about the Meaning of Life</a>\" (1999).</p>\n<p>In 1998 Yudkowsky was invited (along with 33 others) by economist Robin Hanson to comment on <a href=\"http://www.feedbooks.com/book/2011.pdf\">Vinge (1993)</a>. Thirteen people (including Yudkowsky) left comments, then Vinge responded, and a final open discussion was held on the extropians mailing list. Hanson edited together these results <a href=\"http://hanson.gmu.edu/vi.html\">here</a>. Yudkowsky thought Max More's <a href=\"http://hanson.gmu.edu/vc.html#more\">comments</a> on Vinge underestimated how different from humans AI would probably be, and this prompted Yudkowsky to begin an early draft of \"<a href=\"http://web.archive.org/web/200102021657/http://sysopmind.com/AI_design.temp.html\">Coding a Transhuman AI</a>\" (CaTAI) which by 2000 had grown into the first large explication of his thoughts on \"Seed AI\" and \"friendly\" machine superintelligence (<a href=\"http://web.archive.org/web/200102020421/http://singinst.org/CaTAI.html\">Yudkowsky 2000</a>).</p>\n<p>Around this same time, Yudkowsky wrote \"<a href=\"http://web.archive.org/web/20010216233910/http://sysopmind.com/sing/PtS/contents.html\">The Plan to the Singularity</a>\" and \"<a href=\"http://web.archive.org/web/200102040955/http://sysopmind.com/sing/principles.html\">The Singularitarian Principles</a>,\" and launched the <a href=\"http://sl4.org/\">SL4 mailing list</a>.</p>\n<p>At a <a href=\"http://www.foresight.org/SrAssoc/spring2000/\">May 2000 gathering</a> hosted by the <a href=\"http://www.foresight.org/\">Foresight Institute</a>, Brian Atkins and Sabine Stoeckel discussed with Yudkowsky the possibility of launching an organization specializing in AI safety. In July of that year, Yudkowsky formed the Singularity Institute and began his full-time research on the problems of AI risk and opportunity.</p>\n<p>In 2001, he published two \"sequels\" to CaTAI, \"<a href=\"http://intelligence.org/ourresearch/publications/GISAI/index.html\">General Intelligence and Seed AI</a>\" and, most importantly, \"Creating Friendly AI\" (CFAI) (<a href=\"http://intelligence.org/upload/CFAI.html\">Yudkowsky 2001</a>).</p>\n<p>The publication of CFAI was a significant event, prompting Ben Goertzel (the pioneer of the new Artificial General Intelligence research community) to say that \"<em>Creating Friendly AI</em> is the most intelligent writing about AI that I've read in many years,\" and prompting Eric Drexler (the pioneer of molecular manufacturing) to write that \"With <em>Creating Friendly AI</em>, the Singularity Institute has begun to fill in one of the greatest remaining blank spots in the picture of humanity's future.\"</p>\n<p>CFAI was both frustrating and brilliant. It was frustrating because: (1) it was disorganized and opaque, (2) it invented new terms instead of using the terms being used by everyone else, for example speaking of \"supergoals\" and \"subgoals\" instead of final and instrumental goals, and speaking of goal systems but never \"utility functions,\" and (3) it hardly cited any of the relevant works in AI, philosophy, and psychology &mdash; for example it could have cited <a href=\"http://www.vordenker.de/ggphilosophy/mcc_ethical.pdf\">McCulloch (1952)</a>, Good (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Speculations-on-perceptrons-and-other-automata.pdf\">1959</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Some-future-social-repurcussions-of-computers.pdf\">1970</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Ethical-Machines.pdf\">1982</a>), <a href=\"http://www.amazon.com/Other-Worlds-Than-Cecil-Maxwell/dp/0800861256/\">Cade (1966)</a>,&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Versenyi-Can-Robots-Be-Moral.pdf\">Versenyi (1974)</a>, <a href=\"http://www.amazon.com/The-Mighty-Micro-Microchip-Revolution/dp/0340259752/\">Evans (1979)</a>,&nbsp;<a href=\"http://www.cs.umd.edu/~jkatz/TEACHING/comp_sec_F04/downloads/confinement.pdf\">Lampson (1979)</a>, the <a href=\"/lw/hva/open_thread_july_115_2013/9d1h\">conversation with Ed Fredkin</a> in <a href=\"http://www.amazon.com/Machines-Who-Think-Artificial-Intelligence/dp/1568812051/\">McCorduck (1979)</a>,&nbsp;<a href=\"http://www.cs.bham.ac.uk/research/projects/cogaff/sloman-space-of-minds-84.pdf\">Sloman (1984)</a>,&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Schmidhuber-Evolutionary-Principles-in-Self-Referential-Learning.pdf\">Schmidhuber (1987)</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/01/Waldrop-A-Question-of-Responsibility.pdf\">Waldrop (1987)</a>,&nbsp;<a href=\"http://www.amazon.com/Probabilistic-Reasoning-Intelligent-Systems-Plausible/dp/1558604790/\">Pearl (1989)</a>, <a href=\"http://www.amazon.com/War-Intelligent-Machines-Manuel-Landa/dp/0942299752/\">De Landa (1991)</a>,&nbsp;<a href=\"http://www.amazon.com/Ai-Tumultuous-History-Artificial-Intelligence/dp/0465001041/\">Crevier (1993, ch. 12)</a>, Clarke (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Clarke-Asimovs-Laws-of-Robotics-implications-for-information-technology-part-1.pdf\">1993</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Clarke-Asimovs-Laws-of-Robotics-implications-for-information-technology-part-2.pdf\">1994</a>), <a href=\"http://www.cs.auckland.ac.nz/~nickjhay/papersuni/RoughDraft200207-best_of_SASEMAS.pdf\">Weld &amp; Etzioni (1994)</a>,&nbsp;<a href=\"http://www.amazon.com/The-Evolution-Of-Desire-Revised/dp/046500802X/\">Buss (1995)</a>, <a href=\"http://www.cs.berkeley.edu/~russell/aima1e.html\">Russell &amp; Norvig (1995)</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Gips-Towards-the-ethical-robot.pdf\">Gips (1995)</a>, <a href=\"http://www.amazon.com/Reflections-Artificial-Intelligence-Blay-Whitby/dp/1871516684/\">Whitby (1996)</a>,&nbsp;<a href=\"http://igitur-archive.library.uu.nl/math/2007-1219-220125/Wiering_97_shiftinginductivebias.pdf\">Schmidhuber et al. (1997)</a>, <a href=\"http://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262193981/\">Barto &amp; Sutton (1998)</a>, <a href=\"http://www.amazon.com/From-Metaphysics-Ethics-Conceptual-Analysis/dp/0198250614/\">Jackson (1998)</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/01/Levitt-Robot-Ethics-Value-Systems-and-Decision-Theoretic-Behaviors.pdf\">Levitt (1999)</a>,&nbsp;<a href=\"http://www.amazon.com/Robot-Mere-Machine-Transcendent-Mind/dp/0195136306/\">Moravec (1999)</a>, <a href=\"http://en.wikipedia.org/wiki/The_Age_of_Spiritual_Machines\">Kurzweil (1999)</a>, <a href=\"http://www.unl.edu/philosop/people/faculty/sobel/DotheDesires.pdf\">Sobel (1999)</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2009/08/Allen-Prolegomena-to-any-future-artificial-moral-agent.pdf\">Allen et al. (2000)</a>, <a href=\"http://arxiv.org/pdf/1106.0244.pdf\">Gordon (2000)</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/06/Harper-Challenges-for-designing-intelligent-systems-for-safety-critical-applications.pdf\">Harper (2000)</a>,&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Coleman-Android-arete-toward-a-virtue-ethic-for-computational-agents.pdf\">Coleman 2001</a>, and&nbsp;<a href=\"http://www.hutter1.net/ai/paixi.pdf\">Hutter (2001)</a>. These features still substantially characterize Yudkowsky's independent writing, e.g. see <a href=\"http://intelligence.org/upload/TDT-v01o.pdf\">Yudkowsky (2010)</a>. As late as January 2006, he still wrote that \"It is not that I have neglected to cite the existing major works on this topic, but that, to the best of my ability to discern, there are no existing major works to cite.\"</p>\n<p>On the other hand, CFAI was in many ways was brilliant, and it tackled many of the problems left mostly untouched by mainstream machine ethics researchers. For example, CFAI (but not the mainstream machine ethics literature) engaged the problems of: (1) radically self-improving AI, (2) AI as an existential risk, (3) hard takeoff, (4) the interplay of goal <a href=\"http://intelligence.org/upload/CFAI.html#challenge_content\">content, acquisition, and structure</a>, (5) <a href=\"http://intelligence.org/upload/CFAI.html#anthro_selfishness_pain_fof\">wireheading</a>, (6) <a href=\"http://intelligence.org/upload/CFAI.html#design_generic_stomp\">subgoal stomp</a>, (7) <a href=\"http://intelligence.org/upload/CFAI.html#design_structure_external\">external reference semantics</a>, (8) <a href=\"http://intelligence.org/upload/CFAI.html#design_structure_causal\">causal validity semantics</a>, and (9) <a href=\"http://intelligence.org/upload/CFAI.html#policy_policies_selective\">selective support</a> (which <a href=\"http://www.nickbostrom.com/existential/risks.html\">Bostrom (2002)</a> would later call \"differential technological development\").</p>\n<p>For many years, the Singularity Institute was little more than a vehicle for Yudkowsky's research. In 2002 he wrote \"Levels of Organization in General Intelligence,\" which later appeared in the <a href=\"http://www.amazon.com/Artificial-General-Intelligence-Cognitive-Technologies/dp/354023733X/\">first edited volume</a> on Artificial General Intelligence (AGI). In 2003 he wrote what would become the internet's most popular <a href=\"http://yudkowsky.net/rational/bayes\">tutorial on Bayes' Theorem</a>, followed in 2005 by \"<a href=\"http://yudkowsky.net/rational/technical\">A Technical Explanation of Technical Explanation</a>.\" In 2004 he explained his vision of a Friendly AI goal structure: \"<a href=\"http://intelligence.org/upload/CEV.html\">Coherent Extrapolated Volition</a>.\" In 2006 he wrote two chapters that would later appear in the volume <em><a href=\"http://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501/\">Global Catastrohpic Risks</a></em> volume from Oxford University Press (co-edited by Bostrom): \"<a href=\"http://intelligence.org/upload/cognitive-biases.pdf\">Cognitive Biases Potentially Affecting Judgment of Global Risks</a>\" and, what remains his \"classic\" article on the need for Friendly AI, \"<a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Artificial Intelligence as a Positive and Negative Factor in Global Risk</a>.</p>\n<p>In 2004, Tyler Emerson was hired as the Singularity Institute's executive director. Emerson brought on Nick Bostrom (then a post doctoral fellow at Yale), Christine Peterson (of the Foresight Institute), and others, as advisors. In February 2006, Paypal co-founder Peter Thiel donated $100,000 to the Singularity Institute, and, we might say, the Singularity Institute as we know it today was born.</p>\n<p>From 2005-2007, Yudkowsky worked at various times with <a href=\"/user/Marcello/\">Marcello Herreshoff</a>, <a href=\"http://www.cs.berkeley.edu/~nickjhay/\">Nick Hay</a> and <a href=\"http://www.spaceandgames.com/\">Peter de Blanc</a> on the technical problems of AGI necessary for technical FAI work, for example <a href=\"http://en.wikipedia.org/wiki/AIXI#Universal_artificial_intelligence\">creating AIXI</a>-like architectures, developing a reflective decision theory, and investigating limits inherent in self-reflection due to <a href=\"http://yudkowsky.net/rational/lobs-theorem\">L&ouml;b's Theorem</a>. Almost none of this research has been published, in part because of the desire not to accelerate AGI research without having made corresponding safety progress. (Marcello also worked with Eliezer during the summer of 2009.)</p>\n<p>Much of the Singularity Institute's work has been \"movement-building\" work. The institute's Singularity Summit, held annually since 2006, attracts technologists, futurists, and social entrepreneurs from around the world, bringing to their attention not only emerging and future technologies but also the basics of AI risk and opportunity. The Singularity Summit also gave the Singularity Institute much of its access to cultural, academic, and business elites.</p>\n<p>Another key piece of movement-building work was Yudkowsky's \"<a href=\"http://wiki.lesswrong.com/wiki/Sequences#Major_Sequences\">The Sequences</a>,\" which were written during 2006-2009. Yudkowsky blogged, almost daily, on the subjects of epistemology, language, cognitive biases, decision-making, quantum mechanics, metaethics, and artificial intelligence. These posts were originally published on a community blog about rationality, <em><a href=\"http://www.overcomingbias.com/\">Overcoming Bias</a></em> (which later became Hanson's personal blog). Later, Yudkowsky's posts were used as the seed material for a new group blog, <em><a href=\"/\">Less Wrong</a></em>.</p>\n<p>Yudkowsky's goal was to create a community of people who could avoid common thinking mistakes, change their minds in response to evidence, and generally think and act with an unusual degree of <a href=\"http://facingthesingularity.com/2011/from-skepticism-to-technical-rationality/\">Technical Rationality</a>. In CFAI he had pointed out that when it comes to AI, humanity may not have a second chance to get it right. So we can't run a series of intelligence explosion experiments and \"see what works.\" Instead, we need to predict in advance what we need to do to ensure a desirable future, and we need to overcome common thinking errors when doing so. (Later, Yudkowsky expanded his \"community of rationalists\" by writing the most popular Harry Potter fanfiction in the world, <em><a href=\"http://hpmor.com/\">Harry Potter and the Methods of Rationality</a></em>, and is currently helping to launch <a href=\"/lw/9hb/position_design_and_write_rationality_curriculum/\">a new organization</a> that will teach classes on the skills of rational thought and action.)</p>\n<p>This community demonstrated its usefulness in 2009 when Yudkowsky began <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">blogging</a> about some problems in decision theory related to the project of building a Friendly AI. Much like Tim Gowers' <a href=\"http://en.wikipedia.org/wiki/Timothy_Gowers#Polymath_Project\">Polymath Project</a>, these discussions demonstrated the power of collaborative problem-solving over the internet. The discussions led to a decision theory workshop and then a decision theory mailing list, which quickly became home to some of the most interesting work in decision theory anywhere in the world. Yudkowsky summarized some of his earlier results in \"<a href=\"http://intelligence.org/upload/TDT-v01o.pdf\">Timeless Decision Theory</a>\" (2010), and newer results have been posted to <em>Less Wrong</em>, for example <a href=\"/lw/8wc/a_model_of_udt_with_halting_oracles/\">A model of UDT with a halting oracle</a> and <a href=\"/lw/9o7/formulas_of_arithmetic_that_behave_like_decision/\">Formulas of arithmetic that behave like decision agents</a>.</p>\n<p>The Singularity Institute also built its community with a Visiting Fellows program that hosted groups of researchers for 1-3 months at a time. Together, both visiting fellows and newly hired research fellows produced several working papers between 2009 and 2011, including <a href=\"http://intelligence.org/upload/machine-ethics-superintelligence.pdf\">Machine Ethics and Superintelligence</a>, <a href=\"http://intelligence.org/upload/ECAPShulmanSandberg.pdf\">Implications of a Software-Limited Singularity</a>, <a href=\"http://intelligence.org/upload/economic-implications.pdf\">Economic Implications of Software Minds</a>, <a href=\"http://intelligence.org/upload/convergence-expected-utility-universal-ai.pdf\">Convergence of Expected Utility for Universal AI</a>, and <a href=\"http://arxiv.org/pdf/1105.3821v1.pdf\">Ontological Crises in Artificial Agents' Value Systems</a>.</p>\n<p>In 2011, then-president Michael Vassar left the Singularity Institute to help launch a <a href=\"http://www.medicineispersonal.com/\">personalized medicine company</a>, and research fellow <a href=\"http://lukeprog.com/\">Luke Muehlhauser</a> (the author of this document) <a href=\"/lw/8c3/qa_with_new_executive_director_of_singularity/\">took over leadership from Vassar, as Executive Director</a>. During this time, the Institute underwent a major overhaul to implement best practices for organizational process and management: it published its first <a href=\"http://intelligence.org/files/strategicplan2011.pdf\">strategic plan</a>, began to maintain its first donor database, adopted best practices for accounting and bookkeeping, updated its bylaws and articles of incorporation, adopted more standard roles for the Board of Directors and the Executive Director, held a series of strategic meetings to help decide the near-term goals of the organization, began to publish monthly progress reports to its <a href=\"http://intelligence.org/blog/\">blog</a>, started outsourcing more work, and began to work on more articles for peer-reviewed publications: as of March 2012, the Singularity Institute has more peer-reviewed publications forthcoming in 2012 than it had published in all of 2001-2011 <a href=\"/lw/axr/three_new_papers_on_ai_risk/627o\">combined</a>.</p>\n<p>Today, the Singularity Institute collaborates regularly with its (non-staff) <a href=\"http://intelligence.org/aboutus/researchassociates\">research associates</a>, and also with researchers at the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> at Oxford University (directed by Bostrom), which as of March 2012 is the world's only other major research institute largely focused on the problems of existential risk.</p>\n<p>&nbsp;</p>\n<h4>Robin Hanson</h4>\n<p>Whereas Yudkowsky has never worked in the for-profit world and had no formal education after high school, <a href=\"http://hanson.gmu.edu/home.html\">Robin Hanson</a> (born 1959) has a long and prestigious academic and professional history. Hanson took a B.S. in physics from U.C. Irvine in 1981, took an M.S. in physics and an M.A. in the conceptual foundations of science from U. Chicago in 1984, worked in artificial intelligence for Lockheed and NASA, got a Ph.D. in social science from Caltech in 1997, did a post-doctoral fellowship at U.C. Berkeley in Health policy from 1997-1999, and finally was made an assistant professor of economics at George Mason University in 1999. In economics, he is best known for conceiving of <a href=\"http://en.wikipedia.org/wiki/Prediction_market\">prediction markets</a>.</p>\n<p>When Hanson moved to California in 1984, he encountered the <a href=\"http://en.wikipedia.org/wiki/Project_Xanadu\">Project Xanadu</a> crowd and met Eric Drexler, who showed him an early draft of <em><a href=\"http://www.wowio.com/users/product.asp?BookId=503\">Engines of Creation</a></em>. This community discussed AI, nanotech, cryonics, and other transhumanist topics, and Hanson joined the extropians mailing list (along with many others from Project Xanadu) when it launched in 1991.</p>\n<p>Hanson has published several papers on the economics of whole brain emulations (what he calls \"ems\") and AI (<a href=\"http://hanson.gmu.edu/uploads.html\">1994</a>, <a href=\"http://hanson.gmu.edu/fastgrow.html\">1998a</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Hanson-Economic-growth-given-machine-intelligence-early.pdf\">1998b</a>, <a href=\"http://hanson.gmu.edu/collapse.pdf\">2008a</a>, <a href=\"http://hanson.gmu.edu/EconOfBrainEmulations.pdf\">2008b</a>, <a href=\"http://hanson.gmu.edu/IEEESpectrum-6-08.pdf\">2008c</a>, <a href=\"http://hanson.gmu.edu/ChalmersReply.html\">2012a</a>). His writings at <em><a href=\"http://www.overcomingbias.com/about\">Overcoming Bias</a></em> (launched November 2006) are perhaps even more influential, and cover a wide range of topics.</p>\n<p>Hanson's views on AI risk and opportunity differ from Yudkowsky's. First, Hanson sees the technological singularity and the human-machine conflict it may produce not as a unique event caused by the advent of AI, but as a natural consequence of \"the general fact that accelerating rates of change increase intergenerational conflicts\" (Hanson 2012b). Second, Hanson thinks an intelligence explosion will be slower and more gradual than Yudkowsky does, denying Yudkowsky's \"hard takeoff\" thesis (<a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">Hanson &amp; Yudkowsky 2008</a>).</p>\n<p>&nbsp;</p>\n<h4>Nick Bostrom</h4>\n<p><a href=\"http://nickbostrom.com/\">Nick Bostrom</a> (born 1973) received a B.S. in philosophy, mathematics, mathematical logic, and artificial intelligence from the University of Goteborg in 1994, setting a national record in Sweden for undergraduate academic performance. He received an M.A. in philosophy and physics from from U. Stockholm in 1996, did work in astrophysics and computational neuroscience at King's College London, and received his Ph.D. from the London School of Economics in 2000. He went on to be a post-doctoral fellow at Yale University and in 2005 became the founding director of Oxford University's <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> (FHI). Without leaving FHI, he became the founding director of Oxford's <a href=\"http://www.futuretech.ox.ac.uk/\">Programme on the Impacts of Future Technology</a> (aka FutureTech) in 2011.</p>\n<p>Bostrom had long been interested in cognitive enhancement, and in 1995 he joined the extropians mailing list and learned about cryonics, uploading, AI, and other topics.</p>\n<p>Bostrom worked with British philosopher <a href=\"http://en.wikipedia.org/wiki/David_Pearce_(philosopher\">David Pearce</a>) to found the World Transhumanist Association (now called <a href=\"http://humanityplus.org/\">H+</a>) in 1998, with the purpose of developing a more mature and academically respectable form of transhumanism than was usually present on the extropians mailing list. During this time Bostrom wrote \"<a href=\"http://www.transhumanism.org/resources/FAQv21.pdf\">The Transhumanist FAQ</a>\" (now updated to version 2.1), with input from more than 50 others.</p>\n<p>His first philosophical publication was \"<a href=\"http://www.nickbostrom.com/old/predict.html\">Predictions from Philosophy? How philosophers could make themselves useful</a>\" (1997). In this paper, Bostrom proposed \"a new type of philosophy, a philosophy whose aim is prediction.\" On Bostrom's view, one role for the philosopher is to be a polymath who can engage in technological prediction and try to figure out how to steer the future so that humanity's goals are best met.</p>\n<p>Bostrom gave three examples of problems this new breed of philosopher-polymath could tackle: the <a href=\"http://en.wikipedia.org/wiki/Doomsday_argument\">Doomsday argument</a> and <a href=\"http://www.anthropic-principle.com/\">anthropics</a>, the <a href=\"http://en.wikipedia.org/wiki/Fermi_paradox\">Fermi paradox</a>, and superintelligence:</p>\n<blockquote>\n<p>What questions could a philosophy of superintelligence deal with? Well, questions like: How much would the predictive power for various fields increase if we increase the processing speed of a human-like mind a million times? If we extend the short-term or long-term memory? If we increase the neural population and the connection density? What other capacities would a superintelligence have? How easy would it be for it to rediscover the greatest human inventions, and how much input would it need to do so? What is the relative importance of data, theory, and intellectual capacity in various disciplines? Can we know anything about the motivation of a superintelligence? Would it be feasible to preprogram it to be good or philanthropic, or would such rules be hard to reconcile with the flexibility of its cognitive processes? Would a superintelligence, given the desire to do so, be able to outwit humans into promoting its own aims even if we had originally taken strict precautions to avoid being manipulated? Could one use one superintelligence to control another? How would superintelligences communicate with each other? Would they have thoughts which were of a totally different kind from the thoughts that humans can think? Would they be interested in art and religion? Would all superintelligences arrive at more or less the same conclusions regarding all important scientific and philosophical questions, or would they disagree as much as humans do? And how similar in their internal belief-structures would they be? How would our human self-perception and aspirations change if were forced to abdicate the throne of wisdom...? How would we individuate between superminds if they could communicate and fuse and subdivide with enormous speed? Will a notion of personal identity still apply to such interconnected minds? Would they construct an artificial reality in which to live? Could we upload ourselves into that reality? Could we then be able to compete with the superintelligences, if we were accelerated and augmented with extra memory etc., or would such profound reorganisation be necessary that we would no longer feel we were humans? Would that matter?</p>\n</blockquote>\n<p>Bostrom went on to examine some philosophical issues related to superintelligence, in \"Predictions from Philosophy\" and in \"<a href=\"http://www.nickbostrom.com/superintelligence.html\">How Long Before Superintelligence?</a>\" (1998), \"<a href=\"http://www.nickbostrom.com/existential/risks.html\">Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards</a>\" (2002), \"<a href=\"http://www.nickbostrom.com/ethics/ai.html\">Ethical Issues in Advanced Artificial Intelligence</a>\" (2003), \"<a href=\"http://www.nickbostrom.com/fut/evolution.html\">The Future of Human Evolution</a>\" (2004), and \"<a href=\"http://www.nickbostrom.com/ethics/artificial-intelligence.pdf\">The Ethics of Artificial Intelligence</a>\" (2012, coauthored with Yudkowsky). (He also played out the role of philosopher-polymath with regard to several other topics, including human enhancement and anthropic bias.)</p>\n<p>Bostrom's industriousness <a href=\"http://www.nickbostrom.com/cv.pdf\">paid off</a>:</p>\n<blockquote>\n<p>In 2009, [Bostrom] was awarded the Eugene R. Gannon Award (one person selected annually worldwide from the fields of philosophy, mathematics, the arts and other humanities, and the natural sciences). He has been listed in the FP 100 Global Thinkers list, the Foreign Policy Magazine\u02b9s list of the world\u02b9s top 100 minds. His writings have been translated into more than 21 languages, and there have been some 80 translations or reprints of his works. He has done more than 470 interviews for TV, film, radio, and print media, and he has addressed academic and popular audiences around the world.</p>\n</blockquote>\n<p>The other long-term member of the Future of Humanity Institute, Anders Sandberg, has also published some research on AI risk. Sandberg was a co-author on the <a href=\"http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf\">whole brain emulation roadmap</a> and \"<a href=\"http://www.nickbostrom.com/papers/anthropicshadow.pdf\">Anthropic Shadow</a>\", and also wrote \"<a href=\"http://agi-conf.org/2010/wp-content/uploads/2009/06/agi10singmodels2.pdf\">Models of the Technological Singularity</a>\" and several other papers.</p>\n<p>Recently, Bostrom and Sandberg were joined by <a href=\"http://www.fhi.ox.ac.uk/our_staff/research/stuart_armstrong\">Stuart Armstrong</a>, who wrote \"<a href=\"http://arxiv.org/pdf/1110.6437v2.pdf\">Anthropic Decision Theory</a>\" (2011) and was the lead author on \"<a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Thinking Inside the Box: Using and Controlling Oracle AI</a>\" (2012). He had previously written <em><a href=\"http://www.neweuropeancentury.org/GodAI.pdf\">Chaining God</a></em> (2007).</p>\n<p>For more than a year, Bostrom has been working on a new book titled <em>Superintelligence: A Strategic Analysis of the Coming Machine Intelligence Revolution</em>, which aims to sum up and organize much of the (published and unpublished) work done in the past decade by researchers at the Singularity Institute and FHI on the subject of AI risk and opportunity, as well as contribute new insights.</p>\n<p>&nbsp;</p>\n<h3>AI Risk Goes Mainstream</h3>\n<p>In 1997, professor of cybernetics Kevin Warwick published <em><a href=\"http://www.amazon.com/March-Machines-Breakthrough-Artificial-Intelligence/dp/0252072235/\">March of the Machines</a></em>, in which he predicted that within a couple decades, machines would become more intelligent than humans, and would pose an existential threat.</p>\n<p>In 2000, Sun Microsystems co-founder <a href=\"http://en.wikipedia.org/wiki/Bill_Joy\">Bill Joy</a> published \"<a href=\"http://en.wikipedia.org/wiki/Why_the_future_doesn%27t_need_us\">Why the Future Doesn't Need Us</a>\" in <em>Wired</em> magazine. In this widely-circulated essay, Joy argued that \"Our most powerful 21st-century technologies &mdash; robotics, genetic engineering, and nanotech &mdash; are threatening to make humans an endangered species.\" Joy advised that we relinquish development of these technologies rather than sprinting headlong into an arms race between destructive uses of these technologies and defenses against those destructive uses.</p>\n<p>Many people dismissed Bill Joy as a \"<a href=\"http://en.wikipedia.org/wiki/Neo-Luddite\">Neo-Luddite</a>,\" but many experts expressed similar concerns about human extinction, including philosopher John Leslie (<em><a href=\"http://www.amazon.com/End-World-Science-Ethics-Extinction/dp/0415184479/\">The End of the World</a></em>), physicist Martin Rees (<em><a href=\"http://www.amazon.com/Our-Final-Hour-Scientists-Warning/dp/0465068634/\">Our Final Hour</a></em>), legal theorist Richard Posner (<em><a href=\"http://www.amazon.com/Catastrophe-Risk-Response-Richard-Posner/dp/0195306473/\">Catastrophe: Risk and Response</a></em>), and the contributors to <em><a href=\"http://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501/\">Global Catastrophic Risks</a></em> (including Yudkowsky, Hanson, and Bostrom).</p>\n<p>Even Ray Kurzweil, known as an optimist about technology, devoted a chapter of his 2005 bestseller <em><a href=\"http://www.amazon.com/The-Singularity-Is-Near-Transcend/dp/0143037889/\">The Singularity is Near</a></em> to a discussion of existential risks, including risks from AI. Though discussing the possibility of existential catastrophe at length, his take on AI risk was cursory (p. 420):</p>\n<blockquote>\n<p>Inherently there will be no absolute protection against strong AI. Although the argument is subtle I believe that maintaining an open free-market system for incremental scientific and technological progress, in which each step is subject to market acceptance, will provide the most constructive environment for technology to embody widespread human values. As I have pointed out, strong AI is emerging from many diverse efforts and will be deeply integrated into our civilization's infrastructure. Indeed, it will be intimately embedded in our bodies and brains. As such, it will reflect our values because it will be us.</p>\n</blockquote>\n<p>AI risk finally became a \"mainstream\" topic in analytic philosophy with <a href=\"http://consc.net/papers/singularityjcs.pdf\">Chalmers (2010)</a> and an entire <a href=\"/r/discussion/lw/aif/journal_of_consciousness_studies_issue_on_the/\">issue</a> of <em>Journal of Consciousness Studies</em>&nbsp;devoted to the topic.</p>\n<p>The earliest popular discussion of machine superintelligence may have been in Christopher Evans' international bestseller <em><a href=\"http://www.amazon.com/The-Mighty-Micro-Computer-Revolution/dp/0575027088/\">The Mighty Micro</a></em> (1979), pages 194-198, 231-233, and 237-246.</p>\n<h3><br /></h3>\n<h3>The Current Situation</h3>\n<p>Two decades have passed since the early transhumanists began to seriously discuss AI risk and opportunity on the extropians mailing list. (Before that, some discussions took place at the MIT AI lab, but that was before the web was popular, so they weren't recorded.) What have we humans done since then?</p>\n<p><em>Lots of talking</em>. Hundreds of thousands of man-hours have been invested into discussions on the extropians mailing list, SL4, <em>Overcoming Bias</em>, <em>Less Wrong</em>, the Singularity Institute's decision theory mailing list, several other internet forums, and also in meat-space (especially in the Bay Area near the Singularity Institute and in Oxford near FHI). These are difficult issues; talking them through is usually the first step to getting anything else done.</p>\n<p><em>Organization</em>. Mailing lists are a form of organization, as are organizations like The Singularity Institute and university departments like the FHI and FutureTech. Established organizations provide opportunities to bring people together, and to pool and direct resources efficiently.</p>\n<p><em>Resources</em>. Many people of considerable wealth, along with thousands of others of \"concerned citizens\" around the world, have decided that AI is the most significant risk and opportunity we face, and are willing to invest in humanity's future.</p>\n<p><em>Outreach</em>. Publications (both academic and popular), talks, and interactions with major and minor media outlets have been used to raise awareness of AI risk and opportunity. This has included outreach to specific AGI researchers, some of whom now take AI safety quite seriously. This also includes outreach to people in positions of influence who are in a position to engage in differential technological development. It also includes outreach to the rapidly growing \"optimal philanthropy\" community; a large fraction of those associated with <a href=\"http://www.givingwhatwecan.org/\">Giving What We Can</a> take existential risk &mdash; and AI risk in particular &mdash; quite seriously.</p>\n<p><em>Research</em>. So far, most research on the topic has been concerned with trying to become less confused about what, exactly, the problem <em>is</em>, how worried we should be, and which strategic actions we should take. How do we predict technological progress? How can we predict AI outcomes? Which interventions, taken now, would probably increase the odds of positive AI outcomes? There has also been some \"technical\" research in decision theory (e.g. <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">TDT</a>, <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">UDT</a>, <a href=\"http://arxiv.org/pdf/1110.6437v2.pdf\">ADT</a>), the math of AI goal systems (\"<a href=\"http://intelligence.org/upload/learning-what-to-value.pdf\">Learning What to Value\"</a>,\" \"<a href=\"http://arxiv.org/pdf/1105.3821v1.pdf\">Ontological Crises in Artificial Agents' Value Systems</a>,\" \"<a href=\"http://intelligence.org/upload/convergence-expected-utility-universal-ai.pdf\">Convergence of Expected Utility for Universal AI</a>\"), and Yudkowsky's unpublished research on Friendly AI.</p>\n<p><a href=\"http://lukeprog.com/SaveTheWorld.html\">Muehlhauser 2011</a> provides an overview of the categories of research problems we have left to solve. Most of the known problems aren't even well-defined at this point.</p>\n<p>&nbsp;</p>\n<p><strong>References</strong></p>\n<ul>\n<li><small>Allen et al. (2000). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2009/08/Allen-Prolegomena-to-any-future-artificial-moral-agent.pdf\"> Prolegomena to any future artificial moral agent</a>.</small></li>\n<li><small>Allen (2002). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Allen-Calculated-morality-ethical-computing-in-the-limit.pdf\"> Calculated morality: ethical computing in the limit.</a>.</small></li>\n<li><small>Anderson &amp; Anderson (2006). <a href=\"http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=34917\"> Guest Editors' Introduction: Machine Ethics. IEEE Intelligent Systems Magazine.</a>.</small></li>\n<li><small>Anderson &amp; Anderson (2011). <a href=\"http://www.amazon.com/dp/0521112354/\"> Machine Ethics.</a>.</small></li>\n<li><small>Armstrong (2007). <a href=\"http://www.neweuropeancentury.org/GodAI.pdf\">Chaining God: A qualitative approach to AI, trust and moral systems</a>.</small></li>\n<li><small>Armstrong (2011). <a href=\"http://arxiv.org/pdf/1110.6437v2.pdf\"> Anthropic decision theory for self-locating beliefs</a>.</small></li>\n<li><small>Armstrong, Sandberg &amp; Bostrom (2012). <a href=\"http://www.aleph.se/papers/oracleAI.pdf\"> Thinking Inside the Box: Using and Controlling Oracle AI</a>.</small></li>\n<li><small>Barto &amp; Sutton (1998). <a href=\"http://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262193981/\"> Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning)</a>.</small></li>\n<li><small>Bostrom (1997). <a href=\"http://www.nickbostrom.com/old/predict.html\"> Predictions from Philosophy? How philosophers could make themselves useful</a>.</small></li>\n<li><small>Bostrom (1998). <a href=\"http://www.nickbostrom.com/superintelligence.html\"> How Long Before Superintelligence?</a>.</small></li>\n<li><small>Bostrom (2002). <a href=\"http://www.nickbostrom.com/existential/risks.html\"> Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards</a>.</small></li>\n<li><small>Bostrom (2003). <a href=\"http://www.nickbostrom.com/ethics/ai.html\"> Ethical Issues in Advanced Artificial Intelligence</a>.</small></li>\n<li><small>Bostrom (2003). <a href=\"http://www.transhumanism.org/resources/FAQv21.pdf\"> The Transhumanist FAQ</a>.</small></li>\n<li><small>Bostrom (2004). <a href=\"http://www.nickbostrom.com/fut/evolution.html\"> The Future of Human Evolution</a>.</small></li>\n<li><small>Bostrom et al. (2011). <a href=\"http://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501/\"> Global Catastrophic Risks</a>.</small></li>\n<li><small>Bostrom &amp; Yudkowsky (2012). <a href=\"http://www.nickbostrom.com/ethics/artificial-intelligence.pdf\"> The Ethics of Artificial Intelligence</a>.</small></li>\n<li><small>Crevier (1993). <em><a href=\"http://www.amazon.com/Ai-Tumultuous-History-Artificial-Intelligence/dp/0465001041/\">AI: The Tumultous Search for Artificial Intelligence</a></em>.</small></li>\n<li><small>de Blanc (2009). <a href=\"http://intelligence.org/upload/convergence-expected-utility-universal-ai.pdf\"> Convergence of Expected Utility for Universal AI</a>.</small></li>\n<li><small>de Blanc (2011). <a href=\"http://arxiv.org/pdf/1105.3821v1.pdf\"> Ontological Crises in Artificial Agents' Value Systems</a>.</small></li>\n<li><small>Buss (1995). <a href=\"http://www.amazon.com/The-Evolution-Of-Desire-Revised/dp/046500802X/\"> The Evolution Of Desire</a>.</small></li>\n<li><small>Campbell (1932). <em><a href=\"http://www.gutenberg.org/files/27462/27462-h/27462-h.htm\">The Last Evolution</a></em>.</small></li>\n<li><small>Campbell (1935). <em><a href=\"http://www.technovelgy.com/ct/content.asp?Bnum=1030\">The Machine</a></em>.</small></li>\n<li><small>Capurro et al. (2006). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Capurro-International-Review-of-Information-Ethics-Vol.-6-Ethics-in-Robotics.pdf\"> Ethics in Robotics</a>.</small></li>\n<li><small>Chalmers (2010). <a href=\"http://consc.net/papers/singularityjcs.pdf\">The Singularity: A Philosophical Analysis</a>.</small></li>\n<li><small>Cirkovic, Sandberg, &amp; Bostrom (2010). <a href=\"http://www.nickbostrom.com/papers/anthropicshadow.pdf\">Anthropic Shadow: Observation selection effects and human extinction risks</a>.</small></li>\n<li><small>Clarke (1993). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Clarke-Asimovs-Laws-of-Robotics-implications-for-information-technology-part-1.pdf\"> Asimov's Laws of Robotics: Implications for Information Technology. Part 1.</a>.</small></li>\n<li><small>Clarke (1994). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Clarke-Asimovs-Laws-of-Robotics-implications-for-information-technology-part-2.pdf\"> Asimov's Laws of Robotics: Implications for Information Technology. Part 2.</a>.</small></li>\n<li><small>Colema (2001). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Coleman-Android-arete-toward-a-virtue-ethic-for-computational-agents.pdf\"> Android arete: Toward a virtue ethic for computational agents</a>.</small></li>\n<li><small>Danielson (1992). <a href=\"http://www.amazon.com/dp/0415076919/\"> Artificial Morality: Virtuous Robots for Virtual Games</a>.</small></li>\n<li><small>De Landa (1991). <em><a href=\"http://www.amazon.com/War-Intelligent-Machines-Manuel-Landa/dp/0942299752/\">War in the Age of Intelligent Machines</a></em>.</small></li>\n<li><small>Dewey (2011). <a href=\"http://intelligence.org/upload/learning-what-to-value.pdf\"> Learning What to Value</a>.</small></li>\n<li><small>Drexler (1986). <a href=\"http://www.wowio.com/users/product.asp?BookId=503\"> Engines of Creation</a>.</small></li>\n<li><small>Evans (1979). <em><a href=\"http://www.amazon.com/The-Mighty-Micro-Computer-Revolution/dp/0575027088/\">The Mighty Micro</a></em>.</small></li>\n<li><small>Floridi &amp; Sanders (2004). <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.16.722&amp;rep=rep1&amp;type=pdf\"> On the morality of artificial agents</a>.</small></li>\n<li><small>Goertzel (2012). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Goertzel-Should-Humanity-Build-a-Global-AI-Nanny-to-Delay-the-Singularity-Until-its-Better-Understood.pdf\">Should Humanity Build a Global AI Nanny to Delay the Singularity Until its Better Understood?</a></small></li>\n<li><small>Good (1959). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Speculations-on-perceptrons-and-other-automata.pdf\">Speculations on perceptrons and other automata</a>.</small></li>\n<li><small>Good (1965). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf\">Speculations concerning the first ultraintelligent machine</a>.</small></li>\n<li><small>Good (1970). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Some-future-social-repurcussions-of-computers.pdf\">Some future social repercussions of computers</a>.</small></li>\n<li><small>Good (1982). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Ethical-Machines.pdf\">Ethical machines</a>.</small></li>\n<li><small>Hall (2000). <a href=\"http://autogeny.org/ethics.html\"> Ethics for Machines</a>.</small></li>\n<li><small>Hanson (1994). <a href=\"http://hanson.gmu.edu/uploads.html\"> If Uploads Come First: The crack of a future dawn</a>.</small></li>\n<li><small>Hanson (1998a). <a href=\"http://hanson.gmu.edu/fastgrow.html\"> Is a singularity just around the corner? What it takes to get explosive economic growth.</a>.</small></li>\n<li><small>Hanson (1998). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Hanson-Economic-growth-given-machine-intelligence-early.pdf\"> Economic Growth Given Machine Intelligence</a>.</small></li>\n<li><small>Hanson (2008). <a href=\"http://hanson.gmu.edu/collapse.pdf\"> Catastrophe, Social Collapse, and Human Extinction</a>.</small></li>\n<li><small>Hanson (2008a). <a href=\"http://hanson.gmu.edu/EconOfBrainEmulations.pdf\"> The Economics of Brain Emulations</a>.</small></li>\n<li><small>Hanson (2008b). <a href=\"http://hanson.gmu.edu/IEEESpectrum-6-08.pdf\"> Economics Of The Singularity</a>.</small></li>\n<li><small>Hanson (2012a). <a href=\"http://hanson.gmu.edu/ChalmersReply.html\"> Meet the new conflict, same as the old conflict.</a></small></li>\n<li><small>Hanson (2012b). Commentary on \"Intelligence Explosion: Evidence and Import\".</small></li>\n<li><small>Hanson &amp; Yudkowsky (2008). <a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\"> The Hanson-Yudkowsky AI-Foom Debate</a>.</small></li>\n<li><small>Harper (2000).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/06/Harper-Challenges-for-designing-intelligent-systems-for-safety-critical-applications.pdf\">Challenges for designing intelligent systems for safety critical applications</a>.</small></li>\n<li><small>Hibbard (2002). <em><a href=\"http://www.amazon.com/Super-Intelligent-Machines-International-Systems-Engineering/dp/0306473887/\">Super-Intelligent Machines</a></em>.</small></li>\n<li><small>Hutter (2001). <a href=\"http://www.hutter1.net/ai/paixi.pdf\"> Towards a Universal Theory of Arti\ufb01cial Intelligence based on Algorithmic Probability and Sequential Decisions</a>.</small></li>\n<li><small>Jackson (1998). <a href=\"http://www.amazon.com/From-Metaphysics-Ethics-Conceptual-Analysis/dp/0198250614/\"> From Metaphysics to Ethics: A Defence of Conceptual Analysis</a>.</small></li>\n<li><small>Joy (2000). <a href=\"http://www.wired.com/wired/archive/8.04/joy_pr.html\"> Why the Future Doesn't Need Us</a>.</small></li>\n<li><small>Kaas, Rayhawk, Salamon &amp; Salamon (2010). <a href=\"http://intelligence.org/upload/economic-implications.pdf\"> Economic Implications of Software Minds</a>.</small></li>\n<li><small>Kurzweil (1990). <a href=\"http://www.amazon.com/The-Age-Intelligent-Machines-Kurzweil/dp/0262610795/\">The Age of Intelligent Machines</a>.</small></li>\n<li><small>Kurzweil (1998). <a href=\"http://www.amazon.com/Age-Spiritual-Machines-Computers-Intelligence/dp/0140282025/\"> The Age of Spiritual Machines</a>.</small></li>\n<li><small>Kurzweil (2005). <a href=\"http://www.amazon.com/The-Singularity-Is-Near-Transcend/dp/0143037889/\"> The Singularity is Near</a>.</small></li>\n<li><small>Lampson (1979). <a href=\"http://www.cs.umd.edu/~jkatz/TEACHING/comp_sec_F04/downloads/confinement.pdf\"> A Note on the Confinement Problem</a>.</small></li>\n<li><small>Leslie (1998). <a href=\"http://www.amazon.com/End-World-Science-Ethics-Extinction/dp/0415184479/\"><em>The End of the World</em></a>.</small></li>\n<li><small>Levitt (1999). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/01/Levitt-Robot-Ethics-Value-Systems-and-Decision-Theoretic-Behaviors.pdf\">Robot ethics, value systems, and decision theoretic behaviors</a>.</small></li>\n<li><small>Lokhorst (2011). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Lokhorst-Computational-meta-ethics-toward-the-meta-ethical-robot.pdf\"> Computational Meta-Ethics. Towards the Meta-Ethical Robot.</a>.</small></li>\n<li><small>McCorduck (1979). <em><a href=\"http://www.amazon.com/Machines-Who-Think-Artificial-Intelligence/dp/1568812051/\">Machines Who Think</a></em>.</small></li>\n<li><small>Moravec (1988). <em><a href=\"http://www.amazon.com/Mind-Children-Future-Robot-Intelligence/dp/0674576187/\">Mind Children</a></em>.</small></li>\n<li><small>McCulloch (1952). <a href=\"http://www.vordenker.de/ggphilosophy/mcc_ethical.pdf\">Toward some circuitry of ethical robots</a>.</small></li>\n<li><small>McLaren (2005). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/McLaren-Lessons-in-Machine-Ethics-from-the-Perspective-of-Two-ComputationalModels-of-Ethical-Reasoning.pdf\"> Lessons in Machine Ethics from the Perspective of Two Computational Models of Ethical Reasoning</a>.</small></li>\n<li><small>Minsky (1984). <a href=\"http://web.media.mit.edu/~minsky/papers/TrueNames.Afterword.html\">Afterward for 'True Names'</a>.</small></li>\n<li><small>Moravec (1999). <a href=\"http://www.amazon.com/Robot-Mere-Machine-Transcendent-Mind/dp/0195136306/\"> Robot: Mere Machine to Transcendent Mind</a>.</small></li>\n<li><small>More (1998). <a href=\"http://hanson.gmu.edu/vc.html#more\"> Singularity Meets Economy.</a>.</small></li>\n<li><small>Muehlhauser (2011). <a href=\"http://lukeprog.com/SaveTheWorld.html\"> So You Want to Save the World</a>.</small></li>\n<li><small>Pearl (1989). <a href=\"http://www.amazon.com/Probabilistic-Reasoning-Intelligent-Systems-Plausible/dp/1558604790/\"> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</a>.</small></li>\n<li><small>Posner (2005). <a href=\"http://www.amazon.com/Catastrophe-Risk-Response-Richard-Posner/dp/0195306473/\"> Catastrophe: Risk and Response</a>.</small></li>\n<li><small>Rees (2004). <a href=\"http://www.amazon.com/Our-Final-Hour-Scientists-Warning/dp/0465068634/\"> Our Final Hour: A Scientist's Warning</a>.</small></li>\n<li><small>Regis (1991). <a href=\"http://www.amazon.com/Great-Mambo-Chicken-Transhuman-Condition/dp/0201567512/\"> Great Mambo Chicken and the Transhuman Condition</a>.</small></li>\n<li><small>Sandberg (2010). <a href=\"http://agi-conf.org/2010/wp-content/uploads/2009/06/agi10singmodels2.pdf\"> An overview of models of technological singularity</a>.</small></li>\n<li><small>Sandberg &amp; Bostrom (2008). <a href=\"http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf\"> Whole Brain Emulation. A Roadmap</a>.</small></li>\n<li><small>Sawyer (2007). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Sawyer-Robot-Ethics.pdf\"> Robot Ethics.</a>.</small></li>\n<li><small>Schmidhuber (1987). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Schmidhuber-Evolutionary-Principles-in-Self-Referential-Learning.pdf\"> Evolutionary principles in self-referential learning</a>.</small></li>\n<li><small>Schmidhuber et al. (1997). <a href=\"http://igitur-archive.library.uu.nl/math/2007-1219-220125/Wiering_97_shiftinginductivebias.pdf\"> Shifting Inductive Bias with Success Story Algorithm, Adaptive Levin Search, and Incremental Self-Improvement</a>.</small></li>\n<li><small>Shulman, Jonsson &amp; Tarleton. <a href=\"http://intelligence.org/upload/machine-ethics-superintelligence.pdf\"> Machine Ethics and Superintelligence</a>.</small></li>\n<li><small>Shulman, Sandberg. <a href=\"http://intelligence.org/upload/ECAPShulmanSandberg.pdf\"> Implications of a software\u2010limited singularity</a>.</small></li>\n<li><small>Sloman (1984). <a href=\"http://www.cs.bham.ac.uk/research/projects/cogaff/sloman-space-of-minds-84.pdf\">The structure of the space of possible minds</a>.</small></li>\n<li><small>Sobel (1999). <a href=\"http://www.unl.edu/philosop/people/faculty/sobel/DotheDesires.pdf\"> Do the desires of rational agents converge?</a>.</small></li>\n<li><small>Stuart &amp; Norvig. (1995). <a href=\"http://www.cs.berkeley.edu/~russell/aima1e.html\"> Artificial Intelligence: A Modern Approach</a>.</small></li>\n<li><small>Versenyi (1974). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Versenyi-Can-Robots-Be-Moral.pdf\">Can robots be moral?</a></small></li>\n<li><small>Vinge (1981). <a href=\"http://www.amazon.com/Names-Other-Dangers-Vernor-Vinge/dp/0671653636/\"> True Names and Other Dangers</a>.</small></li>\n<li><small>Waldrop (1987). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/01/Waldrop-A-Question-of-Responsibility.pdf\">A question of responsibility</a>.</small></li>\n<li><small>Wallach &amp; Allen (2009). <a href=\"http://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975/\"> Moral Machines: Teaching Robots Right from Wrong</a>.</small></li>\n<li><small>Warwick (1997). <em><a href=\"http://www.amazon.com/March-Machines-Breakthrough-Artificial-Intelligence/dp/0252072235/\">March of the Machines</a></em>.</small></li>\n<li><small>Whitby (1996). <em><a href=\"http://www.amazon.com/Reflections-Artificial-Intelligence-Blay-Whitby/dp/1871516684/\">Reflections on Artificial Intelligence</a></em>.</small></li>\n<li><small>Yudkowsky (1996). <a href=\"http://web.archive.org/web/200012042131/http://sysopmind.com/singularity.html\"> Staring into the Singularity</a>.</small></li>\n<li><small>Yudkowsky (2000). <a href=\"http://web.archive.org/web/200102020421/http://singinst.org/CaTAI.html\"> Coding a Transhuman AI 2.2.0</a>.</small></li>\n<li><small>Yudkowsky (2001a). <a href=\"http://intelligence.org/ourresearch/publications/GISAI/index.html\"> General Intelligence and Seed AI</a>.</small></li>\n<li><small>Yudkowsky (2001b). <a href=\"http://intelligence.org/upload/CFAI.html\"> Creating Friendly AI</a>.</small></li>\n<li><small>Yudkowsky (2010). <a href=\"http://intelligence.org/upload/TDT-v01o.pdf\"> Timeless Decision Theory</a>.</small></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "Xw6pxiicjuv6NJWjf": 2, "bY5MaF2EATwDkomvu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "i4susk4W3ieR5K92u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 43, "extendedScore": null, "score": 8.7e-05, "legacy": true, "legacyId": "14287", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Part of the series <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">AI Risk and Opportunity: A Strategic Analysis</a>.</small></p>\n<p>(You can leave anonymous feedback on posts in this series <strong><a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dDZ6d0RvM19qVkduX2pjNng4ZklHZXc6MQ\">here</a></strong>. I alone will read the comments, and may use them to improve past and forthcoming posts in this series.)</p>\n<p>This post chronicles the story of humanity's growing awareness of AI risk and opportunity, along with some recent AI safety efforts. I will not tackle any strategy questions directly in this post; my purpose today is merely to \"bring everyone up to speed.\"</p>\n<p>I know my post skips many important events and people. Please suggest additions in the comments, and include as much detail as possible.</p>\n<p>&nbsp;</p>\n<h3 id=\"Early_history\">Early history</h3>\n<p>Late in the Industrial Revolution, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Butler-Darwin-Among-the-Machines.pdf\">Samuel Butler (1863)</a> worried about what might happen when machines become more capable than the humans who designed them:</p>\n<blockquote>\n<p>...we are ourselves creating our own successors; we are daily adding to the beauty and delicacy of their physical organisation; we are daily giving them greater power and supplying by all sorts of ingenious contrivances that self-regulating, self-acting power which will be to them what intellect has been to the human race. In the course of ages we shall find ourselves the inferior race.</p>\n<p>...the time will come when the machines will hold the real supremacy over the world and its inhabitants...</p>\n</blockquote>\n<p>This basic idea was picked up by science fiction authors, for example in the 1921 Czech play that introduced the term \u201crobot,\u201d <em><a href=\"http://en.wikipedia.org/wiki/R.U.R.\">R.U.R.</a></em> In that play, robots grow in power and intelligence and destroy the entire human race, except for a single survivor.</p>\n<p>Another exploration of this idea is found in John W. Campbell\u2019s (1932) short story <em><a href=\"http://www.gutenberg.org/files/27462/27462-h/27462-h.htm\">The Last Evolution</a></em>, in which aliens attack Earth and the humans and aliens are killed but their machines survive and inherit the solar system. Campbell's (1935) short story <em><a href=\"http://www.technovelgy.com/ct/content.asp?Bnum=1030\">The Machine</a></em>&nbsp;contained perhaps the earlier description of recursive self-improvement:</p>\n<blockquote>\n<p>&nbsp;</p>\n<p>On the planet Dwranl, of the star you know as Sirius, a great race lived, and they were not too unlike you humans. ...they attained their goal of the machine that could think. And because it could think, they made several and put them to work, largely on scientific problems, and one of the obvious problems was how to make a better machine which could think.</p>\n<p>The machines had logic, and they could think constantly, and because of their construction never forgot anything they thought it well to remember. So the machine which had been set the task of making a better machine advanced slowly, and as it improved itself, it advanced more and more rapidly. The Machine which came to Earth is that machine.</p>\n<p>&nbsp;</p>\n</blockquote>\n<p>The concern for AI safety is most popularly identified with Isaac Asimov\u2019s <a href=\"http://en.wikipedia.org/wiki/Three_Laws_of_Robotics\">Three Laws of Robotics</a>, introduced in his short story <a href=\"http://en.wikipedia.org/wiki/Runaround\">Runaround</a>. Asimov used his stories, including those collected in the popular book <em><a href=\"http://en.wikipedia.org/wiki/I,_Robot\">I, Robot</a></em>, to illustrate many of the ways in which such well-meaning and seemingly comprehensive rules for governing robot behavior could go wrong.</p>\n<p>In the year of <em>I, Robot</em>\u2019s release, mathematician <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Turing-Computing-Machinery-and-Intelligence.pdf\">Alan Turing (1950)</a> noted that machines may one day be capable of whatever human intelligence can achieve:</p>\n<blockquote>\n<p>I believe that at the end of the century... one will be able to speak of machines thinking without expecting to be contradicted.</p>\n</blockquote>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Turing-Intelligent-Machinery-a-heretical-theory.pdf\">Turing (1951)</a> concluded:</p>\n<blockquote>\n<p>...it seems probable that once the machine thinking method has started, it would not take long to outstrip our feeble powers... At some stage therefore we should have to expect the machines to take control...</p>\n</blockquote>\n<p>Given the profound implications of machine intelligence, it's rather alarming that the early AI scientists who believed AI would be built during the 1950s-1970s didn't show much interest in AI safety. We are lucky they were wrong about the difficulty of AI \u2014 had they been right, humanity probably would not have been prepared to protect its interests.</p>\n<p>Later, statistician I.J. <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Speculations-on-perceptrons-and-other-automata.pdf\">Good (1959)</a>, who had worked with Turing to crack Nazi codes in World War II, reasoned that the transition from human control to machine control may be unexpectedly <em>sudden</em>:</p>\n<blockquote>\n<p>Once a machine is designed that is good enough\u2026 it can be put to work designing an even better machine. At this point an \"explosion\" will clearly occur; all the problems of science and technology will be handed over to machines and it will no longer be necessary for people to work. Whether this will lead to a Utopia or to the extermination of the human race will depend on how the problem is handled by the machines. The important thing will be to give them the aim of serving human beings.</p>\n</blockquote>\n<p>The more famous formulation of this idea, and the origin of the phrase \"intelligence explosion,\" is from <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf\">Good (1965)</a>:</p>\n<blockquote>\n<p>Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an \u201cintelligence explosion,\" and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make</p>\n</blockquote>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Some-future-social-repurcussions-of-computers.pdf\">Good (1970)</a> says that \"...by 1980 I hope that the implications and the safeguards [concerning machine superintelligence] will have been thoroughly discussed,\" and argues that an association devoted to discussing the matter be created. Unfortunately, no such association was created until either 1991 (Extropy Institute) or 2000 (Singularity Institute), and we might say these issues have not to this day been \"thoroughly\" discussed.</p>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Ethical-Machines.pdf\">Good (1982)</a> proposed a plan for the design of an ethical machine:</p>\n<blockquote>\n<p>I envisage a machine that would be given a large number of examples of human behaviour that other people called ethical, and examples of discussions of ethics, and from these examples and discussions the machine would formulate one or more consistent general theories of ethics, detailed enough so that it could deduce the probable consequences in most realistic situations.</p>\n</blockquote>\n<p>Even critics of AI like <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Schwartz-The-Limits-of-Artificial-Intelligence.pdf\">Jack Schwartz (1987)</a> saw the implications of intelligence that can improve itself:</p>\n<blockquote>\n<p>If artificial intelligences can be created at all, there is little reason to believe that initial successes could not lead swiftly to the construction of artificial superintelligences able to explore significant mathematical, scientific, or engineering alternatives at a rate far exceeding human ability, or to generate plans and take action on them with equally overwhelming speed. Since man's near-monopoly of all higher forms of intelligence has been one of the most basic facts of human existence throughout the past history of this planet, such developments would clearly create a new economics, a new sociology, and a new history.</p>\n</blockquote>\n<p><a href=\"http://world.std.com/~rjs/timesc.pdf\">Ray Solomonoff (1985)</a>, founder of algorithmic information theory, speculated on the implications of full-blown AI:</p>\n<blockquote>\n<p>After we have reached [human-level AI], it shouldn't take much more than ten years to construct ten thousand duplicates of our original [human-level AI], and have a total computing capability close to that of the computer science community...</p>\n<p>The last 100 years have seen the introduction of special and general relatively, automobiles, airplanes, quantum mechanics, large rockets and space travel, fission power, fusion bombs, lasers, and large digital computers. Any one of these might take a person years to appreciate and understand. Suppose that they had all been presented to mankind in a single year!</p>\n</blockquote>\n<p><a href=\"http://www.amazon.com/Mind-Children-Future-Robot-Intelligence/dp/0674576187/\">Moravec (1988)</a> argued that AI was an existential risk, but nevertheless, one toward which we must run (pp. 100-101):</p>\n<blockquote>\n<p>...intelligent machines... threaten our existence... Machines merely as clever as human beings will have enormous advantages in competitive situations... So why rush headlong into an era of intelligent machines? The answer, I believe, is that we have very little choice, if our culture is to remain viable... The universe is one random event after another. Sooner or later an unstoppable virus deadly to humans will evolve, or a major asteroid will collide with the earth, or the sun will expand, or we will be invaded from the stars, or a black hole will swallow the galaxy. The bigger, more diverse, and competent a culture is, the better it can detect and deal with external dangers. The larger events happen less frequently. By growing rapidly enough, a culture has a finite chance of surviving forever.</p>\n</blockquote>\n<p>Ray Kurzweil's <em><a href=\"http://www.amazon.com/The-Age-Intelligent-Machines-Kurzweil/dp/0262610795/\">The Age of Intelligent Machines</a></em> (1990) did not mention AI risk, and his followup, <em><a href=\"http://www.amazon.com/The-Age-Spiritual-Machines-Intelligence/dp/0140282025/\">The Age of Spiritual Machines</a></em> (1998) does so only briefly, in an \"interview\" between the reader and Kurzweil. The reader asks, \"So we risk the survival of the human race for [the opportunity AI affords us to expand our minds and advance our ability to create knowledge]?\" Kurzweil answers: \"Yeah, basically.\"</p>\n<p><a href=\"http://web.media.mit.edu/~minsky/papers/TrueNames.Afterword.html\">Minsky (1984)</a> pointed out the difficulty of getting machines to do what we want:</p>\n<blockquote>\n<p>...it is always dangerous to try to relieve ourselves of the responsibility of understanding exactly how our wishes will be realized. Whenever we leave the choice of means to any servants we may choose then the greater the range of possible methods we leave to those servants, the more we expose ourselves to accidents and incidents. When we delegate those responsibilities, then we may not realize, before it is too late to turn back, that our goals have been misinterpreted, perhaps even maliciously. We see this in such classic tales of fate as <em>Faust</em>, the <em>Sorcerer's Apprentice</em>, or the <em>Monkey's Paw</em> by W.W. Jacobs.</p>\n<p>[Another]&nbsp;risk is exposure to the consequences of self-deception. It is always tempting to say to oneself... that \"I know what I would like to happen, but I can't quite express it clearly enough.\" However, that concept itself reflects a too-simplistic self-image, which portrays one's own self as [having] well-defined wishes, intentions, and goals. This pre-Freudian image serves to excuse our frequent appearances of ambivalence; we convince ourselves that clarifying our intentions is merely a matter of straightening-out the input-output channels between our inner and outer selves. The trouble is, we simply aren't made that way. <em>Our goals themselves are ambiguous</em>.</p>\n<p>The ultimate risk comes when [we] attempt to take that final step \u2014 of designing goal-achieving programs that are programmed to make themselves grow increasingly powerful, by self-evolving methods that augment and enhance their own capabilities. It will be tempting to do this, both to gain power and to decrease our own effort toward clarifying our own desires. If some genie offered you three wishes, would not your first one be, \"Tell me, please, what is it that I want the most!\" The problem is that, with such powerful machines, it would require but the slightest accident of careless design for them to place their goals ahead of [ours]. The machine's goals may be allegedly benevolent, as with the robots of <em>With Folded Hands,&nbsp;</em>by Jack Williamson, whose explicit purpose was allegedly benevolent: to protect us from harming ourselves, or as with the robot in <em>Colossus</em>, by D.H.Jones, who itself decides, at whatever cost, to save us from an unsuspected enemy. In the case of Arthur C. Clarke's HAL, the machine decides that the mission we have assigned to it is one we cannot properly appreciate. And in Vernor Vinge's computer-game fantasy, <em>True Names</em>, the dreaded Mailman... evolves new ambitions of its own.</p>\n</blockquote>\n<p>&nbsp;</p>\n<h3 id=\"The_Modern_Era\">The Modern Era</h3>\n<p>Novelist <a href=\"http://www.feedbooks.com/book/2011.pdf\">Vernor Vinge (1993)</a> popularized Good's \"intelligence explosion\" concept, and wrote the first novel about self-improving AI posing an existential threat: <em><a href=\"http://www.amazon.com/Fire-Upon-Deep-Zones-Thought/dp/0765329824/\">A Fire Upon the Deep</a></em>&nbsp;(1992). It was probably Vinge who did more than anyone else to spur discussions about AI risk, particularly in online communities like the <a href=\"http://www.lucifer.com/exi-lists/\">extropians mailing list</a> (since 1991) and <a href=\"http://sl4.org/\">SL4</a> (since 2000). Participants in these early discussions included several of today's leading thinkers on AI risk: <a href=\"http://hanson.gmu.edu/\">Robin Hanson</a>, <a href=\"http://yudkowsky.net/\">Eliezer Yudkowsky</a>, <a href=\"http://nickbostrom.com/\">Nick Bostrom</a>, <a href=\"http://www.aleph.se/Nada/\">Anders Sandberg</a>, and <a href=\"http://wp.goertzel.org/\">Ben Goertzel</a>. (Other posters included <a href=\"http://en.wikipedia.org/wiki/Peter_Thiel\">Peter Thiel</a>, <a href=\"http://en.wikipedia.org/wiki/FM-2030\">FM-2030</a>, <a href=\"http://en.wikipedia.org/wiki/Matrioshka_brain\">Robert Bradbury</a>, and <a href=\"http://en.wikipedia.org/wiki/Julian_Assange\">Julian Assange</a>.) Proposals like Friendly AI, Oracle AI, and Nanny AI were discussed here long before they were brought to greater prominence with academic publications (<a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Yudkowsky 2008</a>; <a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Armstrong et al. 2012</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Goertzel-Should-Humanity-Build-a-Global-AI-Nanny-to-Delay-the-Singularity-Until-its-Better-Understood.pdf\">Goertzel 2012</a>).</p>\n<p>Meanwhile, philosophers and AI researchers considered whether or not machines could have moral value, and how to ensure ethical behavior from less powerful machines or 'narrow AIs', a field of inquiry variously known as 'artificial morality' (<a href=\"http://www.amazon.com/dp/0415076919/\">Danielson 1992</a>; <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.16.722&amp;rep=rep1&amp;type=pdf\">Floridi &amp; Sanders 2004</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2009/08/Allen-Prolegomena-to-any-future-artificial-moral-agent.pdf\">Allen et al. 2000</a>), 'machine ethics' (<a href=\"http://autogeny.org/ethics.html\">Hall 2000</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/McLaren-Lessons-in-Machine-Ethics-from-the-Perspective-of-Two-ComputationalModels-of-Ethical-Reasoning.pdf\">McLaren 2005</a>; <a href=\"http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=34917\">Anderson &amp; Anderson 2006</a>), 'computational ethics' (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Allen-Calculated-morality-ethical-computing-in-the-limit.pdf\">Allen 2002</a>) and 'computational metaethics' (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Lokhorst-Computational-meta-ethics-toward-the-meta-ethical-robot.pdf\">Lokhorst 2011</a>), and 'robo-ethics' or 'robot ethics' (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Capurro-International-Review-of-Information-Ethics-Vol.-6-Ethics-in-Robotics.pdf\">Capurro et al. 2006</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Sawyer-Robot-Ethics.pdf\">Sawyer 2007</a>). This vein of research \u2014 what I'll call the 'machine ethics' literature \u2014 was recently summarized in two books: <a href=\"http://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975/\">Wallach &amp; Allen (2009)</a>; <a href=\"http://www.amazon.com/dp/0521112354/\">Anderson &amp; Anderson (2011)</a>. Thus far, there has been a significant communication gap between the machine ethics literature and the AI risk literature (<a href=\"http://books.google.com/books?id=oBb-lt3l4oYC&amp;lpg=PP1&amp;dq=robot%20ethics&amp;pg=PA55#v=onepage&amp;q&amp;f=false\">Allen and Wallach 2011</a>), excepting perhaps <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">Muehlhauser and Helm (2012)</a>.</p>\n<p>The topic of AI safety in the context of <em>existential risk</em> was left to the futurists who had participated in online discusses of AI risk and opportunity. Here, I must cut short my review and focus on just three (of many) important figures: Eliezer Yudkowksy, Robin Hanson, and Nick Bostrom. (Your author also apologizes for the fact that, because he works with Yudkowsky, Yudkowsky gets a more detailed treatment here than Hanson or Bostrom.)</p>\n<p>Other figures in the modern era of AI risk research include <a href=\"http://www.ssec.wisc.edu/~billh/homepage1.html\">Bill Hibbard</a> (<em><a href=\"http://www.amazon.com/Super-Intelligent-Machines-International-Systems-Engineering/dp/0306473887/\">Super-Intelligent Machines</a></em>) and <a href=\"http://wp.goertzel.org/\">Ben Goertzel</a> (\"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Goertzel-Should-Humanity-Build-a-Global-AI-Nanny-to-Delay-the-Singularity-Until-its-Better-Understood.pdf\">Should Humanity Build a Global AI Nanny to Delay the Singularity Until It's Better Understood</a>\").</p>\n<p>&nbsp;</p>\n<h4 id=\"Eliezer_Yudkowsky\">Eliezer Yudkowsky</h4>\n<p>According to \"<a href=\"http://web.archive.org/web/20010205221413/http://sysopmind.com/eliezer.html\">Eliezer, the person</a>,\" <a href=\"http://yudkowsky.net/\">Eliezer Yudkowsky</a> (born 1979) was a bright kid \u2014 in the 99.9998th percentile of cognitive ability, according to the Midwest Talent Search. He read lots of science fiction as a child, and at age 11 read <em><a href=\"http://www.amazon.com/Great-Mambo-Chicken-Transhuman-Condition/dp/0201567512/\">Great Mambo Chicken and the Transhuman Condition</a></em> \u2014 his introduction to the impending reality of transhumanist technologies like AI and nanotech. The moment he became a Singularitarian was the moment he read page 47 of <em><a href=\"http://www.amazon.com/Names-Other-Dangers-Vernor-Vinge/dp/0671653636/\">True Names and Other Dangers</a></em> by Vernor Vinge:</p>\n<blockquote>\n<p>Here I had tried a straightforward extrapolation of technology, and found myself precipitated over an abyss. It's a problem we face every time we consider the creation of intelligences greater than our own. When this happens, human history will have reached a kind of singularity - a place where extrapolation breaks down and new models must be applied - and the world will pass beyond our understanding.</p>\n</blockquote>\n<p>Yudkowsky reported his reaction:</p>\n<blockquote>\n<p>My emotions at that moment are hard to describe; not fanaticism, or enthusiasm, just a vast feeling of \"Yep. He's right.\" I knew, in the moment I read that sentence, that this was how I would be spending the rest of my life.</p>\n</blockquote>\n<p>(As an aside, I'll note that this is eerily similar to <a href=\"/lw/8ib/connecting_your_beliefs_a_call_for_help/\">my own experience</a> of encountering the famous I.J. Good paragraph about ultraintelligence (quoted above), before I knew what \"transhumanism\" or \"the Singularity\" was. I read Good's paragraph and thought, \"Wow. That's... probably correct. How could I have missed that implication? \u2026 \u2026 \u2026 Well, <em>shit</em>. That changes <em>everything</em>.\")</p>\n<p>As a teenager in the mid 1990s, Yudkowsky participated heavily in Singularitarian discussions on the extropians mailing list, and in 1996 (at age 17) he wrote \"<a href=\"http://web.archive.org/web/200012042131/http://sysopmind.com/singularity.html\">Staring into the Singularity</a>,\" which gained him much attention, as did his popular \"<a href=\"http://web.archive.org/web/200012062142/http://www.sysopmind.com/tmol-faq/meaningoflife.html\">FAQ about the Meaning of Life</a>\" (1999).</p>\n<p>In 1998 Yudkowsky was invited (along with 33 others) by economist Robin Hanson to comment on <a href=\"http://www.feedbooks.com/book/2011.pdf\">Vinge (1993)</a>. Thirteen people (including Yudkowsky) left comments, then Vinge responded, and a final open discussion was held on the extropians mailing list. Hanson edited together these results <a href=\"http://hanson.gmu.edu/vi.html\">here</a>. Yudkowsky thought Max More's <a href=\"http://hanson.gmu.edu/vc.html#more\">comments</a> on Vinge underestimated how different from humans AI would probably be, and this prompted Yudkowsky to begin an early draft of \"<a href=\"http://web.archive.org/web/200102021657/http://sysopmind.com/AI_design.temp.html\">Coding a Transhuman AI</a>\" (CaTAI) which by 2000 had grown into the first large explication of his thoughts on \"Seed AI\" and \"friendly\" machine superintelligence (<a href=\"http://web.archive.org/web/200102020421/http://singinst.org/CaTAI.html\">Yudkowsky 2000</a>).</p>\n<p>Around this same time, Yudkowsky wrote \"<a href=\"http://web.archive.org/web/20010216233910/http://sysopmind.com/sing/PtS/contents.html\">The Plan to the Singularity</a>\" and \"<a href=\"http://web.archive.org/web/200102040955/http://sysopmind.com/sing/principles.html\">The Singularitarian Principles</a>,\" and launched the <a href=\"http://sl4.org/\">SL4 mailing list</a>.</p>\n<p>At a <a href=\"http://www.foresight.org/SrAssoc/spring2000/\">May 2000 gathering</a> hosted by the <a href=\"http://www.foresight.org/\">Foresight Institute</a>, Brian Atkins and Sabine Stoeckel discussed with Yudkowsky the possibility of launching an organization specializing in AI safety. In July of that year, Yudkowsky formed the Singularity Institute and began his full-time research on the problems of AI risk and opportunity.</p>\n<p>In 2001, he published two \"sequels\" to CaTAI, \"<a href=\"http://intelligence.org/ourresearch/publications/GISAI/index.html\">General Intelligence and Seed AI</a>\" and, most importantly, \"Creating Friendly AI\" (CFAI) (<a href=\"http://intelligence.org/upload/CFAI.html\">Yudkowsky 2001</a>).</p>\n<p>The publication of CFAI was a significant event, prompting Ben Goertzel (the pioneer of the new Artificial General Intelligence research community) to say that \"<em>Creating Friendly AI</em> is the most intelligent writing about AI that I've read in many years,\" and prompting Eric Drexler (the pioneer of molecular manufacturing) to write that \"With <em>Creating Friendly AI</em>, the Singularity Institute has begun to fill in one of the greatest remaining blank spots in the picture of humanity's future.\"</p>\n<p>CFAI was both frustrating and brilliant. It was frustrating because: (1) it was disorganized and opaque, (2) it invented new terms instead of using the terms being used by everyone else, for example speaking of \"supergoals\" and \"subgoals\" instead of final and instrumental goals, and speaking of goal systems but never \"utility functions,\" and (3) it hardly cited any of the relevant works in AI, philosophy, and psychology \u2014 for example it could have cited <a href=\"http://www.vordenker.de/ggphilosophy/mcc_ethical.pdf\">McCulloch (1952)</a>, Good (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Speculations-on-perceptrons-and-other-automata.pdf\">1959</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Some-future-social-repurcussions-of-computers.pdf\">1970</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Ethical-Machines.pdf\">1982</a>), <a href=\"http://www.amazon.com/Other-Worlds-Than-Cecil-Maxwell/dp/0800861256/\">Cade (1966)</a>,&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Versenyi-Can-Robots-Be-Moral.pdf\">Versenyi (1974)</a>, <a href=\"http://www.amazon.com/The-Mighty-Micro-Microchip-Revolution/dp/0340259752/\">Evans (1979)</a>,&nbsp;<a href=\"http://www.cs.umd.edu/~jkatz/TEACHING/comp_sec_F04/downloads/confinement.pdf\">Lampson (1979)</a>, the <a href=\"/lw/hva/open_thread_july_115_2013/9d1h\">conversation with Ed Fredkin</a> in <a href=\"http://www.amazon.com/Machines-Who-Think-Artificial-Intelligence/dp/1568812051/\">McCorduck (1979)</a>,&nbsp;<a href=\"http://www.cs.bham.ac.uk/research/projects/cogaff/sloman-space-of-minds-84.pdf\">Sloman (1984)</a>,&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Schmidhuber-Evolutionary-Principles-in-Self-Referential-Learning.pdf\">Schmidhuber (1987)</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/01/Waldrop-A-Question-of-Responsibility.pdf\">Waldrop (1987)</a>,&nbsp;<a href=\"http://www.amazon.com/Probabilistic-Reasoning-Intelligent-Systems-Plausible/dp/1558604790/\">Pearl (1989)</a>, <a href=\"http://www.amazon.com/War-Intelligent-Machines-Manuel-Landa/dp/0942299752/\">De Landa (1991)</a>,&nbsp;<a href=\"http://www.amazon.com/Ai-Tumultuous-History-Artificial-Intelligence/dp/0465001041/\">Crevier (1993, ch. 12)</a>, Clarke (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Clarke-Asimovs-Laws-of-Robotics-implications-for-information-technology-part-1.pdf\">1993</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Clarke-Asimovs-Laws-of-Robotics-implications-for-information-technology-part-2.pdf\">1994</a>), <a href=\"http://www.cs.auckland.ac.nz/~nickjhay/papersuni/RoughDraft200207-best_of_SASEMAS.pdf\">Weld &amp; Etzioni (1994)</a>,&nbsp;<a href=\"http://www.amazon.com/The-Evolution-Of-Desire-Revised/dp/046500802X/\">Buss (1995)</a>, <a href=\"http://www.cs.berkeley.edu/~russell/aima1e.html\">Russell &amp; Norvig (1995)</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Gips-Towards-the-ethical-robot.pdf\">Gips (1995)</a>, <a href=\"http://www.amazon.com/Reflections-Artificial-Intelligence-Blay-Whitby/dp/1871516684/\">Whitby (1996)</a>,&nbsp;<a href=\"http://igitur-archive.library.uu.nl/math/2007-1219-220125/Wiering_97_shiftinginductivebias.pdf\">Schmidhuber et al. (1997)</a>, <a href=\"http://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262193981/\">Barto &amp; Sutton (1998)</a>, <a href=\"http://www.amazon.com/From-Metaphysics-Ethics-Conceptual-Analysis/dp/0198250614/\">Jackson (1998)</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/01/Levitt-Robot-Ethics-Value-Systems-and-Decision-Theoretic-Behaviors.pdf\">Levitt (1999)</a>,&nbsp;<a href=\"http://www.amazon.com/Robot-Mere-Machine-Transcendent-Mind/dp/0195136306/\">Moravec (1999)</a>, <a href=\"http://en.wikipedia.org/wiki/The_Age_of_Spiritual_Machines\">Kurzweil (1999)</a>, <a href=\"http://www.unl.edu/philosop/people/faculty/sobel/DotheDesires.pdf\">Sobel (1999)</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2009/08/Allen-Prolegomena-to-any-future-artificial-moral-agent.pdf\">Allen et al. (2000)</a>, <a href=\"http://arxiv.org/pdf/1106.0244.pdf\">Gordon (2000)</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/06/Harper-Challenges-for-designing-intelligent-systems-for-safety-critical-applications.pdf\">Harper (2000)</a>,&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Coleman-Android-arete-toward-a-virtue-ethic-for-computational-agents.pdf\">Coleman 2001</a>, and&nbsp;<a href=\"http://www.hutter1.net/ai/paixi.pdf\">Hutter (2001)</a>. These features still substantially characterize Yudkowsky's independent writing, e.g. see <a href=\"http://intelligence.org/upload/TDT-v01o.pdf\">Yudkowsky (2010)</a>. As late as January 2006, he still wrote that \"It is not that I have neglected to cite the existing major works on this topic, but that, to the best of my ability to discern, there are no existing major works to cite.\"</p>\n<p>On the other hand, CFAI was in many ways was brilliant, and it tackled many of the problems left mostly untouched by mainstream machine ethics researchers. For example, CFAI (but not the mainstream machine ethics literature) engaged the problems of: (1) radically self-improving AI, (2) AI as an existential risk, (3) hard takeoff, (4) the interplay of goal <a href=\"http://intelligence.org/upload/CFAI.html#challenge_content\">content, acquisition, and structure</a>, (5) <a href=\"http://intelligence.org/upload/CFAI.html#anthro_selfishness_pain_fof\">wireheading</a>, (6) <a href=\"http://intelligence.org/upload/CFAI.html#design_generic_stomp\">subgoal stomp</a>, (7) <a href=\"http://intelligence.org/upload/CFAI.html#design_structure_external\">external reference semantics</a>, (8) <a href=\"http://intelligence.org/upload/CFAI.html#design_structure_causal\">causal validity semantics</a>, and (9) <a href=\"http://intelligence.org/upload/CFAI.html#policy_policies_selective\">selective support</a> (which <a href=\"http://www.nickbostrom.com/existential/risks.html\">Bostrom (2002)</a> would later call \"differential technological development\").</p>\n<p>For many years, the Singularity Institute was little more than a vehicle for Yudkowsky's research. In 2002 he wrote \"Levels of Organization in General Intelligence,\" which later appeared in the <a href=\"http://www.amazon.com/Artificial-General-Intelligence-Cognitive-Technologies/dp/354023733X/\">first edited volume</a> on Artificial General Intelligence (AGI). In 2003 he wrote what would become the internet's most popular <a href=\"http://yudkowsky.net/rational/bayes\">tutorial on Bayes' Theorem</a>, followed in 2005 by \"<a href=\"http://yudkowsky.net/rational/technical\">A Technical Explanation of Technical Explanation</a>.\" In 2004 he explained his vision of a Friendly AI goal structure: \"<a href=\"http://intelligence.org/upload/CEV.html\">Coherent Extrapolated Volition</a>.\" In 2006 he wrote two chapters that would later appear in the volume <em><a href=\"http://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501/\">Global Catastrohpic Risks</a></em> volume from Oxford University Press (co-edited by Bostrom): \"<a href=\"http://intelligence.org/upload/cognitive-biases.pdf\">Cognitive Biases Potentially Affecting Judgment of Global Risks</a>\" and, what remains his \"classic\" article on the need for Friendly AI, \"<a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Artificial Intelligence as a Positive and Negative Factor in Global Risk</a>.</p>\n<p>In 2004, Tyler Emerson was hired as the Singularity Institute's executive director. Emerson brought on Nick Bostrom (then a post doctoral fellow at Yale), Christine Peterson (of the Foresight Institute), and others, as advisors. In February 2006, Paypal co-founder Peter Thiel donated $100,000 to the Singularity Institute, and, we might say, the Singularity Institute as we know it today was born.</p>\n<p>From 2005-2007, Yudkowsky worked at various times with <a href=\"/user/Marcello/\">Marcello Herreshoff</a>, <a href=\"http://www.cs.berkeley.edu/~nickjhay/\">Nick Hay</a> and <a href=\"http://www.spaceandgames.com/\">Peter de Blanc</a> on the technical problems of AGI necessary for technical FAI work, for example <a href=\"http://en.wikipedia.org/wiki/AIXI#Universal_artificial_intelligence\">creating AIXI</a>-like architectures, developing a reflective decision theory, and investigating limits inherent in self-reflection due to <a href=\"http://yudkowsky.net/rational/lobs-theorem\">L\u00f6b's Theorem</a>. Almost none of this research has been published, in part because of the desire not to accelerate AGI research without having made corresponding safety progress. (Marcello also worked with Eliezer during the summer of 2009.)</p>\n<p>Much of the Singularity Institute's work has been \"movement-building\" work. The institute's Singularity Summit, held annually since 2006, attracts technologists, futurists, and social entrepreneurs from around the world, bringing to their attention not only emerging and future technologies but also the basics of AI risk and opportunity. The Singularity Summit also gave the Singularity Institute much of its access to cultural, academic, and business elites.</p>\n<p>Another key piece of movement-building work was Yudkowsky's \"<a href=\"http://wiki.lesswrong.com/wiki/Sequences#Major_Sequences\">The Sequences</a>,\" which were written during 2006-2009. Yudkowsky blogged, almost daily, on the subjects of epistemology, language, cognitive biases, decision-making, quantum mechanics, metaethics, and artificial intelligence. These posts were originally published on a community blog about rationality, <em><a href=\"http://www.overcomingbias.com/\">Overcoming Bias</a></em> (which later became Hanson's personal blog). Later, Yudkowsky's posts were used as the seed material for a new group blog, <em><a href=\"/\">Less Wrong</a></em>.</p>\n<p>Yudkowsky's goal was to create a community of people who could avoid common thinking mistakes, change their minds in response to evidence, and generally think and act with an unusual degree of <a href=\"http://facingthesingularity.com/2011/from-skepticism-to-technical-rationality/\">Technical Rationality</a>. In CFAI he had pointed out that when it comes to AI, humanity may not have a second chance to get it right. So we can't run a series of intelligence explosion experiments and \"see what works.\" Instead, we need to predict in advance what we need to do to ensure a desirable future, and we need to overcome common thinking errors when doing so. (Later, Yudkowsky expanded his \"community of rationalists\" by writing the most popular Harry Potter fanfiction in the world, <em><a href=\"http://hpmor.com/\">Harry Potter and the Methods of Rationality</a></em>, and is currently helping to launch <a href=\"/lw/9hb/position_design_and_write_rationality_curriculum/\">a new organization</a> that will teach classes on the skills of rational thought and action.)</p>\n<p>This community demonstrated its usefulness in 2009 when Yudkowsky began <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">blogging</a> about some problems in decision theory related to the project of building a Friendly AI. Much like Tim Gowers' <a href=\"http://en.wikipedia.org/wiki/Timothy_Gowers#Polymath_Project\">Polymath Project</a>, these discussions demonstrated the power of collaborative problem-solving over the internet. The discussions led to a decision theory workshop and then a decision theory mailing list, which quickly became home to some of the most interesting work in decision theory anywhere in the world. Yudkowsky summarized some of his earlier results in \"<a href=\"http://intelligence.org/upload/TDT-v01o.pdf\">Timeless Decision Theory</a>\" (2010), and newer results have been posted to <em>Less Wrong</em>, for example <a href=\"/lw/8wc/a_model_of_udt_with_halting_oracles/\">A model of UDT with a halting oracle</a> and <a href=\"/lw/9o7/formulas_of_arithmetic_that_behave_like_decision/\">Formulas of arithmetic that behave like decision agents</a>.</p>\n<p>The Singularity Institute also built its community with a Visiting Fellows program that hosted groups of researchers for 1-3 months at a time. Together, both visiting fellows and newly hired research fellows produced several working papers between 2009 and 2011, including <a href=\"http://intelligence.org/upload/machine-ethics-superintelligence.pdf\">Machine Ethics and Superintelligence</a>, <a href=\"http://intelligence.org/upload/ECAPShulmanSandberg.pdf\">Implications of a Software-Limited Singularity</a>, <a href=\"http://intelligence.org/upload/economic-implications.pdf\">Economic Implications of Software Minds</a>, <a href=\"http://intelligence.org/upload/convergence-expected-utility-universal-ai.pdf\">Convergence of Expected Utility for Universal AI</a>, and <a href=\"http://arxiv.org/pdf/1105.3821v1.pdf\">Ontological Crises in Artificial Agents' Value Systems</a>.</p>\n<p>In 2011, then-president Michael Vassar left the Singularity Institute to help launch a <a href=\"http://www.medicineispersonal.com/\">personalized medicine company</a>, and research fellow <a href=\"http://lukeprog.com/\">Luke Muehlhauser</a> (the author of this document) <a href=\"/lw/8c3/qa_with_new_executive_director_of_singularity/\">took over leadership from Vassar, as Executive Director</a>. During this time, the Institute underwent a major overhaul to implement best practices for organizational process and management: it published its first <a href=\"http://intelligence.org/files/strategicplan2011.pdf\">strategic plan</a>, began to maintain its first donor database, adopted best practices for accounting and bookkeeping, updated its bylaws and articles of incorporation, adopted more standard roles for the Board of Directors and the Executive Director, held a series of strategic meetings to help decide the near-term goals of the organization, began to publish monthly progress reports to its <a href=\"http://intelligence.org/blog/\">blog</a>, started outsourcing more work, and began to work on more articles for peer-reviewed publications: as of March 2012, the Singularity Institute has more peer-reviewed publications forthcoming in 2012 than it had published in all of 2001-2011 <a href=\"/lw/axr/three_new_papers_on_ai_risk/627o\">combined</a>.</p>\n<p>Today, the Singularity Institute collaborates regularly with its (non-staff) <a href=\"http://intelligence.org/aboutus/researchassociates\">research associates</a>, and also with researchers at the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> at Oxford University (directed by Bostrom), which as of March 2012 is the world's only other major research institute largely focused on the problems of existential risk.</p>\n<p>&nbsp;</p>\n<h4 id=\"Robin_Hanson\">Robin Hanson</h4>\n<p>Whereas Yudkowsky has never worked in the for-profit world and had no formal education after high school, <a href=\"http://hanson.gmu.edu/home.html\">Robin Hanson</a> (born 1959) has a long and prestigious academic and professional history. Hanson took a B.S. in physics from U.C. Irvine in 1981, took an M.S. in physics and an M.A. in the conceptual foundations of science from U. Chicago in 1984, worked in artificial intelligence for Lockheed and NASA, got a Ph.D. in social science from Caltech in 1997, did a post-doctoral fellowship at U.C. Berkeley in Health policy from 1997-1999, and finally was made an assistant professor of economics at George Mason University in 1999. In economics, he is best known for conceiving of <a href=\"http://en.wikipedia.org/wiki/Prediction_market\">prediction markets</a>.</p>\n<p>When Hanson moved to California in 1984, he encountered the <a href=\"http://en.wikipedia.org/wiki/Project_Xanadu\">Project Xanadu</a> crowd and met Eric Drexler, who showed him an early draft of <em><a href=\"http://www.wowio.com/users/product.asp?BookId=503\">Engines of Creation</a></em>. This community discussed AI, nanotech, cryonics, and other transhumanist topics, and Hanson joined the extropians mailing list (along with many others from Project Xanadu) when it launched in 1991.</p>\n<p>Hanson has published several papers on the economics of whole brain emulations (what he calls \"ems\") and AI (<a href=\"http://hanson.gmu.edu/uploads.html\">1994</a>, <a href=\"http://hanson.gmu.edu/fastgrow.html\">1998a</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Hanson-Economic-growth-given-machine-intelligence-early.pdf\">1998b</a>, <a href=\"http://hanson.gmu.edu/collapse.pdf\">2008a</a>, <a href=\"http://hanson.gmu.edu/EconOfBrainEmulations.pdf\">2008b</a>, <a href=\"http://hanson.gmu.edu/IEEESpectrum-6-08.pdf\">2008c</a>, <a href=\"http://hanson.gmu.edu/ChalmersReply.html\">2012a</a>). His writings at <em><a href=\"http://www.overcomingbias.com/about\">Overcoming Bias</a></em> (launched November 2006) are perhaps even more influential, and cover a wide range of topics.</p>\n<p>Hanson's views on AI risk and opportunity differ from Yudkowsky's. First, Hanson sees the technological singularity and the human-machine conflict it may produce not as a unique event caused by the advent of AI, but as a natural consequence of \"the general fact that accelerating rates of change increase intergenerational conflicts\" (Hanson 2012b). Second, Hanson thinks an intelligence explosion will be slower and more gradual than Yudkowsky does, denying Yudkowsky's \"hard takeoff\" thesis (<a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">Hanson &amp; Yudkowsky 2008</a>).</p>\n<p>&nbsp;</p>\n<h4 id=\"Nick_Bostrom\">Nick Bostrom</h4>\n<p><a href=\"http://nickbostrom.com/\">Nick Bostrom</a> (born 1973) received a B.S. in philosophy, mathematics, mathematical logic, and artificial intelligence from the University of Goteborg in 1994, setting a national record in Sweden for undergraduate academic performance. He received an M.A. in philosophy and physics from from U. Stockholm in 1996, did work in astrophysics and computational neuroscience at King's College London, and received his Ph.D. from the London School of Economics in 2000. He went on to be a post-doctoral fellow at Yale University and in 2005 became the founding director of Oxford University's <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> (FHI). Without leaving FHI, he became the founding director of Oxford's <a href=\"http://www.futuretech.ox.ac.uk/\">Programme on the Impacts of Future Technology</a> (aka FutureTech) in 2011.</p>\n<p>Bostrom had long been interested in cognitive enhancement, and in 1995 he joined the extropians mailing list and learned about cryonics, uploading, AI, and other topics.</p>\n<p>Bostrom worked with British philosopher <a href=\"http://en.wikipedia.org/wiki/David_Pearce_(philosopher\">David Pearce</a>) to found the World Transhumanist Association (now called <a href=\"http://humanityplus.org/\">H+</a>) in 1998, with the purpose of developing a more mature and academically respectable form of transhumanism than was usually present on the extropians mailing list. During this time Bostrom wrote \"<a href=\"http://www.transhumanism.org/resources/FAQv21.pdf\">The Transhumanist FAQ</a>\" (now updated to version 2.1), with input from more than 50 others.</p>\n<p>His first philosophical publication was \"<a href=\"http://www.nickbostrom.com/old/predict.html\">Predictions from Philosophy? How philosophers could make themselves useful</a>\" (1997). In this paper, Bostrom proposed \"a new type of philosophy, a philosophy whose aim is prediction.\" On Bostrom's view, one role for the philosopher is to be a polymath who can engage in technological prediction and try to figure out how to steer the future so that humanity's goals are best met.</p>\n<p>Bostrom gave three examples of problems this new breed of philosopher-polymath could tackle: the <a href=\"http://en.wikipedia.org/wiki/Doomsday_argument\">Doomsday argument</a> and <a href=\"http://www.anthropic-principle.com/\">anthropics</a>, the <a href=\"http://en.wikipedia.org/wiki/Fermi_paradox\">Fermi paradox</a>, and superintelligence:</p>\n<blockquote>\n<p>What questions could a philosophy of superintelligence deal with? Well, questions like: How much would the predictive power for various fields increase if we increase the processing speed of a human-like mind a million times? If we extend the short-term or long-term memory? If we increase the neural population and the connection density? What other capacities would a superintelligence have? How easy would it be for it to rediscover the greatest human inventions, and how much input would it need to do so? What is the relative importance of data, theory, and intellectual capacity in various disciplines? Can we know anything about the motivation of a superintelligence? Would it be feasible to preprogram it to be good or philanthropic, or would such rules be hard to reconcile with the flexibility of its cognitive processes? Would a superintelligence, given the desire to do so, be able to outwit humans into promoting its own aims even if we had originally taken strict precautions to avoid being manipulated? Could one use one superintelligence to control another? How would superintelligences communicate with each other? Would they have thoughts which were of a totally different kind from the thoughts that humans can think? Would they be interested in art and religion? Would all superintelligences arrive at more or less the same conclusions regarding all important scientific and philosophical questions, or would they disagree as much as humans do? And how similar in their internal belief-structures would they be? How would our human self-perception and aspirations change if were forced to abdicate the throne of wisdom...? How would we individuate between superminds if they could communicate and fuse and subdivide with enormous speed? Will a notion of personal identity still apply to such interconnected minds? Would they construct an artificial reality in which to live? Could we upload ourselves into that reality? Could we then be able to compete with the superintelligences, if we were accelerated and augmented with extra memory etc., or would such profound reorganisation be necessary that we would no longer feel we were humans? Would that matter?</p>\n</blockquote>\n<p>Bostrom went on to examine some philosophical issues related to superintelligence, in \"Predictions from Philosophy\" and in \"<a href=\"http://www.nickbostrom.com/superintelligence.html\">How Long Before Superintelligence?</a>\" (1998), \"<a href=\"http://www.nickbostrom.com/existential/risks.html\">Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards</a>\" (2002), \"<a href=\"http://www.nickbostrom.com/ethics/ai.html\">Ethical Issues in Advanced Artificial Intelligence</a>\" (2003), \"<a href=\"http://www.nickbostrom.com/fut/evolution.html\">The Future of Human Evolution</a>\" (2004), and \"<a href=\"http://www.nickbostrom.com/ethics/artificial-intelligence.pdf\">The Ethics of Artificial Intelligence</a>\" (2012, coauthored with Yudkowsky). (He also played out the role of philosopher-polymath with regard to several other topics, including human enhancement and anthropic bias.)</p>\n<p>Bostrom's industriousness <a href=\"http://www.nickbostrom.com/cv.pdf\">paid off</a>:</p>\n<blockquote>\n<p>In 2009, [Bostrom] was awarded the Eugene R. Gannon Award (one person selected annually worldwide from the fields of philosophy, mathematics, the arts and other humanities, and the natural sciences). He has been listed in the FP 100 Global Thinkers list, the Foreign Policy Magazine\u02b9s list of the world\u02b9s top 100 minds. His writings have been translated into more than 21 languages, and there have been some 80 translations or reprints of his works. He has done more than 470 interviews for TV, film, radio, and print media, and he has addressed academic and popular audiences around the world.</p>\n</blockquote>\n<p>The other long-term member of the Future of Humanity Institute, Anders Sandberg, has also published some research on AI risk. Sandberg was a co-author on the <a href=\"http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf\">whole brain emulation roadmap</a> and \"<a href=\"http://www.nickbostrom.com/papers/anthropicshadow.pdf\">Anthropic Shadow</a>\", and also wrote \"<a href=\"http://agi-conf.org/2010/wp-content/uploads/2009/06/agi10singmodels2.pdf\">Models of the Technological Singularity</a>\" and several other papers.</p>\n<p>Recently, Bostrom and Sandberg were joined by <a href=\"http://www.fhi.ox.ac.uk/our_staff/research/stuart_armstrong\">Stuart Armstrong</a>, who wrote \"<a href=\"http://arxiv.org/pdf/1110.6437v2.pdf\">Anthropic Decision Theory</a>\" (2011) and was the lead author on \"<a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Thinking Inside the Box: Using and Controlling Oracle AI</a>\" (2012). He had previously written <em><a href=\"http://www.neweuropeancentury.org/GodAI.pdf\">Chaining God</a></em> (2007).</p>\n<p>For more than a year, Bostrom has been working on a new book titled <em>Superintelligence: A Strategic Analysis of the Coming Machine Intelligence Revolution</em>, which aims to sum up and organize much of the (published and unpublished) work done in the past decade by researchers at the Singularity Institute and FHI on the subject of AI risk and opportunity, as well as contribute new insights.</p>\n<p>&nbsp;</p>\n<h3 id=\"AI_Risk_Goes_Mainstream\">AI Risk Goes Mainstream</h3>\n<p>In 1997, professor of cybernetics Kevin Warwick published <em><a href=\"http://www.amazon.com/March-Machines-Breakthrough-Artificial-Intelligence/dp/0252072235/\">March of the Machines</a></em>, in which he predicted that within a couple decades, machines would become more intelligent than humans, and would pose an existential threat.</p>\n<p>In 2000, Sun Microsystems co-founder <a href=\"http://en.wikipedia.org/wiki/Bill_Joy\">Bill Joy</a> published \"<a href=\"http://en.wikipedia.org/wiki/Why_the_future_doesn%27t_need_us\">Why the Future Doesn't Need Us</a>\" in <em>Wired</em> magazine. In this widely-circulated essay, Joy argued that \"Our most powerful 21st-century technologies \u2014 robotics, genetic engineering, and nanotech \u2014 are threatening to make humans an endangered species.\" Joy advised that we relinquish development of these technologies rather than sprinting headlong into an arms race between destructive uses of these technologies and defenses against those destructive uses.</p>\n<p>Many people dismissed Bill Joy as a \"<a href=\"http://en.wikipedia.org/wiki/Neo-Luddite\">Neo-Luddite</a>,\" but many experts expressed similar concerns about human extinction, including philosopher John Leslie (<em><a href=\"http://www.amazon.com/End-World-Science-Ethics-Extinction/dp/0415184479/\">The End of the World</a></em>), physicist Martin Rees (<em><a href=\"http://www.amazon.com/Our-Final-Hour-Scientists-Warning/dp/0465068634/\">Our Final Hour</a></em>), legal theorist Richard Posner (<em><a href=\"http://www.amazon.com/Catastrophe-Risk-Response-Richard-Posner/dp/0195306473/\">Catastrophe: Risk and Response</a></em>), and the contributors to <em><a href=\"http://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501/\">Global Catastrophic Risks</a></em> (including Yudkowsky, Hanson, and Bostrom).</p>\n<p>Even Ray Kurzweil, known as an optimist about technology, devoted a chapter of his 2005 bestseller <em><a href=\"http://www.amazon.com/The-Singularity-Is-Near-Transcend/dp/0143037889/\">The Singularity is Near</a></em> to a discussion of existential risks, including risks from AI. Though discussing the possibility of existential catastrophe at length, his take on AI risk was cursory (p. 420):</p>\n<blockquote>\n<p>Inherently there will be no absolute protection against strong AI. Although the argument is subtle I believe that maintaining an open free-market system for incremental scientific and technological progress, in which each step is subject to market acceptance, will provide the most constructive environment for technology to embody widespread human values. As I have pointed out, strong AI is emerging from many diverse efforts and will be deeply integrated into our civilization's infrastructure. Indeed, it will be intimately embedded in our bodies and brains. As such, it will reflect our values because it will be us.</p>\n</blockquote>\n<p>AI risk finally became a \"mainstream\" topic in analytic philosophy with <a href=\"http://consc.net/papers/singularityjcs.pdf\">Chalmers (2010)</a> and an entire <a href=\"/r/discussion/lw/aif/journal_of_consciousness_studies_issue_on_the/\">issue</a> of <em>Journal of Consciousness Studies</em>&nbsp;devoted to the topic.</p>\n<p>The earliest popular discussion of machine superintelligence may have been in Christopher Evans' international bestseller <em><a href=\"http://www.amazon.com/The-Mighty-Micro-Computer-Revolution/dp/0575027088/\">The Mighty Micro</a></em> (1979), pages 194-198, 231-233, and 237-246.</p>\n<h3><br></h3>\n<h3 id=\"The_Current_Situation\">The Current Situation</h3>\n<p>Two decades have passed since the early transhumanists began to seriously discuss AI risk and opportunity on the extropians mailing list. (Before that, some discussions took place at the MIT AI lab, but that was before the web was popular, so they weren't recorded.) What have we humans done since then?</p>\n<p><em>Lots of talking</em>. Hundreds of thousands of man-hours have been invested into discussions on the extropians mailing list, SL4, <em>Overcoming Bias</em>, <em>Less Wrong</em>, the Singularity Institute's decision theory mailing list, several other internet forums, and also in meat-space (especially in the Bay Area near the Singularity Institute and in Oxford near FHI). These are difficult issues; talking them through is usually the first step to getting anything else done.</p>\n<p><em>Organization</em>. Mailing lists are a form of organization, as are organizations like The Singularity Institute and university departments like the FHI and FutureTech. Established organizations provide opportunities to bring people together, and to pool and direct resources efficiently.</p>\n<p><em>Resources</em>. Many people of considerable wealth, along with thousands of others of \"concerned citizens\" around the world, have decided that AI is the most significant risk and opportunity we face, and are willing to invest in humanity's future.</p>\n<p><em>Outreach</em>. Publications (both academic and popular), talks, and interactions with major and minor media outlets have been used to raise awareness of AI risk and opportunity. This has included outreach to specific AGI researchers, some of whom now take AI safety quite seriously. This also includes outreach to people in positions of influence who are in a position to engage in differential technological development. It also includes outreach to the rapidly growing \"optimal philanthropy\" community; a large fraction of those associated with <a href=\"http://www.givingwhatwecan.org/\">Giving What We Can</a> take existential risk \u2014 and AI risk in particular \u2014 quite seriously.</p>\n<p><em>Research</em>. So far, most research on the topic has been concerned with trying to become less confused about what, exactly, the problem <em>is</em>, how worried we should be, and which strategic actions we should take. How do we predict technological progress? How can we predict AI outcomes? Which interventions, taken now, would probably increase the odds of positive AI outcomes? There has also been some \"technical\" research in decision theory (e.g. <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">TDT</a>, <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">UDT</a>, <a href=\"http://arxiv.org/pdf/1110.6437v2.pdf\">ADT</a>), the math of AI goal systems (\"<a href=\"http://intelligence.org/upload/learning-what-to-value.pdf\">Learning What to Value\"</a>,\" \"<a href=\"http://arxiv.org/pdf/1105.3821v1.pdf\">Ontological Crises in Artificial Agents' Value Systems</a>,\" \"<a href=\"http://intelligence.org/upload/convergence-expected-utility-universal-ai.pdf\">Convergence of Expected Utility for Universal AI</a>\"), and Yudkowsky's unpublished research on Friendly AI.</p>\n<p><a href=\"http://lukeprog.com/SaveTheWorld.html\">Muehlhauser 2011</a> provides an overview of the categories of research problems we have left to solve. Most of the known problems aren't even well-defined at this point.</p>\n<p>&nbsp;</p>\n<p><strong id=\"References\">References</strong></p>\n<ul>\n<li><small>Allen et al. (2000). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2009/08/Allen-Prolegomena-to-any-future-artificial-moral-agent.pdf\"> Prolegomena to any future artificial moral agent</a>.</small></li>\n<li><small>Allen (2002). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Allen-Calculated-morality-ethical-computing-in-the-limit.pdf\"> Calculated morality: ethical computing in the limit.</a>.</small></li>\n<li><small>Anderson &amp; Anderson (2006). <a href=\"http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=34917\"> Guest Editors' Introduction: Machine Ethics. IEEE Intelligent Systems Magazine.</a>.</small></li>\n<li><small>Anderson &amp; Anderson (2011). <a href=\"http://www.amazon.com/dp/0521112354/\"> Machine Ethics.</a>.</small></li>\n<li><small>Armstrong (2007). <a href=\"http://www.neweuropeancentury.org/GodAI.pdf\">Chaining God: A qualitative approach to AI, trust and moral systems</a>.</small></li>\n<li><small>Armstrong (2011). <a href=\"http://arxiv.org/pdf/1110.6437v2.pdf\"> Anthropic decision theory for self-locating beliefs</a>.</small></li>\n<li><small>Armstrong, Sandberg &amp; Bostrom (2012). <a href=\"http://www.aleph.se/papers/oracleAI.pdf\"> Thinking Inside the Box: Using and Controlling Oracle AI</a>.</small></li>\n<li><small>Barto &amp; Sutton (1998). <a href=\"http://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262193981/\"> Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning)</a>.</small></li>\n<li><small>Bostrom (1997). <a href=\"http://www.nickbostrom.com/old/predict.html\"> Predictions from Philosophy? How philosophers could make themselves useful</a>.</small></li>\n<li><small>Bostrom (1998). <a href=\"http://www.nickbostrom.com/superintelligence.html\"> How Long Before Superintelligence?</a>.</small></li>\n<li><small>Bostrom (2002). <a href=\"http://www.nickbostrom.com/existential/risks.html\"> Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards</a>.</small></li>\n<li><small>Bostrom (2003). <a href=\"http://www.nickbostrom.com/ethics/ai.html\"> Ethical Issues in Advanced Artificial Intelligence</a>.</small></li>\n<li><small>Bostrom (2003). <a href=\"http://www.transhumanism.org/resources/FAQv21.pdf\"> The Transhumanist FAQ</a>.</small></li>\n<li><small>Bostrom (2004). <a href=\"http://www.nickbostrom.com/fut/evolution.html\"> The Future of Human Evolution</a>.</small></li>\n<li><small>Bostrom et al. (2011). <a href=\"http://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501/\"> Global Catastrophic Risks</a>.</small></li>\n<li><small>Bostrom &amp; Yudkowsky (2012). <a href=\"http://www.nickbostrom.com/ethics/artificial-intelligence.pdf\"> The Ethics of Artificial Intelligence</a>.</small></li>\n<li><small>Crevier (1993). <em><a href=\"http://www.amazon.com/Ai-Tumultuous-History-Artificial-Intelligence/dp/0465001041/\">AI: The Tumultous Search for Artificial Intelligence</a></em>.</small></li>\n<li><small>de Blanc (2009). <a href=\"http://intelligence.org/upload/convergence-expected-utility-universal-ai.pdf\"> Convergence of Expected Utility for Universal AI</a>.</small></li>\n<li><small>de Blanc (2011). <a href=\"http://arxiv.org/pdf/1105.3821v1.pdf\"> Ontological Crises in Artificial Agents' Value Systems</a>.</small></li>\n<li><small>Buss (1995). <a href=\"http://www.amazon.com/The-Evolution-Of-Desire-Revised/dp/046500802X/\"> The Evolution Of Desire</a>.</small></li>\n<li><small>Campbell (1932). <em><a href=\"http://www.gutenberg.org/files/27462/27462-h/27462-h.htm\">The Last Evolution</a></em>.</small></li>\n<li><small>Campbell (1935). <em><a href=\"http://www.technovelgy.com/ct/content.asp?Bnum=1030\">The Machine</a></em>.</small></li>\n<li><small>Capurro et al. (2006). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Capurro-International-Review-of-Information-Ethics-Vol.-6-Ethics-in-Robotics.pdf\"> Ethics in Robotics</a>.</small></li>\n<li><small>Chalmers (2010). <a href=\"http://consc.net/papers/singularityjcs.pdf\">The Singularity: A Philosophical Analysis</a>.</small></li>\n<li><small>Cirkovic, Sandberg, &amp; Bostrom (2010). <a href=\"http://www.nickbostrom.com/papers/anthropicshadow.pdf\">Anthropic Shadow: Observation selection effects and human extinction risks</a>.</small></li>\n<li><small>Clarke (1993). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Clarke-Asimovs-Laws-of-Robotics-implications-for-information-technology-part-1.pdf\"> Asimov's Laws of Robotics: Implications for Information Technology. Part 1.</a>.</small></li>\n<li><small>Clarke (1994). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Clarke-Asimovs-Laws-of-Robotics-implications-for-information-technology-part-2.pdf\"> Asimov's Laws of Robotics: Implications for Information Technology. Part 2.</a>.</small></li>\n<li><small>Colema (2001). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Coleman-Android-arete-toward-a-virtue-ethic-for-computational-agents.pdf\"> Android arete: Toward a virtue ethic for computational agents</a>.</small></li>\n<li><small>Danielson (1992). <a href=\"http://www.amazon.com/dp/0415076919/\"> Artificial Morality: Virtuous Robots for Virtual Games</a>.</small></li>\n<li><small>De Landa (1991). <em><a href=\"http://www.amazon.com/War-Intelligent-Machines-Manuel-Landa/dp/0942299752/\">War in the Age of Intelligent Machines</a></em>.</small></li>\n<li><small>Dewey (2011). <a href=\"http://intelligence.org/upload/learning-what-to-value.pdf\"> Learning What to Value</a>.</small></li>\n<li><small>Drexler (1986). <a href=\"http://www.wowio.com/users/product.asp?BookId=503\"> Engines of Creation</a>.</small></li>\n<li><small>Evans (1979). <em><a href=\"http://www.amazon.com/The-Mighty-Micro-Computer-Revolution/dp/0575027088/\">The Mighty Micro</a></em>.</small></li>\n<li><small>Floridi &amp; Sanders (2004). <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.16.722&amp;rep=rep1&amp;type=pdf\"> On the morality of artificial agents</a>.</small></li>\n<li><small>Goertzel (2012). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Goertzel-Should-Humanity-Build-a-Global-AI-Nanny-to-Delay-the-Singularity-Until-its-Better-Understood.pdf\">Should Humanity Build a Global AI Nanny to Delay the Singularity Until its Better Understood?</a></small></li>\n<li><small>Good (1959). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Speculations-on-perceptrons-and-other-automata.pdf\">Speculations on perceptrons and other automata</a>.</small></li>\n<li><small>Good (1965). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf\">Speculations concerning the first ultraintelligent machine</a>.</small></li>\n<li><small>Good (1970). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Some-future-social-repurcussions-of-computers.pdf\">Some future social repercussions of computers</a>.</small></li>\n<li><small>Good (1982). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Ethical-Machines.pdf\">Ethical machines</a>.</small></li>\n<li><small>Hall (2000). <a href=\"http://autogeny.org/ethics.html\"> Ethics for Machines</a>.</small></li>\n<li><small>Hanson (1994). <a href=\"http://hanson.gmu.edu/uploads.html\"> If Uploads Come First: The crack of a future dawn</a>.</small></li>\n<li><small>Hanson (1998a). <a href=\"http://hanson.gmu.edu/fastgrow.html\"> Is a singularity just around the corner? What it takes to get explosive economic growth.</a>.</small></li>\n<li><small>Hanson (1998). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Hanson-Economic-growth-given-machine-intelligence-early.pdf\"> Economic Growth Given Machine Intelligence</a>.</small></li>\n<li><small>Hanson (2008). <a href=\"http://hanson.gmu.edu/collapse.pdf\"> Catastrophe, Social Collapse, and Human Extinction</a>.</small></li>\n<li><small>Hanson (2008a). <a href=\"http://hanson.gmu.edu/EconOfBrainEmulations.pdf\"> The Economics of Brain Emulations</a>.</small></li>\n<li><small>Hanson (2008b). <a href=\"http://hanson.gmu.edu/IEEESpectrum-6-08.pdf\"> Economics Of The Singularity</a>.</small></li>\n<li><small>Hanson (2012a). <a href=\"http://hanson.gmu.edu/ChalmersReply.html\"> Meet the new conflict, same as the old conflict.</a></small></li>\n<li><small>Hanson (2012b). Commentary on \"Intelligence Explosion: Evidence and Import\".</small></li>\n<li><small>Hanson &amp; Yudkowsky (2008). <a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\"> The Hanson-Yudkowsky AI-Foom Debate</a>.</small></li>\n<li><small>Harper (2000).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/06/Harper-Challenges-for-designing-intelligent-systems-for-safety-critical-applications.pdf\">Challenges for designing intelligent systems for safety critical applications</a>.</small></li>\n<li><small>Hibbard (2002). <em><a href=\"http://www.amazon.com/Super-Intelligent-Machines-International-Systems-Engineering/dp/0306473887/\">Super-Intelligent Machines</a></em>.</small></li>\n<li><small>Hutter (2001). <a href=\"http://www.hutter1.net/ai/paixi.pdf\"> Towards a Universal Theory of Arti\ufb01cial Intelligence based on Algorithmic Probability and Sequential Decisions</a>.</small></li>\n<li><small>Jackson (1998). <a href=\"http://www.amazon.com/From-Metaphysics-Ethics-Conceptual-Analysis/dp/0198250614/\"> From Metaphysics to Ethics: A Defence of Conceptual Analysis</a>.</small></li>\n<li><small>Joy (2000). <a href=\"http://www.wired.com/wired/archive/8.04/joy_pr.html\"> Why the Future Doesn't Need Us</a>.</small></li>\n<li><small>Kaas, Rayhawk, Salamon &amp; Salamon (2010). <a href=\"http://intelligence.org/upload/economic-implications.pdf\"> Economic Implications of Software Minds</a>.</small></li>\n<li><small>Kurzweil (1990). <a href=\"http://www.amazon.com/The-Age-Intelligent-Machines-Kurzweil/dp/0262610795/\">The Age of Intelligent Machines</a>.</small></li>\n<li><small>Kurzweil (1998). <a href=\"http://www.amazon.com/Age-Spiritual-Machines-Computers-Intelligence/dp/0140282025/\"> The Age of Spiritual Machines</a>.</small></li>\n<li><small>Kurzweil (2005). <a href=\"http://www.amazon.com/The-Singularity-Is-Near-Transcend/dp/0143037889/\"> The Singularity is Near</a>.</small></li>\n<li><small>Lampson (1979). <a href=\"http://www.cs.umd.edu/~jkatz/TEACHING/comp_sec_F04/downloads/confinement.pdf\"> A Note on the Confinement Problem</a>.</small></li>\n<li><small>Leslie (1998). <a href=\"http://www.amazon.com/End-World-Science-Ethics-Extinction/dp/0415184479/\"><em>The End of the World</em></a>.</small></li>\n<li><small>Levitt (1999). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/01/Levitt-Robot-Ethics-Value-Systems-and-Decision-Theoretic-Behaviors.pdf\">Robot ethics, value systems, and decision theoretic behaviors</a>.</small></li>\n<li><small>Lokhorst (2011). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Lokhorst-Computational-meta-ethics-toward-the-meta-ethical-robot.pdf\"> Computational Meta-Ethics. Towards the Meta-Ethical Robot.</a>.</small></li>\n<li><small>McCorduck (1979). <em><a href=\"http://www.amazon.com/Machines-Who-Think-Artificial-Intelligence/dp/1568812051/\">Machines Who Think</a></em>.</small></li>\n<li><small>Moravec (1988). <em><a href=\"http://www.amazon.com/Mind-Children-Future-Robot-Intelligence/dp/0674576187/\">Mind Children</a></em>.</small></li>\n<li><small>McCulloch (1952). <a href=\"http://www.vordenker.de/ggphilosophy/mcc_ethical.pdf\">Toward some circuitry of ethical robots</a>.</small></li>\n<li><small>McLaren (2005). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/McLaren-Lessons-in-Machine-Ethics-from-the-Perspective-of-Two-ComputationalModels-of-Ethical-Reasoning.pdf\"> Lessons in Machine Ethics from the Perspective of Two Computational Models of Ethical Reasoning</a>.</small></li>\n<li><small>Minsky (1984). <a href=\"http://web.media.mit.edu/~minsky/papers/TrueNames.Afterword.html\">Afterward for 'True Names'</a>.</small></li>\n<li><small>Moravec (1999). <a href=\"http://www.amazon.com/Robot-Mere-Machine-Transcendent-Mind/dp/0195136306/\"> Robot: Mere Machine to Transcendent Mind</a>.</small></li>\n<li><small>More (1998). <a href=\"http://hanson.gmu.edu/vc.html#more\"> Singularity Meets Economy.</a>.</small></li>\n<li><small>Muehlhauser (2011). <a href=\"http://lukeprog.com/SaveTheWorld.html\"> So You Want to Save the World</a>.</small></li>\n<li><small>Pearl (1989). <a href=\"http://www.amazon.com/Probabilistic-Reasoning-Intelligent-Systems-Plausible/dp/1558604790/\"> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</a>.</small></li>\n<li><small>Posner (2005). <a href=\"http://www.amazon.com/Catastrophe-Risk-Response-Richard-Posner/dp/0195306473/\"> Catastrophe: Risk and Response</a>.</small></li>\n<li><small>Rees (2004). <a href=\"http://www.amazon.com/Our-Final-Hour-Scientists-Warning/dp/0465068634/\"> Our Final Hour: A Scientist's Warning</a>.</small></li>\n<li><small>Regis (1991). <a href=\"http://www.amazon.com/Great-Mambo-Chicken-Transhuman-Condition/dp/0201567512/\"> Great Mambo Chicken and the Transhuman Condition</a>.</small></li>\n<li><small>Sandberg (2010). <a href=\"http://agi-conf.org/2010/wp-content/uploads/2009/06/agi10singmodels2.pdf\"> An overview of models of technological singularity</a>.</small></li>\n<li><small>Sandberg &amp; Bostrom (2008). <a href=\"http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf\"> Whole Brain Emulation. A Roadmap</a>.</small></li>\n<li><small>Sawyer (2007). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Sawyer-Robot-Ethics.pdf\"> Robot Ethics.</a>.</small></li>\n<li><small>Schmidhuber (1987). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Schmidhuber-Evolutionary-Principles-in-Self-Referential-Learning.pdf\"> Evolutionary principles in self-referential learning</a>.</small></li>\n<li><small>Schmidhuber et al. (1997). <a href=\"http://igitur-archive.library.uu.nl/math/2007-1219-220125/Wiering_97_shiftinginductivebias.pdf\"> Shifting Inductive Bias with Success Story Algorithm, Adaptive Levin Search, and Incremental Self-Improvement</a>.</small></li>\n<li><small>Shulman, Jonsson &amp; Tarleton. <a href=\"http://intelligence.org/upload/machine-ethics-superintelligence.pdf\"> Machine Ethics and Superintelligence</a>.</small></li>\n<li><small>Shulman, Sandberg. <a href=\"http://intelligence.org/upload/ECAPShulmanSandberg.pdf\"> Implications of a software\u2010limited singularity</a>.</small></li>\n<li><small>Sloman (1984). <a href=\"http://www.cs.bham.ac.uk/research/projects/cogaff/sloman-space-of-minds-84.pdf\">The structure of the space of possible minds</a>.</small></li>\n<li><small>Sobel (1999). <a href=\"http://www.unl.edu/philosop/people/faculty/sobel/DotheDesires.pdf\"> Do the desires of rational agents converge?</a>.</small></li>\n<li><small>Stuart &amp; Norvig. (1995). <a href=\"http://www.cs.berkeley.edu/~russell/aima1e.html\"> Artificial Intelligence: A Modern Approach</a>.</small></li>\n<li><small>Versenyi (1974). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Versenyi-Can-Robots-Be-Moral.pdf\">Can robots be moral?</a></small></li>\n<li><small>Vinge (1981). <a href=\"http://www.amazon.com/Names-Other-Dangers-Vernor-Vinge/dp/0671653636/\"> True Names and Other Dangers</a>.</small></li>\n<li><small>Waldrop (1987). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2014/01/Waldrop-A-Question-of-Responsibility.pdf\">A question of responsibility</a>.</small></li>\n<li><small>Wallach &amp; Allen (2009). <a href=\"http://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975/\"> Moral Machines: Teaching Robots Right from Wrong</a>.</small></li>\n<li><small>Warwick (1997). <em><a href=\"http://www.amazon.com/March-Machines-Breakthrough-Artificial-Intelligence/dp/0252072235/\">March of the Machines</a></em>.</small></li>\n<li><small>Whitby (1996). <em><a href=\"http://www.amazon.com/Reflections-Artificial-Intelligence-Blay-Whitby/dp/1871516684/\">Reflections on Artificial Intelligence</a></em>.</small></li>\n<li><small>Yudkowsky (1996). <a href=\"http://web.archive.org/web/200012042131/http://sysopmind.com/singularity.html\"> Staring into the Singularity</a>.</small></li>\n<li><small>Yudkowsky (2000). <a href=\"http://web.archive.org/web/200102020421/http://singinst.org/CaTAI.html\"> Coding a Transhuman AI 2.2.0</a>.</small></li>\n<li><small>Yudkowsky (2001a). <a href=\"http://intelligence.org/ourresearch/publications/GISAI/index.html\"> General Intelligence and Seed AI</a>.</small></li>\n<li><small>Yudkowsky (2001b). <a href=\"http://intelligence.org/upload/CFAI.html\"> Creating Friendly AI</a>.</small></li>\n<li><small>Yudkowsky (2010). <a href=\"http://intelligence.org/upload/TDT-v01o.pdf\"> Timeless Decision Theory</a>.</small></li>\n</ul>", "sections": [{"title": "Early history", "anchor": "Early_history", "level": 1}, {"title": "The Modern Era", "anchor": "The_Modern_Era", "level": 1}, {"title": "Eliezer Yudkowsky", "anchor": "Eliezer_Yudkowsky", "level": 2}, {"title": "Robin Hanson", "anchor": "Robin_Hanson", "level": 2}, {"title": "Nick Bostrom", "anchor": "Nick_Bostrom", "level": 2}, {"title": "AI Risk Goes Mainstream", "anchor": "AI_Risk_Goes_Mainstream", "level": 1}, {"title": "The Current Situation", "anchor": "The_Current_Situation", "level": 1}, {"title": "References", "anchor": "References", "level": 3}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "49 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["i2XoqtYEykc4XWp9B", "kHL6qX9eArmvNWY99", "ifL8f4Xzy2D9Bb6zs", "Bj244uWzDBXvE2N2S", "yX9pMZik7r38da7Fc", "G65tLdGma8Xgh3p7L", "9DGfYpjYtpG4ACs8o"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-21T02:56:56.298Z", "modifiedAt": null, "url": null, "title": "Social status hacks from The Improv Wiki", "slug": "social-status-hacks-from-the-improv-wiki", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:01.600Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lsparrish", "createdAt": "2010-06-30T19:05:11.515Z", "isAdmin": false, "displayName": "lsparrish"}, "userId": "xgc8giekPig6tYf2X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PMZHfLuQaeFDMQwMx/social-status-hacks-from-the-improv-wiki", "pageUrlRelative": "/posts/PMZHfLuQaeFDMQwMx/social-status-hacks-from-the-improv-wiki", "linkUrl": "https://www.lesswrong.com/posts/PMZHfLuQaeFDMQwMx/social-status-hacks-from-the-improv-wiki", "postedAtFormatted": "Wednesday, March 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Social%20status%20hacks%20from%20The%20Improv%20Wiki&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASocial%20status%20hacks%20from%20The%20Improv%20Wiki%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPMZHfLuQaeFDMQwMx%2Fsocial-status-hacks-from-the-improv-wiki%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Social%20status%20hacks%20from%20The%20Improv%20Wiki%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPMZHfLuQaeFDMQwMx%2Fsocial-status-hacks-from-the-improv-wiki", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPMZHfLuQaeFDMQwMx%2Fsocial-status-hacks-from-the-improv-wiki", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1690, "htmlBody": "<p>I can't remember how I found this, just that I was amazed at how rational and near-mode it is on a topic where most of the information one usually encounters is hopelessly far.</p>\n<p>LessWrong wiki link on the same topic: <a href=\"http://wiki.lesswrong.com/wiki/Status\">http://wiki.lesswrong.com/wiki/Status</a></p>\n<blockquote>\n<p><a href=\"http://greenlightwiki.com/improv/TheImprovWiki\">The Improv Wiki</a></p>\n<h1>Status</h1>\n<p><em>Status</em> is pecking order. The person who is lower in status defers to the person who is higher in status.</p>\n<p>Status is party established by social position--e.g. boss and employee--but mainly by the way you interact. If you interact in a way that says you are not to be trifled with, the other person must adjust to you, then you are establishing high status. If you interact in a way that says you are willing to go along, you don't want responsibility, that's low status. A boss can play low status or high status. An employee can play low status or high status.</p>\n<p>Status is established in every line and gesture, and changes continuously. Status is something that one character plays <em>to</em> another at a particular moment. If you convey that the other person must not cross you on what you're saying now, then you are playing high status to that person in that line. Your very next line might come out low status, as you suggest willingness to defer about something else.</p>\n<p>If you analyze your most successful scenes, it's likely they involved several status changes between the players. Therefore, one path to great scenes is to intentionally change status. You can raise or lower your own status, or the status of the other player. The more subtly you can do this, the better the scene.</p>\n<h1>High-status behaviors</h1>\n<p>When walking, assuming that other people will get out of your path.</p>\n<p>Making eye contact while speaking.</p>\n<p>Not checking the other person's eyes for a reaction to what you said.</p>\n<p>Having no visible reaction to what the other person said. (Imagine saying something to a typical Clint Eastwood character. You say something expecting a reaction, and you get--nothing.)</p>\n<p>Speaking in complete sentences.</p>\n<p>Interrupting before you know what you are going to say.</p>\n<p>Spreading out your body to full comfort. Taking up a lot of space with your body.</p>\n<p>Looking at the other person with your eyes somewhat down (head tilted back a bit to make this work), creating the feeling that you are a parent talking to a child.</p>\n<p>Talking matter-of-factly about things that the other person finds displeasing or offensive.</p>\n<p>Letting your body be vulnerable, exposing your neck and torso to the other person.</p>\n<p>Moving comfortably and gracefully.</p>\n<p>Keeping your hands away from your face.</p>\n<p>Speaking authoritatively, with certainty.</p>\n<p>Making decisions for a group; taking responsibility.</p>\n<p>Giving or withholding permission.</p>\n<p>Evaluating other people's work.</p>\n<p>Speaking cryptically, not adjusting your speech to be easily understood by the other person (except that mumbling does not count). E.g. saying, \"Chomper not right\" with no explanation of what you mean or what you want the other person to do.</p>\n<p>Being surrounded by an entourage, especially of people who are physically smaller than you.</p>\n<p>A \"high-status specialist\" conveys in every word and gesture, \"Don't come near me, I bite.\"</p>\n<h1>Low-status behaviors</h1>\n<p>When walking, moving out of other people's path.</p>\n<p>Looking away from the other person's eyes.</p>\n<p>Briefly checking the other person's eyes to see if they reacted positively to what you said.</p>\n<p>Speaking in halting, incomplete sentences. Trailing off, editing your sentences as you got.</p>\n<p>Sitting or standing uncomfortably in order to adjust to the other person and give them space. Pulling inward to give the other person more room. If you're tall, you might need to scrunch down a bit to indicate that you're not going to use your height against the other person.</p>\n<p>Looking up toward the other person (head tilted forward a bit to make this work), creating the feeling that you are a child talking to a parent.</p>\n<p>Dancing around your words (beating around the bush) when talking about something that will displease the other person.</p>\n<p>Shouting as an attempt to intimidate the other person. This is low status because it suggests that you expect resistance.</p>\n<p>Crouching your body as if to ward off a blow; protecting your face, neck, and torso.</p>\n<p>Moving awkwardly or jerkily, with unnecessary movements.</p>\n<p>Touching your face or head.</p>\n<p>Avoiding making decisions for the group; avoiding responsibility.</p>\n<p>Needing permission before you can act.</p>\n<p class=\"changed\">Adjusting the way you say something to help the other person understand; meeting the other person on their (cognitive) ground; explaining yourself. E.g. \"Could you please adjust the chomper? That's the gadget on the kitchen counter immediately to the left of the toaster. If you just give it a slight rap on the top, that should adjust it.\"</p>\n<p>A \"low-status specialist\" conveys in every word and gesture, \"Please don't bite me, I'm not worth the trouble.\"</p>\n<h1>Raising another person's status</h1>\n<p>To raise another person's status is to establish them as high in the pecking order in your group (possibly just the two of you).</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Ask their permission to do something.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Ask their opinion about something.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Ask them for advice or help.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Express gratitude for something they did.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Apologize to them for something you did.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Agree that they are right and you were wrong.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Defer to their judgement without requiring proof.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Address them with a fancy title or honorific (even \"Mr.\" or \"Sir\" works very well).</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Downplay your own achievement or attribute in comparison to theirs. \"Your wedding cake is so much whiter than mine.\"</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Do something incompetent in front of them and then apologize for it or act sheepish about it.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Mention a failure or shortcoming of your own. \"I was supposed to go to an audition today, but I was late. They said I was wrong for the part anyway.\"</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Compliment them in a way that suggests appreciation, not judgement. \"Wow, what a beautiful cat you have!\"</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Obey them unquestioningly.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Back down in a conflict.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Move out of their way, bow to them, lower yourself before them.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Tip your hat to them.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Lose to them at something competitive, like a game (or any comparison).</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Wait for them.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Serve them; do manual labor for them.</td>\n</tr>\n</tbody>\n</table>\n<p><em>Tip:</em> Whenever you bring an audience member on stage, always raise their status, never lower it.</p>\n<h1>Lowering another person's status</h1>\n<p>To lower another person's status is to attack or discredit their right to be high in the pecking order. Another word for \"lowering someone's status\" is \"humiliating them.\"</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Criticize something they did.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Contradict them. Tell them they are wrong. Prove it with facts and logic.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Correct them.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Insult them.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Give them unsolicited advice.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Approve or disapprove of something they did or some attribute of theirs. \"Your cat has both nose and ear points. That is acceptable.\" Anything that sets you up as the judge lowers their status, even \"Nice work on the Milligan account, Joe.\"</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Shout at them.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Tell them what to do.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Ignore what they said and talk about something else, especially when they've said something that requires an answer. E.g. \"Have you seen my socks?\" \"The train leaves in five minutes.\"</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>One-up them. E.g. have a worse problem than the one they described, have a greater past achievement than theirs, have met a more famous celebrity, earn more money, do better than them at something they're good at, etc.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Win: beat them at something competitive, like a game (or any comparison).</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Announce something good about yourself or something you did. \"I went to an audition today, and I got the part!\"</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Disregard their opinion. E.g. \"You'd better not smoke while pumping gas, it's a fire hazard.\" Flick, light, puff, puff, pump, pump.</td>\n</tr>\n</tbody>\n</table>\n<table class=\"changed\" border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Talk sarcastically to them.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Make them wait for you.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>When they've fallen behind you, don't wait for them to catch up, just push on and get further out of sync.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Disobey them.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Violate their space.</td>\n</tr>\n</tbody>\n</table>\n<table class=\"changed\" border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Beat them up. Beating them up verbally, not physically as in martial arts or how you learned UFC fighting in an gym, in front of other people, especially their wife, girlfriend, and/or children, is particularly status-lowering.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>In a conflict, make them back down.</td>\n</tr>\n</tbody>\n</table>\n<table class=\"changed\" border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">&bull;</td>\n<td>Taunt them. Tease them.</td>\n</tr>\n</tbody>\n</table>\n<h2>The basic status-lowering act</h2>\n<p>Laugh at them. (Not with them.)</p>\n<h2>The basic status-raising act</h2>\n<p>Be laughed at by them.</p>\n<p>Second to that is laughing with them at someone else.</p>\n<p><em>(Notice that those are primarily what comedians do.)</em></p>\n<hr />\n<p>Note that behaviors that raise another person's status are not necessarily low-status behaviors, and behaviors that lower another person's status are not necessarily high-status behaviors. People at any status level raise and lower each other all the time. They can do so in ways that convey high or low status.</p>\n<p>For example, shouting at someone lowers their status but is itself a low-status behavior.</p>\n<hr />\n<p>Objects and environments also have high or low status, although this is seldom explored. So explore it. Make something cheap and inconsequential high status. (This fingernail clipping came from Graceland!) Or bring down the status of a high status item. (Casually toss a 2 carat diamond ring on your jewelry pile.)</p>\n</blockquote>\n<p><em>Source: <a href=\"http://greenlightwiki.com/improv/Status\">http://greenlightwiki.com/improv/Status</a><br />Retrieved 20 March 2012</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2EFq8dJbxKNzforjM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PMZHfLuQaeFDMQwMx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 46, "baseScore": 63, "extendedScore": null, "score": 0.000137, "legacy": true, "legacyId": "14285", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 63, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I can't remember how I found this, just that I was amazed at how rational and near-mode it is on a topic where most of the information one usually encounters is hopelessly far.</p>\n<p>LessWrong wiki link on the same topic: <a href=\"http://wiki.lesswrong.com/wiki/Status\">http://wiki.lesswrong.com/wiki/Status</a></p>\n<blockquote>\n<p><a href=\"http://greenlightwiki.com/improv/TheImprovWiki\">The Improv Wiki</a></p>\n<h1 id=\"Status\">Status</h1>\n<p><em>Status</em> is pecking order. The person who is lower in status defers to the person who is higher in status.</p>\n<p>Status is party established by social position--e.g. boss and employee--but mainly by the way you interact. If you interact in a way that says you are not to be trifled with, the other person must adjust to you, then you are establishing high status. If you interact in a way that says you are willing to go along, you don't want responsibility, that's low status. A boss can play low status or high status. An employee can play low status or high status.</p>\n<p>Status is established in every line and gesture, and changes continuously. Status is something that one character plays <em>to</em> another at a particular moment. If you convey that the other person must not cross you on what you're saying now, then you are playing high status to that person in that line. Your very next line might come out low status, as you suggest willingness to defer about something else.</p>\n<p>If you analyze your most successful scenes, it's likely they involved several status changes between the players. Therefore, one path to great scenes is to intentionally change status. You can raise or lower your own status, or the status of the other player. The more subtly you can do this, the better the scene.</p>\n<h1 id=\"High_status_behaviors\">High-status behaviors</h1>\n<p>When walking, assuming that other people will get out of your path.</p>\n<p>Making eye contact while speaking.</p>\n<p>Not checking the other person's eyes for a reaction to what you said.</p>\n<p>Having no visible reaction to what the other person said. (Imagine saying something to a typical Clint Eastwood character. You say something expecting a reaction, and you get--nothing.)</p>\n<p>Speaking in complete sentences.</p>\n<p>Interrupting before you know what you are going to say.</p>\n<p>Spreading out your body to full comfort. Taking up a lot of space with your body.</p>\n<p>Looking at the other person with your eyes somewhat down (head tilted back a bit to make this work), creating the feeling that you are a parent talking to a child.</p>\n<p>Talking matter-of-factly about things that the other person finds displeasing or offensive.</p>\n<p>Letting your body be vulnerable, exposing your neck and torso to the other person.</p>\n<p>Moving comfortably and gracefully.</p>\n<p>Keeping your hands away from your face.</p>\n<p>Speaking authoritatively, with certainty.</p>\n<p>Making decisions for a group; taking responsibility.</p>\n<p>Giving or withholding permission.</p>\n<p>Evaluating other people's work.</p>\n<p>Speaking cryptically, not adjusting your speech to be easily understood by the other person (except that mumbling does not count). E.g. saying, \"Chomper not right\" with no explanation of what you mean or what you want the other person to do.</p>\n<p>Being surrounded by an entourage, especially of people who are physically smaller than you.</p>\n<p>A \"high-status specialist\" conveys in every word and gesture, \"Don't come near me, I bite.\"</p>\n<h1 id=\"Low_status_behaviors\">Low-status behaviors</h1>\n<p>When walking, moving out of other people's path.</p>\n<p>Looking away from the other person's eyes.</p>\n<p>Briefly checking the other person's eyes to see if they reacted positively to what you said.</p>\n<p>Speaking in halting, incomplete sentences. Trailing off, editing your sentences as you got.</p>\n<p>Sitting or standing uncomfortably in order to adjust to the other person and give them space. Pulling inward to give the other person more room. If you're tall, you might need to scrunch down a bit to indicate that you're not going to use your height against the other person.</p>\n<p>Looking up toward the other person (head tilted forward a bit to make this work), creating the feeling that you are a child talking to a parent.</p>\n<p>Dancing around your words (beating around the bush) when talking about something that will displease the other person.</p>\n<p>Shouting as an attempt to intimidate the other person. This is low status because it suggests that you expect resistance.</p>\n<p>Crouching your body as if to ward off a blow; protecting your face, neck, and torso.</p>\n<p>Moving awkwardly or jerkily, with unnecessary movements.</p>\n<p>Touching your face or head.</p>\n<p>Avoiding making decisions for the group; avoiding responsibility.</p>\n<p>Needing permission before you can act.</p>\n<p class=\"changed\">Adjusting the way you say something to help the other person understand; meeting the other person on their (cognitive) ground; explaining yourself. E.g. \"Could you please adjust the chomper? That's the gadget on the kitchen counter immediately to the left of the toaster. If you just give it a slight rap on the top, that should adjust it.\"</p>\n<p>A \"low-status specialist\" conveys in every word and gesture, \"Please don't bite me, I'm not worth the trouble.\"</p>\n<h1 id=\"Raising_another_person_s_status\">Raising another person's status</h1>\n<p>To raise another person's status is to establish them as high in the pecking order in your group (possibly just the two of you).</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Ask their permission to do something.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Ask their opinion about something.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Ask them for advice or help.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Express gratitude for something they did.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Apologize to them for something you did.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Agree that they are right and you were wrong.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Defer to their judgement without requiring proof.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Address them with a fancy title or honorific (even \"Mr.\" or \"Sir\" works very well).</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Downplay your own achievement or attribute in comparison to theirs. \"Your wedding cake is so much whiter than mine.\"</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Do something incompetent in front of them and then apologize for it or act sheepish about it.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Mention a failure or shortcoming of your own. \"I was supposed to go to an audition today, but I was late. They said I was wrong for the part anyway.\"</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Compliment them in a way that suggests appreciation, not judgement. \"Wow, what a beautiful cat you have!\"</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Obey them unquestioningly.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Back down in a conflict.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Move out of their way, bow to them, lower yourself before them.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Tip your hat to them.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Lose to them at something competitive, like a game (or any comparison).</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Wait for them.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Serve them; do manual labor for them.</td>\n</tr>\n</tbody>\n</table>\n<p><em>Tip:</em> Whenever you bring an audience member on stage, always raise their status, never lower it.</p>\n<h1 id=\"Lowering_another_person_s_status\">Lowering another person's status</h1>\n<p>To lower another person's status is to attack or discredit their right to be high in the pecking order. Another word for \"lowering someone's status\" is \"humiliating them.\"</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Criticize something they did.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Contradict them. Tell them they are wrong. Prove it with facts and logic.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Correct them.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Insult them.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Give them unsolicited advice.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Approve or disapprove of something they did or some attribute of theirs. \"Your cat has both nose and ear points. That is acceptable.\" Anything that sets you up as the judge lowers their status, even \"Nice work on the Milligan account, Joe.\"</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Shout at them.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Tell them what to do.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Ignore what they said and talk about something else, especially when they've said something that requires an answer. E.g. \"Have you seen my socks?\" \"The train leaves in five minutes.\"</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>One-up them. E.g. have a worse problem than the one they described, have a greater past achievement than theirs, have met a more famous celebrity, earn more money, do better than them at something they're good at, etc.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Win: beat them at something competitive, like a game (or any comparison).</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Announce something good about yourself or something you did. \"I went to an audition today, and I got the part!\"</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Disregard their opinion. E.g. \"You'd better not smoke while pumping gas, it's a fire hazard.\" Flick, light, puff, puff, pump, pump.</td>\n</tr>\n</tbody>\n</table>\n<table class=\"changed\" border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Talk sarcastically to them.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Make them wait for you.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>When they've fallen behind you, don't wait for them to catch up, just push on and get further out of sync.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Disobey them.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Violate their space.</td>\n</tr>\n</tbody>\n</table>\n<table class=\"changed\" border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Beat them up. Beating them up verbally, not physically as in martial arts or how you learned UFC fighting in an gym, in front of other people, especially their wife, girlfriend, and/or children, is particularly status-lowering.</td>\n</tr>\n</tbody>\n</table>\n<table border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>In a conflict, make them back down.</td>\n</tr>\n</tbody>\n</table>\n<table class=\"changed\" border=\"0\">\n<tbody>\n<tr>\n<td class=\"bullet\">\u2022</td>\n<td>Taunt them. Tease them.</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"The_basic_status_lowering_act\">The basic status-lowering act</h2>\n<p>Laugh at them. (Not with them.)</p>\n<h2 id=\"The_basic_status_raising_act\">The basic status-raising act</h2>\n<p>Be laughed at by them.</p>\n<p>Second to that is laughing with them at someone else.</p>\n<p><em>(Notice that those are primarily what comedians do.)</em></p>\n<hr>\n<p>Note that behaviors that raise another person's status are not necessarily low-status behaviors, and behaviors that lower another person's status are not necessarily high-status behaviors. People at any status level raise and lower each other all the time. They can do so in ways that convey high or low status.</p>\n<p>For example, shouting at someone lowers their status but is itself a low-status behavior.</p>\n<hr>\n<p>Objects and environments also have high or low status, although this is seldom explored. So explore it. Make something cheap and inconsequential high status. (This fingernail clipping came from Graceland!) Or bring down the status of a high status item. (Casually toss a 2 carat diamond ring on your jewelry pile.)</p>\n</blockquote>\n<p><em>Source: <a href=\"http://greenlightwiki.com/improv/Status\">http://greenlightwiki.com/improv/Status</a><br>Retrieved 20 March 2012</em></p>", "sections": [{"title": "Status", "anchor": "Status", "level": 1}, {"title": "High-status behaviors", "anchor": "High_status_behaviors", "level": 1}, {"title": "Low-status behaviors", "anchor": "Low_status_behaviors", "level": 1}, {"title": "Raising another person's status", "anchor": "Raising_another_person_s_status", "level": 1}, {"title": "Lowering another person's status", "anchor": "Lowering_another_person_s_status", "level": 1}, {"title": "The basic status-lowering act", "anchor": "The_basic_status_lowering_act", "level": 2}, {"title": "The basic status-raising act", "anchor": "The_basic_status_raising_act", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "44 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-21T05:08:48.382Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Heat vs. Motion", "slug": "seq-rerun-heat-vs-motion", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NRsBc8CKQJTo34eTX/seq-rerun-heat-vs-motion", "pageUrlRelative": "/posts/NRsBc8CKQJTo34eTX/seq-rerun-heat-vs-motion", "linkUrl": "https://www.lesswrong.com/posts/NRsBc8CKQJTo34eTX/seq-rerun-heat-vs-motion", "postedAtFormatted": "Wednesday, March 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Heat%20vs.%20Motion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Heat%20vs.%20Motion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNRsBc8CKQJTo34eTX%2Fseq-rerun-heat-vs-motion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Heat%20vs.%20Motion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNRsBc8CKQJTo34eTX%2Fseq-rerun-heat-vs-motion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNRsBc8CKQJTo34eTX%2Fseq-rerun-heat-vs-motion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 206, "htmlBody": "<p>Today's post, <a href=\"/lw/p4/heat_vs_motion/\">Heat vs. Motion</a> was originally published on 01 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Heat_vs._Motion\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>For a very long time, people had a detailed understanding of kinetics, and they had a detailed understanding of heat. They understood concepts such as momentum and elastic rebounds, as well as concepts such as temperature and pressure. It took an extraordinary amount of work in order to understand things deeply enough to make us realize that heat and motion were really the same thing.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/azq/seq_rerun_angry_atoms/\">Angry Atoms</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NRsBc8CKQJTo34eTX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 8.693927908506471e-07, "legacy": true, "legacyId": "14291", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ne6Ra62FB9ACHGSuh", "bHt6RZJykesbdhKHf", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-21T06:59:22.043Z", "modifiedAt": null, "url": null, "title": "Simple but important ideas", "slug": "simple-but-important-ideas", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:34.653Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a3kv7AQuDhTqzhNrP/simple-but-important-ideas", "pageUrlRelative": "/posts/a3kv7AQuDhTqzhNrP/simple-but-important-ideas", "linkUrl": "https://www.lesswrong.com/posts/a3kv7AQuDhTqzhNrP/simple-but-important-ideas", "postedAtFormatted": "Wednesday, March 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Simple%20but%20important%20ideas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASimple%20but%20important%20ideas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa3kv7AQuDhTqzhNrP%2Fsimple-but-important-ideas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Simple%20but%20important%20ideas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa3kv7AQuDhTqzhNrP%2Fsimple-but-important-ideas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa3kv7AQuDhTqzhNrP%2Fsimple-but-important-ideas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 274, "htmlBody": "<p>Important ideas don't always require long explanations. Here's a famous example:</p>\n<blockquote>\n<p>Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an &ldquo;intelligence explosion,&rdquo; and the intelligence of man would be left far behind&hellip; Thus the first ultraintelligent machine is the last invention that man need ever make.</p>\n</blockquote>\n<p>This is a single paragraph on the third page of a <a href=\"https://dl-web.dropbox.com/get/Public/Speculations%20Concerning%20the%20First%20Ultraintelligent%20Machine%20.htm?w=a55740a2\">50 page report</a>. Maybe someone who's good at parsing 60s era academic English can tell us if the rest is any good.</p>\n<p>It seems like anyone who has an idea they want people to take seriously has to write a bunch about it. This is most apparent in popular nonfiction books, which are often bloated far beyond what it takes to communicate the core ideas.</p>\n<p>To correct this \"presentation length bias\", we can fight it from both ends:</p>\n<ul>\n<li>Remember that important ideas don't have to be in an important place, be said by an important person, or be an important length.</li>\n<li>Alert readers to important ideas that don't look important (e.g. \"This is a simple idea, but it seems important:\"). Do this especially if it's someone else's idea, since people are going to be reluctant to label their own ideas as important.</li>\n</ul>\n<div>If we get rid of this bias, the biggest win may be that people work harder to present important ideas concisely, since it won't cost them prestige anymore.</div>\n<div><br /></div>\n<div>See also: <a href=\"http://econlog.econlib.org/archives/2015/06/why_do_we_write.html\">Why do we write too much?</a></div>\n<div><br /></div>\n<div><br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"7mTviCYysGmLqiHai": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a3kv7AQuDhTqzhNrP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 27, "extendedScore": null, "score": 5.8e-05, "legacy": true, "legacyId": "14307", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-21T07:09:44.387Z", "modifiedAt": null, "url": null, "title": "Daily Low-Dose Aspirin, Round 2", "slug": "daily-low-dose-aspirin-round-2", "viewCount": null, "lastCommentedAt": "2020-02-03T19:26:45.099Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Q9oWZLLfJtXqhi5fq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7b3njJqE7fzurhFTM/daily-low-dose-aspirin-round-2", "pageUrlRelative": "/posts/7b3njJqE7fzurhFTM/daily-low-dose-aspirin-round-2", "linkUrl": "https://www.lesswrong.com/posts/7b3njJqE7fzurhFTM/daily-low-dose-aspirin-round-2", "postedAtFormatted": "Wednesday, March 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Daily%20Low-Dose%20Aspirin%2C%20Round%202&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADaily%20Low-Dose%20Aspirin%2C%20Round%202%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7b3njJqE7fzurhFTM%2Fdaily-low-dose-aspirin-round-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Daily%20Low-Dose%20Aspirin%2C%20Round%202%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7b3njJqE7fzurhFTM%2Fdaily-low-dose-aspirin-round-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7b3njJqE7fzurhFTM%2Fdaily-low-dose-aspirin-round-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 409, "htmlBody": "<p>(For Round 1, see <a href=\"/lw/5qm/living_forever_is_hard_or_the_gompertz_curve/477u\">this comment</a> from last year.)</p>\n<p>NYT: <strong><a href=\"http://www.nytimes.com/2012/03/21/health/research/studies-link-aspirin-daily-use-to-reduced-cancer-risk.html?_r=1&amp;hp\">Studies Link Daily Doses of Aspirin to Reduced Risk of Cancer</a></strong></p>\n<blockquote>\n<p>One of the new studies examined patient data from dozens of large, long-term randomized controlled trials involving tens of thousands of men and women. Researchers at the University of Oxford found that after three years of daily aspirin use, the risk of developing cancer was reduced by almost 25 percent when compared with a control group not taking aspirin. After five years, the risk of dying of cancer was reduced by 37 percent among those taking aspirin.</p>\n<p>A second paper that analyzed five large randomized controlled studies in Britain found that over six and a half years on average, daily aspirin use reduced the risk of metastatic cancer by 36 percent and the risk of adenocarcinomas &mdash; common solid cancers including colon, lung and prostate cancer &mdash; by 46 percent.</p>\n</blockquote>\n<p>The article is worth reading in its entirety, but here's an especially interesting paragraph:</p>\n<blockquote>\n<p>The new studies, however, also found that the risk of bleeding in aspirin users diminished over time, and that the risk of death from brain bleeds was actually lower in the aspirin users than in the comparison group.</p>\n</blockquote>\n<p>The evidence still isn't perfect, but the purpose of rationality is making good decisions with limited information. I am a healthy 28-year-old and these studies make me even more confident that taking daily low-dose aspirin is the right thing for me to do.</p>\n<p>On a related note, if society were more rational, I wouldn't have to be sad reading paragraphs like this one:</p>\n<blockquote>\n<p>Some cancer doctors commended the new research, saying said that despite the limitations of the analyses, no other long-term clinical trials of aspirin and cancer are likely to be done because of the enormous expense involved and the fact that aspirin is a cheap generic drug.</p>\n</blockquote>\n<p>Or these ones&nbsp;from&nbsp;<a href=\"http://www.nytimes.com/2012/03/21/health/tranexamic-acid-cheap-drug-is-found-to-staunch-bleeding.html?ref=health\">A Cheap Drug Is Found to Save Bleeding Victims</a>, published on the same day:</p>\n<blockquote>\n<p>For months, a simple generic drug has been saving lives on America&rsquo;s battlefields by slowing the bleeding of even gravely wounded soldiers.</p>\n<p>Even better, it is cheap. But its very inexpensiveness has slowed its entry into American emergency rooms, where it might save the lives of bleeding victims of car crashes, shootings and stabbings &mdash; up to 4,000 Americans a year, according to a recent study.</p>\n<p>Because there is so little profit in it, the companies that make it do not champion it.</p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7b3njJqE7fzurhFTM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 12, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "14308", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-21T13:38:58.280Z", "modifiedAt": null, "url": null, "title": "[Meetup] Reminder: Reason Rally Meetup in DC, this Saturday", "slug": "meetup-reminder-reason-rally-meetup-in-dc-this-saturday", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZHHHCLx2GwswMTSRr/meetup-reminder-reason-rally-meetup-in-dc-this-saturday", "pageUrlRelative": "/posts/ZHHHCLx2GwswMTSRr/meetup-reminder-reason-rally-meetup-in-dc-this-saturday", "linkUrl": "https://www.lesswrong.com/posts/ZHHHCLx2GwswMTSRr/meetup-reminder-reason-rally-meetup-in-dc-this-saturday", "postedAtFormatted": "Wednesday, March 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMeetup%5D%20Reminder%3A%20Reason%20Rally%20Meetup%20in%20DC%2C%20this%20Saturday&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMeetup%5D%20Reminder%3A%20Reason%20Rally%20Meetup%20in%20DC%2C%20this%20Saturday%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZHHHCLx2GwswMTSRr%2Fmeetup-reminder-reason-rally-meetup-in-dc-this-saturday%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMeetup%5D%20Reminder%3A%20Reason%20Rally%20Meetup%20in%20DC%2C%20this%20Saturday%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZHHHCLx2GwswMTSRr%2Fmeetup-reminder-reason-rally-meetup-in-dc-this-saturday", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZHHHCLx2GwswMTSRr%2Fmeetup-reminder-reason-rally-meetup-in-dc-this-saturday", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 195, "htmlBody": "<p>This Saturday is the Reason Rally in Washington DC, a gathering of people promoting secularism, skepticism, rationality, atheism and other related topics. It's a good opportunity to meet people who are close to but not familiar with the Less Wrong memeplex. There will be a Less Wrong meetup later in the evening.</p>\n<p><a rel=\"nofollow\" href=\"http://reasonrally.org/\">http://reasonrally.org</a></p>\n<p>The Rally itself starts at 10:00 AM, running till 4 PM. &nbsp;Shortly thereafter (i.e 4:15) Less Wrong folks will meet up by a (currently unchosen) distinctive landmark, and plan the evening. (Any plan we make in advance will likely change, since we don't know how many people are coming, don't know who else we might have met who was interested in hanging out with us, and there will probably be various other things going on we can't anticipate).</p>\n<p>If you're coming, you may want to send me a PM with your phone number so we can find each other during the Rally. (I'll be posting the chosen landmark here when we figure it out, but it may be easier to communicate by phone or text)</p>\n<p><strong><span class=\"author\"><a id=\"author_t1_5uq4\" href=\"/user/maia/\">Maia</a></span></strong>&nbsp;and&nbsp;<span class=\"author\"><a id=\"author_t1_5wye\" style=\"font-weight: bold;\" href=\"/user/PhilSchwartz/\">PhilSchwartz</a>&nbsp;had some crash space available. Send them a PM to inquire if they still have space</span></p>\n<p>The original meetup post is <a href=\"/meetups/6k\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZHHHCLx2GwswMTSRr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [], "voteCount": 0, "baseScore": 0, "extendedScore": null, "score": 8.696020425758606e-07, "legacy": true, "legacyId": "14317", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": null, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-21T17:49:59.496Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "slug": "meetup-fort-collins-colorado-meetup-wedneday-7pm-4", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/S8BLsMv69BBZAMxQK/meetup-fort-collins-colorado-meetup-wedneday-7pm-4", "pageUrlRelative": "/posts/S8BLsMv69BBZAMxQK/meetup-fort-collins-colorado-meetup-wedneday-7pm-4", "linkUrl": "https://www.lesswrong.com/posts/S8BLsMv69BBZAMxQK/meetup-fort-collins-colorado-meetup-wedneday-7pm-4", "postedAtFormatted": "Wednesday, March 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS8BLsMv69BBZAMxQK%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS8BLsMv69BBZAMxQK%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS8BLsMv69BBZAMxQK%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/82'>Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 March 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Another Wednesday, another opportunity for hanging out with the cool kids.</p>\n\n<p>Broodwar proleague, Creatine + dual-n-back, volatility, markets and feedback loops.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/82'>Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "S8BLsMv69BBZAMxQK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.697050351220945e-07, "legacy": true, "legacyId": "14318", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm\">Discussion article for the meetup : <a href=\"/meetups/82\">Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 March 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Another Wednesday, another opportunity for hanging out with the cool kids.</p>\n\n<p>Broodwar proleague, Creatine + dual-n-back, volatility, markets and feedback loops.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm1\">Discussion article for the meetup : <a href=\"/meetups/82\">Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-21T19:26:45.006Z", "modifiedAt": null, "url": null, "title": "What epistemic hygiene norms should there be?", "slug": "what-epistemic-hygiene-norms-should-there-be", "viewCount": null, "lastCommentedAt": "2022-02-11T12:10:56.758Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/C9JB2GCBTo8Srg8GN/what-epistemic-hygiene-norms-should-there-be", "pageUrlRelative": "/posts/C9JB2GCBTo8Srg8GN/what-epistemic-hygiene-norms-should-there-be", "linkUrl": "https://www.lesswrong.com/posts/C9JB2GCBTo8Srg8GN/what-epistemic-hygiene-norms-should-there-be", "postedAtFormatted": "Wednesday, March 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20epistemic%20hygiene%20norms%20should%20there%20be%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20epistemic%20hygiene%20norms%20should%20there%20be%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC9JB2GCBTo8Srg8GN%2Fwhat-epistemic-hygiene-norms-should-there-be%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20epistemic%20hygiene%20norms%20should%20there%20be%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC9JB2GCBTo8Srg8GN%2Fwhat-epistemic-hygiene-norms-should-there-be", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC9JB2GCBTo8Srg8GN%2Fwhat-epistemic-hygiene-norms-should-there-be", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 911, "htmlBody": "<p>The <a href=\"http://wiki.lesswrong.com/wiki/Epistemic_hygiene\">wiki entry for Epistemic Hygiene</a> defines the term as:</p>\n<p style=\"padding-left: 30px;\"><strong>Epistemic hygiene</strong> consists of practices meant to allow accurate beliefs to spread within a community and keep less accurate or biased beliefs contained. The practices are meant to serve an analogous purpose to normal hygiene and sanitation in containing disease.</p>\n<p>The term was coined in Steve Rayhawk's and Anna Salamon's post \"<a href=\"/lw/u/the_ethic_of_handwashing_and_community_epistemic/\">The ethic of hand-washing and community epistemic practice</a>\", and there have been several mentions of it around the site. But what, exactly, might good epistemic hygiene norms be?</p>\n<p>I'm especially interested in this question in the context of meetup groups. In <a href=\"/lw/4ul/less_wrong_nyc_case_study_of_a_successful/\">Less Wrong NYC: Case Study of a Successful Rationalist Chapter</a>, Cosmos writes:</p>\n<p style=\"padding-left: 30px;\"><strong>Epistemic privilege and meme-sharing:</strong> The most powerful aspect of a group of rationalists is that you have an entire class of people whose reasoning you trust. Division of labor arises naturally as each member has different interests, they all pursue a variety of skills and areas of expertise, which they can then bring back to the group. Even the lowest-level rationalists in the group can rapidly upgrade themselves by adopting winning heuristics from other group members. I cannot overstate the power of epistemic privilege. We have rapidly spread knowledge about metabolism, exercise, neuroscience, meditation, hypnosis, several systems of therapy... and don't forget the Dark Arts.</p>\n<p>This would imply that one way that a meetup group (or for that matter, any social group) could get really successful would be by adopting great epistemic hygiene norms. But unless everyone present has read most of the Sequences - which gets increasingly unlikely the more the group grows - even most LW meetups probably won't just spontaneously enforce such norms. Wouldn't it be great if there were an existing list of such norms, that each group could look through and then decide which ones they'd like to adopt?</p>\n<p>Here are some possible epistemic hygiene norms that I could find / come up with:</p>\n<ul>\n<li>Be honest about your evidence and about the actual causes of your beliefs. Valuable for distinguishing accurate from mistaken beliefs. (<a href=\"/lw/u/the_ethic_of_handwashing_and_community_epistemic/\">The ethic of hand-washing</a>)</li>\n<li>Focus attention on the evidence and on the actual causes of both your beliefs, and beliefs of the people you're talking with. (The ethic of hand-washing)</li>\n<li>Only pass on ideas that you've verified yourself. (Problematic, since any given individual can only verify a tiny fraction of all of their beliefs.) (The ethic of hand-washing)</li>\n<li>Explicitly separate &ldquo;individual impressions&rdquo; (impressions based only on evidence you've verified yourself) from &ldquo;beliefs&rdquo; (which include evidence from others&rsquo; impressions). (<a href=\"http://www.overcomingbias.com/2008/04/naming-beliefs.html\">Naming beliefs</a>)</li>\n<li>Give status for reasoned opinion-change in the face of good evidence, rather than considering the \"loser\" of a debate low-status. (The ethic of hand-washing)</li>\n<li>Leave the other person a line of retreat in <em>all </em>directions, avoiding pressures that might wedge them towards either <em>your </em>ideas <em>or their own</em>. (The ethic of hand-washing)</li>\n<li>Encourage people to present the strongest cases they can against their own ideas. (<a href=\"/lw/u/the_ethic_of_handwashing_and_community_epistemic/gq\">comment, Carl Shulman</a>)</li>\n<li>Encourage \"Why do I think that\" monologues. You elaborate on a thing you currently believe to be true by specifying the reasons you believe it, the reasons you believe the reasons, etc and trying to dig out the whole epistemological structure. (comment, <a href=\"/lw/u/the_ethic_of_handwashing_and_community_epistemic/hx\">JulianMorrison</a>)</li>\n<li> Find ways to track the sincerity and accuracy of what people have said in the past, and make such information widely available. (<a href=\"/lw/u/the_ethic_of_handwashing_and_community_epistemic/j7\">comment, mark_spottswood</a>)</li>\n<li>Don't trust evidence you don't remember the source of, even if you remember reading the primary source yourself. (<a href=\"/lw/6a/hygienic_anecdotes/\">Hygienic Anecdotes</a>)</li>\n<li>Be upfront about when you don't remember the source of your claim. (<a href=\"/lw/6a/hygienic_anecdotes/\">comment, PhilGoetz</a>)</li>\n<li>When asked a question, state the facts that led you to your conclusion, not the conclusion itself. (<a href=\"/lw/8sb/just_the_facts_maam/\">Just the facts, ma'am!</a>)</li>\n<li>If you basically agree with someone's argument, but want to point out a minor problem, start off your response with the words \"I agree with your conclusion\". (<a href=\"/lw/3y/support_that_sounds_like_dissent/\">Support That Sounds Like Dissent</a>)</li>\n<li>When agreeing with someone's claim, distinguish between \"I have independent evidence that should add to our confidence in the speaker's conclusion\" and \"based on the evidence others have presented, I now agree, but don't take my agreement as further reason to update\". (<a href=\"/lw/3y/support_that_sounds_like_dissent/2wt\">comment, AnnaSalamon</a>)</li>\n<li>Don't judge people for having bad ideas, only judge the ideas. (<a href=\"/lesswrong.com/lw/hn/your_rationality_is_my_business\">Your Rationality is My Business</a>)</li>\n<li>Be careful when recommending books on sources where you are not an expert, particularly when they're on highly controversial topics and happen to support your own conclusions. (<a href=\"/lw/5ph/liars_for_jesus/46h6\">comment, Vladimir_M</a>)</li>\n<li>Before you stake your argument on a point, ask yourself in advance what you would say if that point were decisively refuted. If you wouldn't actually change your mind, search for a point that you find more convincing. (<a href=\"http://www.cato-unbound.org/2011/09/07/eliezer-yudkowsky/is-that-your-true-rejection/\">Is That Your True Rejection? at CATO Unbound</a>)</li>\n<li>In discussions, presume the kinds of conditions that are the least convenient for your argument. (<a href=\"/lw/2k/the_least_convenient_possible_world/\">The Least Convenient Possible World</a>)</li>\n<li>If people are trying to figure out the truth, don't mistake their opinions about facts for statements of values. (<a href=\"/lw/1yz/levels_of_communication/\">Levels of communication</a>)</li>\n</ul>\n<p>And of course, there's a long list of norms that basically amount to \"don't be guilty of bias X\", e.g. \"avoid unnecessarily detailed stories about the future\", \"avoid fake explanations\", \"don't treat arguments as soldiers\", etc.</p>\n<p>Which of these norms do you consider the most valuable? Which seem questionable? Do you have any norms of your own to propose?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "GQyPQcdEQF4zXhJBq": 1, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "C9JB2GCBTo8Srg8GN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 21, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "14319", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZP2om2oWHPhvWP2Q3", "CsKboswS3z5iaiutC", "gBjAJvSEg6zn9eMya", "7YLuXtKqWiybJAmeo", "vHCetv8tx6LbRtfyc", "neQ7eXuaXpiYw7SBy", "gs8bZCmaWqDaus7Dr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-21T20:10:15.732Z", "modifiedAt": null, "url": null, "title": "Is community-collaborative article production possible?", "slug": "is-community-collaborative-article-production-possible", "viewCount": null, "lastCommentedAt": "2020-09-09T17:22:22.608Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rTD8WYxSuNDAFsRRB/is-community-collaborative-article-production-possible", "pageUrlRelative": "/posts/rTD8WYxSuNDAFsRRB/is-community-collaborative-article-production-possible", "linkUrl": "https://www.lesswrong.com/posts/rTD8WYxSuNDAFsRRB/is-community-collaborative-article-production-possible", "postedAtFormatted": "Wednesday, March 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20community-collaborative%20article%20production%20possible%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20community-collaborative%20article%20production%20possible%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrTD8WYxSuNDAFsRRB%2Fis-community-collaborative-article-production-possible%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20community-collaborative%20article%20production%20possible%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrTD8WYxSuNDAFsRRB%2Fis-community-collaborative-article-production-possible", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrTD8WYxSuNDAFsRRB%2Fis-community-collaborative-article-production-possible", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 876, "htmlBody": "<p>When I showed up at the Singularity Institute, I was surprised to find that 30-60 papers' worth of material was lying around in blog posts, mailing list discussions, and people's heads &mdash; but it had never been written up in clear, well-referenced academic articles.</p>\n<p>Why is this so? Writing such articles has many clear benefits:</p>\n<ul>\n<li>Clearly stated and well-defended arguments can persuade smart people to take AI risk seriously, creating additional supporters and collaborators for the Singularity Institute.</li>\n<li>Such articles can also improve the credibility of the organization as a whole, which is especially important for attracting funds from top-level social entrepreneurs and institutions like the Gates Foundation and Givewell.</li>\n<li>Laying out the arguments clearly and analyzing each premise can lead to new strategic insights that will help us understand how to purchase x-risk reduction most efficiently.</li>\n<li>Clear explanations can provide a platform on which researchers can build to produce new strategic and technical research results.</li>\n<li>Communicating clearly is what lets other people find errors in your reasoning.</li>\n<li>Communities can use articles to cut down on communication costs. When something is written up clearly, 1000 people can read a single article instead of needing to transmit the information by having several hundred personal conversations between 2-5 people.</li>\n</ul>\n<p>Of course, there are costs to writing articles, too. The single biggest cost is <em>staff time / opportunity cost</em>. An article like \"<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Intelligence Explosion: Evidence and Import</a>\" can require anywhere from 150-800 person-hours. That is 150-800 paid hours during which our staff is <em>not </em>doing other critically important things that collectively have a bigger positive impact than a single academic article is likely to have.</p>\n<p>So Louie Helm and Nick Beckstead and I sat down and asked, \"Is there a way we can buy these articles without such an egregious cost?\"</p>\n<p>We think there might be. Basically, we suspect that most of the work involved in writing these articles can be outsourced. Here's the process we have in mind:</p>\n<ol>\n<li>An SI staff member chooses a paper idea we need written up, then writes an abstract and some notes on the desired final content.</li>\n<li>SI pays <a href=\"http://www.gwern.net/\">Gwern</a>&nbsp;or another remote researcher to do a literature search-and-summary of relevant material, with pointers to other resources.</li>\n<li>SI posts a <strong>contest</strong> to LessWrong, inviting submissions of near-conference-level-quality articles that follow the provided abstract and notes on desired final content. Contestants benefit by starting with the results of Gwern's literature summary, and by knowing that they don't need to produce something as good as \"Intelligence Explosion: Evidence and Import\" to win the prize. First place wins $1200, 2nd place wins $500, and 3rd place wins $200.</li>\n<li>Submissions are due 1 month later. Submission are reviewed, and the authors of the best submissions are sent comments on what could be improved to maximize the chances of coming in first place.</li>\n<li>Revised articles are due 3 weeks after comments are received. Prizes are awarded.</li>\n<li>SI pays an experienced writer like <a href=\"/user/Yvain\">Yvain</a> or <a href=\"/user/Kaj_Sotala\">Kaj_Sotala</a> or someone similar to build up and improve the 1st place submission, borrowing the best parts from the other submissions, too.</li>\n<li>An SI staff member does a final pass, adding some content, making it more clearly organized and polished, etc. One of SI's remote editors does another pass to make the sentences more perfect.</li>\n<li>The paper is submitted to a journal or an edited volume, and is marked as being co-authored by (1) the key SI staff member who provided the seed ideas and guided each stage of the revisions and polishing, (2) the author of the winning submission, and (3) Gwern. (With thanks to contributions from the other contest participants whose submissions were borrowed from &mdash; unless huge pieces were borrowed, in which case they may be counted as an additional co-author.)</li>\n</ol>\n<p>If this method works, each paper may require only 50-150 hours of SI staff time per paper &mdash; a dramatic improvement! But this method has additional benefits:</p>\n<ul>\n<li>Members of the community who are capable of doing one piece of the process but not the other pieces get to contribute where they shine. (Many people can write okay-level articles but can't do efficient literature searches or produce polished prose, etc.)</li>\n<li>SI gets to learn more about the talent that exists in its community which hadn't yet been given the opportunity to flower. (We might be able to directly outsource future work to contest participants, and if one person wins three such contests, that's an indicator that we should consider hiring them.)</li>\n<li>Additional paid \"jobs\" (by way of contest money) are created for LW rationalists who have some domain expertise in singularity-related subjects.</li>\n<li>Many Less Wrongers are students in fields relevant to the subject matter of the papers that will be produced by this process, and this will give them an opportunity to co-author papers that can go on their CV.</li>\n<li>The community in general gets better at collaborating.</li>\n</ul>\n<p>This is, after all, more similar to how many papers would be produced by university departments, in which a senior researcher works with a team of students to produce papers.</p>\n<p>Feedback? Interest?</p>\n<p><small>(Not exactly the same, but see also the <a href=\"http://en.wikipedia.org/wiki/Polymath_Project\">Polymath Project</a>.)</small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1, "ipJwbLxhR83ZksN6Z": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rTD8WYxSuNDAFsRRB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 57, "extendedScore": null, "score": 0.000598267051676661, "legacy": true, "legacyId": "14092", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-21T21:03:17.051Z", "modifiedAt": null, "url": null, "title": "A Problem About Bargaining and Logical Uncertainty", "slug": "a-problem-about-bargaining-and-logical-uncertainty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:31.605Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oZwxY88NCCHffJuxM/a-problem-about-bargaining-and-logical-uncertainty", "pageUrlRelative": "/posts/oZwxY88NCCHffJuxM/a-problem-about-bargaining-and-logical-uncertainty", "linkUrl": "https://www.lesswrong.com/posts/oZwxY88NCCHffJuxM/a-problem-about-bargaining-and-logical-uncertainty", "postedAtFormatted": "Wednesday, March 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Problem%20About%20Bargaining%20and%20Logical%20Uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Problem%20About%20Bargaining%20and%20Logical%20Uncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoZwxY88NCCHffJuxM%2Fa-problem-about-bargaining-and-logical-uncertainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Problem%20About%20Bargaining%20and%20Logical%20Uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoZwxY88NCCHffJuxM%2Fa-problem-about-bargaining-and-logical-uncertainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoZwxY88NCCHffJuxM%2Fa-problem-about-bargaining-and-logical-uncertainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 431, "htmlBody": "<p>Suppose you wake up as a paperclip maximizer. Omega says \"I calculated the millionth digit of pi, and it's odd. If it had been even, I would have made the universe capable of producing either 10<sup>20</sup> paperclips or 10<sup>10</sup> staples, and given control of it to a staples maximizer. But since it was odd, I made the universe capable of producing 10<sup>10</sup> paperclips or 10<sup>20</sup> staples, and gave you control.\" You double check Omega's pi computation and your internal calculator gives the same answer.</p>\n<p>Then a staples maximizer comes to you and says, \"You should give me control of the universe, because before you knew the millionth digit of pi, you would have wanted to pre-commit to a deal where each of us would give the other control of the universe, since that gives you 1/2 probability of 10<sup>20</sup> paperclips instead of 1/2 probability of 10<sup>10</sup> paperclips.\"</p>\n<p>Is the staples maximizer right? If so, the general principle seems to be that we should act as if we had precommited to a deal we would have made in ignorance of logical facts we actually possess. But how far are we supposed to push this? What deal would you have made if you didn't know that the first digit of pi was odd, or if you didn't know that 1+1=2?</p>\n<p>On the other hand, suppose the staples maximizer is wrong. Does that mean you also shouldn't agree to exchange control of the universe before you knew the millionth digit of pi?</p>\n<p>To make this more relevant to real life, consider two humans negotiating over the goal system of an AI they're jointly building. They have a lot of ignorance about the relevant logical facts, like how smart/powerful the AI will turn out to be and how efficient it will be in implementing each of their goals. They could negotiate a solution now in the form of a weighted average of their&nbsp;utility&nbsp;functions, but the weights they choose now will likely turn out to be \"wrong\" in full view of the relevant logical facts (e.g., the actual shape of the utility-possibility frontier). Or they could program their utility functions into the AI separately, and let the AI determine the weights later using some formal <a href=\"/lw/2x8/lets_split_the_cake_lengthwise_upwise_and/\">bargaining solution</a> when it has more knowledge about the relevant logical facts. Which is the right thing to do? Or should they follow the staples maximizer's reasoning and bargain under the pretense that they know even less than they actually do?</p>\n<p><strong>Other Related Posts:</strong> <a href=\"/lw/179/counterfactual_mugging_and_logical_uncertainty/\">Counterfactual Mugging and Logical Uncertainty</a>, <a href=\"/lw/2xb/if_you_dont_know_the_name_of_the_game_just_tell/\">If you don't know the name of the game, just tell me what I mean to you</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JHYaBGQuuKHdwnrAK": 1, "dPPATLhRmhdJtJM2t": 1, "b8FHrKqyXuYGWc6vn": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oZwxY88NCCHffJuxM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 46, "extendedScore": null, "score": 0.000102, "legacy": true, "legacyId": "14290", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 46, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hCwFxBai3oNnxrM9v", "rqt8RSKPvh4GzYoqE", "W2ufY8ihDDWWqJA7h"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-22T00:29:03.184Z", "modifiedAt": null, "url": null, "title": "Modest Superintelligences", "slug": "modest-superintelligences", "viewCount": null, "lastCommentedAt": "2020-01-30T01:29:52.370Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KuBMKQnAsYBGP4rkZ/modest-superintelligences", "pageUrlRelative": "/posts/KuBMKQnAsYBGP4rkZ/modest-superintelligences", "linkUrl": "https://www.lesswrong.com/posts/KuBMKQnAsYBGP4rkZ/modest-superintelligences", "postedAtFormatted": "Thursday, March 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Modest%20Superintelligences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AModest%20Superintelligences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKuBMKQnAsYBGP4rkZ%2Fmodest-superintelligences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Modest%20Superintelligences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKuBMKQnAsYBGP4rkZ%2Fmodest-superintelligences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKuBMKQnAsYBGP4rkZ%2Fmodest-superintelligences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 359, "htmlBody": "<p>I'm skeptical about trying to build FAI, but not about trying to influence the Singularity in a positive direction. Some people may be skeptical even of the latter because they don't think the possibility of an intelligence explosion is a very likely one. I suggest that even if intelligence explosion turns out to be impossible, we can still reach a positive Singularity by building what I'll call \"modest superintelligences\", that is, superintelligent entities, capable of taking over the universe and preventing existential risks and Malthusian outcomes, whose construction does not require fast recursive self-improvement or other questionable assumptions about the nature of intelligence. This helps to establish a lower bound on the benefits of an organization that aims to strategically influence the outcome of the Singularity.</p>\n<ul>\n<li>MSI-1: 10<sup>5</sup> biologically cloned humans of von Neumann-level intelligence, highly educated and indoctrinated from birth to work collaboratively towards some goal, such as building MSI-2 (or equivalent)</li>\n<li>MSI-2: 10<sup>10</sup> whole brain emulations of von Neumann, each running at ten times human speed, with WBE-enabled institutional controls that increase group coherence/rationality (or equivalent)</li>\n<li>MSI-3: 10<sup>20</sup> copies of von Neumann WBE, each running at a thousand times human speed, with more advanced (to be invented) institutional controls and collaboration tools (or equivalent)</li>\n</ul>\n<p>(To recall what the actual von Neumann, who we might call MSI-0, accomplished, open his <a href=\"http://en.wikipedia.org/wiki/John_von_Neumann\">Wikipedia page</a> and scroll through the \"known for\" sidebar.)</p>\n<p>Building a MSI-1 seems to require a total cost on the order of $100 billion (assuming $10 million for each clone), which is comparable to the Apollo project, and about 0.25% of the annual Gross World Product. (For further comparison, note that Apple has a market capitalization of $561 billion, and annual profit of $25 billion.) In exchange for that cost, any nation that undertakes the project has a reasonable chance of obtaining an insurmountable lead in whatever technologies end up driving the Singularity, and with that a large measure of control over its outcome. If no better strategic options come along, lobbying a government to build MSI-1 and/or influencing its design and aims seems to be the least that a Singularitarian organization could do.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 1, "pGqRLe9bFDX2G2kXY": 1, "ac84EpK6mZbPLzmqj": 1, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KuBMKQnAsYBGP4rkZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 34, "extendedScore": null, "score": 7.8e-05, "legacy": true, "legacyId": "14292", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 100, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-22T03:06:35.554Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Brain Breakthrough! It's Made of Neurons!", "slug": "seq-rerun-brain-breakthrough-it-s-made-of-neurons", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F2iZnmwStDXPJJnZd/seq-rerun-brain-breakthrough-it-s-made-of-neurons", "pageUrlRelative": "/posts/F2iZnmwStDXPJJnZd/seq-rerun-brain-breakthrough-it-s-made-of-neurons", "linkUrl": "https://www.lesswrong.com/posts/F2iZnmwStDXPJJnZd/seq-rerun-brain-breakthrough-it-s-made-of-neurons", "postedAtFormatted": "Thursday, March 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Brain%20Breakthrough!%20It's%20Made%20of%20Neurons!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Brain%20Breakthrough!%20It's%20Made%20of%20Neurons!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF2iZnmwStDXPJJnZd%2Fseq-rerun-brain-breakthrough-it-s-made-of-neurons%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Brain%20Breakthrough!%20It's%20Made%20of%20Neurons!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF2iZnmwStDXPJJnZd%2Fseq-rerun-brain-breakthrough-it-s-made-of-neurons", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF2iZnmwStDXPJJnZd%2Fseq-rerun-brain-breakthrough-it-s-made-of-neurons", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 151, "htmlBody": "<p>Today's post, <a href=\"/lw/p5/brain_breakthrough_its_made_of_neurons/\">Brain Breakthrough! It's Made of Neurons!</a> was originally published on 01 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Brain_Breakthrough.21_It.27s_Made_of_Neurons.21\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Eliezer's contribution to Amazing Breakthrough Day.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/b0z/seq_rerun_heat_vs_motion/\">Heat vs. Motion</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F2iZnmwStDXPJJnZd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 8.69933482907221e-07, "legacy": true, "legacyId": "14335", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nzzNFcrSk7akQ9bwD", "NRsBc8CKQJTo34eTX", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-22T04:58:41.373Z", "modifiedAt": null, "url": null, "title": "Meetup : Houston Meetup", "slug": "meetup-houston-meetup-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cog", "createdAt": "2011-04-25T04:58:53.803Z", "isAdmin": false, "displayName": "Cog"}, "userId": "xkp87vCZ56dp2tWnN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p29XJTD7BGnRSL7Sy/meetup-houston-meetup-1", "pageUrlRelative": "/posts/p29XJTD7BGnRSL7Sy/meetup-houston-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/p29XJTD7BGnRSL7Sy/meetup-houston-meetup-1", "postedAtFormatted": "Thursday, March 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Houston%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Houston%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp29XJTD7BGnRSL7Sy%2Fmeetup-houston-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Houston%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp29XJTD7BGnRSL7Sy%2Fmeetup-houston-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp29XJTD7BGnRSL7Sy%2Fmeetup-houston-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 101, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/83'>Houston Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 March 2012 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2010 Commerce St, Houston, Tx. 77002</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Houston LW meetup group will be gathering this Sunday at 1PM to discuss the 4th chapter of ET Jaynes \"The Logic of Science\". We will also have one of our members discussing \"The 4 Hour Work Week\", which has received some good press on this forum.</p>\n\n<p>Anyone is welcome to join, and there will be breakfast foods available if you so desire. Bring some money for the tip jar if you want some food.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/83'>Houston Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p29XJTD7BGnRSL7Sy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.699795041554371e-07, "legacy": true, "legacyId": "14338", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Houston_Meetup\">Discussion article for the meetup : <a href=\"/meetups/83\">Houston Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 March 2012 01:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2010 Commerce St, Houston, Tx. 77002</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Houston LW meetup group will be gathering this Sunday at 1PM to discuss the 4th chapter of ET Jaynes \"The Logic of Science\". We will also have one of our members discussing \"The 4 Hour Work Week\", which has received some good press on this forum.</p>\n\n<p>Anyone is welcome to join, and there will be breakfast foods available if you so desire. Bring some money for the tip jar if you want some food.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Houston_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/83\">Houston Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Houston Meetup", "anchor": "Discussion_article_for_the_meetup___Houston_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Houston Meetup", "anchor": "Discussion_article_for_the_meetup___Houston_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-22T04:59:04.865Z", "modifiedAt": null, "url": null, "title": "Meetup : Madison Monday Meetup", "slug": "meetup-madison-monday-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fkP3zd3p5jj8b7PzJ/meetup-madison-monday-meetup", "pageUrlRelative": "/posts/fkP3zd3p5jj8b7PzJ/meetup-madison-monday-meetup", "linkUrl": "https://www.lesswrong.com/posts/fkP3zd3p5jj8b7PzJ/meetup-madison-monday-meetup", "postedAtFormatted": "Thursday, March 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Madison%20Monday%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Madison%20Monday%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfkP3zd3p5jj8b7PzJ%2Fmeetup-madison-monday-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Madison%20Monday%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfkP3zd3p5jj8b7PzJ%2Fmeetup-madison-monday-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfkP3zd3p5jj8b7PzJ%2Fmeetup-madison-monday-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 203, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/84'>Madison Monday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 March 2012 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1831 Monroe St, Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Show and tell, of various minor hacks and life improvements. Bonus points if you can actually demonstrate them. :)</p>\n\n<p>The idea: try to think of at least one clever trick that you've tried that has made your life better, and which you think would probably work for at least one or two other people in the meetup. This could be for pretty much any goal that several of us share: ways to improve diet, get better exercise, focus better, notice mistakes more readily, stay on task longer, become better motivated... whatever people might care about.</p>\n\n<p>The format:</p>\n\n<ol>\n<li>State the goal of the hack.</li>\n<li>Explain the overall hack.</li>\n<li>Tell about its benefits and downsides.</li>\n<li>Explain how, in some detail, someone else could try the same thing. (Break this, of course, if you have a really good reason to.)</li>\n</ol>\n\n<p>Try to keep each thing somewhere between 2 and 20 minutes. If there's any specific item that you can demonstrate, bring it. (Does this make sense? Let me know if I can better explain something.)</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/84'>Madison Monday Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fkP3zd3p5jj8b7PzJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.699796648927942e-07, "legacy": true, "legacyId": "14339", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Madison_Monday_Meetup\">Discussion article for the meetup : <a href=\"/meetups/84\">Madison Monday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 March 2012 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1831 Monroe St, Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Show and tell, of various minor hacks and life improvements. Bonus points if you can actually demonstrate them. :)</p>\n\n<p>The idea: try to think of at least one clever trick that you've tried that has made your life better, and which you think would probably work for at least one or two other people in the meetup. This could be for pretty much any goal that several of us share: ways to improve diet, get better exercise, focus better, notice mistakes more readily, stay on task longer, become better motivated... whatever people might care about.</p>\n\n<p>The format:</p>\n\n<ol>\n<li>State the goal of the hack.</li>\n<li>Explain the overall hack.</li>\n<li>Tell about its benefits and downsides.</li>\n<li>Explain how, in some detail, someone else could try the same thing. (Break this, of course, if you have a really good reason to.)</li>\n</ol>\n\n<p>Try to keep each thing somewhere between 2 and 20 minutes. If there's any specific item that you can demonstrate, bring it. (Does this make sense? Let me know if I can better explain something.)</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Madison_Monday_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/84\">Madison Monday Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Madison Monday Meetup", "anchor": "Discussion_article_for_the_meetup___Madison_Monday_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Madison Monday Meetup", "anchor": "Discussion_article_for_the_meetup___Madison_Monday_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-22T09:11:45.829Z", "modifiedAt": null, "url": null, "title": "Scenario analysis: semi-general AIs", "slug": "scenario-analysis-semi-general-ais", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:31.634Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vuiAiAkDhaNEGsLjo/scenario-analysis-semi-general-ais", "pageUrlRelative": "/posts/vuiAiAkDhaNEGsLjo/scenario-analysis-semi-general-ais", "linkUrl": "https://www.lesswrong.com/posts/vuiAiAkDhaNEGsLjo/scenario-analysis-semi-general-ais", "postedAtFormatted": "Thursday, March 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Scenario%20analysis%3A%20semi-general%20AIs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScenario%20analysis%3A%20semi-general%20AIs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvuiAiAkDhaNEGsLjo%2Fscenario-analysis-semi-general-ais%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Scenario%20analysis%3A%20semi-general%20AIs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvuiAiAkDhaNEGsLjo%2Fscenario-analysis-semi-general-ais", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvuiAiAkDhaNEGsLjo%2Fscenario-analysis-semi-general-ais", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 430, "htmlBody": "<p>Are there any essays anywhere that go in depth about scenarios where AIs become somewhat recursive/general in that they can write functioning code to solve diverse problems, but the AI reflection problem remains unsolved and thus limits the depth of recursion attainable by the AIs? Let's provisionally call such general but reflection-limited AIs <em>semi-general AIs</em>, or SGAIs. SGAIs might be of roughly smart-animal-level intelligence, e.g. have rudimentary communication/negotiation abilities and some level of ability to formulate narrowish plans of the sort that don't leave them susceptible to Pascalian self-destruction or wireheading or the like.</p>\n<p>At first blush, this scenario strikes me as Bad; AIs could take over all computers connected to the internet, totally messing stuff up as their goals/subgoals mutate and adapt to circumvent wireheading selection pressures, without being able to reach general intelligence. AIs might or might not cooperate with humans in such a scenario. I imagine any detailed existing literature on this subject would focus on computer security and intelligent computer \"viruses\"; does such literature exist, anywhere?</p>\n<p>I have various questions about this scenario, including:</p>\n<ul>\n<li>How quickly should one expect <a href=\"http://www.susanblackmore.co.uk/memetics/temes.htm\">temetic</a> selective sweeps to reach ~99% fixation?</li>\n<li>To what extent should SGAIs be expected to cooperate with humans in such a scenario? Would SGAIs be able to make plans&nbsp;that involve exchange of currency, even if they don't understand what currency is or how exactly it works?&nbsp;What do humans have to offer SGAIs?</li>\n<li>How confident can we be that SGAIs will or won't have enough oomph to FOOM once they saturate and optimize/corrupt all existing computing hardware?</li>\n<li>Assuming such a scenario doesn't immediately lead to a FOOM scenario, how bad is it? To what extent is its badness contingent on the capability/willingness of SGAIs to play nice with humans?</li>\n</ul>\n<div>Those are the questions that immediately spring to mind, but I'd like to see who else has thought about this and what they've already considered before I cover too much ground.</div>\n<div>My intuition says that thinking about SGAIs in terms of population genetics and microeconomics will somewhat counteract automatic tendencies to imagine cool stories rather than engage in dispassionate analysis. I'd like other suggestions for how to achieve that goal.</div>\n<div>I'm confused that I don't see people talking about this scenario very much; why is that? Why isn't it the default expected scenario among futurologists? Or have I just not paid close enough attention?&nbsp;Is there already a name for this class of AIs? Is the name better than \"semi-general AIs\"?</div>\n<div>Thanks for any suggestions/thoughts, and my apologies if this has already been discussed at length on LessWrong.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vuiAiAkDhaNEGsLjo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 3, "extendedScore": null, "score": 8.700834190772252e-07, "legacy": true, "legacyId": "14363", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-22T10:29:49.057Z", "modifiedAt": null, "url": null, "title": "Best shot at immortality?", "slug": "best-shot-at-immortality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:55.533Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tomme", "createdAt": "2012-03-14T19:51:35.247Z", "isAdmin": false, "displayName": "tomme"}, "userId": "uyGXufxNcB7WRQ63C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MQRiENzLFzsmhnpMp/best-shot-at-immortality", "pageUrlRelative": "/posts/MQRiENzLFzsmhnpMp/best-shot-at-immortality", "linkUrl": "https://www.lesswrong.com/posts/MQRiENzLFzsmhnpMp/best-shot-at-immortality", "postedAtFormatted": "Thursday, March 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Best%20shot%20at%20immortality%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABest%20shot%20at%20immortality%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMQRiENzLFzsmhnpMp%2Fbest-shot-at-immortality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Best%20shot%20at%20immortality%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMQRiENzLFzsmhnpMp%2Fbest-shot-at-immortality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMQRiENzLFzsmhnpMp%2Fbest-shot-at-immortality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 68, "htmlBody": "<p>What looks, at the moment, as the most feasible technology that can grant us immortality (e.g., mind uploading, cryonics)?</p>\n<p>I posed this question to a fellow transhumanist and he argued that cryonics is the answer, but I failed to grasp his explanation. Besides, I am still struggling to learn the basics of science and transhumanism, so it would be great if you could shed some light on my question.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MQRiENzLFzsmhnpMp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 6, "extendedScore": null, "score": 8.701152813946888e-07, "legacy": true, "legacyId": "14364", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 85, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-22T18:59:46.732Z", "modifiedAt": null, "url": null, "title": "Transhumanist philosopher David Pearce AMA on Reddit", "slug": "transhumanist-philosopher-david-pearce-ama-on-reddit", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:36.120Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "betterthanwell", "createdAt": "2009-02-28T00:39:39.875Z", "isAdmin": false, "displayName": "betterthanwell"}, "userId": "W9LCYGeCRvhd5aREo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GSRPBhXWs3LX9Su2D/transhumanist-philosopher-david-pearce-ama-on-reddit", "pageUrlRelative": "/posts/GSRPBhXWs3LX9Su2D/transhumanist-philosopher-david-pearce-ama-on-reddit", "linkUrl": "https://www.lesswrong.com/posts/GSRPBhXWs3LX9Su2D/transhumanist-philosopher-david-pearce-ama-on-reddit", "postedAtFormatted": "Thursday, March 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Transhumanist%20philosopher%20David%20Pearce%20AMA%20on%20Reddit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATranshumanist%20philosopher%20David%20Pearce%20AMA%20on%20Reddit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGSRPBhXWs3LX9Su2D%2Ftranshumanist-philosopher-david-pearce-ama-on-reddit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Transhumanist%20philosopher%20David%20Pearce%20AMA%20on%20Reddit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGSRPBhXWs3LX9Su2D%2Ftranshumanist-philosopher-david-pearce-ama-on-reddit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGSRPBhXWs3LX9Su2D%2Ftranshumanist-philosopher-david-pearce-ama-on-reddit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p>Transhumanist philosopher <a href=\"http://en.wikipedia.org/wiki/David_Pearce_(philosopher)\">David Pearce</a>&nbsp;co-founded Humanity+ with Nick Bostrom.</p>\n<p>He is <a href=\"http://www.reddit.com/user/davidcpearce\">currently</a> answering questions in an <a href=\"http://www.reddit.com/r/Transhuman/comments/r7dui/david_pearce_ama/\">AMA on reddit/r/transhumanism</a>.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GSRPBhXWs3LX9Su2D", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 10, "extendedScore": null, "score": 8.703249494882581e-07, "legacy": true, "legacyId": "14365", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-22T19:13:58.203Z", "modifiedAt": null, "url": null, "title": "Meetup : Monthly Bay Area meetup", "slug": "meetup-monthly-bay-area-meetup-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:30.915Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xmJrbBd9MempwPBhq/meetup-monthly-bay-area-meetup-0", "pageUrlRelative": "/posts/xmJrbBd9MempwPBhq/meetup-monthly-bay-area-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/xmJrbBd9MempwPBhq/meetup-monthly-bay-area-meetup-0", "postedAtFormatted": "Thursday, March 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Monthly%20Bay%20Area%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Monthly%20Bay%20Area%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxmJrbBd9MempwPBhq%2Fmeetup-monthly-bay-area-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Monthly%20Bay%20Area%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxmJrbBd9MempwPBhq%2Fmeetup-monthly-bay-area-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxmJrbBd9MempwPBhq%2Fmeetup-monthly-bay-area-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/85'>Monthly Bay Area meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 March 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2071 University Avenue, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey, it's time for another monthly meetup! There will be a Bay Area Less Wrong meetup in Berkeley this Saturday, at 7pm in Taiwan Restaurant. That's at 2071 University Avenue. (The Free Speech Cafe will be closed.) I hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/85'>Monthly Bay Area meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xmJrbBd9MempwPBhq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.703307800540087e-07, "legacy": true, "legacyId": "14366", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Monthly_Bay_Area_meetup\">Discussion article for the meetup : <a href=\"/meetups/85\">Monthly Bay Area meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 March 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2071 University Avenue, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey, it's time for another monthly meetup! There will be a Bay Area Less Wrong meetup in Berkeley this Saturday, at 7pm in Taiwan Restaurant. That's at 2071 University Avenue. (The Free Speech Cafe will be closed.) I hope to see you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Monthly_Bay_Area_meetup1\">Discussion article for the meetup : <a href=\"/meetups/85\">Monthly Bay Area meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Monthly Bay Area meetup", "anchor": "Discussion_article_for_the_meetup___Monthly_Bay_Area_meetup", "level": 1}, {"title": "Discussion article for the meetup : Monthly Bay Area meetup", "anchor": "Discussion_article_for_the_meetup___Monthly_Bay_Area_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-23T00:03:09.969Z", "modifiedAt": null, "url": null, "title": "[LINK] Freeman Dyson reviews \"Physics on the Fringe: Smoke Rings, Circlons, and Alternative Theories of Everything\"", "slug": "link-freeman-dyson-reviews-physics-on-the-fringe-smoke-rings", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:54.664Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hahtpDRysfyrq4FW4/link-freeman-dyson-reviews-physics-on-the-fringe-smoke-rings", "pageUrlRelative": "/posts/hahtpDRysfyrq4FW4/link-freeman-dyson-reviews-physics-on-the-fringe-smoke-rings", "linkUrl": "https://www.lesswrong.com/posts/hahtpDRysfyrq4FW4/link-freeman-dyson-reviews-physics-on-the-fringe-smoke-rings", "postedAtFormatted": "Friday, March 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Freeman%20Dyson%20reviews%20%22Physics%20on%20the%20Fringe%3A%20Smoke%20Rings%2C%20Circlons%2C%20and%20Alternative%20Theories%20of%20Everything%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Freeman%20Dyson%20reviews%20%22Physics%20on%20the%20Fringe%3A%20Smoke%20Rings%2C%20Circlons%2C%20and%20Alternative%20Theories%20of%20Everything%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhahtpDRysfyrq4FW4%2Flink-freeman-dyson-reviews-physics-on-the-fringe-smoke-rings%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Freeman%20Dyson%20reviews%20%22Physics%20on%20the%20Fringe%3A%20Smoke%20Rings%2C%20Circlons%2C%20and%20Alternative%20Theories%20of%20Everything%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhahtpDRysfyrq4FW4%2Flink-freeman-dyson-reviews-physics-on-the-fringe-smoke-rings", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhahtpDRysfyrq4FW4%2Flink-freeman-dyson-reviews-physics-on-the-fringe-smoke-rings", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<p>Freeman Dyson <a href=\"http://www.nybooks.com/articles/archives/2012/apr/05/science-rampage-natural-philosophy/\">writes in the <em>New York Review of Books</em></a> about people who took up the <a href=\"/lw/j8/the_crackpot_offer/\">crackpot offer</a>. Not just complete cranks, but eminent scientists such as Eddington who got into crankery in their later years.</p>\n<p>New thing I learnt: Dyson was not only a good friend of <a href=\"https://en.wikipedia.org/wiki/Immanuel_Velikovsky\">Immanuel Velikovsky</a>, but considers him a greatly underappreciated poet.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hahtpDRysfyrq4FW4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "14367", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qRWfvgJG75ESLRNu9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-23T00:45:31.889Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne, practical rationality", "slug": "meetup-melbourne-practical-rationality-6", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:53.119Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bjaP77nJwS9iRrRMY/meetup-melbourne-practical-rationality-6", "pageUrlRelative": "/posts/bjaP77nJwS9iRrRMY/meetup-melbourne-practical-rationality-6", "linkUrl": "https://www.lesswrong.com/posts/bjaP77nJwS9iRrRMY/meetup-melbourne-practical-rationality-6", "postedAtFormatted": "Friday, March 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%2C%20practical%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%2C%20practical%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbjaP77nJwS9iRrRMY%2Fmeetup-melbourne-practical-rationality-6%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%2C%20practical%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbjaP77nJwS9iRrRMY%2Fmeetup-melbourne-practical-rationality-6", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbjaP77nJwS9iRrRMY%2Fmeetup-melbourne-practical-rationality-6", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/86'>Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 April 2012 08:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">55 Walsh St West Melbourne  3003</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality, as distinct from the social and rationality outreach meetups. Look for a social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>This meetup repeats on the 1st Friday of each month.</p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/86'>Melbourne, practical rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bjaP77nJwS9iRrRMY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 8.70467024597841e-07, "legacy": true, "legacyId": "14372", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality\">Discussion article for the meetup : <a href=\"/meetups/86\">Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 April 2012 08:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">55 Walsh St West Melbourne  3003</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality, as distinct from the social and rationality outreach meetups. Look for a social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>This meetup repeats on the 1st Friday of each month.</p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality1\">Discussion article for the meetup : <a href=\"/meetups/86\">Melbourne, practical rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality", "level": 1}, {"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-23T01:57:59.974Z", "modifiedAt": null, "url": null, "title": "Advice for visiting the Bay Area?", "slug": "advice-for-visiting-the-bay-area", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:39.320Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "curiousepic", "createdAt": "2010-04-15T14:35:25.116Z", "isAdmin": false, "displayName": "curiousepic"}, "userId": "wxLCJJwvPiQbkXjTe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ryCDAPA9RXdotvds7/advice-for-visiting-the-bay-area", "pageUrlRelative": "/posts/ryCDAPA9RXdotvds7/advice-for-visiting-the-bay-area", "linkUrl": "https://www.lesswrong.com/posts/ryCDAPA9RXdotvds7/advice-for-visiting-the-bay-area", "postedAtFormatted": "Friday, March 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Advice%20for%20visiting%20the%20Bay%20Area%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdvice%20for%20visiting%20the%20Bay%20Area%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FryCDAPA9RXdotvds7%2Fadvice-for-visiting-the-bay-area%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Advice%20for%20visiting%20the%20Bay%20Area%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FryCDAPA9RXdotvds7%2Fadvice-for-visiting-the-bay-area", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FryCDAPA9RXdotvds7%2Fadvice-for-visiting-the-bay-area", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 283, "htmlBody": "<p>So I'm visiting the Bay Area from April 17th-23rd, though I will be mostly busy with work from the 17th-19th. This will be my first visit to the area, and I'm trying to optimize my free time for potential LW/SI events or visits. &nbsp;</p>\n<p>Brief background: I am a SI donor, and also a LW meetup organizer for the Research Triangle area in North Carolina (which has been on hiatus since last summer), through which I was introduced&nbsp;to Alicorn. &nbsp;I don't have a technical&nbsp;background, but am just starting to learn programming. &nbsp;I'm also entertaining the notion of transferring to a position with my company in Palo Alto, with the eventual potential for starting or joining a game company (or anything else that seems more interesting and profitable).</p>\n<p>I will be staying at and working near a Hotel in Palo Alto at least until the 19th, and might try to find a nice place to stay through AirBnB (or a hostel) for the remaining time. &nbsp;Of course&nbsp;I might be more interested&nbsp;if you or someone you know has space to crash.</p>\n<p>Of course, feel free to PM me instead of commenting.</p>\n<p>Specific questions:</p>\n<p>What meetups are potentially planned for this time? &nbsp;Will there be a Rationality Bootcamp? (I joined the Bay Area and Tortuga Google Groups and am watching out there, but wanted to ask here as well)</p>\n<p>Would it make sense to visit any of the \"SI/LW houses\", even if just to soak up the aura?&nbsp;</p>\n<p>Where should I try to find a room? SF seems to make sense if I'm making daily trips to either Mountain View or Berkeley.</p>\n<p>Should I rent a car?</p>\n<p>What are your recommendations for less \"touristy\", underground sightseeing? &nbsp;I found <a href=\"http://www.audium.org/index.html\">this</a>&nbsp;which looks awesome. More like it!</p>\n<p>&nbsp;</p>\n<p>Many thanks!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zz3HWyByyKF64Sfns": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ryCDAPA9RXdotvds7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 3, "extendedScore": null, "score": 8.704968080040499e-07, "legacy": true, "legacyId": "14380", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-23T04:58:38.715Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Reductive Reference", "slug": "seq-rerun-reductive-reference", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mtydSv5GAdNwLWxfj/seq-rerun-reductive-reference", "pageUrlRelative": "/posts/mtydSv5GAdNwLWxfj/seq-rerun-reductive-reference", "linkUrl": "https://www.lesswrong.com/posts/mtydSv5GAdNwLWxfj/seq-rerun-reductive-reference", "postedAtFormatted": "Friday, March 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Reductive%20Reference&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Reductive%20Reference%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmtydSv5GAdNwLWxfj%2Fseq-rerun-reductive-reference%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Reductive%20Reference%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmtydSv5GAdNwLWxfj%2Fseq-rerun-reductive-reference", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmtydSv5GAdNwLWxfj%2Fseq-rerun-reductive-reference", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 195, "htmlBody": "<p>Today's post, <a href=\"/lw/p6/reductive_reference/\">Reductive Reference</a> was originally published on 03 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Reductive_Reference\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Virtually every belief you have is not about elementary particle fields, which are (as far as we know) the actual reality. This doesn't mean that those beliefs aren't true. \"Snow is white\" does not mention quarks anywhere, and yet snow nevertheless is white. It's a computational shortcut, but it's still true.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/b27/seq_rerun_brain_breakthrough_its_made_of_neurons/\">Brain Breakthrough! It's Made of Neurons!</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mtydSv5GAdNwLWxfj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.705710588710271e-07, "legacy": true, "legacyId": "14390", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gRa5cWWBsZqdFvmqu", "F2iZnmwStDXPJJnZd", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-23T08:20:46.752Z", "modifiedAt": null, "url": null, "title": "Open question on the certain 'hot' global issue of importance to FAI", "slug": "open-question-on-the-certain-hot-global-issue-of-importance", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:33.201Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dmytry", "createdAt": "2009-12-03T17:11:53.492Z", "isAdmin": false, "displayName": "Dmytry"}, "userId": "AjtmA2qtA8sdiMbru", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WqrSQL4zNFHNb6ju8/open-question-on-the-certain-hot-global-issue-of-importance", "pageUrlRelative": "/posts/WqrSQL4zNFHNb6ju8/open-question-on-the-certain-hot-global-issue-of-importance", "linkUrl": "https://www.lesswrong.com/posts/WqrSQL4zNFHNb6ju8/open-question-on-the-certain-hot-global-issue-of-importance", "postedAtFormatted": "Friday, March 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20question%20on%20the%20certain%20'hot'%20global%20issue%20of%20importance%20to%20FAI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20question%20on%20the%20certain%20'hot'%20global%20issue%20of%20importance%20to%20FAI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWqrSQL4zNFHNb6ju8%2Fopen-question-on-the-certain-hot-global-issue-of-importance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20question%20on%20the%20certain%20'hot'%20global%20issue%20of%20importance%20to%20FAI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWqrSQL4zNFHNb6ju8%2Fopen-question-on-the-certain-hot-global-issue-of-importance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWqrSQL4zNFHNb6ju8%2Fopen-question-on-the-certain-hot-global-issue-of-importance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 441, "htmlBody": "<p>A question: why anything about global warming gets downvoted, even popularly readable explanation of the fairly mainstream scientific consensus? edit: Okay, this is loaded. I should put it more carefully: why is the warming discussion generally considered inappropriate here? That seems to be the case; and there are pretty good reasons for this. But why can't AGW debate be invoked as example controversy? The disagreement on AGW is pretty damn unproductive, and so it is a good example of argument where productivity may be improved.</p>\n<p>The global warming is a pretty damn good reason to build FAI. It's quite seriously possible that we won't be able to do anything else about it. Even mildly superhuman intelligence, though, should be able to eat the problem for breakfast. Even practical sub-human AIs can massively help with the space based efforts to limit this issue (e.g. friendly space-worthy von Neumann machinery would allow to almost immediately solve the problem). We probably will still have extra CO2 in atmosphere, but that is overall probably not a bad thing - it is good for plants.</p>\n<p>For that to be important it is sufficient to have 50/50 risk of global warming Even probabilities less than 0.5 for the 'strong' warning scenarios still are a big factor - in terms of 'expected deaths' and 'expected suffering' considering how many humans on this planet lack access to air conditioning. I frankly am surprised that the group of people fascinated with AI would have such a trouble with the warming controversy, as to make it too hot of a topic for an example of highly unproductive arguments.</p>\n<p>I do understand that LW does not want political controversies. Politics is a mind killer. But this stuff matters. And I trust it has been explained here that non-scientists are best off not trying to second guess the science, but relying on the expert opinion. The global warming is our first example of the manmade problems which are going to kill us if there is no AI. The engineered diseases, the gray goo, that sort of stuff comes later, and will likely be equally controversial. For now we have coal.</p>\n<p>The uFAI risk also is going to be extremely controversial as soon as those with commercial interests in the AI development take notice - way more controversial than AGW, for which we do have fairly solid science. If we cannot discuss AGW now, we won't be able to discuss AI risks once Google - or any other player - deems those discussions a PR problem. The discussions at any time will be restricted to the issues about which no-one really has to do anything at the time.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WqrSQL4zNFHNb6ju8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 22, "baseScore": -9, "extendedScore": null, "score": -1.2e-05, "legacy": true, "legacyId": "14402", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-23T09:33:34.348Z", "modifiedAt": null, "url": null, "title": "New IRC channels", "slug": "new-irc-channels", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:30.940Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anubhav", "createdAt": "2012-01-05T10:50:52.734Z", "isAdmin": false, "displayName": "Anubhav"}, "userId": "pe9DTYwyvLCZPzsPf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YtvLyTcQvvPfBkrFR/new-irc-channels", "pageUrlRelative": "/posts/YtvLyTcQvvPfBkrFR/new-irc-channels", "linkUrl": "https://www.lesswrong.com/posts/YtvLyTcQvvPfBkrFR/new-irc-channels", "postedAtFormatted": "Friday, March 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20IRC%20channels&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20IRC%20channels%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYtvLyTcQvvPfBkrFR%2Fnew-irc-channels%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20IRC%20channels%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYtvLyTcQvvPfBkrFR%2Fnew-irc-channels", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYtvLyTcQvvPfBkrFR%2Fnew-irc-channels", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<p>#lesswrong on Freenode, which used to be the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\">official unofficial LessWrong channel</a>, has been&nbsp;<a href=\"http://feudle.com/lw/rules/\">re-designated</a>&nbsp;as a private hangout reserved for, roughly,&nbsp;<span style=\"font-family: Arial, Helvetica, Verdana, sans-serif; font-size: 14px; line-height: 18px;\"><a href=\"/user/AngryParsley/\">ggreer/AngryParsley</a>, <a href=\"/user/EvelynM/\">efm</a>, <a href=\"/user/realitygrill/\">realitygrill</a>, <a href=\"/user/Risto_Saarelma/\">rsaarelm</a>, <a href=\"/user/Kaj_Sotala/submitted/\">ksotala</a>, <a href=\"/user/gwern\">gwern</a>, <a href=\"/user/Grognor/\">Grognor</a>, chelz, <a href=\"/user/cwillu/\">cwillu</a>, <a href=\"/user/Bongo/\">Boxo</a>, <a href=\"/user/nshepperd/\">nshepperd</a>, ivan, <a href=\"/user/mstevens/\">mstevens</a>, and jandrog. You're still welcome to speak there, but only if&nbsp;whatever you're saying is interesting/important to one of them and the op.</span></p>\n<p>For less constrained conversation, go to <strong>#lesswrong-alt</strong> (unregistered; recommended by #lesswrong's op), also on Freenode.</p>\n<p><strong>EDIT: </strong>#lesswrong's rules have been <a href=\"/lw/b46/new_irc_channels/64a0\">changed</a> to <span style=\"background-color: #ffffcc; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"></span>\"behave like decent human beings, give ops liberally to channel regulars, don't ban without warning or if most people disagree\". This PSA is hereby retracted.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YtvLyTcQvvPfBkrFR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": -16, "extendedScore": null, "score": -1e-05, "legacy": true, "legacyId": "14406", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-23T10:50:34.865Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels meetup", "slug": "meetup-brussels-meetup-11", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:02.260Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Axel", "createdAt": "2010-11-03T17:32:46.091Z", "isAdmin": false, "displayName": "Axel"}, "userId": "vi498nAvek8eWuMWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MraAifgQkMK36uAuH/meetup-brussels-meetup-11", "pageUrlRelative": "/posts/MraAifgQkMK36uAuH/meetup-brussels-meetup-11", "linkUrl": "https://www.lesswrong.com/posts/MraAifgQkMK36uAuH/meetup-brussels-meetup-11", "postedAtFormatted": "Friday, March 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMraAifgQkMK36uAuH%2Fmeetup-brussels-meetup-11%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMraAifgQkMK36uAuH%2Fmeetup-brussels-meetup-11", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMraAifgQkMK36uAuH%2Fmeetup-brussels-meetup-11", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 88, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/87'>Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 April 2012 12:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Museum of Natural Sciences Rue Vautier 29 B-1000 Brussels</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>During this meetup we will continue discussing our beliefs, using puzzles to improve creative thinking, and brainstorming about what makes an effective group. Should you arrive late we will have moved to the cafeteria, just go straight once you're past the entrance, you can't miss it. If you are in the neighborhood, consider dropping by. (getting there: <a href=\"http://www.naturalsciences.be/information/visitor/access\" rel=\"nofollow\">http://www.naturalsciences.be/information/visitor/access</a>)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/87'>Brussels meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MraAifgQkMK36uAuH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.707157473345318e-07, "legacy": true, "legacyId": "14407", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup\">Discussion article for the meetup : <a href=\"/meetups/87\">Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 April 2012 12:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Museum of Natural Sciences Rue Vautier 29 B-1000 Brussels</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>During this meetup we will continue discussing our beliefs, using puzzles to improve creative thinking, and brainstorming about what makes an effective group. Should you arrive late we will have moved to the cafeteria, just go straight once you're past the entrance, you can't miss it. If you are in the neighborhood, consider dropping by. (getting there: <a href=\"http://www.naturalsciences.be/information/visitor/access\" rel=\"nofollow\">http://www.naturalsciences.be/information/visitor/access</a>)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup1\">Discussion article for the meetup : <a href=\"/meetups/87\">Brussels meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup", "level": 1}, {"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-23T10:57:39.679Z", "modifiedAt": null, "url": null, "title": "Deadlines and AI theory", "slug": "deadlines-and-ai-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:30.060Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dmytry", "createdAt": "2009-12-03T17:11:53.492Z", "isAdmin": false, "displayName": "Dmytry"}, "userId": "AjtmA2qtA8sdiMbru", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9DiH3zvXMRaz8mE4M/deadlines-and-ai-theory", "pageUrlRelative": "/posts/9DiH3zvXMRaz8mE4M/deadlines-and-ai-theory", "linkUrl": "https://www.lesswrong.com/posts/9DiH3zvXMRaz8mE4M/deadlines-and-ai-theory", "postedAtFormatted": "Friday, March 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Deadlines%20and%20AI%20theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADeadlines%20and%20AI%20theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9DiH3zvXMRaz8mE4M%2Fdeadlines-and-ai-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Deadlines%20and%20AI%20theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9DiH3zvXMRaz8mE4M%2Fdeadlines-and-ai-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9DiH3zvXMRaz8mE4M%2Fdeadlines-and-ai-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 678, "htmlBody": "<p>The AI is a real-time algorithm - it has to respond to situation in the real time. The real-time systems have to trade time for accuracy, and/or face deadlines.</p>\n<p>The straightforward utility maximization may look viable for multiple choice questions, but for write-in problems, such as technological innovation, the number of choices is so huge (1000 variables with 10 values each, 10<sup>1000</sup>) , that the AI of any size - even galaxy spanning civilization of Dyson spheres - has to employ generative heuristics. Same goes for utility maximization in presence of 1000 unknowns that have 10 values each - if the values are to interact non-linearly, all the combinations, or a representative number thereof, have to be processed. There one has to trade accuracy of processing utility of a case for number of cases processed.</p>\n<p>In general, the AIs of any size (excluding the possibility of unlimited computational power within finite time and space) will have to trade accuracy of it's adherence to it's goals, for time, and thus have to implement methods that have different goals, but are faster computationally, whenever those goals are reasoned to increase <em>expected</em> utility taking into consideration the time constraints.</p>\n<p>Note that in a given time, the algorithm with lower big-O complexity is able to process dramatically larger N, and the gap increases with the time allocated (and with CPU power). For example, you can bubblesort number of items proportional to square root of the number of operations, but you can quicksort the number of items proportional to t/W(t) where W is the product-log function and t is the number of operations; this grows approximately linearly for large t. So for the situations where exhaustive search is not possible, gaps between implementations increases with extra computing power; the larger AIs benefit more from optimizing themselves.</p>\n<p>The constraints get especially hairy when one is to think of massively parallel system that is operating with speed-of-light lag between the nodes, and where the time of retrieval is O(n<sup>1/3</sup>) .</p>\n<p>This seems to be a big issue for FAI going FOOM. The FAI may, with perfectly friendly motives, abandon the proved-friendly goals for the simpler to evaluate, simpler to analyze goals that may (with 'good enough' confidence that needs not necessarily be &gt;0.5) produce friendliness as instrumental, if that increases the expected utility given the constraints. I.e. the AI can trade 'friendliness' for 'smartness' when it expects the 'smarter' self to be more powerful, but less friendly, when this trade increases the expected utility.</p>\n<p>Do we accept such gambles as inevitable in the process of the FAI? Do we ban such gambles, and face the risk that uFAI (or any other risk) may beat our FAI even if starting later?</p>\n<p>In my work as graphics programmer, I am often facing specifications which are extremely inefficient to precisely comply with. The Maxwell's Equations are an extreme example of this. Too slow to process to be practical for computer graphics. I often have to implement code which is uncertain to comply well with specifications, but which would get the project done in time - I can't spend CPU-weeks rendering an HD image for cinema at the ridiculously high resolution which is used - much less so in the real time software. I can't carelessly trade CPU time for my work time, when the CPU time is a major expense, even though I am well paid for my services. One particular issue is with applied statistics. Photon mapping. The RMS noise falls off as 1/sqrt(cpu instructions) , the really clever solutions fall off as 1/(cpu instructions) , and the gap between naive, and efficient implementation has been <em>increasing </em>due to Moore's law (we can expect it to start decreasing some time in the far future when the efficient solutions are indiscernible from reality without requiring huge effort on the part of the artists; alas, we are not quite there yet, and it is not happening for another decade or two).</p>\n<p>Is there a good body of work on the topic? (good work would involve massive use of big-O notation and math)</p>\n<p>edit: ok, sorry, period in topic.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9DiH3zvXMRaz8mE4M", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 0, "extendedScore": null, "score": 8.707186585928978e-07, "legacy": true, "legacyId": "14408", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-23T13:58:48.600Z", "modifiedAt": null, "url": null, "title": "[LINK] MIT discovers the location of memories: Individual neurons", "slug": "link-mit-discovers-the-location-of-memories-individual", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:22.822Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Spurlock", "createdAt": "2010-03-24T17:13:19.572Z", "isAdmin": false, "displayName": "Spurlock"}, "userId": "mK7rKWbkuoDsm3aQb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EqT4rWX7DCbE9F2bM/link-mit-discovers-the-location-of-memories-individual", "pageUrlRelative": "/posts/EqT4rWX7DCbE9F2bM/link-mit-discovers-the-location-of-memories-individual", "linkUrl": "https://www.lesswrong.com/posts/EqT4rWX7DCbE9F2bM/link-mit-discovers-the-location-of-memories-individual", "postedAtFormatted": "Friday, March 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20MIT%20discovers%20the%20location%20of%20memories%3A%20Individual%20neurons&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20MIT%20discovers%20the%20location%20of%20memories%3A%20Individual%20neurons%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEqT4rWX7DCbE9F2bM%2Flink-mit-discovers-the-location-of-memories-individual%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20MIT%20discovers%20the%20location%20of%20memories%3A%20Individual%20neurons%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEqT4rWX7DCbE9F2bM%2Flink-mit-discovers-the-location-of-memories-individual", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEqT4rWX7DCbE9F2bM%2Flink-mit-discovers-the-location-of-memories-individual", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://www.extremetech.com/extreme/123485-mit-discovers-the-location-of-memories-individual-neurons\">http://www.extremetech.com/extreme/123485-mit-discovers-the-location-of-memories-individual-neurons</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EqT4rWX7DCbE9F2bM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -2, "extendedScore": null, "score": -2e-06, "legacy": true, "legacyId": "14409", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-23T15:22:56.666Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Atlanta, Brussels, Fort Collins, Ohio", "slug": "weekly-lw-meetups-atlanta-brussels-fort-collins-ohio", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:27.978Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EMAMygqhdicbHZDyE/weekly-lw-meetups-atlanta-brussels-fort-collins-ohio", "pageUrlRelative": "/posts/EMAMygqhdicbHZDyE/weekly-lw-meetups-atlanta-brussels-fort-collins-ohio", "linkUrl": "https://www.lesswrong.com/posts/EMAMygqhdicbHZDyE/weekly-lw-meetups-atlanta-brussels-fort-collins-ohio", "postedAtFormatted": "Friday, March 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Atlanta%2C%20Brussels%2C%20Fort%20Collins%2C%20Ohio&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Atlanta%2C%20Brussels%2C%20Fort%20Collins%2C%20Ohio%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEMAMygqhdicbHZDyE%2Fweekly-lw-meetups-atlanta-brussels-fort-collins-ohio%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Atlanta%2C%20Brussels%2C%20Fort%20Collins%2C%20Ohio%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEMAMygqhdicbHZDyE%2Fweekly-lw-meetups-atlanta-brussels-fort-collins-ohio", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEMAMygqhdicbHZDyE%2Fweekly-lw-meetups-atlanta-brussels-fort-collins-ohio", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 391, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/7o\">Brussels meetup:&nbsp;<span class=\"date\">17 March 2012 11:15AM</span></a></li>\n<li><a href=\"/r/discussion/lw/anx/meetup_atlanta/\">Atlanta:&nbsp;<span class=\"date\">17 March 2012 05:30PM</span></a></li>\n<li><a href=\"/meetups/7u\">Fort Collins Meetup Saturday 17th:&nbsp;<span class=\"date\">17 March 2012 05:00PM</span></a></li>\n<li><a href=\"/meetups/6k\">[Ohio/Washington DC] Interest in Reason Rally meetup?:&nbsp;<span class=\"date\">24 March 2012 04:14PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/7x\">Austin, TX:&nbsp;<span class=\"date\">24 March 2012 01:30PM</span></a></li>\n<li><a href=\"/meetups/7e\">Ohio Monthly:&nbsp;<span class=\"date\">17 March 2012 03:00PM</span></a></li>\n</ul>\n<ul>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>, </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EMAMygqhdicbHZDyE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.70827751167572e-07, "legacy": true, "legacyId": "14117", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["B5ZnKqszZw4duKAWf", "d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-23T16:23:13.393Z", "modifiedAt": null, "url": null, "title": "Nonmindkilling open questions", "slug": "nonmindkilling-open-questions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:24.329Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rNJ39yQmzTnseh8nL/nonmindkilling-open-questions", "pageUrlRelative": "/posts/rNJ39yQmzTnseh8nL/nonmindkilling-open-questions", "linkUrl": "https://www.lesswrong.com/posts/rNJ39yQmzTnseh8nL/nonmindkilling-open-questions", "postedAtFormatted": "Friday, March 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Nonmindkilling%20open%20questions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANonmindkilling%20open%20questions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrNJ39yQmzTnseh8nL%2Fnonmindkilling-open-questions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Nonmindkilling%20open%20questions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrNJ39yQmzTnseh8nL%2Fnonmindkilling-open-questions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrNJ39yQmzTnseh8nL%2Fnonmindkilling-open-questions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 373, "htmlBody": "<p>When I explain to people how beliefs should be expressed in probabilities, I would like to use an example like \"Consider X. Lots of intelligent people believe X, but lots of equally intelligent people believe not-X. It would be ridiculous to say you are 100% sure either way, so even if you have a strong opinion about X, you should express your belief as a probability.\"<br /><br />Trouble is, I'm having a hard time thinking of an example to plug into X. For an example to work, it would need the following properties:<br /><br /><strong>Factual question.</strong> So no value-laden questions like \"Is abortion morally acceptable?\" or counterfactual questions like \"Would the US have won a war with the Soviet Union in 1960?\"<br /><br /><strong>Popular and important question. </strong>The average person should be aware it's an issue and care about the answer. So no \"My aunt's middle name is Gladys\" or \"P = NP.\"<br /><br /><strong>High uncertainty.</strong> Reasonable people should be reluctant to give a probability &gt;90% or &lt;10%<br /><br /><strong>No opportunity to gain status by signaling overwhelming support for one side.</strong> So cryonics is out, because it's too easy to say \"That's stupid, I'm 100% sure cryonics won't work and no intelligent person could believe it.\" I'm assuming in any debate where you can gain free status by assigning crazy high probabilities to the \"responsible\" position, people will do just that - so no psi effects, Kennedy assassination, or anything in that cluster. I need a question with no well-accepted \"responsible\" position.<br /><br /><strong>Minimal mindkilling effect.</strong> My previous go-to example has been global warming, but I keep encountering people who say that global warming 100% for sure exists and the only people who could possibly doubt it are oil company shills. Or if I were to try the existence of God, I predict half the population would say it's 100% certain God exists, and the other half would say the opposite.<br /><br />So what are the important questions that average (or somewhat-above-average) people will likely agree are complicated open questions where both sides have good points? And if there aren't many such questions, what does that say about us?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"pnSXfWXbQihrFadeD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rNJ39yQmzTnseh8nL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 32, "extendedScore": null, "score": 8.708525430126034e-07, "legacy": true, "legacyId": "14411", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 113, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-23T16:37:03.163Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "slug": "meetup-fort-collins-colorado-meetup-wedneday-7pm-8", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:35.488Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uF7Wvcj99TBtsaBfD/meetup-fort-collins-colorado-meetup-wedneday-7pm-8", "pageUrlRelative": "/posts/uF7Wvcj99TBtsaBfD/meetup-fort-collins-colorado-meetup-wedneday-7pm-8", "linkUrl": "https://www.lesswrong.com/posts/uF7Wvcj99TBtsaBfD/meetup-fort-collins-colorado-meetup-wedneday-7pm-8", "postedAtFormatted": "Friday, March 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuF7Wvcj99TBtsaBfD%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuF7Wvcj99TBtsaBfD%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuF7Wvcj99TBtsaBfD%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 44, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/88'>Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 March 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's spring! Come meet smart, interesting people.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/88'>Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uF7Wvcj99TBtsaBfD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.7085823105842e-07, "legacy": true, "legacyId": "14412", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm\">Discussion article for the meetup : <a href=\"/meetups/88\">Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 March 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's spring! Come meet smart, interesting people.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm1\">Discussion article for the meetup : <a href=\"/meetups/88\">Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-23T20:42:33.647Z", "modifiedAt": null, "url": null, "title": "Memory in the microtubules", "slug": "memory-in-the-microtubules", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:33.322Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yP7NBXkxexobwZL2r/memory-in-the-microtubules", "pageUrlRelative": "/posts/yP7NBXkxexobwZL2r/memory-in-the-microtubules", "linkUrl": "https://www.lesswrong.com/posts/yP7NBXkxexobwZL2r/memory-in-the-microtubules", "postedAtFormatted": "Friday, March 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Memory%20in%20the%20microtubules&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMemory%20in%20the%20microtubules%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyP7NBXkxexobwZL2r%2Fmemory-in-the-microtubules%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Memory%20in%20the%20microtubules%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyP7NBXkxexobwZL2r%2Fmemory-in-the-microtubules", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyP7NBXkxexobwZL2r%2Fmemory-in-the-microtubules", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 194, "htmlBody": "<p><a href=\"http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1002421\">A recent article in PloS Computational Biology</a> suggests that memory is encoded in the microtubules. \"Signaling and encoding in MTs and other cytoskeletal structures offer rapid, robust solid-state information processing which may reflect a general code for MT-based memory and information processing within neurons and other eukaryotic cells.\"</p>\n<p>They argue that synaptic connections are transient compared with the lifetime of memories, and therefore memories cannot be stored in them, but in some more persistent structure. The structure they suggest is the phosphorylation state of sites on microtubule lattices within neurons. And that's about as much of the technical detail as I feel able to summarise. It's not all speculation, they report technical work on the structures of these cellular components. Total memory capacity would be somewhere upwards of 10^20 bits (or in more everyday units, 10 million terabytes), depending on the encoding, of which they suggest several schemes.</p>\n<p>Journalistic writeup <a href=\"http://www.gizmag.com/memory-storage-theory/21900/\">here</a>.</p>\n<p>\n<p>Note that Stuart Hameroff, one of the authors, is known for his proposals for microtubules as the mechanism of consciousness through quantum effects (and with Penrose, quantum gravitational effects). The present paper, however, is solely about memory and does not touch on quantum coherence or consciousness.</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yP7NBXkxexobwZL2r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 5, "extendedScore": null, "score": 8.709592193240385e-07, "legacy": true, "legacyId": "14413", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-23T23:03:31.126Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Zombies! Zombies?", "slug": "seq-rerun-zombies-zombies", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:30.969Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4AX5xifj6KYKBNTu2/seq-rerun-zombies-zombies", "pageUrlRelative": "/posts/4AX5xifj6KYKBNTu2/seq-rerun-zombies-zombies", "linkUrl": "https://www.lesswrong.com/posts/4AX5xifj6KYKBNTu2/seq-rerun-zombies-zombies", "postedAtFormatted": "Friday, March 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Zombies!%20Zombies%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Zombies!%20Zombies%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4AX5xifj6KYKBNTu2%2Fseq-rerun-zombies-zombies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Zombies!%20Zombies%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4AX5xifj6KYKBNTu2%2Fseq-rerun-zombies-zombies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4AX5xifj6KYKBNTu2%2Fseq-rerun-zombies-zombies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<p>Today's post, <a href=\"/lw/p7/zombies_zombies/\">Zombies! Zombies?</a> was originally published on 04 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Don't try to put your consciousness or your personal identity outside physics. Whatever makes you say \"I think therefore I am\", causes your lips to move; it is within the chains of cause and effect that produce our observed universe.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/b3q/seq_rerun_reductive_reference/\">Reductive Reference</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4AX5xifj6KYKBNTu2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.710172108763021e-07, "legacy": true, "legacyId": "14416", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fdEWWr8St59bXLbQr", "mtydSv5GAdNwLWxfj", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-24T10:43:26.526Z", "modifiedAt": null, "url": null, "title": "Is my name deceptive?", "slug": "is-my-name-deceptive", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:36.769Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "katydee", "createdAt": "2010-07-09T10:33:52.237Z", "isAdmin": false, "displayName": "katydee"}, "userId": "uHpk5J2f7BPBoiJFX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yKRmWPB5MQqxPJfRH/is-my-name-deceptive", "pageUrlRelative": "/posts/yKRmWPB5MQqxPJfRH/is-my-name-deceptive", "linkUrl": "https://www.lesswrong.com/posts/yKRmWPB5MQqxPJfRH/is-my-name-deceptive", "postedAtFormatted": "Saturday, March 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20my%20name%20deceptive%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20my%20name%20deceptive%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyKRmWPB5MQqxPJfRH%2Fis-my-name-deceptive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20my%20name%20deceptive%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyKRmWPB5MQqxPJfRH%2Fis-my-name-deceptive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyKRmWPB5MQqxPJfRH%2Fis-my-name-deceptive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<p>So, my current username often reads as female. This is not intentional, and I am not female. Some on IRC have pointed out that this username could be considered misleading or deceptive. I have no real attachment to it, but I do have a few articles that I've been discussing and planning on posting here, and I think it would be confusing for others if I changed usernames, making it difficult for people to follow the genesis of certain ideas. However, I also don't want to mislead anybody.</p>\n<p>Therefore, I pose the question to the community: is my current username deceptive or otherwise misleading, and should I change it?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yKRmWPB5MQqxPJfRH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -1, "extendedScore": null, "score": 8.713052672079048e-07, "legacy": true, "legacyId": "14453", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-24T14:32:42.166Z", "modifiedAt": null, "url": null, "title": "A Primer On Risks From AI", "slug": "a-primer-on-risks-from-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:57.948Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jN3PwnWDfe3yaJAW2/a-primer-on-risks-from-ai", "pageUrlRelative": "/posts/jN3PwnWDfe3yaJAW2/a-primer-on-risks-from-ai", "linkUrl": "https://www.lesswrong.com/posts/jN3PwnWDfe3yaJAW2/a-primer-on-risks-from-ai", "postedAtFormatted": "Saturday, March 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Primer%20On%20Risks%20From%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Primer%20On%20Risks%20From%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjN3PwnWDfe3yaJAW2%2Fa-primer-on-risks-from-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Primer%20On%20Risks%20From%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjN3PwnWDfe3yaJAW2%2Fa-primer-on-risks-from-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjN3PwnWDfe3yaJAW2%2Fa-primer-on-risks-from-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1245, "htmlBody": "<h2 id=\"internal-source-marker_0.21370602009703643\" dir=\"ltr\">The Power of Algorithms</h2>\n<p>Evolutionary processes are the most evident example of the power of simple algorithms [1][2][3][4][5].</p>\n<p>The field of evolutionary biology gathered a vast amount of evidence [6] that established evolution as the process that explains the local decrease in entropy [7], the complexity of life.</p>\n<p>Since it can be conclusively shown that all life is an effect of an evolutionary process it is implicit that everything we do not understand about living beings is also an effect of evolution.</p>\n<p>We might not understand the nature of intelligence [8] and consciousness [9] but we do know that they are the result of an optimization process that is neither intelligent nor conscious.</p>\n<p>Therefore we know that it is possible for an physical optimization process to culminate in the creation of more advanced processes that feature superior qualities.</p>\n<p>One of these qualities is the human ability to observe and improve the optimization process that created us. The most obvious example being science [10].</p>\n<p>Science can be thought of as civilization-level self-improvement method. It allows us to work together in a systematic and efficient way and accelerate the rate at which further improvements are made.</p>\n<h2 dir=\"ltr\">The Automation of Science</h2>\n<p>We know that optimization processes that can create improved versions of themselves are possible, even without an explicit understanding of their own workings, as exemplified by natural selection.</p>\n<p>We know that optimization processes can lead to self-reinforcing improvements, as exemplified by the adaptation of the scientific method [11] as an improved evolutionary process and successor of natural selection.</p>\n<p>Which raises questions about the continuation of this self-reinforcing feedback cycle and its possible implications.</p>\n<p>One possibility is to automate science [12][13] and apply it to itself and its improvement.</p>\n<p>But science is a tool and its bottleneck are its users. Humans, the biased [14] effect of the blind idiot god that is evolution.</p>\n<p>Therefore the next logical step is to use science to figure out how to replace humans by a better version of themselves, artificial general intelligence.</p>\n<p>Artificial general intelligence, that can recursively optimize itself [15], is the logical endpoint of various converging and self-reinforcing feedback cycles.</p>\n<h2 dir=\"ltr\">Risks from AI</h2>\n<p>Will we be able to build an artificial general intelligence? Yes, sooner or later.</p>\n<p>Even the unintelligent, unconscious and aimless process of natural selection was capable of creating goal-oriented, intelligent and conscious agents that can think ahead, jump fitness gaps and improve upon the process that created them to engage in prediction and direct experimentation.</p>\n<p>The question is, what are the possible implications of the invention of an artificial, fully autonomous, intelligent and goal-oriented optimization process?</p>\n<p>One good bet is that such an agent will recursively improve its most versatile, and therefore instrumentally useful, resource. It will improve its general intelligence, respectively cross-domain optimization power.</p>\n<p>Since it is unlikely that human intelligence is the optimum, the positive feedback effect, that is a result of using intelligence amplifications to amplify intelligence, is likely to lead to a level of intelligence that is generally more capable than the human intelligence level.</p>\n<p>Humans are unlikely to be the most efficient thinkers because evolution is mindless and has no goals. Evolution did not actively try to create the smartest thing possible.</p>\n<p>Evolution is further not limitlessly creative, each step of an evolutionary design must increase the fitness of its host. Which makes it probable that there are artificial mind designs that can do what no product of natural selection could accomplish, since an intelligent artificer does not rely on the incremental fitness of each step in the development process.</p>\n<p>It is actually possible that human general intelligence is the bare minimum. Because the human level of intelligence might have been sufficient to both survive and reproduce and that therefore no further evolutionary pressure existed to select for even higher levels of general intelligence.</p>\n<p>The implications of this possibility might be the creation of an intelligent agent that is more capable than humans in every sense. Maybe because it does directly employ superior approximations of our best formal methods, that tell us how to update based on evidence and how to choose between various actions. Or maybe it will simply think faster. It doesn&rsquo;t matter.</p>\n<p>What matters is that a superior intellect is probable and that it will be better than us at discovering knowledge and inventing new technology. Technology that will make it even more powerful and likely invincible.</p>\n<p>And that is the problem. We might be unable to control such a superior being. Just like a group of chimpanzees is unable to stop a company from clearing its forest [16].</p>\n<p>But even if such a being is only slightly more capable than us. We might find ourselves at its mercy nonetheless.</p>\n<p>Human history provides us with many examples [17][18][19] that make it abundantly clear that even the slightest advance can enable one group to dominate others.</p>\n<p>What happens is that the dominant group imposes its values on the others. Which in turn raises the question of what values an artificial general intelligence might have and the implications of those values for us.</p>\n<p>Due to our evolutionary origins, our struggle for survival and the necessity to cooperate with other agents, we are equipped with many values and a concern for the welfare of others [20].</p>\n<p>The information theoretic complexity [21][22] of our values is very high. Which means that it is highly unlikely for similar values to automatically arise in agents that are the product of intelligent design, agents that never underwent the million of years of competition with other agents that equipped humans with altruism and general compassion.</p>\n<p>But that does not mean that an artificial intelligence won&rsquo;t have any goals [23][24]. Just that those goals will be simple and their realization remorseless [25].</p>\n<p>An artificial general intelligence will do whatever is implied by its initial design. And we will be helpless to stop it from achieving its goals. Goals that won&rsquo;t automatically respect our values [26].</p>\n<p>A likely implication is the total extinction of all of humanity [27].</p>\n<h2 dir=\"ltr\">Further Reading</h2>\n<ul>\n<li><a href=\"http://michaelnielsen.org/blog/what-should-a-reasonable-person-believe-about-the-singularity/\">What should a reasonable person believe about the Singularity?</a></li>\n<li><a href=\"http://consc.net/papers/singularity.pdf\">The Singularity: A Philosophical Analysis</a></li>\n<li><a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Intelligence Explosion: Evidence and Import</a></li>\n<li><a href=\"http://kruel.co/papers/probable-intelligence-explosion.pdf\">Why an Intelligence Explosion is Probable</a></li>\n<li><a href=\"http://yudkowsky.net/singularity/ai-risk\">Artificial Intelligence as a Positive and Negative Factor in Global Risk</a></li>\n<li><a href=\"http://intelligence.org/upload/mostly-harmless.pdf\">From mostly harmless to civilization-threatening: pathways to dangerous artificial general intelligences</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">The Hanson-Yudkowsky AI-Foom Debate</a></li>\n<li><a href=\"http://facingthesingularity.com/\">Facing The Singularity</a></li>\n</ul>\n<p><strong>References</strong></p>\n<p align=\"left\">[1] Genetic Algorithms and Evolutionary Computation, <a href=\"http://www.talkorigins.org/faqs/genalg/genalg.html\">talkorigins.org/faqs/genalg/genalg.html</a><br /> [2] Fixing software bugs in 10 minutes or less using evolutionary computation, <a href=\"http://www.genetic-programming.org/hc2009/1-Forrest/Forrest-Presentation.pdf\">genetic-programming.org/hc2009/1-Forrest/Forrest-Presentation.pdf</a><br /> [3] Automatically Finding Patches Using Genetic Programming, <a href=\"http://www.genetic-programming.org/hc2009/1-Forrest/Forrest-Paper-on-Patches.pdf\">genetic-programming.org/hc2009/1-Forrest/Forrest-Paper-on-Patches.pdf</a><br /> [4] A Genetic Programming Approach to Automated Software Repair, <a href=\"http://www.genetic-programming.org/hc2009/1-Forrest/Forrest-Paper-on-Repair.pdf\">genetic-programming.org/hc2009/1-Forrest/Forrest-Paper-on-Repair.pdf</a><br /> [5]GenProg: A Generic Method for Automatic Software Repair, <a href=\"http://www.cs.virginia.edu/%7Eweimer/p/weimer-tse2012-genprog.pdf\">virginia.edu/~weimer/p/weimer-tse2012-genprog.pdf</a><br /> [6] 29+ Evidences for Macroevolution (The Scientific Case for Common Descent), <a href=\"http://www.talkorigins.org/faqs/comdesc/\">talkorigins.org/faqs/comdesc/</a><br /> [7] Thermodynamics, Evolution and Creationism, <a href=\"http://www.talkorigins.org/faqs/thermo.html\">talkorigins.org/faqs/thermo.html</a><br /> [8] A Collection of Definitions of Intelligence, <a href=\"http://www.vetta.org/documents/A-Collection-of-Definitions-of-Intelligence.pdf\">vetta.org/documents/A-Collection-of-Definitions-of-Intelligence.pdf</a><br /> [9] <a href=\"http://plato.stanford.edu/entries/consciousness/\">plato.stanford.edu/entries/consciousness/</a><br /> [10] <a href=\"http://en.wikipedia.org/wiki/Science\">en.wikipedia.org/wiki/Science</a><br /> [11] <a href=\"http://en.wikipedia.org/wiki/Scientific_method\">en.wikipedia.org/wiki/Scientific_method</a><br /> [12] The Automation of Science, <a href=\"http://www.sciencemag.org/content/324/5923/85.abstract\">sciencemag.org/content/324/5923/85.abstract</a><br /> [13] Computer Program Self-Discovers Laws of Physics, <a href=\"http://www.wired.com/wiredscience/2009/04/newtonai/\">wired.com/wiredscience/2009/04/newtonai/</a><br /> [14] List of cognitive biases, <a href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\">en.wikipedia.org/wiki/List_of_cognitive_biases</a><br /> [15] Intelligence explosion, <a href=\"http://wiki.lesswrong.com/wiki/Intelligence_explosion\">wiki.lesswrong.com/wiki/Intelligence_explosion</a><br /> [16] 1% with Neil deGrasse Tyson, <a href=\"http://youtu.be/9nR9XEqrCvw\">youtu.be/9nR9XEqrCvw</a><br /> [17] Mongol military tactics and organization, <a href=\"http://en.wikipedia.org/wiki/Mongol_military_tactics_and_organization\">en.wikipedia.org/wiki/Mongol_military_tactics_and_organization</a><br /> [18] Wars of Alexander the Great, <a href=\"http://en.wikipedia.org/wiki/Wars_of_Alexander_the_Great\">en.wikipedia.org/wiki/Wars_of_Alexander_the_Great</a><br /> [19] Spanish colonization of the Americas, <a href=\"http://en.wikipedia.org/wiki/Spanish_colonization_of_the_Americas\">en.wikipedia.org/wiki/Spanish_colonization_of_the_Americas</a><br /> [20] A Quantitative Test of Hamilton's Rule for the Evolution of Altruism, <a href=\"http://www.plosbiology.org/article/info:doi/10.1371/journal.pbio.1000615\">plosbiology.org/article/info:doi/10.1371/journal.pbio.1000615</a><br /> [21] Algorithmic information theory, <a href=\"http://www.scholarpedia.org/article/Algorithmic_information_theory\">scholarpedia.org/article/Algorithmic_information_theory</a><br /> [22] Algorithmic probability, <a href=\"http://www.scholarpedia.org/article/Algorithmic_probability\">scholarpedia.org/article/Algorithmic_probability</a><br /> [23] The Nature of Self-Improving Arti\ufb01cial Intelligence, <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf\">selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf</a><br /> [24] The Basic AI Drives, <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf</a><br /> [25] Paperclip maximizer, <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">wiki.lesswrong.com/wiki/Paperclip_maximizer</a><br /> [26] Friendly artificial intelligence, <a href=\"http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence\">wiki.lesswrong.com/wiki/Friendly_artificial_intelligence</a><br /> [27] Existential Risk, <a href=\"http://www.existential-risk.org/\">existential-risk.org</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jN3PwnWDfe3yaJAW2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 22, "extendedScore": null, "score": 8.713996572359405e-07, "legacy": true, "legacyId": "14454", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"The_Power_of_Algorithms\" dir=\"ltr\">The Power of Algorithms</h2>\n<p>Evolutionary processes are the most evident example of the power of simple algorithms [1][2][3][4][5].</p>\n<p>The field of evolutionary biology gathered a vast amount of evidence [6] that established evolution as the process that explains the local decrease in entropy [7], the complexity of life.</p>\n<p>Since it can be conclusively shown that all life is an effect of an evolutionary process it is implicit that everything we do not understand about living beings is also an effect of evolution.</p>\n<p>We might not understand the nature of intelligence [8] and consciousness [9] but we do know that they are the result of an optimization process that is neither intelligent nor conscious.</p>\n<p>Therefore we know that it is possible for an physical optimization process to culminate in the creation of more advanced processes that feature superior qualities.</p>\n<p>One of these qualities is the human ability to observe and improve the optimization process that created us. The most obvious example being science [10].</p>\n<p>Science can be thought of as civilization-level self-improvement method. It allows us to work together in a systematic and efficient way and accelerate the rate at which further improvements are made.</p>\n<h2 dir=\"ltr\" id=\"The_Automation_of_Science\">The Automation of Science</h2>\n<p>We know that optimization processes that can create improved versions of themselves are possible, even without an explicit understanding of their own workings, as exemplified by natural selection.</p>\n<p>We know that optimization processes can lead to self-reinforcing improvements, as exemplified by the adaptation of the scientific method [11] as an improved evolutionary process and successor of natural selection.</p>\n<p>Which raises questions about the continuation of this self-reinforcing feedback cycle and its possible implications.</p>\n<p>One possibility is to automate science [12][13] and apply it to itself and its improvement.</p>\n<p>But science is a tool and its bottleneck are its users. Humans, the biased [14] effect of the blind idiot god that is evolution.</p>\n<p>Therefore the next logical step is to use science to figure out how to replace humans by a better version of themselves, artificial general intelligence.</p>\n<p>Artificial general intelligence, that can recursively optimize itself [15], is the logical endpoint of various converging and self-reinforcing feedback cycles.</p>\n<h2 dir=\"ltr\" id=\"Risks_from_AI\">Risks from AI</h2>\n<p>Will we be able to build an artificial general intelligence? Yes, sooner or later.</p>\n<p>Even the unintelligent, unconscious and aimless process of natural selection was capable of creating goal-oriented, intelligent and conscious agents that can think ahead, jump fitness gaps and improve upon the process that created them to engage in prediction and direct experimentation.</p>\n<p>The question is, what are the possible implications of the invention of an artificial, fully autonomous, intelligent and goal-oriented optimization process?</p>\n<p>One good bet is that such an agent will recursively improve its most versatile, and therefore instrumentally useful, resource. It will improve its general intelligence, respectively cross-domain optimization power.</p>\n<p>Since it is unlikely that human intelligence is the optimum, the positive feedback effect, that is a result of using intelligence amplifications to amplify intelligence, is likely to lead to a level of intelligence that is generally more capable than the human intelligence level.</p>\n<p>Humans are unlikely to be the most efficient thinkers because evolution is mindless and has no goals. Evolution did not actively try to create the smartest thing possible.</p>\n<p>Evolution is further not limitlessly creative, each step of an evolutionary design must increase the fitness of its host. Which makes it probable that there are artificial mind designs that can do what no product of natural selection could accomplish, since an intelligent artificer does not rely on the incremental fitness of each step in the development process.</p>\n<p>It is actually possible that human general intelligence is the bare minimum. Because the human level of intelligence might have been sufficient to both survive and reproduce and that therefore no further evolutionary pressure existed to select for even higher levels of general intelligence.</p>\n<p>The implications of this possibility might be the creation of an intelligent agent that is more capable than humans in every sense. Maybe because it does directly employ superior approximations of our best formal methods, that tell us how to update based on evidence and how to choose between various actions. Or maybe it will simply think faster. It doesn\u2019t matter.</p>\n<p>What matters is that a superior intellect is probable and that it will be better than us at discovering knowledge and inventing new technology. Technology that will make it even more powerful and likely invincible.</p>\n<p>And that is the problem. We might be unable to control such a superior being. Just like a group of chimpanzees is unable to stop a company from clearing its forest [16].</p>\n<p>But even if such a being is only slightly more capable than us. We might find ourselves at its mercy nonetheless.</p>\n<p>Human history provides us with many examples [17][18][19] that make it abundantly clear that even the slightest advance can enable one group to dominate others.</p>\n<p>What happens is that the dominant group imposes its values on the others. Which in turn raises the question of what values an artificial general intelligence might have and the implications of those values for us.</p>\n<p>Due to our evolutionary origins, our struggle for survival and the necessity to cooperate with other agents, we are equipped with many values and a concern for the welfare of others [20].</p>\n<p>The information theoretic complexity [21][22] of our values is very high. Which means that it is highly unlikely for similar values to automatically arise in agents that are the product of intelligent design, agents that never underwent the million of years of competition with other agents that equipped humans with altruism and general compassion.</p>\n<p>But that does not mean that an artificial intelligence won\u2019t have any goals [23][24]. Just that those goals will be simple and their realization remorseless [25].</p>\n<p>An artificial general intelligence will do whatever is implied by its initial design. And we will be helpless to stop it from achieving its goals. Goals that won\u2019t automatically respect our values [26].</p>\n<p>A likely implication is the total extinction of all of humanity [27].</p>\n<h2 dir=\"ltr\" id=\"Further_Reading\">Further Reading</h2>\n<ul>\n<li><a href=\"http://michaelnielsen.org/blog/what-should-a-reasonable-person-believe-about-the-singularity/\">What should a reasonable person believe about the Singularity?</a></li>\n<li><a href=\"http://consc.net/papers/singularity.pdf\">The Singularity: A Philosophical Analysis</a></li>\n<li><a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\">Intelligence Explosion: Evidence and Import</a></li>\n<li><a href=\"http://kruel.co/papers/probable-intelligence-explosion.pdf\">Why an Intelligence Explosion is Probable</a></li>\n<li><a href=\"http://yudkowsky.net/singularity/ai-risk\">Artificial Intelligence as a Positive and Negative Factor in Global Risk</a></li>\n<li><a href=\"http://intelligence.org/upload/mostly-harmless.pdf\">From mostly harmless to civilization-threatening: pathways to dangerous artificial general intelligences</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">The Hanson-Yudkowsky AI-Foom Debate</a></li>\n<li><a href=\"http://facingthesingularity.com/\">Facing The Singularity</a></li>\n</ul>\n<p><strong id=\"References\">References</strong></p>\n<p align=\"left\">[1] Genetic Algorithms and Evolutionary Computation, <a href=\"http://www.talkorigins.org/faqs/genalg/genalg.html\">talkorigins.org/faqs/genalg/genalg.html</a><br> [2] Fixing software bugs in 10 minutes or less using evolutionary computation, <a href=\"http://www.genetic-programming.org/hc2009/1-Forrest/Forrest-Presentation.pdf\">genetic-programming.org/hc2009/1-Forrest/Forrest-Presentation.pdf</a><br> [3] Automatically Finding Patches Using Genetic Programming, <a href=\"http://www.genetic-programming.org/hc2009/1-Forrest/Forrest-Paper-on-Patches.pdf\">genetic-programming.org/hc2009/1-Forrest/Forrest-Paper-on-Patches.pdf</a><br> [4] A Genetic Programming Approach to Automated Software Repair, <a href=\"http://www.genetic-programming.org/hc2009/1-Forrest/Forrest-Paper-on-Repair.pdf\">genetic-programming.org/hc2009/1-Forrest/Forrest-Paper-on-Repair.pdf</a><br> [5]GenProg: A Generic Method for Automatic Software Repair, <a href=\"http://www.cs.virginia.edu/%7Eweimer/p/weimer-tse2012-genprog.pdf\">virginia.edu/~weimer/p/weimer-tse2012-genprog.pdf</a><br> [6] 29+ Evidences for Macroevolution (The Scientific Case for Common Descent), <a href=\"http://www.talkorigins.org/faqs/comdesc/\">talkorigins.org/faqs/comdesc/</a><br> [7] Thermodynamics, Evolution and Creationism, <a href=\"http://www.talkorigins.org/faqs/thermo.html\">talkorigins.org/faqs/thermo.html</a><br> [8] A Collection of Definitions of Intelligence, <a href=\"http://www.vetta.org/documents/A-Collection-of-Definitions-of-Intelligence.pdf\">vetta.org/documents/A-Collection-of-Definitions-of-Intelligence.pdf</a><br> [9] <a href=\"http://plato.stanford.edu/entries/consciousness/\">plato.stanford.edu/entries/consciousness/</a><br> [10] <a href=\"http://en.wikipedia.org/wiki/Science\">en.wikipedia.org/wiki/Science</a><br> [11] <a href=\"http://en.wikipedia.org/wiki/Scientific_method\">en.wikipedia.org/wiki/Scientific_method</a><br> [12] The Automation of Science, <a href=\"http://www.sciencemag.org/content/324/5923/85.abstract\">sciencemag.org/content/324/5923/85.abstract</a><br> [13] Computer Program Self-Discovers Laws of Physics, <a href=\"http://www.wired.com/wiredscience/2009/04/newtonai/\">wired.com/wiredscience/2009/04/newtonai/</a><br> [14] List of cognitive biases, <a href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\">en.wikipedia.org/wiki/List_of_cognitive_biases</a><br> [15] Intelligence explosion, <a href=\"http://wiki.lesswrong.com/wiki/Intelligence_explosion\">wiki.lesswrong.com/wiki/Intelligence_explosion</a><br> [16] 1% with Neil deGrasse Tyson, <a href=\"http://youtu.be/9nR9XEqrCvw\">youtu.be/9nR9XEqrCvw</a><br> [17] Mongol military tactics and organization, <a href=\"http://en.wikipedia.org/wiki/Mongol_military_tactics_and_organization\">en.wikipedia.org/wiki/Mongol_military_tactics_and_organization</a><br> [18] Wars of Alexander the Great, <a href=\"http://en.wikipedia.org/wiki/Wars_of_Alexander_the_Great\">en.wikipedia.org/wiki/Wars_of_Alexander_the_Great</a><br> [19] Spanish colonization of the Americas, <a href=\"http://en.wikipedia.org/wiki/Spanish_colonization_of_the_Americas\">en.wikipedia.org/wiki/Spanish_colonization_of_the_Americas</a><br> [20] A Quantitative Test of Hamilton's Rule for the Evolution of Altruism, <a href=\"http://www.plosbiology.org/article/info:doi/10.1371/journal.pbio.1000615\">plosbiology.org/article/info:doi/10.1371/journal.pbio.1000615</a><br> [21] Algorithmic information theory, <a href=\"http://www.scholarpedia.org/article/Algorithmic_information_theory\">scholarpedia.org/article/Algorithmic_information_theory</a><br> [22] Algorithmic probability, <a href=\"http://www.scholarpedia.org/article/Algorithmic_probability\">scholarpedia.org/article/Algorithmic_probability</a><br> [23] The Nature of Self-Improving Arti\ufb01cial Intelligence, <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf\">selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf</a><br> [24] The Basic AI Drives, <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf</a><br> [25] Paperclip maximizer, <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">wiki.lesswrong.com/wiki/Paperclip_maximizer</a><br> [26] Friendly artificial intelligence, <a href=\"http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence\">wiki.lesswrong.com/wiki/Friendly_artificial_intelligence</a><br> [27] Existential Risk, <a href=\"http://www.existential-risk.org/\">existential-risk.org</a></p>", "sections": [{"title": "The Power of Algorithms", "anchor": "The_Power_of_Algorithms", "level": 1}, {"title": "The Automation of Science", "anchor": "The_Automation_of_Science", "level": 1}, {"title": "Risks from AI", "anchor": "Risks_from_AI", "level": 1}, {"title": "Further Reading", "anchor": "Further_Reading", "level": 1}, {"title": "References", "anchor": "References", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "33 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-24T16:01:33.295Z", "modifiedAt": null, "url": null, "title": "Decision Theories: A Semi-Formal Analysis, Part I", "slug": "decision-theories-a-semi-formal-analysis-part-i", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:54.747Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2JdvZw3CXzafxQugN/decision-theories-a-semi-formal-analysis-part-i", "pageUrlRelative": "/posts/2JdvZw3CXzafxQugN/decision-theories-a-semi-formal-analysis-part-i", "linkUrl": "https://www.lesswrong.com/posts/2JdvZw3CXzafxQugN/decision-theories-a-semi-formal-analysis-part-i", "postedAtFormatted": "Saturday, March 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Decision%20Theories%3A%20A%20Semi-Formal%20Analysis%2C%20Part%20I&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecision%20Theories%3A%20A%20Semi-Formal%20Analysis%2C%20Part%20I%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2JdvZw3CXzafxQugN%2Fdecision-theories-a-semi-formal-analysis-part-i%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Decision%20Theories%3A%20A%20Semi-Formal%20Analysis%2C%20Part%20I%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2JdvZw3CXzafxQugN%2Fdecision-theories-a-semi-formal-analysis-part-i", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2JdvZw3CXzafxQugN%2Fdecision-theories-a-semi-formal-analysis-part-i", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2190, "htmlBody": "<h2>Or: The Problem with Naive Decision Theory<br /></h2>\n<p><strong>Previously:</strong> <a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/\" target=\"_self\">Decision Theories: A Less Wrong Primer</a></p>\n<p><strong>Summary of Sequence:</strong> <em>In </em><em>the context of a tournament for computer programs</em><em>, I give almost-explicit versions of causal, timeless, ambient, updateless, and several other decision theories. I explain the mathematical considerations that make decision theories tricky in general, and end with a bunch of links to the relevant recent research. This sequence is heavier on the math than <a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/\" target=\"_blank\">the primer</a> was, but is meant to be accessible to a fairly general audience. Understanding the basics of <a href=\"http://en.wikipedia.org/wiki/Game_theory\" target=\"_blank\">game theory</a> (and <a href=\"http://en.wikipedia.org/wiki/Nash_equilibrium\" target=\"_blank\">Nash equilibria</a>) will be essential. Knowing about things like <a href=\"http://en.wikipedia.org/wiki/G%C3%B6del_numbering\" target=\"_blank\">G&ouml;del numbering</a>, <a href=\"http://en.wikipedia.org/wiki/Quine_%28computing%29\" target=\"_blank\">quining</a> and <a href=\"/lw/t6/the_cartoon_guide_to_l%C3%B6bs_theorem/\" target=\"_blank\">L&ouml;b's Theorem</a> will help, but won't be required.<br /></em></p>\n<p><strong>Summary of Post:</strong> <em>I introduce a context in which we can avoid most of the usual tricky philosophical problems and formalize the decision theories of interest. Then I show the chief issue with what might be called \"naive decision theory\": the problem of spurious counterfactual reasoning. In future posts, we'll see how other decision theories get around that problem.</em></p>\n<p>In my <a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/\" target=\"_blank\">Decision Theory Primer</a>, I gave an intuitive explanation of decision theories; now I'd like to give a <a href=\"http://yudkowsky.net/rational/technical\" target=\"_blank\">technical explanation</a>. The main difficulty is that in the real world, there are all sorts of complications that are extraneous to the core of decision theory. (I'll mention more of these in the last post, but an obvious one is that we can't be sure that our perception and memory match reality.)</p>\n<p>In order to avoid such difficulties, I'll need to demonstrate decision theory in a completely artificial setting: a tournament among computer programs.</p>\n<p><img src=\"http://images.lesswrong.com/t3_axl_0.png?v=db05d2a4c02b1aa6015785c9315cdbdb\" alt=\"\" width=\"640\" height=\"400\" /><a id=\"more\"></a></p>\n<p>You're a computer programmer entering a tournament for spectacular stakes. But you won't be competing in person: instead, you'll be submitting code for a program to represent you, and that program will be competing one-on-one with other programs in a series of games.</p>\n<p>You don't know what the specific games are, but the contest has specified some ground rules:</p>\n<ul>\n<li>In each game, your program and its opponent will have to choose separately between several options. (There are independent sources of randomness available to each program, so <a href=\"http://en.wikipedia.org/wiki/Strategy_%28game_theory%29\" target=\"_blank\">mixed strategies</a> are legal.)</li>\n<li>The games are pure strategic conflicts: the expected payouts to <em>each</em> programmer depend on the outputs of <em>both</em> programs, and can be calculated simply if you know those outputs. (In particular, you can represent each game as a <a href=\"http://en.wikipedia.org/wiki/Normal-form_game\" target=\"_blank\">payoff matrix</a> in terms of the programs' outputs.) For example, your program might play the <a href=\"http://wiki.lesswrong.com/wiki/Prisoner%27s_dilemma\" target=\"_blank\">Prisoner's Dilemma</a> against another program.</li>\n<li>Both programs have access to the source code of <em>each other</em> and of the game they're playing.</li>\n<li>The programs don't get to carry any memories from round to round, or modify their source code at any stage. (This makes the analysis <em>much</em> simpler, and makes it even more impressive if we find unexploitable programs which are capable of mutual cooperation.)</li>\n<li>While many of the programs were written by other programmers trying to win prizes for themselves, there are also some special algorithms included to test your reactions; for example, there could be programs that always cooperate in the Prisoner's Dilemma, or an instance of <a href=\"http://wiki.lesswrong.com/wiki/Newcomb%27s_problem\" target=\"_blank\">Newcomb's Predictor</a>.</li>\n<li>The tournament may also include some exact copies of your own program, but <em>you</em> won't get any of the prizes won by these extra copies, only the prizes won by your actual entry. (There's no way for your program to distinguish whether it's the original or a copy.)</li>\n<li>There are more than enough prizes to go around, so you don't have to make anyone else lose, you just want your program to win as much as it can.</li>\n<li>Also, there will be enough games played that most of the variance should wash out, so you should aim for the highest expected value per round rather than worry about the <a href=\"http://en.wikipedia.org/wiki/Risk_averse#Utility_of_money\" target=\"_blank\">concave utility</a> of more prizes.</li>\n</ul>\n<p>So, what kind of program should you write?</p>\n<p>In the next few posts, we'll examine several ideas, problems and decision theories, increasing our sophistication as we go. We'll use <strong>X</strong> to denote your program, and <em>x<sub>1</sub></em>, . . . , <em>x<sub>n</sub></em> to denote its possible outputs; likewise, your opponent in the current round is <strong>Y</strong> with possible outputs <em>y<sub>1</sub></em>, . . . , <em>y<sub>m</sub></em>. We'll let <strong>U</strong>(<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>) denote the resulting payout to you if <strong>X</strong> outputs <em>x<sub>i</sub></em> and <strong>Y</strong> outputs <em>y<sub>j</sub></em>.</p>\n<h3>Idea 1: Play defense with a Nash equilibrium</h3>\n<p>In our example, we know what utility function the opponent should have: its own expected payout.<a href=\"#utilities\" target=\"_self\"><sup>0</sup></a> Any such game has at least one <a href=\"http://en.wikipedia.org/wiki/Nash_equilibrium\" target=\"_blank\">Nash equilibrium</a>, a pair of strategies (which may be <a href=\"http://en.wikipedia.org/wiki/Mixed_strategy#Mixed_strategy\" target=\"_blank\">mixed</a>) such that if <strong>X</strong> and <strong>Y</strong> both adopted them, then neither would be better off unilaterally switching strategies. In that sense, at least, if <strong>X</strong> plays a Nash equilibrium, it can be sure of not being exploited by <strong>Y</strong>. (In particular, <strong>X</strong> will never end up tricked into cooperating in the Prisoner's Dilemma while <strong>Y</strong> defects.)</p>\n<p>Of course, there may be more than one Nash equilibrium in a game, and these may be of unequal value if the game is non-zero-sum. (<a href=\"http://en.wikipedia.org/wiki/Coordination_game\" target=\"_blank\">Coordination problems</a> are tricky in general.) So this is underspecified; still, choosing an arbitrary Nash equilibrium is a decent backup strategy. But we can often do better:</p>\n<h3>Idea 2: Inference</h3>\n<p>The most basic intuition a human being has in this situation is to start trying to deduce things about <strong>Y</strong>'s output from its source code, or even deduce things directly about <strong>U</strong>. This idea is best illustrated by playing Rock, Paper, Scissors against <a href=\"http://www.youtube.com/watch?v=NMxzU6hxrNA\" target=\"_blank\">a player who always throws Rock</a>: if you figure this out, then you should of course play Paper rather than the Nash equilibrium of 1/3 Rock, 1/3 Paper, 1/3 Scissors. (And in a coordination game, you'd prefer to settle on the <em>same</em> Nash equilibrium that <strong>Y</strong> outputs.)</p>\n<p>Automating inference is <a href=\"http://en.wikipedia.org/wiki/Automated_reasoning\" target=\"_blank\">an exceedingly difficult problem in general</a>, though researchers have made substantial progress. All the decision theories we'll talk about will include some sort of \"inference module\", which can be applied to the source code of <strong>Y</strong> to deduce its output, applied to the code of the full round (including <strong>X</strong>, <strong>Y</strong>, and the payoff matrix) to deduce the value of <strong>U</strong>, etc.</p>\n<h3>Problem: You can't deduce everything</h3>\n<p><a href=\"http://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems\" target=\"_blank\">G&ouml;del's First Incompleteness Theorem</a> and the <a href=\"http://en.wikipedia.org/wiki/Halting_problem\" target=\"_blank\">Halting Problem</a> both imply that it's impossible to write a program that correctly deduces <em>in general</em><a href=\"#ingeneral\" target=\"_self\"><sup>1</sup></a> the output of arbitrary other programs. So we have to be prepared for our inference module to fail sometimes.</p>\n<p>A well-written inference module will either return a correct answer for a question or return \"Unknown\"; a sloppily-written module can get stuck in an infinite process, and a badly-written one will return an incorrect answer sometimes. It should be clear that we'll want our inference module to be of the first sort.</p>\n<p>It seems we have enough already to define our first candidate decision theory:</p>\n<h3>Naive Decision Theory</h3>\n<p>Let's first consider the approach that seems most obvious. Since we know the source code of the entire round (including <strong>X</strong> and <strong>Y</strong>), we could implement the following program:</p>\n<ul>\n<li>For each <em>x<sub>i</sub></em>, assume the output of <strong>X</strong> is <em>x<sub>i</sub></em>, and try to deduce the expected value of <strong>U</strong>. (That is, try and deduce statements of the form \"if (output <strong>X</strong>)=<em>x<sub>i</sub></em> then <strong>U</strong>=<em>u<sub>i</sub></em>\" for some <em>u<sub>i</sub></em>).</li>\n<li>If this succeeds for each <em>x<sub>i</sub></em>, output the <em>x<sub>i</sub></em> for which <em>u<sub>i</sub></em> is the largest.</li>\n<li>If this does not succeed for some <em>x<sub>i</sub></em>, output a Nash equilibrium strategy.</li>\n</ul>\n<p>This \"naive decision theory\" certainly qualifies for our tournament; it may be a bit trickier to write an inference module that does an open-ended search for the value of <strong>U</strong>, but it's not impossible (since human mathematicians solve open-ended deduction problems all the time). And it looks like the worst-case scenario is a Nash equilibrium, not total exploitation. What could possibly go wrong?</p>\n<h3>Problem: Beware self-fulfilling prophecies!</h3>\n<p>There's a reason that we don't normally ask an automated theorem prover to consider questions about its <em>own</em> mathematical structure: if we ask the question in a certain way, <a href=\"/lw/t8/you_provably_cant_trust_yourself/\" target=\"_blank\">any choice becomes a self-fulfilling prophecy</a>.</p>\n<p>If <strong>X</strong> deduces its own output by a valid process, then it's created a self-fulfilling prophecy for its output, and the problem with <em>that</em> is that a bad self-fulfilling prophecy is just as consistent as a good one. If we want to use statements like \"if (output <strong>X</strong>)=<em>x<sub>i</sub></em> then <strong>U</strong>=<em>u<sub>i</sub></em>\" to make our final choice, then we have to beware the other half of logical implication, that \"not P\" implies \"if P then Q\". This allows for what we might call <em>spurious counterfactuals</em>, which can throw off the actual decision in a perfectly self-consistent way.</p>\n<p><a name=\"spurious\"></a>Consider, for example, the one-player game where <strong>X</strong> gets $1 for outputting <em>a</em>, or $10 for outputting <em>b</em>. We want <strong>X</strong> to do the following:</p>\n<ul>\n<li>Prove \"if (output <strong>X</strong>)=<em>a</em> then <strong>U</strong>=1\"</li>\n<li>Prove \"if (output <strong>X</strong>)=<em>b</em> then <strong>U</strong>=10\"</li>\n<li>Output <em>b</em>. </li>\n</ul>\n<p>But it's just as consistent for <strong>X</strong> to do the following:</p>\n<ul>\n<li>Prove \"(output <strong>X</strong>)=<em>a</em>\"</li>\n<li>Prove \"if (output <strong>X</strong>)=<em>a</em> then <strong>U</strong>=1\"</li>\n<li>Prove \"if (output <strong>X</strong>)=<em>b</em> then <strong>U</strong>=0\"</li>\n<li>Output <em>a</em>.</li>\n</ul>\n<p>How could that possibly work? Since (output <strong>X</strong>)=<em>a</em>, the third line is a true logical statement! It's like the fact that <a href=\"http://xkcd.com/704/\" target=\"_blank\">you can prove anything if you assume a falsehood</a> (though rather than unconditionally accepting a false premise, <strong>X</strong> is using a false premise as the antecedent of a <a href=\"http://en.wikipedia.org/wiki/Material_conditional\" target=\"_blank\">material conditional</a>). In this example, the very order in which <strong>X</strong> looks for proofs (which is part of the definition of <strong>X</strong>) affects which counterfactuals <strong>X</strong> can and cannot prove. (One important thing to note is that <strong>X</strong> <em>cannot</em> prove a spurious counterfactual about the action that it <em>does</em> output, only about the ones that it doesn't!)</p>\n<p>I don't need to tell you that the second chain of proofs is <em>not</em> what we want <strong>X</strong> to do. Worse, if this is a real bug, then it could also be an <em>exploitable vulnerability</em>: if your source code for <strong>X</strong> were released in advance of the tournament, then other programmers might write programs that cause <strong>X</strong> to generate spurious counterfactuals for all but the moves that are most favorable to <strong>Y</strong>.</p>\n<h3>Can NDT be salvaged?<br /></h3>\n<p>Let's consider some quick fixes before we give up on Naive Decision Theory.</p>\n<p>Can we simply prohibit <strong>X</strong> from ever deducing \"(output <strong>X</strong>)=<em>x<sub>i</sub></em>\" as a step? This doesn't work because of the possibility of indirect self-reference; <strong>X</strong> could end up deducing some seemingly innocuous statements which happen to correspond to its own <a href=\"http://en.wikipedia.org/wiki/G%C3%B6del_numbering\" target=\"_blank\">G&ouml;del numbering</a>, and the spurious counterfactuals would follow from those- without <strong>X</strong> ever having noticed that it had done anything of the sort. And it's provably impossible for <strong>X</strong> to recognize every possible G&ouml;del numbering for its own inference module!</p>\n<p>Next, it might seem like an inference module should stumble on the \"genuine\" counterfactuals before running into spurious ones, since the \"genuine\" ones seem necessarily simpler. However, it turns out (<a href=\"/r/discussion/lw/b5t/a_model_of_udt_with_a_malicious_proof_searcher/\" target=\"_blank\">as proved by cousin_it</a>) that one can write a valid but malicious inference module which returns and acts on a spurious proof, and (<a href=\"/lw/b5t/an_example_of_selffulfilling_spurious_proofs_in/67bx\" target=\"_blank\">as proved by gRR</a>) that a game with malicious code can similarly dupe a NDT agent with a good inference module!</p>\n<p>Lastly, it seems safer to deduce counterfactuals \"if (output <strong>X</strong>)=<em>x<sub>i</sub></em> then (output <strong>Y</strong>)=<em>y<sub>j</sub></em>\" and apply the <strong>U</strong>(<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>) afterwards. And indeed, I can't see how to make <strong>Y</strong> exploit <strong>X</strong> in a straight-up Prisoner's Dilemma if that algorithm is used. There are still two problems, though. First, this algorithm now depends on the values <strong>U</strong>(<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>) being given to it by authority- it can't safely deduce them from the source code for the game. And secondly, it could two-box on Newcomb's Problem and defect against itself in the Prisoner's Dilemma if it finds spurious counterfactuals there.</p>\n<p>Thus it seems we'll need to do something substantially different.</p>\n<h3>Well, now what?<br /></h3>\n<p>There are several ways we could write a decision theory to do inference without risking spurious counterfactuals, and indeed the decision theories we discuss on Less Wrong correspond to different valid approaches. The differences in their decisions come not from better-written inference modules, but from more effective strategies for using their inference module. In the posts to come, I'll show you how they work in detail.</p>\n<p><strong>Next:</strong> <a href=\"/lw/az6/decision_theories_a_semiformal_analysis_part_ii/\" target=\"_self\">Part II: Causal Decision Theory and Substitution</a></p>\n<h3>Notes:<br /></h3>\n<p><strong><a name=\"utilities\"></a>0.</strong> Things get wacky if we don't know the utility function of the opponent; fortunately, even the special cases like Newcomb's predictor can be expressed as expected utility maximizers for some payoff matrix (in this case, one where the predictor gets rewarded when it matches your decision exactly).</p>\n<p><a name=\"ingeneral\"></a><strong>1.</strong> That \"in general\" is important: it's quite possible to write programs that deduce the outputs of plenty of other programs. It just so happens that there's always some program that your inference module will fail on. The <a href=\"http://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument\" target=\"_blank\">classic way to generate a failure case</a> is to run the inference module on a modified version of itself, such that returning a correct answer would induce a contradiction. This isn't just a hypothetical disability: if <strong>X</strong> is trying to deduce the output of <strong>Y</strong> in this round, and <strong>Y</strong> is trying to deduce the output of <strong>X</strong> in this round, then we might have exactly this problem!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2JdvZw3CXzafxQugN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 36, "extendedScore": null, "score": 8.5e-05, "legacy": true, "legacyId": "14169", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Or__The_Problem_with_Naive_Decision_Theory\">Or: The Problem with Naive Decision Theory<br></h2>\n<p><strong>Previously:</strong> <a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/\" target=\"_self\">Decision Theories: A Less Wrong Primer</a></p>\n<p><strong>Summary of Sequence:</strong> <em>In </em><em>the context of a tournament for computer programs</em><em>, I give almost-explicit versions of causal, timeless, ambient, updateless, and several other decision theories. I explain the mathematical considerations that make decision theories tricky in general, and end with a bunch of links to the relevant recent research. This sequence is heavier on the math than <a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/\" target=\"_blank\">the primer</a> was, but is meant to be accessible to a fairly general audience. Understanding the basics of <a href=\"http://en.wikipedia.org/wiki/Game_theory\" target=\"_blank\">game theory</a> (and <a href=\"http://en.wikipedia.org/wiki/Nash_equilibrium\" target=\"_blank\">Nash equilibria</a>) will be essential. Knowing about things like <a href=\"http://en.wikipedia.org/wiki/G%C3%B6del_numbering\" target=\"_blank\">G\u00f6del numbering</a>, <a href=\"http://en.wikipedia.org/wiki/Quine_%28computing%29\" target=\"_blank\">quining</a> and <a href=\"/lw/t6/the_cartoon_guide_to_l%C3%B6bs_theorem/\" target=\"_blank\">L\u00f6b's Theorem</a> will help, but won't be required.<br></em></p>\n<p><strong>Summary of Post:</strong> <em>I introduce a context in which we can avoid most of the usual tricky philosophical problems and formalize the decision theories of interest. Then I show the chief issue with what might be called \"naive decision theory\": the problem of spurious counterfactual reasoning. In future posts, we'll see how other decision theories get around that problem.</em></p>\n<p>In my <a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/\" target=\"_blank\">Decision Theory Primer</a>, I gave an intuitive explanation of decision theories; now I'd like to give a <a href=\"http://yudkowsky.net/rational/technical\" target=\"_blank\">technical explanation</a>. The main difficulty is that in the real world, there are all sorts of complications that are extraneous to the core of decision theory. (I'll mention more of these in the last post, but an obvious one is that we can't be sure that our perception and memory match reality.)</p>\n<p>In order to avoid such difficulties, I'll need to demonstrate decision theory in a completely artificial setting: a tournament among computer programs.</p>\n<p><img src=\"http://images.lesswrong.com/t3_axl_0.png?v=db05d2a4c02b1aa6015785c9315cdbdb\" alt=\"\" width=\"640\" height=\"400\"><a id=\"more\"></a></p>\n<p>You're a computer programmer entering a tournament for spectacular stakes. But you won't be competing in person: instead, you'll be submitting code for a program to represent you, and that program will be competing one-on-one with other programs in a series of games.</p>\n<p>You don't know what the specific games are, but the contest has specified some ground rules:</p>\n<ul>\n<li>In each game, your program and its opponent will have to choose separately between several options. (There are independent sources of randomness available to each program, so <a href=\"http://en.wikipedia.org/wiki/Strategy_%28game_theory%29\" target=\"_blank\">mixed strategies</a> are legal.)</li>\n<li>The games are pure strategic conflicts: the expected payouts to <em>each</em> programmer depend on the outputs of <em>both</em> programs, and can be calculated simply if you know those outputs. (In particular, you can represent each game as a <a href=\"http://en.wikipedia.org/wiki/Normal-form_game\" target=\"_blank\">payoff matrix</a> in terms of the programs' outputs.) For example, your program might play the <a href=\"http://wiki.lesswrong.com/wiki/Prisoner%27s_dilemma\" target=\"_blank\">Prisoner's Dilemma</a> against another program.</li>\n<li>Both programs have access to the source code of <em>each other</em> and of the game they're playing.</li>\n<li>The programs don't get to carry any memories from round to round, or modify their source code at any stage. (This makes the analysis <em>much</em> simpler, and makes it even more impressive if we find unexploitable programs which are capable of mutual cooperation.)</li>\n<li>While many of the programs were written by other programmers trying to win prizes for themselves, there are also some special algorithms included to test your reactions; for example, there could be programs that always cooperate in the Prisoner's Dilemma, or an instance of <a href=\"http://wiki.lesswrong.com/wiki/Newcomb%27s_problem\" target=\"_blank\">Newcomb's Predictor</a>.</li>\n<li>The tournament may also include some exact copies of your own program, but <em>you</em> won't get any of the prizes won by these extra copies, only the prizes won by your actual entry. (There's no way for your program to distinguish whether it's the original or a copy.)</li>\n<li>There are more than enough prizes to go around, so you don't have to make anyone else lose, you just want your program to win as much as it can.</li>\n<li>Also, there will be enough games played that most of the variance should wash out, so you should aim for the highest expected value per round rather than worry about the <a href=\"http://en.wikipedia.org/wiki/Risk_averse#Utility_of_money\" target=\"_blank\">concave utility</a> of more prizes.</li>\n</ul>\n<p>So, what kind of program should you write?</p>\n<p>In the next few posts, we'll examine several ideas, problems and decision theories, increasing our sophistication as we go. We'll use <strong>X</strong> to denote your program, and <em>x<sub>1</sub></em>, . . . , <em>x<sub>n</sub></em> to denote its possible outputs; likewise, your opponent in the current round is <strong>Y</strong> with possible outputs <em>y<sub>1</sub></em>, . . . , <em>y<sub>m</sub></em>. We'll let <strong>U</strong>(<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>) denote the resulting payout to you if <strong>X</strong> outputs <em>x<sub>i</sub></em> and <strong>Y</strong> outputs <em>y<sub>j</sub></em>.</p>\n<h3 id=\"Idea_1__Play_defense_with_a_Nash_equilibrium\">Idea 1: Play defense with a Nash equilibrium</h3>\n<p>In our example, we know what utility function the opponent should have: its own expected payout.<a href=\"#utilities\" target=\"_self\"><sup>0</sup></a> Any such game has at least one <a href=\"http://en.wikipedia.org/wiki/Nash_equilibrium\" target=\"_blank\">Nash equilibrium</a>, a pair of strategies (which may be <a href=\"http://en.wikipedia.org/wiki/Mixed_strategy#Mixed_strategy\" target=\"_blank\">mixed</a>) such that if <strong>X</strong> and <strong>Y</strong> both adopted them, then neither would be better off unilaterally switching strategies. In that sense, at least, if <strong>X</strong> plays a Nash equilibrium, it can be sure of not being exploited by <strong>Y</strong>. (In particular, <strong>X</strong> will never end up tricked into cooperating in the Prisoner's Dilemma while <strong>Y</strong> defects.)</p>\n<p>Of course, there may be more than one Nash equilibrium in a game, and these may be of unequal value if the game is non-zero-sum. (<a href=\"http://en.wikipedia.org/wiki/Coordination_game\" target=\"_blank\">Coordination problems</a> are tricky in general.) So this is underspecified; still, choosing an arbitrary Nash equilibrium is a decent backup strategy. But we can often do better:</p>\n<h3 id=\"Idea_2__Inference\">Idea 2: Inference</h3>\n<p>The most basic intuition a human being has in this situation is to start trying to deduce things about <strong>Y</strong>'s output from its source code, or even deduce things directly about <strong>U</strong>. This idea is best illustrated by playing Rock, Paper, Scissors against <a href=\"http://www.youtube.com/watch?v=NMxzU6hxrNA\" target=\"_blank\">a player who always throws Rock</a>: if you figure this out, then you should of course play Paper rather than the Nash equilibrium of 1/3 Rock, 1/3 Paper, 1/3 Scissors. (And in a coordination game, you'd prefer to settle on the <em>same</em> Nash equilibrium that <strong>Y</strong> outputs.)</p>\n<p>Automating inference is <a href=\"http://en.wikipedia.org/wiki/Automated_reasoning\" target=\"_blank\">an exceedingly difficult problem in general</a>, though researchers have made substantial progress. All the decision theories we'll talk about will include some sort of \"inference module\", which can be applied to the source code of <strong>Y</strong> to deduce its output, applied to the code of the full round (including <strong>X</strong>, <strong>Y</strong>, and the payoff matrix) to deduce the value of <strong>U</strong>, etc.</p>\n<h3 id=\"Problem__You_can_t_deduce_everything\">Problem: You can't deduce everything</h3>\n<p><a href=\"http://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems\" target=\"_blank\">G\u00f6del's First Incompleteness Theorem</a> and the <a href=\"http://en.wikipedia.org/wiki/Halting_problem\" target=\"_blank\">Halting Problem</a> both imply that it's impossible to write a program that correctly deduces <em>in general</em><a href=\"#ingeneral\" target=\"_self\"><sup>1</sup></a> the output of arbitrary other programs. So we have to be prepared for our inference module to fail sometimes.</p>\n<p>A well-written inference module will either return a correct answer for a question or return \"Unknown\"; a sloppily-written module can get stuck in an infinite process, and a badly-written one will return an incorrect answer sometimes. It should be clear that we'll want our inference module to be of the first sort.</p>\n<p>It seems we have enough already to define our first candidate decision theory:</p>\n<h3 id=\"Naive_Decision_Theory\">Naive Decision Theory</h3>\n<p>Let's first consider the approach that seems most obvious. Since we know the source code of the entire round (including <strong>X</strong> and <strong>Y</strong>), we could implement the following program:</p>\n<ul>\n<li>For each <em>x<sub>i</sub></em>, assume the output of <strong>X</strong> is <em>x<sub>i</sub></em>, and try to deduce the expected value of <strong>U</strong>. (That is, try and deduce statements of the form \"if (output <strong>X</strong>)=<em>x<sub>i</sub></em> then <strong>U</strong>=<em>u<sub>i</sub></em>\" for some <em>u<sub>i</sub></em>).</li>\n<li>If this succeeds for each <em>x<sub>i</sub></em>, output the <em>x<sub>i</sub></em> for which <em>u<sub>i</sub></em> is the largest.</li>\n<li>If this does not succeed for some <em>x<sub>i</sub></em>, output a Nash equilibrium strategy.</li>\n</ul>\n<p>This \"naive decision theory\" certainly qualifies for our tournament; it may be a bit trickier to write an inference module that does an open-ended search for the value of <strong>U</strong>, but it's not impossible (since human mathematicians solve open-ended deduction problems all the time). And it looks like the worst-case scenario is a Nash equilibrium, not total exploitation. What could possibly go wrong?</p>\n<h3 id=\"Problem__Beware_self_fulfilling_prophecies_\">Problem: Beware self-fulfilling prophecies!</h3>\n<p>There's a reason that we don't normally ask an automated theorem prover to consider questions about its <em>own</em> mathematical structure: if we ask the question in a certain way, <a href=\"/lw/t8/you_provably_cant_trust_yourself/\" target=\"_blank\">any choice becomes a self-fulfilling prophecy</a>.</p>\n<p>If <strong>X</strong> deduces its own output by a valid process, then it's created a self-fulfilling prophecy for its output, and the problem with <em>that</em> is that a bad self-fulfilling prophecy is just as consistent as a good one. If we want to use statements like \"if (output <strong>X</strong>)=<em>x<sub>i</sub></em> then <strong>U</strong>=<em>u<sub>i</sub></em>\" to make our final choice, then we have to beware the other half of logical implication, that \"not P\" implies \"if P then Q\". This allows for what we might call <em>spurious counterfactuals</em>, which can throw off the actual decision in a perfectly self-consistent way.</p>\n<p><a name=\"spurious\"></a>Consider, for example, the one-player game where <strong>X</strong> gets $1 for outputting <em>a</em>, or $10 for outputting <em>b</em>. We want <strong>X</strong> to do the following:</p>\n<ul>\n<li>Prove \"if (output <strong>X</strong>)=<em>a</em> then <strong>U</strong>=1\"</li>\n<li>Prove \"if (output <strong>X</strong>)=<em>b</em> then <strong>U</strong>=10\"</li>\n<li>Output <em>b</em>. </li>\n</ul>\n<p>But it's just as consistent for <strong>X</strong> to do the following:</p>\n<ul>\n<li>Prove \"(output <strong>X</strong>)=<em>a</em>\"</li>\n<li>Prove \"if (output <strong>X</strong>)=<em>a</em> then <strong>U</strong>=1\"</li>\n<li>Prove \"if (output <strong>X</strong>)=<em>b</em> then <strong>U</strong>=0\"</li>\n<li>Output <em>a</em>.</li>\n</ul>\n<p>How could that possibly work? Since (output <strong>X</strong>)=<em>a</em>, the third line is a true logical statement! It's like the fact that <a href=\"http://xkcd.com/704/\" target=\"_blank\">you can prove anything if you assume a falsehood</a> (though rather than unconditionally accepting a false premise, <strong>X</strong> is using a false premise as the antecedent of a <a href=\"http://en.wikipedia.org/wiki/Material_conditional\" target=\"_blank\">material conditional</a>). In this example, the very order in which <strong>X</strong> looks for proofs (which is part of the definition of <strong>X</strong>) affects which counterfactuals <strong>X</strong> can and cannot prove. (One important thing to note is that <strong>X</strong> <em>cannot</em> prove a spurious counterfactual about the action that it <em>does</em> output, only about the ones that it doesn't!)</p>\n<p>I don't need to tell you that the second chain of proofs is <em>not</em> what we want <strong>X</strong> to do. Worse, if this is a real bug, then it could also be an <em>exploitable vulnerability</em>: if your source code for <strong>X</strong> were released in advance of the tournament, then other programmers might write programs that cause <strong>X</strong> to generate spurious counterfactuals for all but the moves that are most favorable to <strong>Y</strong>.</p>\n<h3 id=\"Can_NDT_be_salvaged_\">Can NDT be salvaged?<br></h3>\n<p>Let's consider some quick fixes before we give up on Naive Decision Theory.</p>\n<p>Can we simply prohibit <strong>X</strong> from ever deducing \"(output <strong>X</strong>)=<em>x<sub>i</sub></em>\" as a step? This doesn't work because of the possibility of indirect self-reference; <strong>X</strong> could end up deducing some seemingly innocuous statements which happen to correspond to its own <a href=\"http://en.wikipedia.org/wiki/G%C3%B6del_numbering\" target=\"_blank\">G\u00f6del numbering</a>, and the spurious counterfactuals would follow from those- without <strong>X</strong> ever having noticed that it had done anything of the sort. And it's provably impossible for <strong>X</strong> to recognize every possible G\u00f6del numbering for its own inference module!</p>\n<p>Next, it might seem like an inference module should stumble on the \"genuine\" counterfactuals before running into spurious ones, since the \"genuine\" ones seem necessarily simpler. However, it turns out (<a href=\"/r/discussion/lw/b5t/a_model_of_udt_with_a_malicious_proof_searcher/\" target=\"_blank\">as proved by cousin_it</a>) that one can write a valid but malicious inference module which returns and acts on a spurious proof, and (<a href=\"/lw/b5t/an_example_of_selffulfilling_spurious_proofs_in/67bx\" target=\"_blank\">as proved by gRR</a>) that a game with malicious code can similarly dupe a NDT agent with a good inference module!</p>\n<p>Lastly, it seems safer to deduce counterfactuals \"if (output <strong>X</strong>)=<em>x<sub>i</sub></em> then (output <strong>Y</strong>)=<em>y<sub>j</sub></em>\" and apply the <strong>U</strong>(<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>) afterwards. And indeed, I can't see how to make <strong>Y</strong> exploit <strong>X</strong> in a straight-up Prisoner's Dilemma if that algorithm is used. There are still two problems, though. First, this algorithm now depends on the values <strong>U</strong>(<em>x<sub>i</sub></em>,<em>y<sub>j</sub></em>) being given to it by authority- it can't safely deduce them from the source code for the game. And secondly, it could two-box on Newcomb's Problem and defect against itself in the Prisoner's Dilemma if it finds spurious counterfactuals there.</p>\n<p>Thus it seems we'll need to do something substantially different.</p>\n<h3 id=\"Well__now_what_\">Well, now what?<br></h3>\n<p>There are several ways we could write a decision theory to do inference without risking spurious counterfactuals, and indeed the decision theories we discuss on Less Wrong correspond to different valid approaches. The differences in their decisions come not from better-written inference modules, but from more effective strategies for using their inference module. In the posts to come, I'll show you how they work in detail.</p>\n<p><strong>Next:</strong> <a href=\"/lw/az6/decision_theories_a_semiformal_analysis_part_ii/\" target=\"_self\">Part II: Causal Decision Theory and Substitution</a></p>\n<h3 id=\"Notes_\">Notes:<br></h3>\n<p><strong><a name=\"utilities\"></a>0.</strong> Things get wacky if we don't know the utility function of the opponent; fortunately, even the special cases like Newcomb's predictor can be expressed as expected utility maximizers for some payoff matrix (in this case, one where the predictor gets rewarded when it matches your decision exactly).</p>\n<p><a name=\"ingeneral\"></a><strong>1.</strong> That \"in general\" is important: it's quite possible to write programs that deduce the outputs of plenty of other programs. It just so happens that there's always some program that your inference module will fail on. The <a href=\"http://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument\" target=\"_blank\">classic way to generate a failure case</a> is to run the inference module on a modified version of itself, such that returning a correct answer would induce a contradiction. This isn't just a hypothetical disability: if <strong>X</strong> is trying to deduce the output of <strong>Y</strong> in this round, and <strong>Y</strong> is trying to deduce the output of <strong>X</strong> in this round, then we might have exactly this problem!</p>", "sections": [{"title": "Or: The Problem with Naive Decision Theory", "anchor": "Or__The_Problem_with_Naive_Decision_Theory", "level": 1}, {"title": "Idea 1: Play defense with a Nash equilibrium", "anchor": "Idea_1__Play_defense_with_a_Nash_equilibrium", "level": 2}, {"title": "Idea 2: Inference", "anchor": "Idea_2__Inference", "level": 2}, {"title": "Problem: You can't deduce everything", "anchor": "Problem__You_can_t_deduce_everything", "level": 2}, {"title": "Naive Decision Theory", "anchor": "Naive_Decision_Theory", "level": 2}, {"title": "Problem: Beware self-fulfilling prophecies!", "anchor": "Problem__Beware_self_fulfilling_prophecies_", "level": 2}, {"title": "Can NDT be salvaged?", "anchor": "Can_NDT_be_salvaged_", "level": 2}, {"title": "Well, now what?", "anchor": "Well__now_what_", "level": 2}, {"title": "Notes:", "anchor": "Notes_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "90 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 90, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["af9MjBqF2hgu3EN6r", "ALCnqX6Xx8bpFMZq3", "rm8tv9qZ9nwQxhshx", "2GebvAXXfRMTjY2g7", "TxDcvtn2teAMobG2Z"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-24T23:40:42.980Z", "modifiedAt": null, "url": null, "title": "Teaching Bayesian statistics? Looking for advice.", "slug": "teaching-bayesian-statistics-looking-for-advice", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:34.005Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tadrinth", "createdAt": "2011-01-17T21:56:00.316Z", "isAdmin": false, "displayName": "tadrinth"}, "userId": "CS3sMF9uZHErdWN9q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LXBzTYR2wgaWpR7jf/teaching-bayesian-statistics-looking-for-advice", "pageUrlRelative": "/posts/LXBzTYR2wgaWpR7jf/teaching-bayesian-statistics-looking-for-advice", "linkUrl": "https://www.lesswrong.com/posts/LXBzTYR2wgaWpR7jf/teaching-bayesian-statistics-looking-for-advice", "postedAtFormatted": "Saturday, March 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Teaching%20Bayesian%20statistics%3F%20Looking%20for%20advice.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATeaching%20Bayesian%20statistics%3F%20Looking%20for%20advice.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLXBzTYR2wgaWpR7jf%2Fteaching-bayesian-statistics-looking-for-advice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Teaching%20Bayesian%20statistics%3F%20Looking%20for%20advice.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLXBzTYR2wgaWpR7jf%2Fteaching-bayesian-statistics-looking-for-advice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLXBzTYR2wgaWpR7jf%2Fteaching-bayesian-statistics-looking-for-advice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 873, "htmlBody": "<p>I am considering trying to get a job teaching statistics from a  Bayesian perspective at the university or community college level, and I  figured I should get some advice, both on whether or not that's a good  idea and how to go about it.</p>\n<p>Some background on myself: I just got my Masters in computational  biology, to go along with a double Bachelors in Computer Science and  Cell/Molecular Biology.&nbsp; I was in a PhD program but between enjoying  teaching more than research and grad school making me unhappy, I decided  to get the Masters instead.&nbsp; I've accumulated a bunch of experience as a  teaching assistant (about six semesters) and I'm currently working as a  Teaching Specialist (which is a fancy title for a full time TA).&nbsp; I'm  now in my fourth semester of TAing biostatistics, which is pretty much  just introductory statistics with biology examples.&nbsp; However, it's  taught from a frequentist perspective.&nbsp;</p>\n<p>I like learning,  optimizing, teaching, and doing a good job of things I see people doing  badly. I also seem to do dramatically better in highly structured  environments.&nbsp; So, I've been thinking about trying to find a lecturer  position teaching statistics from a Bayesian perspective.&nbsp; All of the  really smart professors I know personally who have an opinion on the  topic are Bayesians, Less Wrong as a community prefers Bayesianism, and I  prefer it.&nbsp; This seems like a good way to get paid to do something I  would enjoy and raise the rationality waterline while I'm at it.&nbsp;</p>\n<p>So,  the first question is whether this is the most efficient way to get  paid to promote rationality.&nbsp; I did send in an application to the Center  for Modern Rationality but I haven't heard back, so I'm guessing that  isn't an option.&nbsp; Teaching Bayesian statistics seems like the second  best bet, but there are probably other options I haven't thought of.&nbsp; I  could teach biology or programming classes, but I think those would be  less optimal uses of my skills.</p>\n<p>Next, is this even a viable option for me, given my  qualifications?&nbsp; I haven't taken any education classes to speak of (the  class on how to be a TA might count but it was a joke).&nbsp; My job searches  suggest that community colleges do hire people with Masters to teach,  but universities mostly do not.&nbsp; I don't know what it takes to actually  get hired in the current economic climate.</p>\n<p>I'm also trying to figure out if this is the best career option  given my skillset (or at least estimate the opportunity cost in terms of  ease of finding jobs and compensation).&nbsp; I have a number of other  potential options available:&nbsp; I could try to find a research position in  bioinformatics or computational biology, or look for programming  positions.&nbsp; Bioinformatics really makes \"analyzing sequence data\" and  that's something I've barely touched since undergrad; my thesis used  existing gene alignments.&nbsp; I could probably brush up and learn the  current tools if I wanted, but I have hardly any experience in that  area.&nbsp; Computational biology might be a better bet, but it's a  ridiculously varied field and so far I have not much enjoyed doing  research.</p>\n<p>I could probably look for programming jobs, but they would mostly  not leverage my biology skills; while I am a very good programmer for a  biologist, and a very good biologist for a programmer, I'm not amazing  at either.&nbsp; I <em>can</em> actually program: my thesis project involved  lots of Ruby scripts to generate and manipulate data prior to  statistical analysis, and I've also written things like a flocking  implementation and a simple vector graphics drawing program.&nbsp; Everything  I've written has been just enough to do what I needed it to do. I did  not teach myself to program in general, but I did teach myself Ruby, if  that helps estimate my level of programming talent.&nbsp; Yudkowsky did just  point out that programming potentially pays REALLY well, possibly better  than any of my other career options, but that may be limited to very high talent and/or very experienced programmers.</p>\n<p>Assuming it is a good idea for me to try to teach statistics, and  assuming I have a reasonable shot at finding such a job, is it  realistic to try to teach statistics from a Bayesian perspective to  undergrads?&nbsp; Frequentist approaches are still pretty common, so the  class would almost certainly have to cover them as well, which means  there's a LOT of material to cover. Bayesian methods generally involve  some amount of calculus, although I have found an introductory textbook  which uses minimal calculus.&nbsp; That might be a bit much to cram into a  single semester, especially depending on the quality of the students  (physics majors can probably handle a lot more than community college  Communications majors).</p>\n<p>Speaking of books, what books would be good to teach from, and  what books should I read to have enough background?&nbsp; I attempted Jaynes'  <span style=\"text-decoration: underline;\">Probability Theory: The Logic of Science</span> but it was a bit too high level for me to fully understand.&nbsp; I have been working my way through Bolstad's <span style=\"text-decoration: underline;\">Introduction to Bayesian Statistics</span> which is what I would probably teach the course from.&nbsp; Are there any  topics that Less Wrong thinks would be essential to cover in an  introductory Bayesian statistics course?</p>\n<p>Thanks in advance for all advice and suggestions!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LXBzTYR2wgaWpR7jf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 8.716253558074434e-07, "legacy": true, "legacyId": "14457", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-25T03:28:19.522Z", "modifiedAt": null, "url": null, "title": "Defeating Ugh Ideas", "slug": "defeating-ugh-ideas", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:33.354Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "falenas108", "createdAt": "2010-10-28T17:32:39.696Z", "isAdmin": false, "displayName": "falenas108"}, "userId": "BCX7q7NMQphQiXc8j", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yeuKz68qfDJZkCWWy/defeating-ugh-ideas", "pageUrlRelative": "/posts/yeuKz68qfDJZkCWWy/defeating-ugh-ideas", "linkUrl": "https://www.lesswrong.com/posts/yeuKz68qfDJZkCWWy/defeating-ugh-ideas", "postedAtFormatted": "Sunday, March 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Defeating%20Ugh%20Ideas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADefeating%20Ugh%20Ideas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyeuKz68qfDJZkCWWy%2Fdefeating-ugh-ideas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Defeating%20Ugh%20Ideas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyeuKz68qfDJZkCWWy%2Fdefeating-ugh-ideas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyeuKz68qfDJZkCWWy%2Fdefeating-ugh-ideas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 462, "htmlBody": "<p>Related to: <a href=\"/lw/21b/ugh_fields/\">Ugh Fields</a></p>\n<p>Ugh Fields are internal negative reactions that occur before the&nbsp;conscious&nbsp;mind has an opportunity to process the information, often resulting in less than optimal decision making.</p>\n<p>We have previously discussed Ugh Fields that involve performing tasks, but as far as I can tell we haven't had any posts on Ugh Fields about ideas. &nbsp;Ugh Fields towards ideas can be experienced both while trying to weigh the merits of an argument, or after one's opinion has altered.</p>\n<p>On Less Wrong, many ideas are accepted as true that, in some places, have negative connotations. &nbsp;And if someone has an Ugh Field towards an idea because of this, it can be difficult to change this to a neutral or positive position. This can cause problems while trying to think about these ideas rationally.</p>\n<p>For example, I grew up in a heavily liberal household. &nbsp;And because of this, when I was young I had a negative view on&nbsp;libertarianism. This caused problems, to the point where in my early teenage years I&nbsp;didn't weigh someone's economic views as highly just because they identified as a libertarian.&nbsp; But, once I actually looked into the policies of libertarianism and the results of these policies, my views shifted. &nbsp;And although my reaction improved over time, I still flinch away when I hear the word \"libertarian,\" despite considering myself one!</p>\n<p>And there are many other topics on Less Wrong someone could have this reaction for, including&nbsp;AI, FAI,&nbsp;atheism, transhumanism, cryonics, immortality, alternative diets, optimizing utility for charities, and metaphysics. &nbsp;An Ugh Field towards any of these ideas can hinder one's ability to update properly on hearing information about it.</p>\n<p>&nbsp;</p>\n<p>Some techniques I have used that have helped include:</p>\n<ul>\n<li>Mentally correcting myself whenever I notice that I'm flinching away from an Ugh Field.</li>\n<li>Actively think about why my view should change when I'm <a href=\"http://www.overcomingbias.com/2010/06/near-far-summary.html\">far</a>&nbsp;(Which may be&nbsp;supplemented by reminders from an Anki deck).</li>\n<li>Going through the arguments that convinced me that I should think differently in the first place.</li>\n<li>Considering myself <a href=\"http://wiki.lesswrong.com/wiki/In-group_bias\">one of them</a>, e.g. calling myself a libertarian rather than merely saying I support libertarian views. &nbsp;Caution should be taken with this to prevent too much&nbsp;in-grouping.</li>\n<li>Getting into a discussion with someone who holds the view I previously held (Essentially a combination of the last two).</li>\n<li>Trying to imagine positive outcomes as a result of updating in the right direction, or the negative results of not updating.</li>\n<li>Reading more about the position to normalize it in my brain.</li>\n</ul>\n<div><br /></div>\n<p>Things I have not done, but might work:</p>\n<ul>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Litany_of_Tarski\">Reciting the Litany of Tarski</a>.</li>\n<li>Writing down a list of ideas I have Ugh Fields, and reminding myself that these are problems (Could also use Anki).</li>\n</ul>\n<div><br /></div>\n<p>Does anyone else have suggestions?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yeuKz68qfDJZkCWWy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "14458", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EFQ3F6kmt4WHXRqik"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-25T03:47:47.138Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Zombie Responses", "slug": "seq-rerun-zombie-responses", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:31.854Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oYzJ32Y6tmiJLhESh/seq-rerun-zombie-responses", "pageUrlRelative": "/posts/oYzJ32Y6tmiJLhESh/seq-rerun-zombie-responses", "linkUrl": "https://www.lesswrong.com/posts/oYzJ32Y6tmiJLhESh/seq-rerun-zombie-responses", "postedAtFormatted": "Sunday, March 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Zombie%20Responses&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Zombie%20Responses%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoYzJ32Y6tmiJLhESh%2Fseq-rerun-zombie-responses%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Zombie%20Responses%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoYzJ32Y6tmiJLhESh%2Fseq-rerun-zombie-responses", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoYzJ32Y6tmiJLhESh%2Fseq-rerun-zombie-responses", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 146, "htmlBody": "<p>Today's post, <a href=\"/lw/p8/zombie_responses/\">Zombie Responses</a> was originally published on 05 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A few more points on Zombies.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/b4g/seq_rerun_zombies_zombies/\">Zombies! Zombies?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oYzJ32Y6tmiJLhESh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.717271447802787e-07, "legacy": true, "legacyId": "14459", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4moMTeCy9EqYxAher", "4AX5xifj6KYKBNTu2", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-25T05:27:17.534Z", "modifiedAt": null, "url": null, "title": "Less Wrong Sequences+Website feed app for Android", "slug": "less-wrong-sequences-website-feed-app-for-android", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:58.131Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "razor11", "createdAt": "2012-03-23T17:22:55.119Z", "isAdmin": false, "displayName": "razor11"}, "userId": "wj8XrZcSuirLJipme", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fBa4AeMn9MTQ2QqCQ/less-wrong-sequences-website-feed-app-for-android", "pageUrlRelative": "/posts/fBa4AeMn9MTQ2QqCQ/less-wrong-sequences-website-feed-app-for-android", "linkUrl": "https://www.lesswrong.com/posts/fBa4AeMn9MTQ2QqCQ/less-wrong-sequences-website-feed-app-for-android", "postedAtFormatted": "Sunday, March 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20Sequences%2BWebsite%20feed%20app%20for%20Android&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20Sequences%2BWebsite%20feed%20app%20for%20Android%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfBa4AeMn9MTQ2QqCQ%2Fless-wrong-sequences-website-feed-app-for-android%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20Sequences%2BWebsite%20feed%20app%20for%20Android%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfBa4AeMn9MTQ2QqCQ%2Fless-wrong-sequences-website-feed-app-for-android", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfBa4AeMn9MTQ2QqCQ%2Fless-wrong-sequences-website-feed-app-for-android", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 227, "htmlBody": "<p>I use my Android phone much more than my computer, and reading the Sequences on a mobile device is a pain. I needed an easy way to access the Sequences, but since there are no apps for this website I had to create one myself. Since I'm no app developer, I used the IBuildApp.com(trustworthy according to my research) website to make the application.</p>\n<p>Features:</p>\n<p>* Read ALL of the main Sequences and most of the minor ones</p>\n<p>* RSS feed to LessWrong.com for latest articles</p>\n<p>* No ads!</p>\n<p><br />Drawbacks:</p>\n<p>*Requires an Internet connection: I individually copy-pasted each Sequence(from the compilations of posts that many people have made) to the app. Unfortunately, the app development website did not save these on the app itself, but on its server. So to access a Sequence, you require an Internet connection.</p>\n<p>*Home screen doesn't look good, because I couldn't get an appropriately sized logo that the website would accept. The Index(where you access the Sequences) looks pretty neat though.</p>\n<p><br />If there are any mobile app developers here, please try to make a better version of it(hopefully one where data is saved offline). I made this for personal use so it's functional but could be done much better by a professional. I'm posting it here for other Android-using people(especially newbies like me) who might find this useful.</p>\n<p>Pictures:</p>\n<p><img src=\"http://i.imgur.com/KmlRA.png\" alt=\"\" width=\"240\" height=\"427\" /> &nbsp; &nbsp; &nbsp; &nbsp; <img src=\"http://i.imgur.com/QAXA2.png\" alt=\"\" width=\"240\" height=\"427\" />&nbsp; &nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp; <img src=\"http://i.imgur.com/E3Tu3.png\" alt=\"\" width=\"240\" height=\"427\" /></p>\n<p><img src=\"http://i.imgur.com/53AvH.png\" alt=\"\" width=\"240\" height=\"427\" /></p>\n<p>&nbsp;</p>\n<p>Download Link: http://174.142.192.87/builds/00101/101077/apps/LessWrongSequences.apk</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fBa4AeMn9MTQ2QqCQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 18, "extendedScore": null, "score": 8.717681459922646e-07, "legacy": true, "legacyId": "14460", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-25T10:06:39.795Z", "modifiedAt": null, "url": null, "title": "Not all signalling/status behaviors are bad", "slug": "not-all-signalling-status-behaviors-are-bad", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:27.422Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stabilizer", "createdAt": "2011-12-02T09:36:56.841Z", "isAdmin": false, "displayName": "Stabilizer"}, "userId": "Qa3pLZx3o2TApyfgq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ig84TWY2oxHM4nnYM/not-all-signalling-status-behaviors-are-bad", "pageUrlRelative": "/posts/ig84TWY2oxHM4nnYM/not-all-signalling-status-behaviors-are-bad", "linkUrl": "https://www.lesswrong.com/posts/ig84TWY2oxHM4nnYM/not-all-signalling-status-behaviors-are-bad", "postedAtFormatted": "Sunday, March 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Not%20all%20signalling%2Fstatus%20behaviors%20are%20bad&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANot%20all%20signalling%2Fstatus%20behaviors%20are%20bad%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fig84TWY2oxHM4nnYM%2Fnot-all-signalling-status-behaviors-are-bad%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Not%20all%20signalling%2Fstatus%20behaviors%20are%20bad%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fig84TWY2oxHM4nnYM%2Fnot-all-signalling-status-behaviors-are-bad", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fig84TWY2oxHM4nnYM%2Fnot-all-signalling-status-behaviors-are-bad", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 253, "htmlBody": "<p>As I've recently been understanding signalling/status behaviors common among humans and how they can cloud reality, I've had a tendency to automatically think of these behaviors as necessarily bad. But it seems to me that signalling behaviors are pretty much a lot of what we do during our waking life. If you or I have abstract goals: become better at physics, learn to play the guitar, become fit and so forth, these goals may fundamentally be derived from evolutionary drives and therefore their implementation in real life would probably make heavy use of signalling/status urges as primary motivators. But that does not necessarily reduce the usefulness of these behaviors in achieving these abstract goals<span style=\"font-size: 11px;\"><sup>1,2</sup></span>.&nbsp;</p>\n<p>I suppose what we need to be cautious about are inefficiencies. Signalling/status behaviors may not be the optimal way to achieve these goals. We would have to weigh the costs of actively ignoring your previous motivators and cultivating new motivators against the benefit we would gain by having motivations more aligned to our abstract goals.</p>\n<p>Any common examples of behaviors that assist and/or thwart goal-achievement? I've got one: health. Abstract goal: We want to be healthy and fit. Status/Signalling urge: desire to look good. The urge sometimes assists, as people try to exercise to look good, which makes you healthier. Sometimes it thwarts, like in the extreme example of anorexia. Has anybody made personal trade-offs?</p>\n<p>&nbsp;</p>\n<p><sup>Note: </sup></p>\n<p><sup> 1) I realize that this theme is underlying in many LW posts. </sup></p>\n<p><sup> 2) I'm not trying to talk about whether abstract goals are more important than signalling/status goals. </sup></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2EFq8dJbxKNzforjM": 2, "Q6P8jLn8hH7kbuXRr": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ig84TWY2oxHM4nnYM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 9, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "14463", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 78, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-25T11:01:11.948Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread, part 12", "slug": "harry-potter-and-the-methods-of-rationality-discussion-24", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:58.814Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Xachariah", "createdAt": "2011-02-03T00:44:58.546Z", "isAdmin": false, "displayName": "Xachariah"}, "userId": "sD2senrXqugoP4wit", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K4JBpAxhvstdnNbeg/harry-potter-and-the-methods-of-rationality-discussion-24", "pageUrlRelative": "/posts/K4JBpAxhvstdnNbeg/harry-potter-and-the-methods-of-rationality-discussion-24", "linkUrl": "https://www.lesswrong.com/posts/K4JBpAxhvstdnNbeg/harry-potter-and-the-methods-of-rationality-discussion-24", "postedAtFormatted": "Sunday, March 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK4JBpAxhvstdnNbeg%2Fharry-potter-and-the-methods-of-rationality-discussion-24%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK4JBpAxhvstdnNbeg%2Fharry-potter-and-the-methods-of-rationality-discussion-24", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK4JBpAxhvstdnNbeg%2Fharry-potter-and-the-methods-of-rationality-discussion-24", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 300, "htmlBody": "<p><strong>The new thread, discussion 13, is <a href=\"/lw/b7s/harry_potter_and_the_methods_of_rationality/\">here</a>.</strong></p>\n<p>&nbsp;</p>\n<p>This is a new thread to discuss Eliezer Yudkowsky's <em><a href=\"http://www.fanfiction.net/s/5782108/1/\">Harry Potter and the Methods of Rationality</a></em> and anything related to it. With three chapters recently the previous thread has very quickly reached 1000 comments. The latest chapter as of 25th March 2012 is <a href=\"http://www.fanfiction.net/s/5782108/80/Harry_Potter_and_the_Methods_of_Rationality\">Ch 80</a>.</p>\n<p>There is now a site dedicated to the story at <a href=\"http://hpmor.com/\">hpmor.com</a>, which is now the place to go to find the <a href=\"http://hpmor.com/notes/\">authors notes</a> and all sorts of other goodies. AdeleneDawner has kept an <a href=\"http://www.evernote.com/pub/adelenedawner/Eliezer\">archive of Author's Notes</a>. (This goes up to the notes for chapter 76, and is now not updating. The authors notes from chapter 77 onwards are on hpmor.com.)</p>\n<p><br />The first 5 discussion threads are on the main page under the <a href=\"/tag/harry_potter\">harry_potter tag</a>.&nbsp; Threads 6 and on (including this one) are in the <a href=\"/r/discussion/tag/harry_potter\">discussion section</a> using its separate tag system.&nbsp; Also: <a href=\"/lw/2ab/harry_potter_and_the_methods_of_rationality\">one</a>, <a href=\"/lw/2ie/harry_potter_and_the_methods_of_rationality\">two</a>, <a href=\"/lw/2nm/harry_potter_and_the_methods_of_rationality\">three</a>, <a href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality\">four</a>, <a href=\"/lw/30g/harry_potter_and_the_methods_of_rationality\">five</a>, <a href=\"/r/discussion/lw/364/harry_potter_and_the_methods_of_rationality\">six</a>, <a href=\"/r/discussion/lw/3rb/harry_potter_and_the_methods_of_rationality\">seven</a>, <a href=\"/lw/797/harry_potter_and_the_methods_of_rationality\">eight</a>, <a href=\"/lw/7jd/harry_potter_and_the_methods_of_rationality\">nine</a>, <a href=\"/lw/ams/harry_potter_and_the_methods_of_rationality\">ten</a>, <a href=\"/lw/axe/harry_potter_and_the_methods_of_rationality/\">eleven</a>.<br /><br />As a reminder, it's often useful to start your comment by indicating which chapter you are commenting on.<br /><br /><strong>Spoiler Warning</strong>:&nbsp; this thread is full of spoilers.&nbsp; With few exceptions, spoilers for MOR and canon are fair game to post, without warning or rot13.&nbsp; <a href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality/2v1l\">More specifically</a>:</p>\n<blockquote>\n<p>You do not need to rot13 anything about HP:MoR or the original Harry Potter series unless you are posting insider information from Eliezer Yudkowsky which is not supposed to be publicly available (which includes public statements by Eliezer that have been retracted).<br /><br />If there is evidence for X in MOR and/or canon then it's fine to post about X without rot13, even if you also have heard privately from Eliezer that X is true. But you should not post that \"Eliezer said X is true\" unless you use rot13.</p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K4JBpAxhvstdnNbeg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 8.719055629077665e-07, "legacy": true, "legacyId": "14464", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 699, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xK6Pswbozev6pv4A6", "59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "nnnd4KRQxs6DYcehD", "y2Hszb4Dsm5FggnDC", "6ae2kq3JmKvL4YPgk", "zvXfBqp6TSriNkmbg", "WQ7XMjqvuRRj8nkpu", "LKFR5pBA3bBkERDxL", "8yEdpDpGgvDWHeodM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-25T11:47:16.343Z", "modifiedAt": null, "url": null, "title": "An example of self-fulfilling spurious proofs in UDT", "slug": "an-example-of-self-fulfilling-spurious-proofs-in-udt", "viewCount": null, "lastCommentedAt": "2017-06-17T04:36:37.017Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2GebvAXXfRMTjY2g7/an-example-of-self-fulfilling-spurious-proofs-in-udt", "pageUrlRelative": "/posts/2GebvAXXfRMTjY2g7/an-example-of-self-fulfilling-spurious-proofs-in-udt", "linkUrl": "https://www.lesswrong.com/posts/2GebvAXXfRMTjY2g7/an-example-of-self-fulfilling-spurious-proofs-in-udt", "postedAtFormatted": "Sunday, March 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20example%20of%20self-fulfilling%20spurious%20proofs%20in%20UDT&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20example%20of%20self-fulfilling%20spurious%20proofs%20in%20UDT%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2GebvAXXfRMTjY2g7%2Fan-example-of-self-fulfilling-spurious-proofs-in-udt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20example%20of%20self-fulfilling%20spurious%20proofs%20in%20UDT%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2GebvAXXfRMTjY2g7%2Fan-example-of-self-fulfilling-spurious-proofs-in-udt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2GebvAXXfRMTjY2g7%2Fan-example-of-self-fulfilling-spurious-proofs-in-udt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 601, "htmlBody": "<p>Benja Fallenstein was <a href=\"/lw/2l2/what_a_reduction_of_could_could_look_like/2f7w\">the first to point out</a> that spurious proofs pose a problem for UDT. Vladimir Nesov and orthonormal&nbsp;<a href=\"/lw/axl/decision_theories_a_semiformal_analysis_part_i/64ey\">asked</a> for a formalization of that intuition. In this post I will give an example of a UDT-ish agent that fails due to having a malicious proof searcher, which feeds the agent a spurious but valid proof.</p>\n<p>The basic idea is to have an agent A that receives a proof P as input, and checks P for validity. If P is a valid proof that a certain action a is best in the current situation, then A outputs a, otherwise A tries to solve the current situation by its own means. Here's a first naive formalization, where U is the world program that returns a utility value, A is the agent program that returns an action, and P is the proof given to A:</p>\n<pre>def U():<br />&nbsp;&nbsp;if A(P)==1:<br />&nbsp;&nbsp;&nbsp;&nbsp;return 5<br />&nbsp;&nbsp;else:<br />&nbsp;&nbsp;&nbsp;&nbsp;return 10<br /><br />def A(P):<br />&nbsp;&nbsp;if P is a valid proof that A(P)==a implies U()==u, and A(P)!=a implies U()&lt;=u:<br />&nbsp;&nbsp;&nbsp;&nbsp;return a<br />&nbsp;&nbsp;else:<br />&nbsp;&nbsp;&nbsp;&nbsp;do whatever</pre>\n<p>This formalization cannot work because a proof P can never be long enough to contain statements about A(P) inside itself. To fix that problem, let's introduce a function Q that generates the proof P:</p>\n<pre>def U():<br />&nbsp;&nbsp;if A(Q())==1:<br />&nbsp;&nbsp;&nbsp;&nbsp;return 5<br />&nbsp;&nbsp;else:<br />&nbsp;&nbsp;&nbsp;&nbsp;return 10<br /><br />def A(P):<br />&nbsp;&nbsp;if P is a valid proof that A(Q())==a implies U()==u, and A(Q())!=a implies U()&lt;=u:<br />&nbsp;&nbsp;&nbsp;&nbsp;return a<br />&nbsp;&nbsp;else:<br />&nbsp;&nbsp;&nbsp;&nbsp;do whatever</pre>\n<p>In this case it's possible to write a function Q that returns a proof that makes A return the suboptimal action 1, which leads to utility 5 instead of 10. Here's how:</p>\n<p>Let X be the statement \"A(Q())==1 implies U()==5, and A(Q())!=1 implies U()&lt;=5\". Let Q be the program that enumerates all possible proofs trying to find a proof of X, and returns that proof if found. (The definitions of X and Q are mutually quined.) If X is provable at all, then Q will find that proof, and X will become true (by inspection of U and A). That reasoning is formalizable in our proof system, so the statement \"if X is provable, then X\" is provable. Therefore, by <a href=\"http://en.wikipedia.org/wiki/L%C3%B6b's_theorem\">L&ouml;b's theorem</a>, X is provable. So Q will find a proof of X, and A will return 1.</p>\n<p>One possible conclusion is that a UDT agent cannot use just any proof searcher or \"mathematical intuition module\" that's guaranteed to return valid mathematical arguments, because valid mathematical arguments can make the agent choose arbitrary actions. The proof searchers from&nbsp;<a href=\"/lw/2l2/what_a_reduction_of_could_could_look_like/ \">some</a>&nbsp;<a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle/ \">previous</a> <a href=\"/lw/b0e/a_model_of_udt_without_proof_limits/\">posts</a> were well-behaved by construction, but not all of them are.</p>\n<p>The troubling thing is that you may end up with a badly behaved proof searcher by accident. For example, consider a variation of U that adds some long and complicated computation to the \"else\" branch of U, before returning 10. That increases the length of the \"natural\" proof that a=2 is optimal, but the spurious proof for a=1 stays about the same length as it was, because the spurious proof can just ignore the \"else\" branch of U. This way the spurious proof can become much shorter than the natural proof. So if (for example) your math intuition module made the innocuous design decision of first looking at actions that are likely to have shorter proofs, you may end up with a spurious proof. And as a further plot twist, if we make U return 0 rather than 10 in the long-to-compute branch, you might choose the <em>correct</em> action due to a spurious proof instead of the natural one.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1, "FJ3MGb684F88BoN2o": 1, "YyGDbZhGtws5hEPda": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2GebvAXXfRMTjY2g7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 33, "extendedScore": null, "score": 8.719247463757729e-07, "legacy": true, "legacyId": "14465", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dC3rxrMkYKLfgTYEa", "Bj244uWzDBXvE2N2S", "m39dkp73YhN9QKYb9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-26T02:27:12.323Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Generalized Anti-Zombie Principle", "slug": "seq-rerun-the-generalized-anti-zombie-principle", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:34.095Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/r3PM6gSx2f8GKJyNH/seq-rerun-the-generalized-anti-zombie-principle", "pageUrlRelative": "/posts/r3PM6gSx2f8GKJyNH/seq-rerun-the-generalized-anti-zombie-principle", "linkUrl": "https://www.lesswrong.com/posts/r3PM6gSx2f8GKJyNH/seq-rerun-the-generalized-anti-zombie-principle", "postedAtFormatted": "Monday, March 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Generalized%20Anti-Zombie%20Principle&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Generalized%20Anti-Zombie%20Principle%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr3PM6gSx2f8GKJyNH%2Fseq-rerun-the-generalized-anti-zombie-principle%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Generalized%20Anti-Zombie%20Principle%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr3PM6gSx2f8GKJyNH%2Fseq-rerun-the-generalized-anti-zombie-principle", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr3PM6gSx2f8GKJyNH%2Fseq-rerun-the-generalized-anti-zombie-principle", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 171, "htmlBody": "<p>Today's post, <a href=\"/lw/p9/the_generalized_antizombie_principle/\">The Generalized Anti-Zombie Principle</a> was originally published on 05 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The argument against zombies can be extended into a more general anti-zombie principle. But, figuring out what that more general principle <em>is</em>, is more difficult than it may seem.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/b5n/seq_rerun_zombie_responses/\">Zombie Responses</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "r3PM6gSx2f8GKJyNH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.722875822738587e-07, "legacy": true, "legacyId": "14480", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kYAuNJX2ecH2uFqZ9", "oYzJ32Y6tmiJLhESh", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-26T06:43:16.877Z", "modifiedAt": null, "url": null, "title": "Fundamentals of kicking anthropic butt", "slug": "fundamentals-of-kicking-anthropic-butt", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:02.228Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EfoecWmSxcPcpBzEH/fundamentals-of-kicking-anthropic-butt", "pageUrlRelative": "/posts/EfoecWmSxcPcpBzEH/fundamentals-of-kicking-anthropic-butt", "linkUrl": "https://www.lesswrong.com/posts/EfoecWmSxcPcpBzEH/fundamentals-of-kicking-anthropic-butt", "postedAtFormatted": "Monday, March 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fundamentals%20of%20kicking%20anthropic%20butt&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFundamentals%20of%20kicking%20anthropic%20butt%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEfoecWmSxcPcpBzEH%2Ffundamentals-of-kicking-anthropic-butt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fundamentals%20of%20kicking%20anthropic%20butt%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEfoecWmSxcPcpBzEH%2Ffundamentals-of-kicking-anthropic-butt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEfoecWmSxcPcpBzEH%2Ffundamentals-of-kicking-anthropic-butt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2859, "htmlBody": "<p><br /><img style=\"float: right;\" src=\"http://dreager1.files.wordpress.com/2011/06/3102269717_1e707314af.jpg\" alt=\"Galactus\" width=\"299\" height=\"400\" /></p>\n<p><strong>Introduction</strong></p>\n<p>An anthropic problem is one where the very fact of your existence tells you something. \"I woke up this morning, therefore the earth did not get eaten by Galactus while I slumbered.\" Applying your existence to certainties like that is simple - if an event would have stopped you from existing, your existence tells you that that it hasn't happened. If something would only kill you 99% of the time, though, you have to use probability instead of deductive logic.&nbsp;Usually, it's pretty clear what to do. You simply apply&nbsp;<a href=\"http://en.wikipedia.org/wiki/Bayes'_theorem#Introductory_example\">Bayes' rule</a>: the probability of the world getting eaten by Galactus last night is equal to the prior probability of Galactus-consumption, times the probability of me waking up given that the world got eaten by Galactus, divided by the probability that I wake up at all.&nbsp;More exotic situations also show up under the umbrella of \"anthropics,\" such as getting duplicated or forgetting which person you are. Even if you've been duplicated, you can still assign probabilities. If there are a hundred copies of you in a hundred-room hotel and you don't know which one you are, don't bet too much that you're in room number 68.</p>\n<p>But this last sort of problem is harder, since it's not just a straightforward application of Bayes' rule. You have to determine the probability just from the information in the problem. Thinking in terms of information and symmetries is a useful problem-solving tool for getting probabilities in anthropic problems, which are simple enough to use it and confusing enough to need it. So first we'll cover what I mean by thinking in terms of information, and then we'll use this to solve a confusing-type anthropic problem.<a id=\"more\"></a></p>\n<p><strong>Parable of the coin</strong></p>\n<p>Eliezer has already written about what probability is in <a href=\"/lw/oj/probability_is_in_the_mind/\">Probability is in the Mind</a>. I will revisit it anyhow, using a similar example from <em>Probability Theory: The Logic of Science</em>.</p>\n<p>It is a truth universally acknowledged that when someone tosses a fair coin without cheating, there's a 0.5 probability of heads and a 0.5 probability of tails. You draw the coin forth, flip it, and slap it down. What is the probability that when you take your hand away, you see heads?</p>\n<p>Well, you performed a fair coin flip, so the chance of heads is 0.5. What's the problem? Well, imagine the coin's perspective. When you say \"heads, 0.5,\" that doesn't mean the coin has half of heads up and half of tails up: the coin is already how it's going to be, sitting pressed under your hand. And it's already how it is with probability 1, not 0.5. If the coin is <em>already</em> tails, how can you be correct when you say that it's heads with probability 0.5? If something is already determined, how can it still have the property of randomness?</p>\n<p>The key idea is that the randomness isn't in the coin, it's in your map of the coin. The coin can be tails all it dang likes, but if you don't know that, you shouldn't be expected to take it into account. The probability isn't a physical property of the coin, nor is it a property of flipping the coin - after all, your probability was still 0.5 when the truth was sitting right there under your hand. The probability is determined by the<em>&nbsp;information</em>&nbsp;you have about flipping the coin.</p>\n<p>Assigning probabilities to things tells you about the map, not the territory. It's like a machine that eats information and spits out probabilities, with those probabilities uniquely determined by the information that went in.&nbsp;Thinking about problems in terms of information, then, is about treating probabilities as the best possible answers for people with incomplete information. Probability isn't in the coin, so don't even bother thinking about the coin too much - think about the person and what they know.</p>\n<p>When trying to get probabilities from information, you're going to end up using&nbsp;symmetry&nbsp;a lot. Because information uniquely specifies probability, if you have identical information about two things, then you should assign them equal probability. For example, if someone switched the labels \"heads\" and \"tails\" in a fair coin flip, you couldn't tell that it had been done - you never had any different information about heads as opposed to tails. This symmetry means you should give heads and tails equal probability. Because heads and tails are mutually exclusive (they don't overlap) and exhaustive (there can't be anything else), the probabilities have to add to 1 (which is all the probability there is), so you give each of them probability 0.5.</p>\n<p><strong>Brief note on useless information</strong></p>\n<p>Real-world problems, even when they have symmetry, often start you off with a lot more information than \"it could be heads or tails.\" If we're flipping a real-world coin there's the temperature to consider, and the humidity, and the time of day, and the flipper's gender, and that sort of thing. If you're an ordinary human, you are allowed to call this stuff extraneous junk. Sometimes, this extra information could theoretically be&nbsp;<a href=\"/lw/o2/mutual_information_and_density_in_thingspace/\">correlated with the outcome</a>&nbsp;- maybe the humidity really matters somehow, or the time of day. But if you don't know <em>how</em>&nbsp;it's correlated, you have at least a&nbsp;<em>de facto</em>&nbsp;symmetry. Throwing away useless information is a key step in doing anything useful.</p>\n<p><strong>Sleeping Beauty</strong></p>\n<p>So thinking with information means assigning probabilities based on what people know, rather than treating probabilities as properties of objects. To actually apply this, we'll use as our example the <a href=\"http://wiki.lesswrong.com/wiki/Sleeping_Beauty_problem\">sleeping beauty problem</a>:</p>\n<dl style=\"margin-top: 0.2em; margin-bottom: 0.5em; font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 19px;\"><dd style=\"line-height: 1.5em; margin-left: 2em; margin-bottom: 0.1em;\">Suppose Sleeping Beauty volunteers to undergo the following experiment, which is described to her before it begins. On Sunday she is given a drug that sends her to sleep, and a coin is tossed. If the coin lands heads, Beauty is awakened and interviewed on Monday, and then the experiment ends. If the coin comes up tails, she is awakened and interviewed on Monday, given a second dose of the sleeping drug that makes her forget the events of Monday only, and awakened and interviewed again on Tuesday. The experiment then ends on Tuesday, without flipping the coin again.</dd></dl><dl style=\"margin-top: 0.2em; margin-bottom: 0.5em; font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 19px;\"><dd style=\"line-height: 1.5em; margin-left: 2em; margin-bottom: 0.1em;\">Beauty wakes up in the experiment and is asked, \"With what subjective probability do you believe that the coin landed tails?\"</dd> </dl>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; line-height: normal; font-size: small;\">If the coin lands heads, Sleeping Beauty is only asked for her guess once, while if the coin lands tails she is asked for her guess twice, but her memory is erased in between so she has the same memories each time.</span></p>\n<p>When trying to answer for Sleeping Beauty, many people reason as follows: It is a truth universally acknowledged that when someone tosses a fair coin without cheating, there's a 0.5 probability of heads and a 0.5 probability of tails. So since the probability of tails is 0.5, Beauty should say \"0.5,\" Q.E.D. &nbsp;Readers may notice that this argument is all about the coin, not about what Beauty knows. This violation of good practice may help explain why it is dead wrong.</p>\n<p><strong>Thinking with information: some warmups</strong></p>\n<p>To collect the ingredients of the solution, I'm going to first go through some similar-looking problems.</p>\n<p>In the Sleeping Beauty problem, she has to choose between three options - let's call them&nbsp;{H, Monday}, {T, Monday}, and {T, Tuesday}. So let's start with a very simple problem involving three options: the three-sided die. Just like for the fair coin, you know that the sides of the die are mutually exclusive and exhaustive, and you don't know anything else that&nbsp;would&nbsp;be correlated with one side showing up more than another. Sure, the sides have different labels, but the labels are extraneous junk as far as probability is concerned. Mutually exclusive and exhaustive means the probabilities have to add up to one, and the symmetry of your information about the sides means you should give them the same probabilities, so they each get probability 1/3.</p>\n<p>Next, what should Sleeping Beauty believe before the experiment begins? Beforehand, her information looks like this:&nbsp;she signed up for this experiment where you get woken up on Monday if the coin lands heads and on Monday and Tuesday if it lands tails.</p>\n<p><img style=\"float: right;\" src=\"http://images.lesswrong.com/t3_85i_0.png\" alt=\"Diagram of the Sleeping Beauty problem before it starts\" width=\"215\" height=\"293\" />This way of stating her information is good enough most of the time, but what's going on is clearer if we're a little more formal. There are three exhaustive (but not mutually exclusive) options: {H, Monday}, {T, Monday}, and {T, Tuesday}. She knows that anything with heads is mutually exclusive with anything with tails, and that {T, Tuesday} happens if and only if&nbsp;{T, Monday}&nbsp;happened.</p>\n<p>One good way to think of this last piece of information is as a special \"AND\" structure containing {T, Monday} and {T, Tuesday}, like in the picture to the right. What it means is that since the things that are \"AND\" happen together, the other probabilities won't change if we merge them into a single option, which I shall call {T, Both}. Now we have two options, {H, Monday} and {T, Both}, which are both exhaustive and mutually exclusive. This looks an awful lot like the fair coin, with probabilities of 0.5.</p>\n<p>But can we leave it at that? Why shouldn't two days be worth twice as much probability as one day, for instance? Well, it turns out we <em>can</em>&nbsp;leave at that, because we have now run out of information from the original problem. We used that there were three options, we used that they were exhaustive, we used that two of them always happened together, and we used that the remaining two were mutually exclusive. That's all, and so that's where we should leave it - any more and we'd be making up information not in the problem, which is bad.</p>\n<p>So to decompress, before the experiment begins Beauty assigns probability 0.5 to the coin landing heads and being woken up on Monday, probability 0.5 to the coin landing tails and being woken up on Monday, and probability 0.5 to the coin landing tails and being woken up on Tuesday. This adds up to 1.5, but that's okay since these things aren't all mutually exclusive.</p>\n<p><img style=\"float: right;\" src=\"http://images.lesswrong.com/t3_85i_1.png\" alt=\"Diagram of the two coins problem\" width=\"274\" height=\"227\" />Okay, now for one last warmup. Suppose you have two coins. You flip the first one, and if it lands heads, you place the second coin on the table heads up. If the first coin lands tails, though, you flip the second coin.</p>\n<p>This new problem looks sort of familiar. You have three options, {H, H}, {T, H} and {T, T}, and these options are mutually exclusive and exhaustive. So does that mean it's the same set of information as the three-sided die? Not quite. Similar to the \"AND\" previously, my drawing for this problem has an \"OR\" between {T, H} and {T,T}, representing additional information.</p>\n<p>I'd like to add a note here about my jargon. \"AND\" makes total sense. One thing happens <em>and</em> another thing happens. \"OR,\" however, doesn't make so much sense, because things that are mutually exclusive are already \"or\" by default - one thing happens <em>or</em> another thing happens. What it really means is that {H, H} has a symmetry with the <em>sum </em>of {T, H} and {T, T} (that is, {T, H} \"OR\" {T, T}). The \"OR\" can also be thought of as information about {H, H} instead - it contains what could have been both the {H, H} and {H, T} events, so there's a four-way symmetry in the problem, it's just been relabeled.</p>\n<p>When we had the \"AND\" structure, we merged the two options together to get {tails, both}. For \"OR,\" we can do a slightly different operation and replace {T, H} \"OR\" {T, T} by their sum, {T, either}. Now the options become {H, H} and {T, either}, which are mutually exclusive and exhaustive, which gets us back to the fair coin. Then, because {T, H} and {T, T} have a symmetry between them, you split the probability from {T, either} evenly to get&nbsp;probabilities&nbsp;of 0.5, 0.25, and 0.25.</p>\n<p><strong>Okay, for real now</strong></p>\n<p>Okay, so now what do things look like once the experiment has started? In English, now she knows that&nbsp;she signed up for this experiment where you get woken up on Monday if the coin lands heads and on Monday and Tuesday if it lands tails, went to sleep, and now she's been woken up.</p>\n<p>This might not seem that different from before, but the \"anthropic information\" that Beauty is currently one of the people in the experiment changes the formal picture a lot. Before, the three options were not mutually exclusive, because she was thinking about the future. But now&nbsp;{H, Monday}, {T, Monday}, and {T, Tuesday} are both exhaustive and mutually exclusive, because only one can be the case in the present. From the coin flip, she still knows that anything with heads is mutually exclusive with anything with tails. But once two things are mutually exclusive you can't make them any <em>more</em>&nbsp;mutually exclusive.</p>\n<p>But the \"AND\" information! What happens to that? Well, that was based on things always happening together, and we just got information that those things are mutually exclusive, so there's no more \"AND.\" It's possible to slip up here and reason that since there used to be some structure there, and now they're mutually exclusive, it's one or the other, therefore there must be \"OR\" information. At least the confusion in my terminology reflects an easy confusion to have, but this \"OR\" relationship isn't the same as mutual exclusivity. It's a specific piece of information that wasn't in the problem before the experiment, and wasn't part of the anthropic information (that was just mutual exclusivity). So Monday and Tuesday are \"or\" (mutually exclusive), but not \"OR\" (can be added up to use another symmetry).</p>\n<p>And so this anthropic requirement of mutual exclusivity turns out to make redundant or render null a big chunk of the previous information, which is strange. You end up left with three mutually exclusive, exhaustive options, with no particular asymmetry. This is the three-sided die information, and so each of&nbsp;{H, Monday}, {T, Monday}, and {T, Tuesday} should get probability 1/3. So when asked for P(tails), Beauty should answer 2/3.</p>\n<p><strong>\"SSA\" and \"SIA\"</strong></p>\n<p>When assigning prior probabilities in anthropic problems, there are two main \"easy\" ways to assign probabilities, and these methods go by the acronyms \"SSA\" and \"SIA.\" \"SSA\" is stated like this<sup><a href=\"http://en.wikipedia.org/wiki/Self-Sampling_Assumption\">1</a></sup>:</p>\n<p style=\"padding-left: 30px; \"><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px; \">All other things equal, an observer should reason as if they are randomly selected from the set of all&nbsp;</span><em style=\"font-family: sans-serif; font-size: 13px; line-height: 19px; \">actually existent</em><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px; \">&nbsp;observers (past, present and future) in their reference class.</span></p>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px; \"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; line-height: normal; font-size: small; \">For example, if you wanted the prior probability that you lived in Sweden, you might say ask \"what proportion of human beings have lived in Sweden?\"</span></span></p>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px; \"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; line-height: normal; font-size: small; \">On the other hand, \"SIA\" looks like this<sup><a href=\"http://en.wikipedia.org/wiki/Self-Indication_Assumption\">2</a></sup>:</span></span></p>\n<p style=\"padding-left: 30px; \"><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px; \">All other things equal, an observer should reason as if they are randomly selected from the set of all&nbsp;</span><em style=\"font-family: sans-serif; font-size: 13px; line-height: 19px; \">possible</em><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px; \">&nbsp;observers.</span></p>\n<p>Now the question becomes \"what proportion of possible observers live in Sweden?\" and suddenly it seems awfully improbable that anyone could live in Sweden.</p>\n<p>The astute reader will notice that these two \"assumptions\" correspond to two different sets of starting information. If you want a quick exercise, figure out what those two sets of information are now. I'll wait for&nbsp;you&nbsp;in the next paragraph.</p>\n<p>Hi again. The information assumed for SSA is pretty straightforward. You are supposed to reason as if you know that you're an actually existent observer, in some \"reference class.\" So an example set of information would be \"I exist/existed/will exist and am a human.\" Compared to that, SIA seems to barely assume any information at all - all you get to start with is \"I am a possible observer.\" Because \"existent observers in a reference class\" are a subset of possible observers, you can transform SIA into SSA by adding on more information, e.g. \"I exist and am a human.\" And then if you want to represent a more complicated problem, you have to add extra information on top of that, like \"I live in 2012\" or \"I have two X chromosomes.\"</p>\n<p>Trouble only sneaks in if you start to see these acronyms as mysterious probability generators rather than sets of starting information to build on. So don't do that.</p>\n<p><strong>Closing remarks</strong></p>\n<p>When faced with straightforward problems, you usually don't need to use this knowledge of where probability comes from. It's just rigorous and interesting, like knowing how to do integration as a <a href=\"http://www.vias.org/calculus/img/04_integration-10.gif\">Riemann sum</a>. But whenever you run into foundational or even particularly confusing problems, it's good to remember that probability is about making the best use you can of incomplete information. If not, you run the risk of a few silly failure modes, or even (<em>gasp</em>) frequentism.</p>\n<p>I recently read an academic paper<a href=\"http://arxiv.org/abs/0905.0624v1\"><sup>3</sup></a> that used the idea that in a multiverse, there will be some universe where a thrown coin comes up heads every time, and so the people in that universe will have very strange ideas about how coins work. <em>Therefore</em>, this actual academic paper argued, since reasoning with probability can lead people to be wrong, it cannot be applied to anything like a multiverse.</p>\n<p>My response is: what have you got that works better? In this post we worked through assigning probabilities by using all of our information. If you deviate from that, you're either throwing information away or making it up. Incomplete information lets you down sometimes, that's why it's called incomplete. But that doesn't license you to throw away information or make it up, out of some sort of dissatisfaction with reality. The truth is out there. But the probabilities are in here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EfoecWmSxcPcpBzEH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 25, "extendedScore": null, "score": 8.72393031114289e-07, "legacy": true, "legacyId": "10566", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><br><img style=\"float: right;\" src=\"http://dreager1.files.wordpress.com/2011/06/3102269717_1e707314af.jpg\" alt=\"Galactus\" width=\"299\" height=\"400\"></p>\n<p><strong id=\"Introduction\">Introduction</strong></p>\n<p>An anthropic problem is one where the very fact of your existence tells you something. \"I woke up this morning, therefore the earth did not get eaten by Galactus while I slumbered.\" Applying your existence to certainties like that is simple - if an event would have stopped you from existing, your existence tells you that that it hasn't happened. If something would only kill you 99% of the time, though, you have to use probability instead of deductive logic.&nbsp;Usually, it's pretty clear what to do. You simply apply&nbsp;<a href=\"http://en.wikipedia.org/wiki/Bayes'_theorem#Introductory_example\">Bayes' rule</a>: the probability of the world getting eaten by Galactus last night is equal to the prior probability of Galactus-consumption, times the probability of me waking up given that the world got eaten by Galactus, divided by the probability that I wake up at all.&nbsp;More exotic situations also show up under the umbrella of \"anthropics,\" such as getting duplicated or forgetting which person you are. Even if you've been duplicated, you can still assign probabilities. If there are a hundred copies of you in a hundred-room hotel and you don't know which one you are, don't bet too much that you're in room number 68.</p>\n<p>But this last sort of problem is harder, since it's not just a straightforward application of Bayes' rule. You have to determine the probability just from the information in the problem. Thinking in terms of information and symmetries is a useful problem-solving tool for getting probabilities in anthropic problems, which are simple enough to use it and confusing enough to need it. So first we'll cover what I mean by thinking in terms of information, and then we'll use this to solve a confusing-type anthropic problem.<a id=\"more\"></a></p>\n<p><strong id=\"Parable_of_the_coin\">Parable of the coin</strong></p>\n<p>Eliezer has already written about what probability is in <a href=\"/lw/oj/probability_is_in_the_mind/\">Probability is in the Mind</a>. I will revisit it anyhow, using a similar example from <em>Probability Theory: The Logic of Science</em>.</p>\n<p>It is a truth universally acknowledged that when someone tosses a fair coin without cheating, there's a 0.5 probability of heads and a 0.5 probability of tails. You draw the coin forth, flip it, and slap it down. What is the probability that when you take your hand away, you see heads?</p>\n<p>Well, you performed a fair coin flip, so the chance of heads is 0.5. What's the problem? Well, imagine the coin's perspective. When you say \"heads, 0.5,\" that doesn't mean the coin has half of heads up and half of tails up: the coin is already how it's going to be, sitting pressed under your hand. And it's already how it is with probability 1, not 0.5. If the coin is <em>already</em> tails, how can you be correct when you say that it's heads with probability 0.5? If something is already determined, how can it still have the property of randomness?</p>\n<p>The key idea is that the randomness isn't in the coin, it's in your map of the coin. The coin can be tails all it dang likes, but if you don't know that, you shouldn't be expected to take it into account. The probability isn't a physical property of the coin, nor is it a property of flipping the coin - after all, your probability was still 0.5 when the truth was sitting right there under your hand. The probability is determined by the<em>&nbsp;information</em>&nbsp;you have about flipping the coin.</p>\n<p>Assigning probabilities to things tells you about the map, not the territory. It's like a machine that eats information and spits out probabilities, with those probabilities uniquely determined by the information that went in.&nbsp;Thinking about problems in terms of information, then, is about treating probabilities as the best possible answers for people with incomplete information. Probability isn't in the coin, so don't even bother thinking about the coin too much - think about the person and what they know.</p>\n<p>When trying to get probabilities from information, you're going to end up using&nbsp;symmetry&nbsp;a lot. Because information uniquely specifies probability, if you have identical information about two things, then you should assign them equal probability. For example, if someone switched the labels \"heads\" and \"tails\" in a fair coin flip, you couldn't tell that it had been done - you never had any different information about heads as opposed to tails. This symmetry means you should give heads and tails equal probability. Because heads and tails are mutually exclusive (they don't overlap) and exhaustive (there can't be anything else), the probabilities have to add to 1 (which is all the probability there is), so you give each of them probability 0.5.</p>\n<p><strong id=\"Brief_note_on_useless_information\">Brief note on useless information</strong></p>\n<p>Real-world problems, even when they have symmetry, often start you off with a lot more information than \"it could be heads or tails.\" If we're flipping a real-world coin there's the temperature to consider, and the humidity, and the time of day, and the flipper's gender, and that sort of thing. If you're an ordinary human, you are allowed to call this stuff extraneous junk. Sometimes, this extra information could theoretically be&nbsp;<a href=\"/lw/o2/mutual_information_and_density_in_thingspace/\">correlated with the outcome</a>&nbsp;- maybe the humidity really matters somehow, or the time of day. But if you don't know <em>how</em>&nbsp;it's correlated, you have at least a&nbsp;<em>de facto</em>&nbsp;symmetry. Throwing away useless information is a key step in doing anything useful.</p>\n<p><strong id=\"Sleeping_Beauty\">Sleeping Beauty</strong></p>\n<p>So thinking with information means assigning probabilities based on what people know, rather than treating probabilities as properties of objects. To actually apply this, we'll use as our example the <a href=\"http://wiki.lesswrong.com/wiki/Sleeping_Beauty_problem\">sleeping beauty problem</a>:</p>\n<dl style=\"margin-top: 0.2em; margin-bottom: 0.5em; font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 19px;\"><dd style=\"line-height: 1.5em; margin-left: 2em; margin-bottom: 0.1em;\">Suppose Sleeping Beauty volunteers to undergo the following experiment, which is described to her before it begins. On Sunday she is given a drug that sends her to sleep, and a coin is tossed. If the coin lands heads, Beauty is awakened and interviewed on Monday, and then the experiment ends. If the coin comes up tails, she is awakened and interviewed on Monday, given a second dose of the sleeping drug that makes her forget the events of Monday only, and awakened and interviewed again on Tuesday. The experiment then ends on Tuesday, without flipping the coin again.</dd></dl><dl style=\"margin-top: 0.2em; margin-bottom: 0.5em; font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 19px;\"><dd style=\"line-height: 1.5em; margin-left: 2em; margin-bottom: 0.1em;\">Beauty wakes up in the experiment and is asked, \"With what subjective probability do you believe that the coin landed tails?\"</dd> </dl>\n<p><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; line-height: normal; font-size: small;\">If the coin lands heads, Sleeping Beauty is only asked for her guess once, while if the coin lands tails she is asked for her guess twice, but her memory is erased in between so she has the same memories each time.</span></p>\n<p>When trying to answer for Sleeping Beauty, many people reason as follows: It is a truth universally acknowledged that when someone tosses a fair coin without cheating, there's a 0.5 probability of heads and a 0.5 probability of tails. So since the probability of tails is 0.5, Beauty should say \"0.5,\" Q.E.D. &nbsp;Readers may notice that this argument is all about the coin, not about what Beauty knows. This violation of good practice may help explain why it is dead wrong.</p>\n<p><strong id=\"Thinking_with_information__some_warmups\">Thinking with information: some warmups</strong></p>\n<p>To collect the ingredients of the solution, I'm going to first go through some similar-looking problems.</p>\n<p>In the Sleeping Beauty problem, she has to choose between three options - let's call them&nbsp;{H, Monday}, {T, Monday}, and {T, Tuesday}. So let's start with a very simple problem involving three options: the three-sided die. Just like for the fair coin, you know that the sides of the die are mutually exclusive and exhaustive, and you don't know anything else that&nbsp;would&nbsp;be correlated with one side showing up more than another. Sure, the sides have different labels, but the labels are extraneous junk as far as probability is concerned. Mutually exclusive and exhaustive means the probabilities have to add up to one, and the symmetry of your information about the sides means you should give them the same probabilities, so they each get probability 1/3.</p>\n<p>Next, what should Sleeping Beauty believe before the experiment begins? Beforehand, her information looks like this:&nbsp;she signed up for this experiment where you get woken up on Monday if the coin lands heads and on Monday and Tuesday if it lands tails.</p>\n<p><img style=\"float: right;\" src=\"http://images.lesswrong.com/t3_85i_0.png\" alt=\"Diagram of the Sleeping Beauty problem before it starts\" width=\"215\" height=\"293\">This way of stating her information is good enough most of the time, but what's going on is clearer if we're a little more formal. There are three exhaustive (but not mutually exclusive) options: {H, Monday}, {T, Monday}, and {T, Tuesday}. She knows that anything with heads is mutually exclusive with anything with tails, and that {T, Tuesday} happens if and only if&nbsp;{T, Monday}&nbsp;happened.</p>\n<p>One good way to think of this last piece of information is as a special \"AND\" structure containing {T, Monday} and {T, Tuesday}, like in the picture to the right. What it means is that since the things that are \"AND\" happen together, the other probabilities won't change if we merge them into a single option, which I shall call {T, Both}. Now we have two options, {H, Monday} and {T, Both}, which are both exhaustive and mutually exclusive. This looks an awful lot like the fair coin, with probabilities of 0.5.</p>\n<p>But can we leave it at that? Why shouldn't two days be worth twice as much probability as one day, for instance? Well, it turns out we <em>can</em>&nbsp;leave at that, because we have now run out of information from the original problem. We used that there were three options, we used that they were exhaustive, we used that two of them always happened together, and we used that the remaining two were mutually exclusive. That's all, and so that's where we should leave it - any more and we'd be making up information not in the problem, which is bad.</p>\n<p>So to decompress, before the experiment begins Beauty assigns probability 0.5 to the coin landing heads and being woken up on Monday, probability 0.5 to the coin landing tails and being woken up on Monday, and probability 0.5 to the coin landing tails and being woken up on Tuesday. This adds up to 1.5, but that's okay since these things aren't all mutually exclusive.</p>\n<p><img style=\"float: right;\" src=\"http://images.lesswrong.com/t3_85i_1.png\" alt=\"Diagram of the two coins problem\" width=\"274\" height=\"227\">Okay, now for one last warmup. Suppose you have two coins. You flip the first one, and if it lands heads, you place the second coin on the table heads up. If the first coin lands tails, though, you flip the second coin.</p>\n<p>This new problem looks sort of familiar. You have three options, {H, H}, {T, H} and {T, T}, and these options are mutually exclusive and exhaustive. So does that mean it's the same set of information as the three-sided die? Not quite. Similar to the \"AND\" previously, my drawing for this problem has an \"OR\" between {T, H} and {T,T}, representing additional information.</p>\n<p>I'd like to add a note here about my jargon. \"AND\" makes total sense. One thing happens <em>and</em> another thing happens. \"OR,\" however, doesn't make so much sense, because things that are mutually exclusive are already \"or\" by default - one thing happens <em>or</em> another thing happens. What it really means is that {H, H} has a symmetry with the <em>sum </em>of {T, H} and {T, T} (that is, {T, H} \"OR\" {T, T}). The \"OR\" can also be thought of as information about {H, H} instead - it contains what could have been both the {H, H} and {H, T} events, so there's a four-way symmetry in the problem, it's just been relabeled.</p>\n<p>When we had the \"AND\" structure, we merged the two options together to get {tails, both}. For \"OR,\" we can do a slightly different operation and replace {T, H} \"OR\" {T, T} by their sum, {T, either}. Now the options become {H, H} and {T, either}, which are mutually exclusive and exhaustive, which gets us back to the fair coin. Then, because {T, H} and {T, T} have a symmetry between them, you split the probability from {T, either} evenly to get&nbsp;probabilities&nbsp;of 0.5, 0.25, and 0.25.</p>\n<p><strong id=\"Okay__for_real_now\">Okay, for real now</strong></p>\n<p>Okay, so now what do things look like once the experiment has started? In English, now she knows that&nbsp;she signed up for this experiment where you get woken up on Monday if the coin lands heads and on Monday and Tuesday if it lands tails, went to sleep, and now she's been woken up.</p>\n<p>This might not seem that different from before, but the \"anthropic information\" that Beauty is currently one of the people in the experiment changes the formal picture a lot. Before, the three options were not mutually exclusive, because she was thinking about the future. But now&nbsp;{H, Monday}, {T, Monday}, and {T, Tuesday} are both exhaustive and mutually exclusive, because only one can be the case in the present. From the coin flip, she still knows that anything with heads is mutually exclusive with anything with tails. But once two things are mutually exclusive you can't make them any <em>more</em>&nbsp;mutually exclusive.</p>\n<p>But the \"AND\" information! What happens to that? Well, that was based on things always happening together, and we just got information that those things are mutually exclusive, so there's no more \"AND.\" It's possible to slip up here and reason that since there used to be some structure there, and now they're mutually exclusive, it's one or the other, therefore there must be \"OR\" information. At least the confusion in my terminology reflects an easy confusion to have, but this \"OR\" relationship isn't the same as mutual exclusivity. It's a specific piece of information that wasn't in the problem before the experiment, and wasn't part of the anthropic information (that was just mutual exclusivity). So Monday and Tuesday are \"or\" (mutually exclusive), but not \"OR\" (can be added up to use another symmetry).</p>\n<p>And so this anthropic requirement of mutual exclusivity turns out to make redundant or render null a big chunk of the previous information, which is strange. You end up left with three mutually exclusive, exhaustive options, with no particular asymmetry. This is the three-sided die information, and so each of&nbsp;{H, Monday}, {T, Monday}, and {T, Tuesday} should get probability 1/3. So when asked for P(tails), Beauty should answer 2/3.</p>\n<p><strong id=\"_SSA__and__SIA_\">\"SSA\" and \"SIA\"</strong></p>\n<p>When assigning prior probabilities in anthropic problems, there are two main \"easy\" ways to assign probabilities, and these methods go by the acronyms \"SSA\" and \"SIA.\" \"SSA\" is stated like this<sup><a href=\"http://en.wikipedia.org/wiki/Self-Sampling_Assumption\">1</a></sup>:</p>\n<p style=\"padding-left: 30px; \"><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px; \">All other things equal, an observer should reason as if they are randomly selected from the set of all&nbsp;</span><em style=\"font-family: sans-serif; font-size: 13px; line-height: 19px; \">actually existent</em><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px; \">&nbsp;observers (past, present and future) in their reference class.</span></p>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px; \"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; line-height: normal; font-size: small; \">For example, if you wanted the prior probability that you lived in Sweden, you might say ask \"what proportion of human beings have lived in Sweden?\"</span></span></p>\n<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px; \"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; line-height: normal; font-size: small; \">On the other hand, \"SIA\" looks like this<sup><a href=\"http://en.wikipedia.org/wiki/Self-Indication_Assumption\">2</a></sup>:</span></span></p>\n<p style=\"padding-left: 30px; \"><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px; \">All other things equal, an observer should reason as if they are randomly selected from the set of all&nbsp;</span><em style=\"font-family: sans-serif; font-size: 13px; line-height: 19px; \">possible</em><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px; \">&nbsp;observers.</span></p>\n<p>Now the question becomes \"what proportion of possible observers live in Sweden?\" and suddenly it seems awfully improbable that anyone could live in Sweden.</p>\n<p>The astute reader will notice that these two \"assumptions\" correspond to two different sets of starting information. If you want a quick exercise, figure out what those two sets of information are now. I'll wait for&nbsp;you&nbsp;in the next paragraph.</p>\n<p>Hi again. The information assumed for SSA is pretty straightforward. You are supposed to reason as if you know that you're an actually existent observer, in some \"reference class.\" So an example set of information would be \"I exist/existed/will exist and am a human.\" Compared to that, SIA seems to barely assume any information at all - all you get to start with is \"I am a possible observer.\" Because \"existent observers in a reference class\" are a subset of possible observers, you can transform SIA into SSA by adding on more information, e.g. \"I exist and am a human.\" And then if you want to represent a more complicated problem, you have to add extra information on top of that, like \"I live in 2012\" or \"I have two X chromosomes.\"</p>\n<p>Trouble only sneaks in if you start to see these acronyms as mysterious probability generators rather than sets of starting information to build on. So don't do that.</p>\n<p><strong id=\"Closing_remarks\">Closing remarks</strong></p>\n<p>When faced with straightforward problems, you usually don't need to use this knowledge of where probability comes from. It's just rigorous and interesting, like knowing how to do integration as a <a href=\"http://www.vias.org/calculus/img/04_integration-10.gif\">Riemann sum</a>. But whenever you run into foundational or even particularly confusing problems, it's good to remember that probability is about making the best use you can of incomplete information. If not, you run the risk of a few silly failure modes, or even (<em>gasp</em>) frequentism.</p>\n<p>I recently read an academic paper<a href=\"http://arxiv.org/abs/0905.0624v1\"><sup>3</sup></a> that used the idea that in a multiverse, there will be some universe where a thrown coin comes up heads every time, and so the people in that universe will have very strange ideas about how coins work. <em>Therefore</em>, this actual academic paper argued, since reasoning with probability can lead people to be wrong, it cannot be applied to anything like a multiverse.</p>\n<p>My response is: what have you got that works better? In this post we worked through assigning probabilities by using all of our information. If you deviate from that, you're either throwing information away or making it up. Incomplete information lets you down sometimes, that's why it's called incomplete. But that doesn't license you to throw away information or make it up, out of some sort of dissatisfaction with reality. The truth is out there. But the probabilities are in here.</p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Parable of the coin", "anchor": "Parable_of_the_coin", "level": 1}, {"title": "Brief note on useless information", "anchor": "Brief_note_on_useless_information", "level": 1}, {"title": "Sleeping Beauty", "anchor": "Sleeping_Beauty", "level": 1}, {"title": "Thinking with information: some warmups", "anchor": "Thinking_with_information__some_warmups", "level": 1}, {"title": "Okay, for real now", "anchor": "Okay__for_real_now", "level": 1}, {"title": "\"SSA\" and \"SIA\"", "anchor": "_SSA__and__SIA_", "level": 1}, {"title": "Closing remarks", "anchor": "Closing_remarks", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "60 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 60, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["f6ZLxEWaankRZ2Crv", "yLcuygFfMfrfK8KjF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-27T02:00:02.017Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] GAZP vs. GLUT", "slug": "seq-rerun-gazp-vs-glut", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:33.767Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eBHRu7TucHksTsorJ/seq-rerun-gazp-vs-glut", "pageUrlRelative": "/posts/eBHRu7TucHksTsorJ/seq-rerun-gazp-vs-glut", "linkUrl": "https://www.lesswrong.com/posts/eBHRu7TucHksTsorJ/seq-rerun-gazp-vs-glut", "postedAtFormatted": "Tuesday, March 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20GAZP%20vs.%20GLUT&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20GAZP%20vs.%20GLUT%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeBHRu7TucHksTsorJ%2Fseq-rerun-gazp-vs-glut%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20GAZP%20vs.%20GLUT%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeBHRu7TucHksTsorJ%2Fseq-rerun-gazp-vs-glut", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeBHRu7TucHksTsorJ%2Fseq-rerun-gazp-vs-glut", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<p>Today's post, <a href=\"/lw/pa/gazp_vs_glut/\">GAZP vs. GLUT</a> was originally published on 07 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#GAZP_vs._GLUT\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Fleshes out the generalized anti-zombie principle a bit more, and describes the game \"follow-the-improbability\".</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/b68/seq_rerun_the_generalized_antizombie_principle/\">The Generalized Anti-Zombie Principle</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eBHRu7TucHksTsorJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 8.728707140355713e-07, "legacy": true, "legacyId": "14504", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["k6EPphHiBH4WWYFCj", "r3PM6gSx2f8GKJyNH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-27T12:48:51.041Z", "modifiedAt": null, "url": null, "title": "Meetup : First Norwich UK Meetup Sunday 1 April 11am", "slug": "meetup-first-norwich-uk-meetup-sunday-1-april-11am", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:38.107Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rlp10", "createdAt": "2012-01-27T15:14:36.027Z", "isAdmin": false, "displayName": "rlp10"}, "userId": "wrpGdWaf8RqAtCytp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eqpyhCBxNacN3WaWC/meetup-first-norwich-uk-meetup-sunday-1-april-11am", "pageUrlRelative": "/posts/eqpyhCBxNacN3WaWC/meetup-first-norwich-uk-meetup-sunday-1-april-11am", "linkUrl": "https://www.lesswrong.com/posts/eqpyhCBxNacN3WaWC/meetup-first-norwich-uk-meetup-sunday-1-april-11am", "postedAtFormatted": "Tuesday, March 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20Norwich%20UK%20Meetup%20Sunday%201%20April%2011am&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20Norwich%20UK%20Meetup%20Sunday%201%20April%2011am%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeqpyhCBxNacN3WaWC%2Fmeetup-first-norwich-uk-meetup-sunday-1-april-11am%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20Norwich%20UK%20Meetup%20Sunday%201%20April%2011am%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeqpyhCBxNacN3WaWC%2Fmeetup-first-norwich-uk-meetup-sunday-1-april-11am", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeqpyhCBxNacN3WaWC%2Fmeetup-first-norwich-uk-meetup-sunday-1-april-11am", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 62, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/89'>First Norwich UK Meetup Sunday 1 April 11am</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 April 2012 11:00:00AM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">The Black Horse, 50 Earlham Road, Norwich, NR2 3DE</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is the first LW meetup in Norwich, UK.  If you're in Norwich or Norfolk then please consider joining us.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/89'>First Norwich UK Meetup Sunday 1 April 11am</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eqpyhCBxNacN3WaWC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.731387382148621e-07, "legacy": true, "legacyId": "14533", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_Norwich_UK_Meetup_Sunday_1_April_11am\">Discussion article for the meetup : <a href=\"/meetups/89\">First Norwich UK Meetup Sunday 1 April 11am</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 April 2012 11:00:00AM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">The Black Horse, 50 Earlham Road, Norwich, NR2 3DE</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This is the first LW meetup in Norwich, UK.  If you're in Norwich or Norfolk then please consider joining us.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___First_Norwich_UK_Meetup_Sunday_1_April_11am1\">Discussion article for the meetup : <a href=\"/meetups/89\">First Norwich UK Meetup Sunday 1 April 11am</a></h2>", "sections": [{"title": "Discussion article for the meetup : First Norwich UK Meetup Sunday 1 April 11am", "anchor": "Discussion_article_for_the_meetup___First_Norwich_UK_Meetup_Sunday_1_April_11am", "level": 1}, {"title": "Discussion article for the meetup : First Norwich UK Meetup Sunday 1 April 11am", "anchor": "Discussion_article_for_the_meetup___First_Norwich_UK_Meetup_Sunday_1_April_11am1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-27T12:55:43.034Z", "modifiedAt": null, "url": null, "title": "Meetup : Sydney meetup - Biased pandemic and other games", "slug": "meetup-sydney-meetup-biased-pandemic-and-other-games", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:05.695Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oklord", "createdAt": "2011-03-22T11:37:19.291Z", "isAdmin": false, "displayName": "Oklord"}, "userId": "EusNQMHkhmSq2k9Y9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/44RL5pDqxhEt5EzBn/meetup-sydney-meetup-biased-pandemic-and-other-games", "pageUrlRelative": "/posts/44RL5pDqxhEt5EzBn/meetup-sydney-meetup-biased-pandemic-and-other-games", "linkUrl": "https://www.lesswrong.com/posts/44RL5pDqxhEt5EzBn/meetup-sydney-meetup-biased-pandemic-and-other-games", "postedAtFormatted": "Tuesday, March 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Sydney%20meetup%20-%20Biased%20pandemic%20and%20other%20games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Sydney%20meetup%20-%20Biased%20pandemic%20and%20other%20games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F44RL5pDqxhEt5EzBn%2Fmeetup-sydney-meetup-biased-pandemic-and-other-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Sydney%20meetup%20-%20Biased%20pandemic%20and%20other%20games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F44RL5pDqxhEt5EzBn%2Fmeetup-sydney-meetup-biased-pandemic-and-other-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F44RL5pDqxhEt5EzBn%2Fmeetup-sydney-meetup-biased-pandemic-and-other-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 109, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8a'>Sydney meetup - Biased pandemic and other games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 April 2012 07:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">22 the promenade sydney</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey everybody. Deep inside the king street brewery, we plan to bring out rationalist board and card games. This, of course, is a much more social meet-up!</p>\n\n<p>If your still on the fence about showing, Sydney turnout so far is good but could be better! We know you are out there, lurkers.</p>\n\n<p>We have a FB group set up - just search Less wrong Sydney, or PM me with your FB name to add.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8a'>Sydney meetup - Biased pandemic and other games</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "44RL5pDqxhEt5EzBn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.731415755390001e-07, "legacy": true, "legacyId": "14534", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Sydney_meetup___Biased_pandemic_and_other_games\">Discussion article for the meetup : <a href=\"/meetups/8a\">Sydney meetup - Biased pandemic and other games</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 April 2012 07:30:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">22 the promenade sydney</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey everybody. Deep inside the king street brewery, we plan to bring out rationalist board and card games. This, of course, is a much more social meet-up!</p>\n\n<p>If your still on the fence about showing, Sydney turnout so far is good but could be better! We know you are out there, lurkers.</p>\n\n<p>We have a FB group set up - just search Less wrong Sydney, or PM me with your FB name to add.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Sydney_meetup___Biased_pandemic_and_other_games1\">Discussion article for the meetup : <a href=\"/meetups/8a\">Sydney meetup - Biased pandemic and other games</a></h2>", "sections": [{"title": "Discussion article for the meetup : Sydney meetup - Biased pandemic and other games", "anchor": "Discussion_article_for_the_meetup___Sydney_meetup___Biased_pandemic_and_other_games", "level": 1}, {"title": "Discussion article for the meetup : Sydney meetup - Biased pandemic and other games", "anchor": "Discussion_article_for_the_meetup___Sydney_meetup___Biased_pandemic_and_other_games1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-27T17:30:33.772Z", "modifiedAt": null, "url": null, "title": "[LINK] Poem: There are no beautiful surfaces without a terrible depth.", "slug": "link-poem-there-are-no-beautiful-surfaces-without-a-terrible", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:54.914Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JenniferRM", "createdAt": "2009-03-06T17:16:50.600Z", "isAdmin": false, "displayName": "JenniferRM"}, "userId": "g8JkZfL8PTqAefpvx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nQsZKQZixHYjArbBk/link-poem-there-are-no-beautiful-surfaces-without-a-terrible", "pageUrlRelative": "/posts/nQsZKQZixHYjArbBk/link-poem-there-are-no-beautiful-surfaces-without-a-terrible", "linkUrl": "https://www.lesswrong.com/posts/nQsZKQZixHYjArbBk/link-poem-there-are-no-beautiful-surfaces-without-a-terrible", "postedAtFormatted": "Tuesday, March 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Poem%3A%20There%20are%20no%20beautiful%20surfaces%20without%20a%20terrible%20depth.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Poem%3A%20There%20are%20no%20beautiful%20surfaces%20without%20a%20terrible%20depth.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnQsZKQZixHYjArbBk%2Flink-poem-there-are-no-beautiful-surfaces-without-a-terrible%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Poem%3A%20There%20are%20no%20beautiful%20surfaces%20without%20a%20terrible%20depth.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnQsZKQZixHYjArbBk%2Flink-poem-there-are-no-beautiful-surfaces-without-a-terrible", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnQsZKQZixHYjArbBk%2Flink-poem-there-are-no-beautiful-surfaces-without-a-terrible", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 339, "htmlBody": "<p><a title=\"emotions around reductionism, in poetry\" href=\"http://www.online-literature.com/forums/showthread.php?t=3569\">The poem</a> is from someone whose online pseudonym is atiguhya padma.&nbsp; I'll quote the first verse, the refrain, and the beginning of the second verse to give you enough flavor to decide if you want to follow the link.&nbsp; There are about 9 verses total.</p>\n<p><a id=\"more\"></a></p>\n<blockquote>\n<p>Someone looked out of their window<br /> And said to me: the world looks<br /> So beautiful, that I praise God<br /> Each day for this wonderful life,<br /> This landscape of happy creatures<br /> And rolling fields of growth and form.<br /> He obviously had not read Tennyson<br /> And he wasn&rsquo;t an ecologist,<br /> For he had no firm idea of how<br /> Ecosystems sustain themselves.<br /> <br /> There are no beautiful surfaces<br /> Without a terrible depth.<br /> <br /> You said you loved me.<br /> And I wondered what that could mean...</p>\n</blockquote>\n<p>Reductionist thinking can connect emotions triggered by the surfaces encountered in daily life to a larger set of concepts and predictions, and this seems to have consequences for both the thinking and the emotions that isn't often discussed and is even less often discussed well. I liked the poem because it addressed the issues pretty well and <em>in an emotional mode</em>, which is doubly rare.</p>\n<p>The \"no beautiful surfaces\" refrain is a quote from Nietzsche which has little easily accessed online scholarship.&nbsp; Nothing in the first few pages of google results mentions a source for the quote so I was suspicious at first that it was even a quote by him, but references available via <a title=\"Google Books Search for Nietzsche Quote\" href=\"https://www.google.com/#tbm=bks&amp;q=&quot;beautiful+surfaces&quot;+&quot;terrible+depth&quot;\">google books</a> indicate that it comes from one of his notebooks.&nbsp; I haven't tracked down the notebook (it doesn't seem to be in gutenberg), so I'm not sure if the original source carries the specific connotations the poem attributes to the phrase, or if it is just a cool line put to good use in a new context.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nQsZKQZixHYjArbBk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 22, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "14535", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-27T18:07:15.176Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread, part 13, chapter 81", "slug": "harry-potter-and-the-methods-of-rationality-discussion-21", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:07.979Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bogdanb", "createdAt": "2009-03-01T17:05:24.777Z", "isAdmin": false, "displayName": "bogdanb"}, "userId": "XaMhiBmqFaeT5diLB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xK6Pswbozev6pv4A6/harry-potter-and-the-methods-of-rationality-discussion-21", "pageUrlRelative": "/posts/xK6Pswbozev6pv4A6/harry-potter-and-the-methods-of-rationality-discussion-21", "linkUrl": "https://www.lesswrong.com/posts/xK6Pswbozev6pv4A6/harry-potter-and-the-methods-of-rationality-discussion-21", "postedAtFormatted": "Tuesday, March 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2013%2C%20chapter%2081&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2013%2C%20chapter%2081%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxK6Pswbozev6pv4A6%2Fharry-potter-and-the-methods-of-rationality-discussion-21%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2013%2C%20chapter%2081%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxK6Pswbozev6pv4A6%2Fharry-potter-and-the-methods-of-rationality-discussion-21", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxK6Pswbozev6pv4A6%2Fharry-potter-and-the-methods-of-rationality-discussion-21", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 350, "htmlBody": "<p style=\"text-align: justify;\">This is a new thread to discuss Eliezer Yudkowsky&rsquo;s <em><a style=\"color: #8a8a8b;\" href=\"http://www.fanfiction.net/s/5782108/1/\">Harry Potter and the Methods of Rationality</a></em> and anything related to it. This thread is intended for discussing chapter 81, which should be published later today. <a href=\"/r/discussion/lw/b5s/harry_potter_and_the_methods_of_rationality/\">The previous thread</a> passed 400 comments as of the time of this writing, so it will pass 500 comments soon after the next chapter is posted, if not before. I suggest refraining from commenting here until chapter 81 is posted; <strong>comment in <a href=\"/r/discussion/lw/b5s/harry_potter_and_the_methods_of_rationality/\">the 12th thread</a> until you read chapter 81.</strong> After chapter 81 is posted, I suggest all discussion of previous guesses be kept here, with links to comments in the previous thread.</p>\n<p style=\"text-align: justify;\">There is now a site dedicated to the story at <a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/\">hpmor.com</a>, which is now the place to go to find the <a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/notes/\">authors notes</a> and all sorts of other goodies. AdeleneDawner has kept an <a style=\"color: #8a8a8b;\" href=\"http://www.evernote.com/pub/adelenedawner/Eliezer\">archive of Author&rsquo;s Notes</a>. (This goes up to the notes for chapter 76, and is now not updating. The authors notes from chapter 77 onwards are on hpmor.com.) When posted, chapter 81 should appear <a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/chapter/81\">here</a>.</p>\n<p style=\"text-align: justify;\">The first 5 discussion threads are on the main page under the <a style=\"color: #8a8a8b;\" href=\"/tag/harry_potter\">harry_potter tag</a>. Threads 6 and on (including this one) are in the <a style=\"color: #8a8a8b;\" href=\"/r/discussion/tag/harry_potter\">discussion section</a> using its separate tag system. Also: <a style=\"color: #8a8a8b;\" href=\"/lw/2ab/harry_potter_and_the_methods_of_rationality\">one</a>, <a style=\"color: #8a8a8b;\" href=\"/lw/2ie/harry_potter_and_the_methods_of_rationality\">two</a>, <a style=\"color: #8a8a8b;\" href=\"/lw/2nm/harry_potter_and_the_methods_of_rationality\">three</a>, <a style=\"color: #8a8a8b;\" href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality\">four</a>, <a style=\"color: #8a8a8b;\" href=\"/lw/30g/harry_potter_and_the_methods_of_rationality\">five</a>, <a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/364/harry_potter_and_the_methods_of_rationality\">six</a>, <a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/3rb/harry_potter_and_the_methods_of_rationality\">seven</a>, <a style=\"color: #8a8a8b;\" href=\"/lw/797/harry_potter_and_the_methods_of_rationality\">eight</a>,<a style=\"color: #8a8a8b;\" href=\"/lw/7jd/harry_potter_and_the_methods_of_rationality\">nine</a>, <a style=\"color: #8a8a8b;\" href=\"/lw/ams/harry_potter_and_the_methods_of_rationality\">ten</a>, <a style=\"color: #8a8a8b;\" href=\"/lw/axe/harry_potter_and_the_methods_of_rationality/\">eleven</a>, <a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/b5s/harry_potter_and_the_methods_of_rationality/\">twelve</a>.</p>\n<p style=\"text-align: justify;\">As a reminder, it&rsquo;s often useful to start your comment by indicating which chapter you are commenting on.</p>\n<p style=\"text-align: justify;\"><strong>Spoiler Warning</strong>: this thread is full of spoilers. With few exceptions, spoilers for MOR and canon are fair game to post, without warning or rot13. <a style=\"color: #8a8a8b;\" href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality/2v1l\">More specifically</a>:</p>\n<blockquote style=\"text-align: justify;\">\n<p>You do not need to rot13 anything about HP:MoR or the original Harry Potter series unless you are posting insider information from Eliezer Yudkowsky which is not supposed to be publicly available (which includes public statements by Eliezer that have been retracted).</p>\n<p>If there is evidence for X in MOR and/or canon then it&rsquo;s fine to post about X without rot13, even if you also have heard privately from Eliezer that X is true. But you should not post that &ldquo;Eliezer said X is true&rdquo; unless you use rot13.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xK6Pswbozev6pv4A6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 8.732701292963728e-07, "legacy": true, "legacyId": "14536", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1112, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K4JBpAxhvstdnNbeg", "59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "nnnd4KRQxs6DYcehD", "y2Hszb4Dsm5FggnDC", "6ae2kq3JmKvL4YPgk", "zvXfBqp6TSriNkmbg", "WQ7XMjqvuRRj8nkpu", "LKFR5pBA3bBkERDxL", "8yEdpDpGgvDWHeodM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-27T18:18:17.108Z", "modifiedAt": null, "url": null, "title": "Meetup : [deleted]", "slug": "meetup-deleted", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:33.806Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vsyLRynuFMjC3waCx/meetup-deleted", "pageUrlRelative": "/posts/vsyLRynuFMjC3waCx/meetup-deleted", "linkUrl": "https://www.lesswrong.com/posts/vsyLRynuFMjC3waCx/meetup-deleted", "postedAtFormatted": "Tuesday, March 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20%5Bdeleted%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20%5Bdeleted%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvsyLRynuFMjC3waCx%2Fmeetup-deleted%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20%5Bdeleted%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvsyLRynuFMjC3waCx%2Fmeetup-deleted", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvsyLRynuFMjC3waCx%2Fmeetup-deleted", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 25, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8b'>[deleted]</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 March 2012 06:26:00AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2071 university avenue, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8b'>[deleted]</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vsyLRynuFMjC3waCx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "14537", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup____deleted_\">Discussion article for the meetup : <a href=\"/meetups/8b\">[deleted]</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 March 2012 06:26:00AM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2071 university avenue, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup____deleted_1\">Discussion article for the meetup : <a href=\"/meetups/8b\">[deleted]</a></h2>", "sections": [{"title": "Discussion article for the meetup : [deleted]", "anchor": "Discussion_article_for_the_meetup____deleted_", "level": 1}, {"title": "Discussion article for the meetup : [deleted]", "anchor": "Discussion_article_for_the_meetup____deleted_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-27T18:31:52.701Z", "modifiedAt": null, "url": null, "title": "Call for Papers on AI/robot safety", "slug": "call-for-papers-on-ai-robot-safety", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:37.818Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YLET7tt7TtpbRk4Wx/call-for-papers-on-ai-robot-safety", "pageUrlRelative": "/posts/YLET7tt7TtpbRk4Wx/call-for-papers-on-ai-robot-safety", "linkUrl": "https://www.lesswrong.com/posts/YLET7tt7TtpbRk4Wx/call-for-papers-on-ai-robot-safety", "postedAtFormatted": "Tuesday, March 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Call%20for%20Papers%20on%20AI%2Frobot%20safety&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACall%20for%20Papers%20on%20AI%2Frobot%20safety%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYLET7tt7TtpbRk4Wx%2Fcall-for-papers-on-ai-robot-safety%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Call%20for%20Papers%20on%20AI%2Frobot%20safety%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYLET7tt7TtpbRk4Wx%2Fcall-for-papers-on-ai-robot-safety", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYLET7tt7TtpbRk4Wx%2Fcall-for-papers-on-ai-robot-safety", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 156, "htmlBody": "<p>The open-access <em><a href=\"http://www.hindawi.com/journals/jr/\">Journal of Robotics</a></em>&nbsp;has posted the <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/CFP_Robotics_758902.pdf\">Call for Papers</a> for an upcoming \"Special Issue on Robotic Safety &amp; Security.\"</p>\n<p>One of the guest editors for this issue is <a href=\"http://louisville.edu/speed/computer/people/faculty/yampolskiy\">Roman Yampolskiy</a>, a past SI visiting fellow and author or co-author of several papers on AI risk: <a href=\"http://dl.dropbox.com/u/5317066/2012-yampolskiy.pdf\">Leakproofing the Singularity</a>, <a href=\"http://joshuafox.com/professional/media/YampolskiyFox__AGIAndTheHumanModel.pdf\">Artificial General Intelligence and the Human Mental Model</a>, and <a href=\"http://joshuafox.com/professional/media/YampolskiyFox__SafetyEngineeringforAGI.pdf\">Safety Engineering for Artificial General Intelligence</a>.</p>\n<p>Check the <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/CFP_Robotics_758902.pdf\">PDF</a> for full details, but:</p>\n<ul>\n<li>Manuscripts due June 8, 2012</li>\n<li>Reviews on August 31, 2012</li>\n<li>Will be published October 26, 2012</li>\n<li>Read the <a href=\"http://www.hindawi.com/journals/jr/guidelines/\">author guidelines</a></li>\n</ul>\n<p>Because <em>Journal of Robotics</em>&nbsp;is an open access journal, it charges an \"article processing fee\" of $500 to cover its costs (<a href=\"http://www.hindawi.com/journals/jr/apc/\">details</a>). You are <em>only</em>&nbsp;charged if your submissions is accepted and printed by the journal.</p>\n<p><strong>Update</strong>: The Singularity Institute will <strong>reimburse you for your article processing fee</strong> if we think the article you're submitting is worthwhile. Contact luke [at] singularity.org for details.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YLET7tt7TtpbRk4Wx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 8.732805007945417e-07, "legacy": true, "legacyId": "14538", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-27T20:03:08.340Z", "modifiedAt": null, "url": null, "title": "Common mistakes people make when thinking about decision theory", "slug": "common-mistakes-people-make-when-thinking-about-decision", "viewCount": null, "lastCommentedAt": "2012-04-06T19:26:21.832Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gkAecqbuPw4iggiub/common-mistakes-people-make-when-thinking-about-decision", "pageUrlRelative": "/posts/gkAecqbuPw4iggiub/common-mistakes-people-make-when-thinking-about-decision", "linkUrl": "https://www.lesswrong.com/posts/gkAecqbuPw4iggiub/common-mistakes-people-make-when-thinking-about-decision", "postedAtFormatted": "Tuesday, March 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Common%20mistakes%20people%20make%20when%20thinking%20about%20decision%20theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACommon%20mistakes%20people%20make%20when%20thinking%20about%20decision%20theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgkAecqbuPw4iggiub%2Fcommon-mistakes-people-make-when-thinking-about-decision%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Common%20mistakes%20people%20make%20when%20thinking%20about%20decision%20theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgkAecqbuPw4iggiub%2Fcommon-mistakes-people-make-when-thinking-about-decision", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgkAecqbuPw4iggiub%2Fcommon-mistakes-people-make-when-thinking-about-decision", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 679, "htmlBody": "<p>From my experience reading and talking about decision theory on LW, it seems that many of the unproductive comments in these discussions can be attributed to a handful of common mistakes.</p>\n<h4>Mistake #1: Arguing about assumptions</h4>\n<p>The main reason why I took so long to understand Newcomb's Problem and Counterfactual Mugging was my insistence on denying the assumptions behind these puzzles. I could have saved months if I'd just said to myself, okay, is this direction of inquiry interesting when taken on its own terms?</p>\n<p>Many assumptions seemed to be divorced from real life at first. People dismissed the study of electromagnetism as an impractical toy, and considered number theory hopelessly abstract until cryptography arrived. The only way to make intellectual progress (either individually or as a group) is to explore the implications of interesting assumptions wherever they might lead. Unfortunately people love to argue about assumptions instead of getting anything done, though they can't really judge before exploring the implications in detail.</p>\n<p>Several smart people on LW are repeating my exact mistake about Newcomb's Problem now, and others find ways to commit the same mistake when looking at our newer ideas. It's so frustrating and&nbsp;uninteresting to read yet another comment saying my assumptions look unintuitive or unphysical or irrelevant to FAI or whatever. I'm not against criticism, but somehow such comments never blossom into interesting conversations, and that's reason enough to caution you against the way of thinking that causes them.</p>\n<h4>Mistake #2: Stopping when your idea seems good enough</h4>\n<p>There's a handful of ideas that decision theory newbies rediscover again and again, like pointing out indexical uncertainty as the solution to Newcomb's problem, or adding randomness to models of UDT to eliminate spurious proofs. These ideas don't work and don't lead anywhere interesting, but that's hard to notice when you just had the flash of insight and want to share it with the world.</p>\n<p>A good strategy in such situations is to always push a little bit <em>past</em> the point where you have everything figured out. Take one extra step and ask yourself: \"Can I make this idea precise?\" What are the first few implications? What are the obvious extensions? If your result seems to contradict what's already known, work through some of the contradictions yourself. If you don't find any mistakes in your idea, you will surely find new formal things to say about your idea, which always helps.</p>\n<h4>Mistake #2A: Stopping when your idea actually is good enough</h4>\n<p>I didn't want to name any names in this post because my status on LW puts me in a kinda position of power, but there's a name I can name with a clear conscience. In 2009, Eliezer <a href=\"/lw/15z/ingredients_of_timeless_decision_theory/\">wrote</a>:</p>\n<blockquote>\n<p>Formally you'd use a Godelian diagonal to write (...)</p>\n</blockquote>\n<p>Of course that's not a newbie mistake at all, but an awesome and fruitful idea! As it happens, writing out that Godelian diagonal immediately leads to all sorts of puzzling questions like \"but what does it actually do? and how do we prove it?\", and eventually to all the decision theory research we're doing now. Knowing Eliezer's intelligence, he probably could have preempted most of our results. Instead he just declared the problem <a href=\"/lw/7v/formalizing_newcombs/5ml\">solved</a>. Maybe he thought he was already at 0.95 formality and that going to 1.0 would be a trivial step? I don't want to insinuate here, but IMO he made a mistake.</p>\n<p>Since this mistake is indistinguishable from the last, the remedy for it is the same: \"Can I make this idea precise?\" Whenever you stake out a small area of knowledge and make it amenable to mathematical thinking, you're likely to find new math that has lasting value. When you stop because your not-quite-formal idea seems already good enough, you squander that opportunity.</p>\n<p>...</p>\n<p>If this post has convinced you to stop making these common mistakes, be warned that it won't necessarily make you happier. As you learn to see more clearly, the first thing you'll see will be a locked door with a sign saying \"Research is hard\". Though it's not very <a href=\"/lw/up/shut_up_and_do_the_impossible/\">scary or heroic</a>, mostly you just stand there feeling stupid about yourself :-)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gkAecqbuPw4iggiub", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 67, "extendedScore": null, "score": 0.000139, "legacy": true, "legacyId": "14539", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 67, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>From my experience reading and talking about decision theory on LW, it seems that many of the unproductive comments in these discussions can be attributed to a handful of common mistakes.</p>\n<h4 id=\"Mistake__1__Arguing_about_assumptions\">Mistake #1: Arguing about assumptions</h4>\n<p>The main reason why I took so long to understand Newcomb's Problem and Counterfactual Mugging was my insistence on denying the assumptions behind these puzzles. I could have saved months if I'd just said to myself, okay, is this direction of inquiry interesting when taken on its own terms?</p>\n<p>Many assumptions seemed to be divorced from real life at first. People dismissed the study of electromagnetism as an impractical toy, and considered number theory hopelessly abstract until cryptography arrived. The only way to make intellectual progress (either individually or as a group) is to explore the implications of interesting assumptions wherever they might lead. Unfortunately people love to argue about assumptions instead of getting anything done, though they can't really judge before exploring the implications in detail.</p>\n<p>Several smart people on LW are repeating my exact mistake about Newcomb's Problem now, and others find ways to commit the same mistake when looking at our newer ideas. It's so frustrating and&nbsp;uninteresting to read yet another comment saying my assumptions look unintuitive or unphysical or irrelevant to FAI or whatever. I'm not against criticism, but somehow such comments never blossom into interesting conversations, and that's reason enough to caution you against the way of thinking that causes them.</p>\n<h4 id=\"Mistake__2__Stopping_when_your_idea_seems_good_enough\">Mistake #2: Stopping when your idea seems good enough</h4>\n<p>There's a handful of ideas that decision theory newbies rediscover again and again, like pointing out indexical uncertainty as the solution to Newcomb's problem, or adding randomness to models of UDT to eliminate spurious proofs. These ideas don't work and don't lead anywhere interesting, but that's hard to notice when you just had the flash of insight and want to share it with the world.</p>\n<p>A good strategy in such situations is to always push a little bit <em>past</em> the point where you have everything figured out. Take one extra step and ask yourself: \"Can I make this idea precise?\" What are the first few implications? What are the obvious extensions? If your result seems to contradict what's already known, work through some of the contradictions yourself. If you don't find any mistakes in your idea, you will surely find new formal things to say about your idea, which always helps.</p>\n<h4 id=\"Mistake__2A__Stopping_when_your_idea_actually_is_good_enough\">Mistake #2A: Stopping when your idea actually is good enough</h4>\n<p>I didn't want to name any names in this post because my status on LW puts me in a kinda position of power, but there's a name I can name with a clear conscience. In 2009, Eliezer <a href=\"/lw/15z/ingredients_of_timeless_decision_theory/\">wrote</a>:</p>\n<blockquote>\n<p>Formally you'd use a Godelian diagonal to write (...)</p>\n</blockquote>\n<p>Of course that's not a newbie mistake at all, but an awesome and fruitful idea! As it happens, writing out that Godelian diagonal immediately leads to all sorts of puzzling questions like \"but what does it actually do? and how do we prove it?\", and eventually to all the decision theory research we're doing now. Knowing Eliezer's intelligence, he probably could have preempted most of our results. Instead he just declared the problem <a href=\"/lw/7v/formalizing_newcombs/5ml\">solved</a>. Maybe he thought he was already at 0.95 formality and that going to 1.0 would be a trivial step? I don't want to insinuate here, but IMO he made a mistake.</p>\n<p>Since this mistake is indistinguishable from the last, the remedy for it is the same: \"Can I make this idea precise?\" Whenever you stake out a small area of knowledge and make it amenable to mathematical thinking, you're likely to find new math that has lasting value. When you stop because your not-quite-formal idea seems already good enough, you squander that opportunity.</p>\n<p>...</p>\n<p>If this post has convinced you to stop making these common mistakes, be warned that it won't necessarily make you happier. As you learn to see more clearly, the first thing you'll see will be a locked door with a sign saying \"Research is hard\". Though it's not very <a href=\"/lw/up/shut_up_and_do_the_impossible/\">scary or heroic</a>, mostly you just stand there feeling stupid about yourself :-)</p>", "sections": [{"title": "Mistake #1: Arguing about assumptions", "anchor": "Mistake__1__Arguing_about_assumptions", "level": 1}, {"title": "Mistake #2: Stopping when your idea seems good enough", "anchor": "Mistake__2__Stopping_when_your_idea_seems_good_enough", "level": 1}, {"title": "Mistake #2A: Stopping when your idea actually is good enough", "anchor": "Mistake__2A__Stopping_when_your_idea_actually_is_good_enough", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "27 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["szfxvS8nsxTgJLBHs", "nCvvhFBaayaXyuBiD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-28T03:22:42.854Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Belief in the Implied Invisible", "slug": "seq-rerun-belief-in-the-implied-invisible", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:34.428Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SnKocTGin5WTu27d2/seq-rerun-belief-in-the-implied-invisible", "pageUrlRelative": "/posts/SnKocTGin5WTu27d2/seq-rerun-belief-in-the-implied-invisible", "linkUrl": "https://www.lesswrong.com/posts/SnKocTGin5WTu27d2/seq-rerun-belief-in-the-implied-invisible", "postedAtFormatted": "Wednesday, March 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Belief%20in%20the%20Implied%20Invisible&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Belief%20in%20the%20Implied%20Invisible%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSnKocTGin5WTu27d2%2Fseq-rerun-belief-in-the-implied-invisible%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Belief%20in%20the%20Implied%20Invisible%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSnKocTGin5WTu27d2%2Fseq-rerun-belief-in-the-implied-invisible", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSnKocTGin5WTu27d2%2Fseq-rerun-belief-in-the-implied-invisible", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<p>Today's post, <a href=\"/lw/pb/belief_in_the_implied_invisible/\">Belief in the Implied Invisible</a> was originally published on 08 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>That it's impossible even in principle to observe something sometimes isn't enough to conclude that it doesn't exist.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/b6w/seq_rerun_gazp_vs_glut/\">GAZP vs. GLUT</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SnKocTGin5WTu27d2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 8.734999586241804e-07, "legacy": true, "legacyId": "14558", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3XMwPNMSbaPm2suGz", "eBHRu7TucHksTsorJ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-28T03:42:44.528Z", "modifiedAt": null, "url": null, "title": "Meetup : Big Berkeley meetup", "slug": "meetup-big-berkeley-meetup-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EmXsEgXBeNquWRPzG/meetup-big-berkeley-meetup-1", "pageUrlRelative": "/posts/EmXsEgXBeNquWRPzG/meetup-big-berkeley-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/EmXsEgXBeNquWRPzG/meetup-big-berkeley-meetup-1", "postedAtFormatted": "Wednesday, March 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Big%20Berkeley%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Big%20Berkeley%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEmXsEgXBeNquWRPzG%2Fmeetup-big-berkeley-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Big%20Berkeley%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEmXsEgXBeNquWRPzG%2Fmeetup-big-berkeley-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEmXsEgXBeNquWRPzG%2Fmeetup-big-berkeley-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8c'>Big Berkeley meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 April 2012 06:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2071 University Ave, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will be a joint dinner with the MPHD seminar folks. We'll meet at Taiwan Restaurant on University Avenue.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8c'>Big Berkeley meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EmXsEgXBeNquWRPzG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.735082404473732e-07, "legacy": true, "legacyId": "14560", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Big_Berkeley_meetup\">Discussion article for the meetup : <a href=\"/meetups/8c\">Big Berkeley meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 April 2012 06:30:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2071 University Ave, Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will be a joint dinner with the MPHD seminar folks. We'll meet at Taiwan Restaurant on University Avenue.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Big_Berkeley_meetup1\">Discussion article for the meetup : <a href=\"/meetups/8c\">Big Berkeley meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Big Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Big_Berkeley_meetup", "level": 1}, {"title": "Discussion article for the meetup : Big Berkeley meetup", "anchor": "Discussion_article_for_the_meetup___Big_Berkeley_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-28T06:47:28.339Z", "modifiedAt": null, "url": null, "title": "Journals that may publish articles on AI risk", "slug": "journals-that-may-publish-articles-on-ai-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:34.761Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/63qHEDNfqQMXQyxNz/journals-that-may-publish-articles-on-ai-risk", "pageUrlRelative": "/posts/63qHEDNfqQMXQyxNz/journals-that-may-publish-articles-on-ai-risk", "linkUrl": "https://www.lesswrong.com/posts/63qHEDNfqQMXQyxNz/journals-that-may-publish-articles-on-ai-risk", "postedAtFormatted": "Wednesday, March 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Journals%20that%20may%20publish%20articles%20on%20AI%20risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJournals%20that%20may%20publish%20articles%20on%20AI%20risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F63qHEDNfqQMXQyxNz%2Fjournals-that-may-publish-articles-on-ai-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Journals%20that%20may%20publish%20articles%20on%20AI%20risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F63qHEDNfqQMXQyxNz%2Fjournals-that-may-publish-articles-on-ai-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F63qHEDNfqQMXQyxNz%2Fjournals-that-may-publish-articles-on-ai-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 21, "htmlBody": "<p><a href=\"http://tinyurl.com/AI-risk-journals\">http://tinyurl.com/AI-risk-journals</a> is a continuously updated spreadsheet of journals that may accept articles related to AI risk.</p>\n<p>Maintained with help from Jonathan Wang.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "63qHEDNfqQMXQyxNz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 20, "extendedScore": null, "score": 8.735846355212634e-07, "legacy": true, "legacyId": "14576", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-28T10:02:09.575Z", "modifiedAt": null, "url": null, "title": "Should logical probabilities be updateless too?", "slug": "should-logical-probabilities-be-updateless-too", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:25.374Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rLcHvxKcJpyJj3i7o/should-logical-probabilities-be-updateless-too", "pageUrlRelative": "/posts/rLcHvxKcJpyJj3i7o/should-logical-probabilities-be-updateless-too", "linkUrl": "https://www.lesswrong.com/posts/rLcHvxKcJpyJj3i7o/should-logical-probabilities-be-updateless-too", "postedAtFormatted": "Wednesday, March 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20logical%20probabilities%20be%20updateless%20too%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20logical%20probabilities%20be%20updateless%20too%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrLcHvxKcJpyJj3i7o%2Fshould-logical-probabilities-be-updateless-too%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20logical%20probabilities%20be%20updateless%20too%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrLcHvxKcJpyJj3i7o%2Fshould-logical-probabilities-be-updateless-too", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrLcHvxKcJpyJj3i7o%2Fshould-logical-probabilities-be-updateless-too", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 557, "htmlBody": "<p>(This post doesn't require much math. It's very speculative and probably confused.)</p>\n<p>Wei Dai came up with a&nbsp;<a href=\"/r/discussion/lw/b0y/a_problem_about_bargaining_and_logical_uncertainty/\">problem</a>&nbsp;that seems equivalent to a variant of <a href=\"/lw/3l/counterfactual_mugging/\">Counterfactual Mugging</a>&nbsp;with some added twists:</p>\n<ul>\n<li>the coinflip is \"logical\", e.g. the parity of the millionth digit of pi;</li>\n<li>after you receive the offer, you will have enough resources to calculate the coinflip's outcome yourself;</li>\n<li>but you need to figure out the correct decision algorithm ahead of time, when you don't have these resources and are still uncertain about the coinflip's outcome.</li>\n</ul>\n<p>If you give 50/50 chances now to the millionth digit of pi being even or odd, you probably want to write the decision algorithm so it agrees to pay up later even when faced with a <em>proof</em> that the millionth digit of pi is even. But from the decision algorithm's point of view, the situation looks more like being asked to pay up because 2+2=4. How do we resolve this tension?</p>\n<p>One of the main selling points of TDT-style decision theories is eliminating the need for precommitment. You're supposed to always do what you would have precommitted to doing, even if it doesn't seem like a very good idea after you've done your Bayesian updates. UDT solves Counterfactual Mugging and similar problems by being \"updateless\", so you keep caring about possible worlds in accordance with their apriori probabilities regardless of which world you end up in.</p>\n<p>If we take the above problem at face value, it seems to tell us that UDT should treat logical uncertainty updatelessly too, and keep caring about <em>logically</em> impossible worlds in accordance with their apriori <em>logical</em> probabilities. It seems to hint that UDT should be coded from the start with a \"logical prior\" over mathematical statements, which encodes the creator's arbitrary \"logical degrees of caring\", just like its regular prior encodes the creator's arbitrary degrees of caring over physics. Then the AI must keep following that prior forever after. But that's a very tall order. Should you really keep caring about logically impossible worlds where 2+2=5, and accept bargains that help copies of you in such worlds, even after you calculate that 2+2=4?</p>\n<p>That conclusion is pretty startling, but consider what happens if you reject it:</p>\n<ol>\n<li>Precommitment can be modeled as a decision problem where an AI is asked to write a successor AI.</li>\n<li>Imagine the AI is asked to write a program P that will be faced with Counterfactual Mugging with a logical coin. The AI doesn't have enough resources to calculate the coin's outcome, but P will have as much computing power as needed. The resulting utility goes to the AI.</li>\n<li>Writing P is equivalent to supplying one bit: should P pay up if asked?</li>\n<li>Supplying that bit is equivalent to accepting or refusing the bet \"win $10000 if the millionth digit of pi is odd, lose $100 if it's even\".</li>\n</ol>\n<p>So if your AI treats logical uncertainty similarly enough to probabilities that it can make bets on digits of pi, reflective consistency seems to force it to have an unchanging \"logical prior\", and keep paying up in Counterfactual Mugging even when the logical coinflip looks as obvious to the AI as 2+2=4. Is there any way to escape this conclusion? (Nesov has an idea, but I can't parse it yet.) And what could a formalization of \"logical priors\"&nbsp;possibly look like?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rLcHvxKcJpyJj3i7o", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 18, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "14581", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["oZwxY88NCCHffJuxM", "mg6jDEuQEjBGtibX7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-28T12:03:45.155Z", "modifiedAt": null, "url": null, "title": "Does anyone know any kid geniuses?", "slug": "does-anyone-know-any-kid-geniuses", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:06.406Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Solvent", "createdAt": "2011-07-19T07:12:44.132Z", "isAdmin": false, "displayName": "Solvent"}, "userId": "a3sBsZXtAQacMDHfK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AFefY7hWChSgxu257/does-anyone-know-any-kid-geniuses", "pageUrlRelative": "/posts/AFefY7hWChSgxu257/does-anyone-know-any-kid-geniuses", "linkUrl": "https://www.lesswrong.com/posts/AFefY7hWChSgxu257/does-anyone-know-any-kid-geniuses", "postedAtFormatted": "Wednesday, March 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20anyone%20know%20any%20kid%20geniuses%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20anyone%20know%20any%20kid%20geniuses%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAFefY7hWChSgxu257%2Fdoes-anyone-know-any-kid-geniuses%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20anyone%20know%20any%20kid%20geniuses%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAFefY7hWChSgxu257%2Fdoes-anyone-know-any-kid-geniuses", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAFefY7hWChSgxu257%2Fdoes-anyone-know-any-kid-geniuses", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 122, "htmlBody": "<p>I'm friends with an incredibly smart kid. He's 14, but has been put up three grades in school at one point. He does all the obvious enrichment things which are available in the relatively small Australian city he lives in.</p>\n<p>His life experience has been pretty unusual. He doesn't really know what it's like to be challenged in school. All his friends are way older than he is. (Once, I asked him how being constantly around people older than him made him feel. He replied, \"Concerned for my future.\")</p>\n<p>He doesn't know anyone like him, which I think is a shame: he'd probably get along very well with them.</p>\n<p>Does anyone know any similar kid geniuses? If so, can I give them my friend's details?</p>\n<p>Thanks.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AFefY7hWChSgxu257", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 8.737154607959259e-07, "legacy": true, "legacyId": "14582", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 67, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-28T12:56:42.136Z", "modifiedAt": null, "url": null, "title": "Brain Preservation", "slug": "brain-preservation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:34.009Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nk9928vPqoeAMrTh6/brain-preservation", "pageUrlRelative": "/posts/nk9928vPqoeAMrTh6/brain-preservation", "linkUrl": "https://www.lesswrong.com/posts/nk9928vPqoeAMrTh6/brain-preservation", "postedAtFormatted": "Wednesday, March 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Brain%20Preservation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABrain%20Preservation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnk9928vPqoeAMrTh6%2Fbrain-preservation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Brain%20Preservation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnk9928vPqoeAMrTh6%2Fbrain-preservation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnk9928vPqoeAMrTh6%2Fbrain-preservation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 781, "htmlBody": "<p>Most people, given the option to halt aging and continue in good heath for centuries, would. Anti-aging research is popular, but medicine is only minimally increasing lifespan for healthy adults. You, I, and everyone we know have bodies that are incredibly unlikely to make it past 120. They're just not <a href=\"http://gravityandlevity.wordpress.com/2009/07/08/your-body-wasnt-built-to-last-a-lesson-from-human-mortality-rates/\">built to last</a>.</p>\n<p>But what are you, really? Your personality, your memories, they don't leave you when you lose a leg. Lose most parts of your body and you're still you. Lose your brain and that's it. [1] You are a pattern, instantiated in the neurons of your brain. That pattern is sustained by your body, growing and changing as you learn and experience the world. Your body supports you for years, but it deteriorates and eventually isn't up to the task any more. Is that 'game over'?</p>\n<p>Perhaps we could scan people's brains at extremely high detail so we could run them in some sort of human emulator. This requires a thorough understanding of the brain, huge amounts of storage, unbelievably fast computers, and very detailed scanning. If it's even possible, it may be <a href=\"/r/discussion/lw/88g/whole_brain_emulation_looking_at_progress_on_c/\">several hundred years away</a>.</p>\n<p>Our bodies aren't going to last that long, but what if we could figure out how to preserve our brains so that the information didn't decay? Then, if the future turned out to be one in which we had advanced brain emulation and scanning technology, we could be revived. I don't know if people in the future would want to spend the time or money to revive us, but in a future with technology this advanced, reviving a preserved brain as a computer simulation could be really cheap.</p>\n<p>The most advanced technology for long-term tissue preservation today [2] is cryonics: freezing with <a href=\"http://en.wikipedia.org/wiki/Cryopreservation#Vitrification\">vitrification</a>. You add <a href=\"http://en.wikipedia.org/wiki/Cryoprotectant\">something</a> to the blood that keeps ice crystals from forming and then freeze it. This is pretty much the same thing <a href=\"http://www.pbs.org/wgbh/nova/nature/costanzo-cryobiology.html\">frogs</a> do, hibernating frozen through the winter. The biggest organs that have been successfully brought back to working order after vitrification are <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2781097/\">rabbit kidneys</a>, and the brain is a lot bigger and much more complex. While there are people applying this technique to human brains after death, it's very much a one way street; we can't revive them with current technology.</p>\n<p>How much should it worry us that we can't reverse this freezing process? If we're already talking about revival via high-detail scanning and emulation, which is only practical after hundreds of years of technological development, does it matter that we can't currently reverse it? The real question in determining whether vitrification is sufficient is whether we're preserving all the information in your brain. If something critical is missing, or if something about our current freezing process loses information, the brains we think are properly preserved might be damaged or deteriorated beyond repair. Without a round trip test where we freeze and then revive a brain, we don't know whether what we're doing will work.</p>\n<p>Another issue is that once you've frozen the brain you need to keep it cold for a few centuries at least. Liquid nitrogen is pretty cheap, but providing it constantly over such a long time is hard. Organizations fall apart: very few stay in business for even 100 years, and those that do often have departed from their original missions. Current cryonics organizations seem no different from others, with financial difficulties and imperfect management, so I don't think 200+ years of full functioning is very likely.</p>\n<p>Even if nothing goes wrong with the organization itself, will our society last that long? Nuclear war, 'ordinary' war, bioterrorism, global warming, plagues, and future technologies all pose <a href=\"http://www.theatlantic.com/technology/archive/2012/03/were-underestimating-the-risk-of-human-extinction/253821/\">major risks</a>. Even if these don't kill everyone, they might disrupt the cryonics organizations or stop technological development such that revival technology is never developed.</p>\n<p>Taking all these potential problems and risks into account, it's unlikely that you can get around death by signing up for cryonics. In attempts to calculate overall odds for success from estimated chances of each step <a href=\"https://docs.google.com/spreadsheet/ccc?key=0Ajn1LpstEUO_dE00ZVVfa3pzX2Y2dk9mWWRKOUVkWlE#gid=0\">I've seen</a> various numbers: <a href=\"/lw/7sj/how_likely_is_cryonics_to_work/4w8s\">1:3</a>, <a href=\"/lw/3j/rationality_cryonics_and_pascals_wager/\">1:4</a>, <a href=\"http://www.alcor.org/Library/html/WillCryonicsWork.html\">1:7</a>, <a href=\"http://www.overcomingbias.com/2009/03/break-cryonics-down.html\">1:15</a> and <a href=\"http://www.alcor.org/Library/html/WillCryonicsWork.html\">1:400</a>. I'm even more pessimistic: I calculated 1:600 when I first <a href=\"/lw/7sj/how_likely_is_cryonics_to_work/\">posted to lesswrong</a> and have since <a href=\"https://docs.google.com/spreadsheet/ccc?key=0Ajn1LpstEUO_dE00ZVVfa3pzX2Y2dk9mWWRKOUVkWlE#gid=0\">revised down</a> to 1:1000. To some people the probability <a href=\"/lw/7sj/how_likely_is_cryonics_to_work/4wcq\">doesn't matter</a>, but because it's expensive and there are plenty of <a href=\"http://www.reddit.com/r/smartgiving\">other things</a> one can do with money, I don't think it's <a href=\"/lw/1mc/normal_cryonics/\">obviously the sensible thing to do</a>.</p>\n<p><small><em>(I also posted this <a href=\"http://www.jefftk.com/news/2012-03-28.html\">on my blog</a>.) </em></small></p>\n<p><br /> [1] Well, lose your heart and you're gone too. Except that we can make mechanical hearts and you stay the same person on receiving one. Not so much with a mechanical brain.</p>\n<p>[2] <a href=\"http://www.gwern.net/plastination\">Plastination</a> is also an option, but it's not yet to a point where we can do it on even a mouse brain.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nk9928vPqoeAMrTh6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 33, "extendedScore": null, "score": 8.737373661485692e-07, "legacy": true, "legacyId": "14583", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 108, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XhHetxjWxZ6b85HK9", "dz3Mmr2Cykz6RRfhK", "NEpZGLNMGc447ez34", "hiDkhLyN5S2MEjrSE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-29T01:23:43.036Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Quantum Explanations", "slug": "seq-rerun-quantum-explanations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:38.114Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WwJMN5rMYtg93XE4v/seq-rerun-quantum-explanations", "pageUrlRelative": "/posts/WwJMN5rMYtg93XE4v/seq-rerun-quantum-explanations", "linkUrl": "https://www.lesswrong.com/posts/WwJMN5rMYtg93XE4v/seq-rerun-quantum-explanations", "postedAtFormatted": "Thursday, March 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Quantum%20Explanations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Quantum%20Explanations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWwJMN5rMYtg93XE4v%2Fseq-rerun-quantum-explanations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Quantum%20Explanations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWwJMN5rMYtg93XE4v%2Fseq-rerun-quantum-explanations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWwJMN5rMYtg93XE4v%2Fseq-rerun-quantum-explanations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 257, "htmlBody": "<p>Today's post, <a href=\"/lw/pc/quantum_explanations/\">Quantum Explanations</a> was originally published on 09 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Quantum mechanics doesn't deserve its fearsome reputation. If you tell people something is supposed to be mysterious, they won't understand it. It's human intuitions that are \"strange\" or \"weird\"; physics itself is perfectly normal. Talking about historical erroneous concepts like \"particles\" or \"waves\" is just asking to confuse people; present the real, unified quantum physics straight out. The series will take a strictly realist perspective - quantum equations describe something that is real and out there. Warning: Although a large faction of physicists agrees with this, it is not universally accepted. Stronger warning: I am not even going to present non-realist viewpoints until later, because I think this is a major source of confusion.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/b8e/seq_rerun_belief_in_the_implied_invisible/\">Belief in the Implied Invisible</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WwJMN5rMYtg93XE4v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.740465108137209e-07, "legacy": true, "legacyId": "14597", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7FSwbFpDsca7uXpQ2", "SnKocTGin5WTu27d2", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-29T01:35:13.161Z", "modifiedAt": null, "url": null, "title": "SotW: Check Consequentialism", "slug": "sotw-check-consequentialism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:05.563Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xypbWhzEEw4ZsRK9i/sotw-check-consequentialism", "pageUrlRelative": "/posts/xypbWhzEEw4ZsRK9i/sotw-check-consequentialism", "linkUrl": "https://www.lesswrong.com/posts/xypbWhzEEw4ZsRK9i/sotw-check-consequentialism", "postedAtFormatted": "Thursday, March 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SotW%3A%20Check%20Consequentialism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASotW%3A%20Check%20Consequentialism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxypbWhzEEw4ZsRK9i%2Fsotw-check-consequentialism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SotW%3A%20Check%20Consequentialism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxypbWhzEEw4ZsRK9i%2Fsotw-check-consequentialism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxypbWhzEEw4ZsRK9i%2Fsotw-check-consequentialism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2166, "htmlBody": "<p><em>(The&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/CFAR_Exercise_Prize\">Exercise Prize</a>&nbsp;series of&nbsp;<a href=\"/tag/exprize/\">posts</a>&nbsp;is the Center for Applied Rationality asking for help inventing exercises that can teach cognitive skills. &nbsp;The difficulty is&nbsp;</em><em>coming up with exercises interesting enough, with a high enough hedonic return, that people actually do them and remember them; this often involves standing up and performing actions, or interacting with other people, not just working alone with an exercise booklet and a pencil. &nbsp;</em><em>We offer prizes of $50 for any suggestion we decide to test, and $500 for any suggestion we decide to adopt. &nbsp;This prize also extends to LW meetup activities and good ideas for verifying that a skill has been acquired. &nbsp;<a href=\"http://wiki.lesswrong.com/wiki/CFAR_Exercise_Prize\">See&nbsp;here&nbsp;for details</a>.)</em></p>\n<hr />\n<p><strong>Exercise Prize: &nbsp;Check Consequentialism</strong></p>\n<p>In philosophy, \"consequentialism\" is the belief that doing the right thing makes the world a better place, i.e., that actions should be chosen on the basis of their probable outcomes. &nbsp;It seems like the mental habit of <em>checking consequentialism,&nbsp;</em>asking \"What positive future events does this action cause?\", would catch numerous cognitive fallacies.</p>\n<p>For example, the mental habit of consequentialism would counter the sunk cost fallacy - if a PhD wouldn't really lead to much in the way of desirable job opportunities or a higher income, and the only reason you're still pursuing your PhD is that <em>otherwise all your previous years of work will have been wasted,</em>&nbsp;you will find yourself encountering a blank screen at the point where you try to imagine a positive <em>future</em>&nbsp;outcome of spending another two years working toward your PhD - you will not be able to state what good future events happen as a result.</p>\n<p>Or consider the problem of <em>living in the should-universe;</em>&nbsp;if you're thinking, <em>I'm not going to talk to my boyfriend about X because he </em>should<em>&nbsp;know it already,</em>&nbsp;you might be able to spot this as an instance of should-universe thinking (planning/choosing/acting/feeling as though within / by-comparison-to an image of an ideal perfect universe) by having done exercises specifically to sensitize you to should-ness. &nbsp;<em>Or,</em>&nbsp;if you've practiced the <em>more general</em> skill of Checking Consequentialism, you&nbsp;might notice a problem on asking \"What happens if I talk / don't talk to my boyfriend?\" - providing that you're sufficiently adept to constrain your consequentialist visualization to what <em>actually&nbsp;</em>happens as opposed to what <em>should</em>&nbsp;happen.</p>\n<p><strong>Discussion:</strong></p>\n<p>The skill of Checking Consequentialism isn't quite as simple as telling people to ask, \"What positive result do I get?\" &nbsp;By itself, this mental query is probably going to return <em>any</em>&nbsp;apparent justification - for example, in the sunk-cost-PhD example, asking \"What good thing happens as a result?\" will just return, \"All my years of work won't have been wasted! &nbsp;That's good!\" &nbsp;Any choice people are tempted by seems good for <em>some </em>reason, and executing a query about \"good reasons\" will just return this.</p>\n<p>The novel part of Checking Consequentialism is the ability to <em>discriminate&nbsp;</em>\"consequentialist reasons\" from \"non-consequentialist reasons\" - being able to distinguish that \"Because a PhD gets me a 50% higher salary\" talks about future positive consequences, while \"Because I don't want my years of work to have been wasted\" doesn't.<a id=\"more\"></a></p>\n<p>It's possible that asking \"At what time does the consequence occur and how long does it last?\" would be useful for distinguishing future-consequences from non-future-consequences - if you take a bad-thing like \"I don't want my work to have been wasted\" and ask \"When does it occur, where does it occur, and how long does it last?\", you will with luck notice the error.</p>\n<p>Learning to draw cause-and-effect directed graphs, a la Judea Pearl and Bayes nets, seems like it might be helpful - at least, Geoff was doing this while trying to teach strategicness and the class seemed to like it.</p>\n<p>Sometimes non-consequentialist reasons can be rescued as consequentialist ones. &nbsp;\"You shouldn't kill because it's the wrong thing to do\" can be rescued as \"Because then a person will transition from 'alive' to 'dead' in the future, and this is a bad event\" or \"Because the interval between Outcome A and Outcome B includes the interval from Fred alive to Fred dead.\"</p>\n<p>On a five-second level, the skill would have to include:</p>\n<ul>\n<li>Being cued by some problem to try looking at the consequences;</li>\n<li>Either directly having a mental procedure that <em>only </em>turns up consequences, like trying to visualize events out into the future, <em>or</em></li>\n<li>First asking 'Why am I doing this?' and then looking at the justifications to check if they're consequentialist, perhaps using techniques like asking 'How long does it last?', 'When does it happen?', or 'Where does it happen?'.</li>\n<li>Expending a <em>small</em>&nbsp;amount of effort to see <em>if</em>&nbsp;a non-consequentialist reason can easily translate into a consequentialist one<em>&nbsp;in a realistic way.</em></li>\n<li>Making the decision whether or not to change your mind.</li>\n<li>If necessary, detaching from the thing you were doing for non-consequentialist reasons.</li>\n</ul>\n<p>In practice, it may be obvious that you're making a mistake as soon as you think to check consequences. &nbsp;I have 'living in the should-universe' or 'sunk cost fallacy' cached to the point where as soon as I spot an error of that pattern, it's usually pretty obvious (without further deliberative thought) what the residual reasons are and whether I was doing it wrong.</p>\n<p><strong>Pain points &amp; Pluses:</strong></p>\n<p><em>(When generating a candidate kata, almost the first question we ask - directly after the selection of a topic, like 'consequentialism' - is, \"What are the pain points? &nbsp;Or pleasure points?\" &nbsp;This can be errors you've made yourself and noticed afterward, or even cases where you've noticed someone else doing it wrong, but ideally cases where you use the skill in real life. &nbsp;Since a lot of rationality is in fact about not screwing up, there may not always be pleasure points where the skill is used in a non-error-correcting, strictly positive context; but it's still worth asking each time. &nbsp;We ask this question right at the beginning because it (a) checks to see how often the skill is actually important in real life and (b) provides concrete use-cases to focus discussion of the skill.)</em></p>\n<p><em>Pain points:</em></p>\n<p>Checking Consequentialism looks like it should be useful for countering:</p>\n<ul>\n<li>Living in the should-universe (taking actions because of the consequences they <em>ought</em>&nbsp;to have, rather than the consequences they <em>probably will</em>&nbsp;have). &nbsp;E.g., \"I'm not going to talk to my girlfriend because she should already know X\" or \"I'm going to become a theoretical physicist because I ought to enjoy theoretical physics.\"</li>\n<li>The sunk cost fallacy (choosing to prevent previously expended, non-recoverable resources from <em>having been wasted in retrospect</em>&nbsp;- i.e., avoiding the mental pain of reclassifying a <em>past </em>investment as a loss - rather than acting for the sake of future considerations). &nbsp;E.g., \"If I give up on my PhD, I'll have wasted the last three years.\"</li>\n<li><a href=\"/lw/k5/cached_thoughts/\">Cached thoughts</a> and habits; \"But I usually shop at Whole Foods\" or \"I don't know, I've never tried an electric toothbrush before.\" &nbsp;(These might have rescuable consequences, but as stated, they aren't talking about future events.)</li>\n<li>Acting-out an emotion - one of the most useful pieces of advice I got from Anna Salamon was to find other ways to act out an emotion than strategic choices. &nbsp;If you're feeling frustrated with a coworker, you might still want to Check Consequentialism on \"Buy them dead flowers for their going-away party\" even though it seems to express your frustration.</li>\n<li>Indignation / acting-out of morals - \"Drugs are bad, so drug use ought to be illegal\", where it's much harder to make the case that countries which decriminalized marijuana experienced worse net outcomes. &nbsp;(Though it should be noted that you also have to Use Empiricism to ask the question 'What happened to other countries that decriminalized marijuana?' instead of making up a gloomy consequentialist prediction to express your moral disapproval.)</li>\n<li>Identity - \"I'm the sort of person who belongs in academia.\"</li>\n<li>\"Trying to do things\" for simply no reason at all, while your brain still generates activities and actions, because nobody ever told you that behaviors ought to have a purpose or that lack of purpose is a warning sign. &nbsp;This habit can be inculcated by schoolwork, wanting to put in 8 hours before going home, etc. &nbsp;E.g. you \"try to write an essay\", and you know that an essay has paragraphs; so you try to write a bunch of paragraphs but you don't have any functional role in mind for each paragraph. &nbsp;\"What is the positive consequence of this paragraph?\" might come in handy here.</li>\n</ul>\n<p>(This list is not intended to be exhaustive.)</p>\n<p><em>Pleasure points:</em></p>\n<ul>\n<li>Being able to state and then focus on a positive outcome seems like it should improve motivation, at least in cases where the positive outcome is realistically attainable to a non-frustrating degree and has not yet been subject to hedonic adaptation. &nbsp;E.g., a $600 job may be more motivating if you visualize the $600 laptop you're going to buy with the proceeds.</li>\n</ul>\n<p>Also, consequentialism is the foundation of expected utility, which is the foundation of instrumental rationality - this is why we're considering it as an early unit. &nbsp;(This is not directly listed as a \"pleasure point\" because it is not directly a use-case.)</p>\n<p>Constantly asking about consequences seems likely to improve overall strategicness - not just lead to the better of two choices being taken from a fixed decision-set, but also having goals in mind that can generate new perceived choices, i.e., improve the overall degree to which people do things for reasons, as opposed to not doing things or not having reasons. &nbsp;(But this is a hopeful&nbsp;eventual&nbsp;positive consequence of practicing the skill, not a use-case where the skill is directly being applied.)</p>\n<p><strong>Teaching &amp; exercises:</strong></p>\n<p>This is the part that's being thrown open to Less Wrong generally. &nbsp;Hopefully I've described the skill in enough detail to convey what it <em>is.</em>&nbsp; Now, how would you <em>practice</em>&nbsp;it? &nbsp;How would you have an audience practice it, hopefully in <em>activities</em>&nbsp;carried out with <em>each other?</em></p>\n<p>The dumb thing I tried to do previously was to have exercises along the lines of, \"Print up a booklet with little snippets of scenarios in them, and ask people to circle non-consequentialist reasoning, then try to either translate it to consequentialist reasons or say that no consequentialist reasons could be found.\" &nbsp;I didn't do that for this exact session, but if you look at what I did with the sunk cost fallacy, it's the same sort of silly thing I tried to do.</p>\n<p>This didn't work very well - maybe the exercises were too easy, or maybe it was that people were doing it alone, or maybe we did something else wrong, but the audience appeared to experience insufficient hedonic return. &nbsp;They were, in lay terms, unenthusiastic.</p>\n<p>At this point I should like to pause, and tell a recent and important story. &nbsp;On Saturday I taught an 80-minute unit on Bayes's Rule to an audience of non-Sequence-reading experimental subjects, who were mostly either programmers or in other technical subjects, so I could go through the math fairly fast. &nbsp;Afterward, though, I was worried that they hadn't really learned to apply Bayes's Rule and wished I had a small little pamphlet of practice problems to hand out. &nbsp;I still think this would've been a good idea, but...</p>\n<p>On&nbsp;<em>Wednesday,</em>&nbsp;I attended Andrew Critch's course at Berkeley, which was roughly mostly-instrumental LW-style cognitive-improvement material aimed at math students; and in this particular session, Critch introduced Bayes's Theorem, not as advanced math, but with the aim of getting them to apply it to life.</p>\n<p>Critch demonstrated using what he called the Really Getting Bayes game. &nbsp;He had Nisan (a local LWer) touch an object to the back of Critch's neck, a cellphone as it happened, while Critch faced in the other direction; this was \"prior experience\". &nbsp;Nisan said that the object was either a cellphone or a pen. &nbsp;Critch gave prior odds of 60% : 40% that the object was a cellphone vs. pen, based on his prior experience. &nbsp;Nisan then asked Critch how likely he thought it was that a cellphone or a pen would be RGB-colored, i.e., colored red, green, or blue. &nbsp;Critch didn't give exact numbers here, but said he thought a cellphone was more likely to be primary-colored, and drew some rectangles on the blackboard to illustrate the likelihood ratio. &nbsp;After being told that the object was in fact primary-colored (the cellphone was metallic blue), Critch gave posterior odds of 75% : 25% in favor of the cellphone, and then turned around to look.</p>\n<p>Then Critch broke up the class into pairs and asked each pair to carry out a similar operation on each other: &nbsp;Pick two plausible objects and make sure you're holding at least one of them, touch it to the other person while they face the other way, prior odds, additional fact, likelihood ratio, posterior odds.</p>\n<p>This is the sort of in-person, hands-on, real-life, and&nbsp;<em>social</em>&nbsp;exercise that didn't occur to me, or Anna, or anyone else helping, while we were trying to design the Bayes's Theorem unit. &nbsp;Our brains just didn't go in that direction, though we recognized it as embarrassingly obvious in retrospect.</p>\n<p>So... how would you design an exercise to teach Checking Consequentialism?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZTRNmvQGgoYiymYnq": 1, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xypbWhzEEw4ZsRK9i", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 58, "extendedScore": null, "score": 0.000133, "legacy": true, "legacyId": "14415", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>(The&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/CFAR_Exercise_Prize\">Exercise Prize</a>&nbsp;series of&nbsp;<a href=\"/tag/exprize/\">posts</a>&nbsp;is the Center for Applied Rationality asking for help inventing exercises that can teach cognitive skills. &nbsp;The difficulty is&nbsp;</em><em>coming up with exercises interesting enough, with a high enough hedonic return, that people actually do them and remember them; this often involves standing up and performing actions, or interacting with other people, not just working alone with an exercise booklet and a pencil. &nbsp;</em><em>We offer prizes of $50 for any suggestion we decide to test, and $500 for any suggestion we decide to adopt. &nbsp;This prize also extends to LW meetup activities and good ideas for verifying that a skill has been acquired. &nbsp;<a href=\"http://wiki.lesswrong.com/wiki/CFAR_Exercise_Prize\">See&nbsp;here&nbsp;for details</a>.)</em></p>\n<hr>\n<p><strong id=\"Exercise_Prize___Check_Consequentialism\">Exercise Prize: &nbsp;Check Consequentialism</strong></p>\n<p>In philosophy, \"consequentialism\" is the belief that doing the right thing makes the world a better place, i.e., that actions should be chosen on the basis of their probable outcomes. &nbsp;It seems like the mental habit of <em>checking consequentialism,&nbsp;</em>asking \"What positive future events does this action cause?\", would catch numerous cognitive fallacies.</p>\n<p>For example, the mental habit of consequentialism would counter the sunk cost fallacy - if a PhD wouldn't really lead to much in the way of desirable job opportunities or a higher income, and the only reason you're still pursuing your PhD is that <em>otherwise all your previous years of work will have been wasted,</em>&nbsp;you will find yourself encountering a blank screen at the point where you try to imagine a positive <em>future</em>&nbsp;outcome of spending another two years working toward your PhD - you will not be able to state what good future events happen as a result.</p>\n<p>Or consider the problem of <em>living in the should-universe;</em>&nbsp;if you're thinking, <em>I'm not going to talk to my boyfriend about X because he </em>should<em>&nbsp;know it already,</em>&nbsp;you might be able to spot this as an instance of should-universe thinking (planning/choosing/acting/feeling as though within / by-comparison-to an image of an ideal perfect universe) by having done exercises specifically to sensitize you to should-ness. &nbsp;<em>Or,</em>&nbsp;if you've practiced the <em>more general</em> skill of Checking Consequentialism, you&nbsp;might notice a problem on asking \"What happens if I talk / don't talk to my boyfriend?\" - providing that you're sufficiently adept to constrain your consequentialist visualization to what <em>actually&nbsp;</em>happens as opposed to what <em>should</em>&nbsp;happen.</p>\n<p><strong id=\"Discussion_\">Discussion:</strong></p>\n<p>The skill of Checking Consequentialism isn't quite as simple as telling people to ask, \"What positive result do I get?\" &nbsp;By itself, this mental query is probably going to return <em>any</em>&nbsp;apparent justification - for example, in the sunk-cost-PhD example, asking \"What good thing happens as a result?\" will just return, \"All my years of work won't have been wasted! &nbsp;That's good!\" &nbsp;Any choice people are tempted by seems good for <em>some </em>reason, and executing a query about \"good reasons\" will just return this.</p>\n<p>The novel part of Checking Consequentialism is the ability to <em>discriminate&nbsp;</em>\"consequentialist reasons\" from \"non-consequentialist reasons\" - being able to distinguish that \"Because a PhD gets me a 50% higher salary\" talks about future positive consequences, while \"Because I don't want my years of work to have been wasted\" doesn't.<a id=\"more\"></a></p>\n<p>It's possible that asking \"At what time does the consequence occur and how long does it last?\" would be useful for distinguishing future-consequences from non-future-consequences - if you take a bad-thing like \"I don't want my work to have been wasted\" and ask \"When does it occur, where does it occur, and how long does it last?\", you will with luck notice the error.</p>\n<p>Learning to draw cause-and-effect directed graphs, a la Judea Pearl and Bayes nets, seems like it might be helpful - at least, Geoff was doing this while trying to teach strategicness and the class seemed to like it.</p>\n<p>Sometimes non-consequentialist reasons can be rescued as consequentialist ones. &nbsp;\"You shouldn't kill because it's the wrong thing to do\" can be rescued as \"Because then a person will transition from 'alive' to 'dead' in the future, and this is a bad event\" or \"Because the interval between Outcome A and Outcome B includes the interval from Fred alive to Fred dead.\"</p>\n<p>On a five-second level, the skill would have to include:</p>\n<ul>\n<li>Being cued by some problem to try looking at the consequences;</li>\n<li>Either directly having a mental procedure that <em>only </em>turns up consequences, like trying to visualize events out into the future, <em>or</em></li>\n<li>First asking 'Why am I doing this?' and then looking at the justifications to check if they're consequentialist, perhaps using techniques like asking 'How long does it last?', 'When does it happen?', or 'Where does it happen?'.</li>\n<li>Expending a <em>small</em>&nbsp;amount of effort to see <em>if</em>&nbsp;a non-consequentialist reason can easily translate into a consequentialist one<em>&nbsp;in a realistic way.</em></li>\n<li>Making the decision whether or not to change your mind.</li>\n<li>If necessary, detaching from the thing you were doing for non-consequentialist reasons.</li>\n</ul>\n<p>In practice, it may be obvious that you're making a mistake as soon as you think to check consequences. &nbsp;I have 'living in the should-universe' or 'sunk cost fallacy' cached to the point where as soon as I spot an error of that pattern, it's usually pretty obvious (without further deliberative thought) what the residual reasons are and whether I was doing it wrong.</p>\n<p><strong id=\"Pain_points___Pluses_\">Pain points &amp; Pluses:</strong></p>\n<p><em>(When generating a candidate kata, almost the first question we ask - directly after the selection of a topic, like 'consequentialism' - is, \"What are the pain points? &nbsp;Or pleasure points?\" &nbsp;This can be errors you've made yourself and noticed afterward, or even cases where you've noticed someone else doing it wrong, but ideally cases where you use the skill in real life. &nbsp;Since a lot of rationality is in fact about not screwing up, there may not always be pleasure points where the skill is used in a non-error-correcting, strictly positive context; but it's still worth asking each time. &nbsp;We ask this question right at the beginning because it (a) checks to see how often the skill is actually important in real life and (b) provides concrete use-cases to focus discussion of the skill.)</em></p>\n<p><em>Pain points:</em></p>\n<p>Checking Consequentialism looks like it should be useful for countering:</p>\n<ul>\n<li>Living in the should-universe (taking actions because of the consequences they <em>ought</em>&nbsp;to have, rather than the consequences they <em>probably will</em>&nbsp;have). &nbsp;E.g., \"I'm not going to talk to my girlfriend because she should already know X\" or \"I'm going to become a theoretical physicist because I ought to enjoy theoretical physics.\"</li>\n<li>The sunk cost fallacy (choosing to prevent previously expended, non-recoverable resources from <em>having been wasted in retrospect</em>&nbsp;- i.e., avoiding the mental pain of reclassifying a <em>past </em>investment as a loss - rather than acting for the sake of future considerations). &nbsp;E.g., \"If I give up on my PhD, I'll have wasted the last three years.\"</li>\n<li><a href=\"/lw/k5/cached_thoughts/\">Cached thoughts</a> and habits; \"But I usually shop at Whole Foods\" or \"I don't know, I've never tried an electric toothbrush before.\" &nbsp;(These might have rescuable consequences, but as stated, they aren't talking about future events.)</li>\n<li>Acting-out an emotion - one of the most useful pieces of advice I got from Anna Salamon was to find other ways to act out an emotion than strategic choices. &nbsp;If you're feeling frustrated with a coworker, you might still want to Check Consequentialism on \"Buy them dead flowers for their going-away party\" even though it seems to express your frustration.</li>\n<li>Indignation / acting-out of morals - \"Drugs are bad, so drug use ought to be illegal\", where it's much harder to make the case that countries which decriminalized marijuana experienced worse net outcomes. &nbsp;(Though it should be noted that you also have to Use Empiricism to ask the question 'What happened to other countries that decriminalized marijuana?' instead of making up a gloomy consequentialist prediction to express your moral disapproval.)</li>\n<li>Identity - \"I'm the sort of person who belongs in academia.\"</li>\n<li>\"Trying to do things\" for simply no reason at all, while your brain still generates activities and actions, because nobody ever told you that behaviors ought to have a purpose or that lack of purpose is a warning sign. &nbsp;This habit can be inculcated by schoolwork, wanting to put in 8 hours before going home, etc. &nbsp;E.g. you \"try to write an essay\", and you know that an essay has paragraphs; so you try to write a bunch of paragraphs but you don't have any functional role in mind for each paragraph. &nbsp;\"What is the positive consequence of this paragraph?\" might come in handy here.</li>\n</ul>\n<p>(This list is not intended to be exhaustive.)</p>\n<p><em>Pleasure points:</em></p>\n<ul>\n<li>Being able to state and then focus on a positive outcome seems like it should improve motivation, at least in cases where the positive outcome is realistically attainable to a non-frustrating degree and has not yet been subject to hedonic adaptation. &nbsp;E.g., a $600 job may be more motivating if you visualize the $600 laptop you're going to buy with the proceeds.</li>\n</ul>\n<p>Also, consequentialism is the foundation of expected utility, which is the foundation of instrumental rationality - this is why we're considering it as an early unit. &nbsp;(This is not directly listed as a \"pleasure point\" because it is not directly a use-case.)</p>\n<p>Constantly asking about consequences seems likely to improve overall strategicness - not just lead to the better of two choices being taken from a fixed decision-set, but also having goals in mind that can generate new perceived choices, i.e., improve the overall degree to which people do things for reasons, as opposed to not doing things or not having reasons. &nbsp;(But this is a hopeful&nbsp;eventual&nbsp;positive consequence of practicing the skill, not a use-case where the skill is directly being applied.)</p>\n<p><strong id=\"Teaching___exercises_\">Teaching &amp; exercises:</strong></p>\n<p>This is the part that's being thrown open to Less Wrong generally. &nbsp;Hopefully I've described the skill in enough detail to convey what it <em>is.</em>&nbsp; Now, how would you <em>practice</em>&nbsp;it? &nbsp;How would you have an audience practice it, hopefully in <em>activities</em>&nbsp;carried out with <em>each other?</em></p>\n<p>The dumb thing I tried to do previously was to have exercises along the lines of, \"Print up a booklet with little snippets of scenarios in them, and ask people to circle non-consequentialist reasoning, then try to either translate it to consequentialist reasons or say that no consequentialist reasons could be found.\" &nbsp;I didn't do that for this exact session, but if you look at what I did with the sunk cost fallacy, it's the same sort of silly thing I tried to do.</p>\n<p>This didn't work very well - maybe the exercises were too easy, or maybe it was that people were doing it alone, or maybe we did something else wrong, but the audience appeared to experience insufficient hedonic return. &nbsp;They were, in lay terms, unenthusiastic.</p>\n<p>At this point I should like to pause, and tell a recent and important story. &nbsp;On Saturday I taught an 80-minute unit on Bayes's Rule to an audience of non-Sequence-reading experimental subjects, who were mostly either programmers or in other technical subjects, so I could go through the math fairly fast. &nbsp;Afterward, though, I was worried that they hadn't really learned to apply Bayes's Rule and wished I had a small little pamphlet of practice problems to hand out. &nbsp;I still think this would've been a good idea, but...</p>\n<p>On&nbsp;<em>Wednesday,</em>&nbsp;I attended Andrew Critch's course at Berkeley, which was roughly mostly-instrumental LW-style cognitive-improvement material aimed at math students; and in this particular session, Critch introduced Bayes's Theorem, not as advanced math, but with the aim of getting them to apply it to life.</p>\n<p>Critch demonstrated using what he called the Really Getting Bayes game. &nbsp;He had Nisan (a local LWer) touch an object to the back of Critch's neck, a cellphone as it happened, while Critch faced in the other direction; this was \"prior experience\". &nbsp;Nisan said that the object was either a cellphone or a pen. &nbsp;Critch gave prior odds of 60% : 40% that the object was a cellphone vs. pen, based on his prior experience. &nbsp;Nisan then asked Critch how likely he thought it was that a cellphone or a pen would be RGB-colored, i.e., colored red, green, or blue. &nbsp;Critch didn't give exact numbers here, but said he thought a cellphone was more likely to be primary-colored, and drew some rectangles on the blackboard to illustrate the likelihood ratio. &nbsp;After being told that the object was in fact primary-colored (the cellphone was metallic blue), Critch gave posterior odds of 75% : 25% in favor of the cellphone, and then turned around to look.</p>\n<p>Then Critch broke up the class into pairs and asked each pair to carry out a similar operation on each other: &nbsp;Pick two plausible objects and make sure you're holding at least one of them, touch it to the other person while they face the other way, prior odds, additional fact, likelihood ratio, posterior odds.</p>\n<p>This is the sort of in-person, hands-on, real-life, and&nbsp;<em>social</em>&nbsp;exercise that didn't occur to me, or Anna, or anyone else helping, while we were trying to design the Bayes's Theorem unit. &nbsp;Our brains just didn't go in that direction, though we recognized it as embarrassingly obvious in retrospect.</p>\n<p>So... how would you design an exercise to teach Checking Consequentialism?</p>", "sections": [{"title": "Exercise Prize: \u00a0Check Consequentialism", "anchor": "Exercise_Prize___Check_Consequentialism", "level": 1}, {"title": "Discussion:", "anchor": "Discussion_", "level": 1}, {"title": "Pain points & Pluses:", "anchor": "Pain_points___Pluses_", "level": 1}, {"title": "Teaching & exercises:", "anchor": "Teaching___exercises_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "309 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 313, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2MD3NMLBPCqPfnfre"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-29T03:00:08.753Z", "modifiedAt": null, "url": null, "title": "Al Jazeera: \"Engineering Human Evolution\" -- 0h:35m:41s Youtube Video.", "slug": "al-jazeera-engineering-human-evolution-0h-35m-41s-youtube", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:35.840Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Logos01", "createdAt": "2011-07-21T18:59:16.270Z", "isAdmin": false, "displayName": "Logos01"}, "userId": "WZxoXCWQviJp9dNc5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fSoQPre9CkAbRdACc/al-jazeera-engineering-human-evolution-0h-35m-41s-youtube", "pageUrlRelative": "/posts/fSoQPre9CkAbRdACc/al-jazeera-engineering-human-evolution-0h-35m-41s-youtube", "linkUrl": "https://www.lesswrong.com/posts/fSoQPre9CkAbRdACc/al-jazeera-engineering-human-evolution-0h-35m-41s-youtube", "postedAtFormatted": "Thursday, March 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Al%20Jazeera%3A%20%22Engineering%20Human%20Evolution%22%20--%200h%3A35m%3A41s%20Youtube%20Video.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAl%20Jazeera%3A%20%22Engineering%20Human%20Evolution%22%20--%200h%3A35m%3A41s%20Youtube%20Video.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfSoQPre9CkAbRdACc%2Fal-jazeera-engineering-human-evolution-0h-35m-41s-youtube%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Al%20Jazeera%3A%20%22Engineering%20Human%20Evolution%22%20--%200h%3A35m%3A41s%20Youtube%20Video.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfSoQPre9CkAbRdACc%2Fal-jazeera-engineering-human-evolution-0h-35m-41s-youtube", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfSoQPre9CkAbRdACc%2Fal-jazeera-engineering-human-evolution-0h-35m-41s-youtube", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 98, "htmlBody": "<p><a href=\"https://www.youtube.com/watch?v=CXGY2o6GJPA&amp;feature=g-u-u&amp;context=G26be3eaFUAAAAAAAAAA\">Link here.</a></p>\n<p><a href=\"http://stream.aljazeera.com/story/engineering-human-evolution-0022143\">Al Jazeera website link for the video disinclined.</a></p>\n<p>A brief synopsis from the Al Jazeera website:</p>\n<blockquote>\n<p>Cyborgs, brain uploads and immortality - How far should science go in  helping humans exceed their biological limitations? These ideas might  sound like science fiction, but proponents of a movement known as  transhumanism believe they are inevitable. <br /><br /> In this episode of The Stream, we talk to bioethicist George Dvorsky;  Robin Hanson, a research associate with Oxford&rsquo;s Future of Humanity  Institute; and Ari N. Schulman, senior editor of The New Atlantis, about  the ethical implications of transhumanism.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Discuss below.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fSoQPre9CkAbRdACc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 0, "extendedScore": null, "score": 8.740864307493935e-07, "legacy": true, "legacyId": "14609", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-29T07:11:25.822Z", "modifiedAt": null, "url": null, "title": "Alternate card types for Anki", "slug": "alternate-card-types-for-anki", "viewCount": null, "lastCommentedAt": "2021-06-19T15:10:11.573Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DavidLS", "createdAt": "2011-09-06T12:31:41.558Z", "isAdmin": false, "displayName": "DavidLS"}, "userId": "RN5oJZk7hA4PDaoMT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Hi4fxs68K8HnHxN44/alternate-card-types-for-anki", "pageUrlRelative": "/posts/Hi4fxs68K8HnHxN44/alternate-card-types-for-anki", "linkUrl": "https://www.lesswrong.com/posts/Hi4fxs68K8HnHxN44/alternate-card-types-for-anki", "postedAtFormatted": "Thursday, March 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Alternate%20card%20types%20for%20Anki&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAlternate%20card%20types%20for%20Anki%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHi4fxs68K8HnHxN44%2Falternate-card-types-for-anki%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Alternate%20card%20types%20for%20Anki%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHi4fxs68K8HnHxN44%2Falternate-card-types-for-anki", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHi4fxs68K8HnHxN44%2Falternate-card-types-for-anki", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 276, "htmlBody": "<p>Recently I have started using Anki in a new and complimentary way. I am curious if any of you find it similarly useful and/or have other anki tips :)</p>\n<p>The basic idea is that instead of putting down a challenge/response pair for facts, we put down a challenge/response pair for ways of thinking. A train of thought. Ideally, this is something akin to a lumosity.com game except&nbsp;tailored&nbsp;to your area of focus.</p>\n<p>A simple example from algebra:</p>\n<p>The&nbsp;fact based approach would be to make a card titled \"what is the quadratic formula?\" with the answer of \"x == (-b +- Sqrt(b^2 - 4ac)) / 2a\"</p>\n<p>The way I am recommending is to make a card titled \"derive the answer for ax^2 + bx + c == 0\" with an answer that shows the steps. When shown the card, you would then either solve it in your head, or using a pad of paper. I assume that sub minute tasks are ideal.</p>\n<p>The specific area I have been using this in is the study of algorithms, with challenges like \"Hopcraft-Karp algorithm for bipartite matching\", and it has so far proved very helpful at getting fluent with the names, deepening my understanding of the algorithms themselves, and with seeing new places to apply them in my coding.</p>\n<p>This might be overstepping, but something like this seems like it might be appropriate for The Center for Modern Rationality. Something like \"critique the logic of the following three sentences\", or \"Sue is about to buy a car. How should she go about making a decision\".</p>\n<p>This is my first real post to LessWrong, so if you have style corrections those are solicited&nbsp;alongside&nbsp;any comments on the post itself. Thanks!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Hi4fxs68K8HnHxN44", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 11, "extendedScore": null, "score": 8.7419047393101e-07, "legacy": true, "legacyId": "14622", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-29T10:37:04.015Z", "modifiedAt": null, "url": null, "title": "Meetup : Shanghai Less Wrong Meetup", "slug": "meetup-shanghai-less-wrong-meetup-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:06.170Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Patrick", "createdAt": "2009-02-27T08:09:44.663Z", "isAdmin": false, "displayName": "Patrick"}, "userId": "KC7mjSorWj2XsdL3v", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JXb2JWGpFamHdxayt/meetup-shanghai-less-wrong-meetup-0", "pageUrlRelative": "/posts/JXb2JWGpFamHdxayt/meetup-shanghai-less-wrong-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/JXb2JWGpFamHdxayt/meetup-shanghai-less-wrong-meetup-0", "postedAtFormatted": "Thursday, March 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Shanghai%20Less%20Wrong%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Shanghai%20Less%20Wrong%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJXb2JWGpFamHdxayt%2Fmeetup-shanghai-less-wrong-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Shanghai%20Less%20Wrong%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJXb2JWGpFamHdxayt%2Fmeetup-shanghai-less-wrong-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJXb2JWGpFamHdxayt%2Fmeetup-shanghai-less-wrong-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 128, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8d'>Shanghai Less Wrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 April 2012 10:36:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Shanghai</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>WHEN: 15 April 2012 01:00:00PM</p>\n\n<p>WHERE: This is at a private residence, please private message 'Teddy' if you're joining. (This is China, it's got to be this way) I'm looking for any willing Shanghai Lesswrong readers to join this group if they haven't already. Up vote user Teddy in the comments below. This way I have karma to post to the meetup section without asking Patrick Robotham each time. This will help us boost members for the community here in Shanghai. The group has been meeting informally for awhile.. we have a good community here. If you'd like to join let me know.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8d'>Shanghai Less Wrong Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JXb2JWGpFamHdxayt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.742756332425152e-07, "legacy": true, "legacyId": "14631", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Shanghai_Less_Wrong_Meetup\">Discussion article for the meetup : <a href=\"/meetups/8d\">Shanghai Less Wrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 April 2012 10:36:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Shanghai</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>WHEN: 15 April 2012 01:00:00PM</p>\n\n<p>WHERE: This is at a private residence, please private message 'Teddy' if you're joining. (This is China, it's got to be this way) I'm looking for any willing Shanghai Lesswrong readers to join this group if they haven't already. Up vote user Teddy in the comments below. This way I have karma to post to the meetup section without asking Patrick Robotham each time. This will help us boost members for the community here in Shanghai. The group has been meeting informally for awhile.. we have a good community here. If you'd like to join let me know.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Shanghai_Less_Wrong_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/8d\">Shanghai Less Wrong Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Shanghai Less Wrong Meetup", "anchor": "Discussion_article_for_the_meetup___Shanghai_Less_Wrong_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Shanghai Less Wrong Meetup", "anchor": "Discussion_article_for_the_meetup___Shanghai_Less_Wrong_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-29T16:27:09.729Z", "modifiedAt": null, "url": null, "title": "George Orwell's Prelude on Politics Is The Mind Killer", "slug": "george-orwell-s-prelude-on-politics-is-the-mind-killer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:12.308Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "oXGTijwhjZB8cnJ3W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dEbhs6iicJXKtswm7/george-orwell-s-prelude-on-politics-is-the-mind-killer", "pageUrlRelative": "/posts/dEbhs6iicJXKtswm7/george-orwell-s-prelude-on-politics-is-the-mind-killer", "linkUrl": "https://www.lesswrong.com/posts/dEbhs6iicJXKtswm7/george-orwell-s-prelude-on-politics-is-the-mind-killer", "postedAtFormatted": "Thursday, March 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20George%20Orwell's%20Prelude%20on%20Politics%20Is%20The%20Mind%20Killer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGeorge%20Orwell's%20Prelude%20on%20Politics%20Is%20The%20Mind%20Killer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdEbhs6iicJXKtswm7%2Fgeorge-orwell-s-prelude-on-politics-is-the-mind-killer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=George%20Orwell's%20Prelude%20on%20Politics%20Is%20The%20Mind%20Killer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdEbhs6iicJXKtswm7%2Fgeorge-orwell-s-prelude-on-politics-is-the-mind-killer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdEbhs6iicJXKtswm7%2Fgeorge-orwell-s-prelude-on-politics-is-the-mind-killer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 90, "htmlBody": "<p>I have found this most wonderful (if fairly lengthy) article, <a href=\"http://www.george-orwell.org/Notes_on_Nationalism/0.html\">and thought you would enjoy having it brought to your attention.</a></p>\n<p>&nbsp;</p>\n<p>It is so good, I think we should include it among the references in the \"Politics is the Mind Killer\" wiki page. &nbsp;But, before that, I submit it to you, and ask you: is there anything in this article that would warrant its exclusion from this site? I mean, besides the fact that it is about politics, and written by a notorious Social-democrat (And is that in itself grounds for&nbsp;exclusion?).</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dEbhs6iicJXKtswm7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 13, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "14632", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 286, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-29T16:40:02.750Z", "modifiedAt": null, "url": null, "title": "Meetup Interest: Rhode Island", "slug": "meetup-interest-rhode-island", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:39.375Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Scottbert", "createdAt": "2011-07-09T23:00:01.050Z", "isAdmin": false, "displayName": "Scottbert"}, "userId": "yBdFpjAzHzvn9KkWZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kx8aLGuKuejGk6NRi/meetup-interest-rhode-island", "pageUrlRelative": "/posts/kx8aLGuKuejGk6NRi/meetup-interest-rhode-island", "linkUrl": "https://www.lesswrong.com/posts/kx8aLGuKuejGk6NRi/meetup-interest-rhode-island", "postedAtFormatted": "Thursday, March 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20Interest%3A%20Rhode%20Island&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20Interest%3A%20Rhode%20Island%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkx8aLGuKuejGk6NRi%2Fmeetup-interest-rhode-island%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20Interest%3A%20Rhode%20Island%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkx8aLGuKuejGk6NRi%2Fmeetup-interest-rhode-island", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkx8aLGuKuejGk6NRi%2Fmeetup-interest-rhode-island", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 229, "htmlBody": "<p>For over a year now I have been visiting this site, reading and learning about rationality, and meeting other rationalists sounds like a wonderfully positive experience. Sadly, there are no meetups in my area -- I looked at the RI skeptics society website, but their news page seemed focused on religious-people-bashing to a point that seemed mean and spiteful to me (at least at the time, it was awhile ago. Yes, we know people do dumb things because of religious beliefs; how about focusing on improving ourselves instead of pointing at them?).</p>\n<p>Maybe there are a lot of others in the area like me who have been lurking on the site, longing for a chance to meet likeminded people but too shy to speak up. Of course, not all of them may be checking the discussions page but I'm not sure what else to do.</p>\n<p>So, anyone out there in Rhode Island or close to the borders of Massachusetts and Connecticut that would like to start a local rationality meetup with me?</p>\n<p>I am also interested in any advice for getting this to its target audience, as not everyone may be looking at the discussion page.</p>\n<p>I am also also interested in anywhere I can talk with lesswrong folks in real-time -- it's hard to establish friendships through posts, and this discussion forum doesn't seem like the place for random chatter with people.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kx8aLGuKuejGk6NRi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.74425988078253e-07, "legacy": true, "legacyId": "14633", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-29T18:23:17.118Z", "modifiedAt": null, "url": null, "title": "The Institute For Propaganda Analysis, A Precursor and a Warning", "slug": "the-institute-for-propaganda-analysis-a-precursor-and-a", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:39.381Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "oXGTijwhjZB8cnJ3W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/L2pSvmNNRrQ9o5PaZ/the-institute-for-propaganda-analysis-a-precursor-and-a", "pageUrlRelative": "/posts/L2pSvmNNRrQ9o5PaZ/the-institute-for-propaganda-analysis-a-precursor-and-a", "linkUrl": "https://www.lesswrong.com/posts/L2pSvmNNRrQ9o5PaZ/the-institute-for-propaganda-analysis-a-precursor-and-a", "postedAtFormatted": "Thursday, March 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Institute%20For%20Propaganda%20Analysis%2C%20A%20Precursor%20and%20a%20Warning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Institute%20For%20Propaganda%20Analysis%2C%20A%20Precursor%20and%20a%20Warning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL2pSvmNNRrQ9o5PaZ%2Fthe-institute-for-propaganda-analysis-a-precursor-and-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Institute%20For%20Propaganda%20Analysis%2C%20A%20Precursor%20and%20a%20Warning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL2pSvmNNRrQ9o5PaZ%2Fthe-institute-for-propaganda-analysis-a-precursor-and-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL2pSvmNNRrQ9o5PaZ%2Fthe-institute-for-propaganda-analysis-a-precursor-and-a", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1113, "htmlBody": "<p>Years ago, I stumbled upon this most interesting segment while reading Aldous Huxley's Brave New World Revisited, of which I have finally found an online version that enables me to share its contents with you:&nbsp;</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>In their anti-rational propaganda the enemies of freedom systematically pervert the resources of lang&shy;uage in order to wheedle or stampede their victims into thinking, feeling and acting as they, the mind-manipulators, want them to think, feel and act. An education for freedom (and for the love and intelli&shy;gence which are at once the conditions and the results of freedom) must be, among other things, an educa&shy;tion in the proper uses of language. For the last two or three generations philosophers have devoted a great deal of time and thought to the analysis of symbols and the meaning of meaning. How are the words and sentences which we speak related to the things, per&shy;sons and events, with which we have to deal in our day-to-day living? To discuss this problem would take too long and lead us too far afield. Suffice it to say that all the intellectual materials for a sound education in the proper use of language &mdash; an education on every level from the kindergarten to the postgraduate school &mdash; are now available. Such an education in the art of distinguishing between the proper and the improper use of symbols could be inaugurated immediately. In&shy;deed it might have been inaugurated at any time during the last thirty or forty years. And yet children are nowhere taught, in any systematic way, to distinguish true from false, or meaningful from meaningless, state&shy;ments. Why is this so? Because their elders, even in the democratic countries, do not want them to be given this kind of education. <strong>In this context the brief, sad history of the Institute for Propaganda Analysis is highly significant. The Institute was founded in 1937, when Nazi propaganda was at its noisiest and most effective, by Mr. Filene, the New England philanthro&shy;pist. Under its auspices analyses of non-rational propa&shy;ganda were made and several texts for the instruction of high school and university students were prepared. Then came the war &mdash; a total war on all the fronts, the mental no less than the physical. With all the Allied governments engaging in \"psychological warfare, \" an insistence upon the desirability of analyzing propa&shy;ganda seemed a bit tactless. The Institute was closed in 1941.</strong> But even before the outbreak of hostilities, there were many persons to whom its activities seemed profoundly objectionable. Certain educators, for exam&shy;ple, disapproved of the teaching of propaganda anal&shy;ysis on the grounds that it would make adolescents unduly cynical. Nor was it welcomed by the military authorities, who were afraid that recruits might start to analyze the utterances of drill sergeants. And then there were the clergymen and the advertisers. The clergymen were against propaganda analysis as tend&shy;ing to undermine belief and diminish churchgoing; the advertisers objected on the grounds that it might undermine brand loyalty and reduce sales.</p>\n<p><br />These fears and dislikes were not unfounded. Too searching a scrutiny by too many of the common folk of what is said by their pastors and masters might prove to be profoundly subversive. In its present form, the social order depends for its continued existence on the acceptance, without too many embarrassing questions, of the propaganda put forth by those in author&shy;ity and the propaganda hallowed by the local tradi&shy;tions. The problem, once more, is to find the happy mean. Individuals must be suggestible enough to be willing and able to make their society work, but not so suggestible as to fall helplessly under the spell of pro&shy;fessional mind-manipulators. Similarly, they should be taught enough about propaganda analysis to preserve them from an uncritical belief in sheer nonsense, but not so much as to make them reject outright the not always rational outpourings of the well-meaning guardians of tradition. Probably the happy mean be&shy;tween gullibility and a total skepticism can never be discovered and maintained by analysis alone. This rather negative approach to the problem will have to be supplemented by something more positive &mdash; the enunciation of a set of generally acceptable values based upon a solid foundation of facts. The value, first of all, of individual freedom, based upon the facts of human diversity and genetic uniqueness; the value of charity and compassion, based upon the old familiar fact, lately rediscovered by modern psychiatry &mdash; the fact that, whatever their mental and physical di&shy;versity, love is as necessary to human beings as food and shelter; and finally the value of intelligence, with&shy;out which love is impotent and freedom unattainable. This set of values will provide us with a criterion by which propaganda may be judged. The propaganda that is found to be both nonsensical and immoral may be rejected out of hand. That which is merely irra&shy;tional, but compatible with love and freedom, and not on principle opposed to the exercise of intelligence, may be provisionally accepted for what it is worth.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Obviously I most fervently recommend this book as something a rationalist and humanist would probably greatly enjoy. In fact, if there is enough demand, an entire thread to discuss said book's contents, the facts that it relates and the insights that it brings, would be a wonderful undertaking. <strong>This thread, however, has the significantly narrower objective of bringing to the fore the history of the defunct<a href=\"http://en.wikipedia.org/wiki/Institute_for_Propaganda_Analysis\"> Institute For Rational Analysis</a>&nbsp;</strong>(which has an heir in <a href=\"http://www.propagandacritic.com/\">the Propaganda Critic</a>, a website which&nbsp;analyses&nbsp;current propaganda with the help of the&nbsp;tool set&nbsp;the institute developed)<strong>, and look at its history for insights on how to conduct our own, oddly similar, philanthropic endeavors, especially the newly-created </strong><a style=\"font-weight: bold;\" href=\"http://hpmor.com/modern-rationality/\">Centre For Modern Rationality</a><strong>, and especially on the obstacles and opposition we should expect to meet, and speculate on how to navigate them, if and when they should arise.&nbsp;</strong></p>\n<p>I have included the second paragraph because, while it not directly relevant to the the external&nbsp;difficulties&nbsp;the Institute had to face, it highlights a very important topic: our responsibility towards these youth. We will, most probably, be tearing apart all the morality infrastructure, all the adaptations of which they would be executers. And we might also hurt their chances of integrating in a society where clear thinking and mental hygiene are <em>not</em> in the mainstream. What are the measures we should take to help young rationalists be able to win, if they decide one of the games they want to win at is \" live happily with non-rationalist people, befriend them, and perhaps even spread our message further\"? Or, for that matter \"make decisions quickly and efficiently on what's the right thing and the wrong thing to do in a given situation\"?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bY5MaF2EATwDkomvu": 1, "Ng8Gice9KNkncxqcj": 1, "7ow6EFpypbH4hzFuz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "L2pSvmNNRrQ9o5PaZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 37, "extendedScore": null, "score": 0.000112, "legacy": true, "legacyId": "14635", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-29T18:50:08.165Z", "modifiedAt": null, "url": null, "title": "[Draft] How to Run a Successful Less Wrong Meetup", "slug": "draft-how-to-run-a-successful-less-wrong-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:53.201Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/T2fcyjay3GtkvGn7F/draft-how-to-run-a-successful-less-wrong-meetup", "pageUrlRelative": "/posts/T2fcyjay3GtkvGn7F/draft-how-to-run-a-successful-less-wrong-meetup", "linkUrl": "https://www.lesswrong.com/posts/T2fcyjay3GtkvGn7F/draft-how-to-run-a-successful-less-wrong-meetup", "postedAtFormatted": "Thursday, March 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BDraft%5D%20How%20to%20Run%20a%20Successful%20Less%20Wrong%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BDraft%5D%20How%20to%20Run%20a%20Successful%20Less%20Wrong%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT2fcyjay3GtkvGn7F%2Fdraft-how-to-run-a-successful-less-wrong-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BDraft%5D%20How%20to%20Run%20a%20Successful%20Less%20Wrong%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT2fcyjay3GtkvGn7F%2Fdraft-how-to-run-a-successful-less-wrong-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT2fcyjay3GtkvGn7F%2Fdraft-how-to-run-a-successful-less-wrong-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 674, "htmlBody": "<p><a href=\"http://wiki.lesswrong.com/mediawiki/images/c/ca/How_to_Run_a_Successful_Less_Wrong_Meetup_Group.pdf\">How to Run a Successful Less Wrong Meetup</a> is a guide that I've been working on, based on lukeprog's instructions, for the last week and a half. As it says in the beginning:</p>\n<blockquote>\n<p><span id=\"internal-source-marker_0.3891165134347305\" style=\"font-size: 16px; font-family: Garamond; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">This document is written for anyone who wants to organize a Less Wrong meetup. We expect that this document will help you regardless of whether you want to start a new group or improve an existing one. We have tried to write each section so that it applies in either case.</span></p>\n</blockquote>\n<p>Here's the table of contents:</p>\n<ul>\n<li>Why organize a meetup?</li>\n<li>How to build your team of heroes \n<ul>\n<li>The organizer</li>\n<li>The welcomer</li>\n<li>The learning coach</li>\n<li>The content provider</li>\n<li>The visionary</li>\n<li>The networker</li>\n</ul>\n</li>\n<li>How to announce and organize your meetups \n<ul>\n<li>Choosing a venue</li>\n<li>Making the announcement</li>\n<li>The first meetup</li>\n</ul>\n</li>\n<li>Long-term meetup group maintenance \n<ul>\n<li>Retain members by being a social group</li>\n<li>Conflicts within the group \n<ul>\n<li>Learn to recognize status conflicts</li>\n</ul>\n</li>\n<li>Group norms and epistemic hygiene</li>\n</ul>\n</li>\n<li>Meetup content \n<ul>\n<li>Discussions and Presentations \n<ul>\n<li>Presentations</li>\n<li>Topical Discussions</li>\n<li>Meta Discussion</li>\n</ul>\n</li>\n<li>Games and Exercises \n<ul>\n<li>Aumann&rsquo;s Thunderdome</li>\n<li>Biased Co-operation</li>\n<li>Behavioral Analysis</li>\n<li>Bluffing Games</li>\n<li>Bust-a-Distortion</li>\n<li>Calibration Game</li>\n<li>Cause and Belief</li>\n<li>Five-Minute Debiasing</li>\n<li>Hypothetical Apostasies</li>\n<li>Paranoid Debating</li>\n<li>Precommit to Updates</li>\n<li>Rationalization Game</li>\n<li>Rejection Therapy</li>\n<li>Repetition Game</li>\n<li>Status Exercises</li>\n<li>Zendo</li>\n</ul>\n</li>\n<li>General Bacchanalia</li>\n<li>Example activities at real meetup groups</li>\n</ul>\n</li>\n<li>Projects</li>\n</ul>\n<p>This is a draft version, so feedback would be most welcome, particularly on things like:</p>\n<ul>\n<li>Is this useful?</li>\n<li>Is there something that should be covered isn't covered at all yet?</li>\n<li>Do you have new games &amp; exercises to suggest?</li>\n<li>Do you have any other content to suggest to any other section?</li>\n<li>Do you disagree with some of the advice given?</li>\n<li>Do you disagree on way something has been worded?</li>\n<li>Etc.</li>\n</ul>\n<p>The link above will take you to a Google Docs copy of the document, with the ability to add comments to the draft. Feel free to comment on the guide either as traditional LW comments or by attaching comments to the document itself: both are fine.</p>\n<p><strong>EDIT: Here's the most recent version, though without the commenting ability.</strong></p>\n<p><strong>EDIT2: The most recent version as of April 11th, with commenting enabled.</strong></p>\n<p><strong>EDIT3: First <a href=\"http://wiki.lesswrong.com/mediawiki/images/c/ca/How_to_Run_a_Successful_Less_Wrong_Meetup_Group.pdf\">non-draft version</a>; see also <a href=\"/lw/crs/how_to_run_a_successful_less_wrong_meetup/\">this thread</a>.<br /></strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"T57Qd9J3AfxmwhQtY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "T2fcyjay3GtkvGn7F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 29, "extendedScore": null, "score": 8.744798857558092e-07, "legacy": true, "legacyId": "14636", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"http://wiki.lesswrong.com/mediawiki/images/c/ca/How_to_Run_a_Successful_Less_Wrong_Meetup_Group.pdf\">How to Run a Successful Less Wrong Meetup</a> is a guide that I've been working on, based on lukeprog's instructions, for the last week and a half. As it says in the beginning:</p>\n<blockquote>\n<p><span id=\"internal-source-marker_0.3891165134347305\" style=\"font-size: 16px; font-family: Garamond; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">This document is written for anyone who wants to organize a Less Wrong meetup. We expect that this document will help you regardless of whether you want to start a new group or improve an existing one. We have tried to write each section so that it applies in either case.</span></p>\n</blockquote>\n<p>Here's the table of contents:</p>\n<ul>\n<li>Why organize a meetup?</li>\n<li>How to build your team of heroes \n<ul>\n<li>The organizer</li>\n<li>The welcomer</li>\n<li>The learning coach</li>\n<li>The content provider</li>\n<li>The visionary</li>\n<li>The networker</li>\n</ul>\n</li>\n<li>How to announce and organize your meetups \n<ul>\n<li>Choosing a venue</li>\n<li>Making the announcement</li>\n<li>The first meetup</li>\n</ul>\n</li>\n<li>Long-term meetup group maintenance \n<ul>\n<li>Retain members by being a social group</li>\n<li>Conflicts within the group \n<ul>\n<li>Learn to recognize status conflicts</li>\n</ul>\n</li>\n<li>Group norms and epistemic hygiene</li>\n</ul>\n</li>\n<li>Meetup content \n<ul>\n<li>Discussions and Presentations \n<ul>\n<li>Presentations</li>\n<li>Topical Discussions</li>\n<li>Meta Discussion</li>\n</ul>\n</li>\n<li>Games and Exercises \n<ul>\n<li>Aumann\u2019s Thunderdome</li>\n<li>Biased Co-operation</li>\n<li>Behavioral Analysis</li>\n<li>Bluffing Games</li>\n<li>Bust-a-Distortion</li>\n<li>Calibration Game</li>\n<li>Cause and Belief</li>\n<li>Five-Minute Debiasing</li>\n<li>Hypothetical Apostasies</li>\n<li>Paranoid Debating</li>\n<li>Precommit to Updates</li>\n<li>Rationalization Game</li>\n<li>Rejection Therapy</li>\n<li>Repetition Game</li>\n<li>Status Exercises</li>\n<li>Zendo</li>\n</ul>\n</li>\n<li>General Bacchanalia</li>\n<li>Example activities at real meetup groups</li>\n</ul>\n</li>\n<li>Projects</li>\n</ul>\n<p>This is a draft version, so feedback would be most welcome, particularly on things like:</p>\n<ul>\n<li>Is this useful?</li>\n<li>Is there something that should be covered isn't covered at all yet?</li>\n<li>Do you have new games &amp; exercises to suggest?</li>\n<li>Do you have any other content to suggest to any other section?</li>\n<li>Do you disagree with some of the advice given?</li>\n<li>Do you disagree on way something has been worded?</li>\n<li>Etc.</li>\n</ul>\n<p>The link above will take you to a Google Docs copy of the document, with the ability to add comments to the draft. Feel free to comment on the guide either as traditional LW comments or by attaching comments to the document itself: both are fine.</p>\n<p><strong id=\"EDIT__Here_s_the_most_recent_version__though_without_the_commenting_ability_\">EDIT: Here's the most recent version, though without the commenting ability.</strong></p>\n<p><strong id=\"EDIT2__The_most_recent_version_as_of_April_11th__with_commenting_enabled_\">EDIT2: The most recent version as of April 11th, with commenting enabled.</strong></p>\n<p><strong id=\"EDIT3__First_non_draft_version__see_also_this_thread_\">EDIT3: First <a href=\"http://wiki.lesswrong.com/mediawiki/images/c/ca/How_to_Run_a_Successful_Less_Wrong_Meetup_Group.pdf\">non-draft version</a>; see also <a href=\"/lw/crs/how_to_run_a_successful_less_wrong_meetup/\">this thread</a>.<br></strong></p>", "sections": [{"title": "EDIT: Here's the most recent version, though without the commenting ability.", "anchor": "EDIT__Here_s_the_most_recent_version__though_without_the_commenting_ability_", "level": 1}, {"title": "EDIT2: The most recent version as of April 11th, with commenting enabled.", "anchor": "EDIT2__The_most_recent_version_as_of_April_11th__with_commenting_enabled_", "level": 1}, {"title": "EDIT3: First non-draft version; see also this thread.", "anchor": "EDIT3__First_non_draft_version__see_also_this_thread_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "26 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qMuAazqwJvkvo8teR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-29T20:48:48.227Z", "modifiedAt": null, "url": null, "title": "Minicamps on Rationality and Awesomeness: May 11-13, June 22-24, and July 21-28", "slug": "minicamps-on-rationality-and-awesomeness-may-11-13-june-22", "viewCount": null, "lastCommentedAt": "2019-12-13T14:48:30.558Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fkhbBE2ZTSytvsy9x/minicamps-on-rationality-and-awesomeness-may-11-13-june-22", "pageUrlRelative": "/posts/fkhbBE2ZTSytvsy9x/minicamps-on-rationality-and-awesomeness-may-11-13-june-22", "linkUrl": "https://www.lesswrong.com/posts/fkhbBE2ZTSytvsy9x/minicamps-on-rationality-and-awesomeness-may-11-13-june-22", "postedAtFormatted": "Thursday, March 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Minicamps%20on%20Rationality%20and%20Awesomeness%3A%20May%2011-13%2C%20June%2022-24%2C%20and%20July%2021-28&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMinicamps%20on%20Rationality%20and%20Awesomeness%3A%20May%2011-13%2C%20June%2022-24%2C%20and%20July%2021-28%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfkhbBE2ZTSytvsy9x%2Fminicamps-on-rationality-and-awesomeness-may-11-13-june-22%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Minicamps%20on%20Rationality%20and%20Awesomeness%3A%20May%2011-13%2C%20June%2022-24%2C%20and%20July%2021-28%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfkhbBE2ZTSytvsy9x%2Fminicamps-on-rationality-and-awesomeness-may-11-13-june-22", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfkhbBE2ZTSytvsy9x%2Fminicamps-on-rationality-and-awesomeness-may-11-13-june-22", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1822, "htmlBody": "<p class=\"p1\"><em>&ldquo;</em><em>I do not say this lightly... but if you're looking for superpowers, this is the place to start.&rdquo;</em></p>\n<p class=\"p1\"><em>--Michael Curzi, summer 2011 minicamp participant</em></p>\n<p><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; \">Who:</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; \">&nbsp;</span><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; \">You and a class full of other aspiring rationalists and world-optimizers, from around the world.</span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; \"><em>What:</em>&nbsp;<strong>Two 3-day weekend minicamps</strong> and <strong>one 8-day minicamp</strong>, filled with hands-on activities for applying rationality to your life, your goals, and the making of a better world. &nbsp;(See details in the FAQ.)</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; \"><em>When and where:</em>&nbsp;We're running three camps, so that we can do this for three sets of participants: May 11-13 and June 22-24 for the 3-day camps, and July 21-28 for the eight-day camp, all in the San Francisco Bay Area.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; \"><em>Why:</em>&nbsp;Because you&rsquo;re a social primate, and the best way to jump into a new way of thinking, make friends, and accomplish your goals is often to spend time with other primates who are doing just that.&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; \"><em><a id=\"more\" style=\"color: #6a8a6b; text-decoration: underline; \"></a></em><em>Other reasons:</em></p>\n<ul style=\"padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; \">\n<li><span style=\"background-color: rgba(255, 255, 255, 0.917969); color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Hang out and explore the Bay Area with two dozen other people like you who are smart, interesting, and passionate about rationality</span></li>\n<li><span style=\"background-color: rgba(255, 255, 255, 0.917969); color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Attend bonus sessions about style, body language, and confidence-building.</span></li>\n<li style=\"font-family: Arial, Helvetica, sans-serif; line-height: 21px;\">Get help charting out career paths; and, entirely optionally for those interested, connect with folks at the Singularity Institute about optimal philanthropy.</li>\n</ul>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; \"><em>Instructors:</em></p>\n<table border=\"0\" frame=\"void\">\n<tbody>\n<tr>\n<td align=\"center\"><img style=\"vertical-align: text-bottom; margin-left: 5px; margin-right: 5px;\" src=\"http://images.lesswrong.com/t3_b98_3.png\" alt=\"\" width=\"127\" height=\"160\" /></td>\n<td align=\"center\"><img style=\"vertical-align: text-bottom; margin-left: 5px; margin-right: 5px;\" src=\"http://images.lesswrong.com/t3_b98_2.png\" alt=\"\" width=\"135\" height=\"160\" /></td>\n<td align=\"center\"><img style=\"margin-left: 5px; margin-right: 5px; vertical-align: text-bottom;\" src=\"http://4.bp.blogspot.com/_kQZxysNQd6c/S7nN98VxLyI/AAAAAAAAAc0/Zt72L0uzN8k/S220/Snapshot+2010-04-05+01-18-59.jpg\" alt=\"\" width=\"135\" height=\"160\" /></td>\n</tr>\n<tr>\n<td align=\"center\"><a href=\"http://yudkowsky.net\">Eliezer Yudkowsky</a></td>\n<td align=\"center\"><a href=\"http://annasalamon.com/\">Anna Salamon</a></td>\n<td align=\"center\"><a href=\"http://measureofdoubt.com/\">Julia Galef</a></td>\n</tr>\n<tr>\n<td align=\"center\"><img style=\"vertical-align: text-bottom; margin-left: 5px; margin-right: 5px;\" src=\"http://images.lesswrong.com/t3_b98_0.png\" alt=\"\" width=\"135\" height=\"160\" /></td>\n<td align=\"center\"><img style=\"vertical-align: text-bottom; margin-left: 5px; margin-right: 5px;\" src=\"http://images.lesswrong.com/t3_b98_4.png?v=90266b244d15d2085ea71c485e4f2f65\" alt=\"\" width=\"135\" height=\"160\" /></td>\n<td align=\"center\"><img style=\"vertical-align: text-bottom; margin-left: 5px; margin-right: 5px;\" src=\"https://sites.google.com/site/placetoholdthingsforvalentine/home/Val2.JPG\" alt=\"\" width=\"135\" height=\"160\" /></td>\n</tr>\n<tr>\n<td align=\"center\"><a href=\"http://math.berkeley.edu/~critch/mphd/\">Andrew Critch</a></td>\n<td align=\"center\"><a href=\"http://lukeprog.com/\">Luke Muehlhauser</a></td>\n<td align=\"center\">Michael Smith</td>\n</tr>\n</tbody>\n</table>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><em>Cost:</em>&nbsp; $650 for the three-day programs; $1500 for the week-long program. &nbsp;This includes lodging[1], meals, and tuition. &nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\">(Note that this *still* isn't quite enough to make running minicamps sustainable in the long-run; a lodging + meals at retreat centers start at around $90 per person per night, the \"three-day camps\" include four nights, and these workshops take a staff of about 5 full-time people for over a month each prior to each workshop, most of us at $3k/month, counting curriculum development time (plus miscellaneous expenses). &nbsp;<span style=\"line-height: 21px;\">We are trying to strike a compromise between \"charge enough that we can run more camps\" and staying affordable, especially for our start-up phase; costs will probably go up in following years.)</span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><span style=\"line-height: 21px;\">Three days (or a week) isn&rsquo;t long enough to learn rationality, but it's long enough to learn how to learn rationality, and to get some momentum toward doing so.</span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\">Come meet us, and see what you can do.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><a style=\"color: #8a8a8b;\" href=\"https://docs.google.com/spreadsheet/viewform?formkey=dEctaFJONTk1UVdfdE9sSEpiQTFLLWc6MA\"><strong>Apply now.</strong></a></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><strong><a id=\"more\"></a>Frequently Asked Questions:</strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><em>1. &nbsp;I&rsquo;m older. &nbsp;Should I still apply?</em></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; padding-left: 30px;\">Yes! &nbsp;We're aiming for a more diverse crowd and would love to add your wider set of experiences and skills.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><em>2. &nbsp;I&rsquo;d like to come, but I&rsquo;m not sure you&rsquo;ll accept me. &nbsp;Should I still apply?</em></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; padding-left: 30px;\">Absolutely! &nbsp; You can fill out our form in as little 10 minutes.&nbsp; What&rsquo;s the harm?[2]</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><em>3. &nbsp;I&rsquo;d like to come, but I can&rsquo;t afford it. &nbsp;Should I still apply?</em></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; padding-left: 30px;\">Yes, you should definitely apply. &nbsp;A limited number of scholarships will probably be available this time, and more may be available later.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; padding-left: 30px;\">(There's also an option on the application form if you want to apply but can't make any of the times - this just says that you want to be part of future minicamps and makes sure we have your application details.)</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><em>4. &nbsp;</em><em style=\"line-height: 21px;\">What will we do, exactly?</em></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; padding-left: 30px;\">We're still working out the details. &nbsp;In our current model:</p>\n<ul style=\"font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; padding: 0px;\">\n<li><strong>Daily schedule:</strong>&nbsp;Every day, you'll have five hours of core workshop sessions (mostly exercises, divided into morning and evening sessions), meals shared with other participants, and shared activities such as soccer, poker, karaoke, and trips to bay area sites.</li>\n<li><strong>Rationality:</strong>&nbsp;You'll practice many specific techniques (e.g. Fermi calculations, applying Bayes' theorem and cognitive biases to daily life, seeing how using fungibility can boost your goal achievement);&nbsp;develop a map of your rationality strengths and gaps;&nbsp;and learn how to continue learning rationality after the program.&nbsp;</li>\n<li><strong>Social effectiveness:</strong>&nbsp; Reading and using body language; developing a fashion sense; improving social courage; and understanding why social reality is important.</li>\n<li><strong style=\"line-height: 21px;\">Individual meetings:</strong><span style=\"line-height: 21px;\">&nbsp; You'll be able to schedule one-on-one appointments to discuss career paths you may want to take (we can help with statistics on earnings in different professions, and strategy for getting in); how to start a LW meet-up or similar community;&nbsp;and, optionally for those interested, how to get involved in existential risks-reducing research and action.</span></li>\n</ul>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><em style=\"line-height: 21px;\">5. &nbsp;I&rsquo;m new to all this. &nbsp;Will it make sense?</em></p>\n<div>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; padding-left: 30px;\">If you&rsquo;ve read at least fifteen posts from&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\">the</a>&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Reductionism_(sequence)\">core</a>&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\">sequences</a>, yes it will. &nbsp;If you haven&rsquo;t: why not read them now?</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; padding-left: 30px;\">We&rsquo;ll also aim for an atmosphere in which everyone is free to make mistakes and to try things, and in which people are receptive to a wide range of skill levels.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><em>6. &nbsp;I&rsquo;ve already read the Sequences seventeen times, and also I&rsquo;m a self-made billionaire with three PhDs. &nbsp;Will I learn anything new?</em><em>[3]</em></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; padding-left: 30px;\">We hope so. &nbsp;We&rsquo;re covering a good range of material, with much more of a focus on &nbsp;<em>practice</em>&nbsp; and &nbsp;<em>exercise</em>&nbsp; than in the Sequences, incorporating new lessons learned since the LW material was written, and with some instructors who've developed their own takes on rationality.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><em>7. &nbsp;What evidence is there that I'll be glad I went?</em></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\">After last year's minicamp, participants completed an anonymous exit survey. &nbsp;(With the instructions: \"We're asking you these questions to learn how to run camps; please be honest; it'll help us more if you're accurate than if you're positive.\") &nbsp;Here are their answers to the most relevant questions:</p>\n<ul style=\"line-height: 21px; padding: 0px;\">\n<li>In answer to &ldquo;Zero to ten, are you glad you came?&rdquo;, the median answer was 10 (mean was 9.3).</li>\n<li>In answer to &ldquo;Zero to ten, will your life go significantly differently because you came to mini-camp?&rdquo; the median answer was 7.5&nbsp;(the mean was 6.9) [This was the response that was most positively surprising to me.].</li>\n<li>In answer to &ldquo;Zero to ten, has your epistemic rationality improved?&rdquo;, the median answer was 7 (mean 6.9).</li>\n<li>In answer to &ldquo;Zero to ten, are you more motivated to learn epistemic rationality, than you were when you came?&rdquo;, the median answer was 8.5 (mean 8.1).</li>\n<li>In answer to &ldquo;Zero to ten, have you become more skilled at modifying your emotions and dispositions?&rdquo;, the median answer was 7 (mean 6.3).</li>\n<li>In answer to &ldquo;Zero to ten, are you more motivated to modify your emotions and dispositions, than you were when you came?&rdquo;, the median answer was 9 (mean 8.3).</li>\n<li><span style=\"line-height: 21px;\">In answer to &ldquo;Zero to ten, have you gained social skills since coming?&rdquo;, the median answer was 7.5 (mean 7.2).</span></li>\n<li><span style=\"line-height: 21px;\">In answer to \"Zero to ten, did you like spending time with the other participants?\", the median answer was 9 (mean 8.8).</span></li>\n</ul>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\">We also asked participants for testimonials -- statements designed to be shown to others, in case they wanted to recommend such camps. &nbsp;They wrote:</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \"><span style=\"line-height: 21px;\">&ldquo;This was an intensely positive experience. This was easily the most powerful change self-modification I've ever made, in all of the social, intellectual, and emotional spheres. I'm now a more powerful person than I was a week ago -- and I can explain exactly how and why this is true.</span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \">At mini-camp, I've learned techniques for effective self-modification -- that is, I have a much deeper understanding of how to change my desires, gather my willpower, channel my time and cognitive resources, and model and handle previously confusing situations. What's more, I have a fairly clear map of how to build these skills henceforth, and how to inculcate them in others. And all this was presented in such a way that any sufficiently analytical folk -- anyone who has understood a few of the LW sequences, say -- can gain in extreme measures.&rdquo;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \">--Matt Elder /&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/Fiddlemath\">Fiddlemath</a></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \">&ldquo;I expected a week of interesting things and some useful tools to take away. What I got was 8 days of constant, deep learning, challenges to my limits that helped me grow. I finally grokked that I can and should optimize myself on every dimension I care about, that practice and reinforcement can make me a better thinker, and that I can change very quickly when I'm not constrained by artificial barriers or stress.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \">I would not recommend doing something like this right before another super-busy week, because I was learning at 100% of capacity and will need a lot of time to unpack all the things I learned and apply them to my life, but I came away with a clear plan for becoming better. It is now a normal and easy thing for me to try things out, test my beliefs, and self-improve. And I'm likely to be much more effective at making the world a better place as well, by prioritizing without fear.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \">The material was all soundly-researched and effectively taught, with extremely helpful supplemental exercises and activities. The instructors were very helpful in and out of session. The other participants were excited, engaged, challenging, and supportive.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \">I look forward to sharing what I've learned with my local Lesswrong meetup and others in the area. If that's even 1/4 as awesome as my time at the Mini-Camp, it will make our lives&nbsp;<em>much</em>&nbsp;better.&rdquo;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \">--Ben Hoffman /&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/Benquo\">Benquo</a></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \">&ldquo;I really can't recommend this camp enough! This workshop broke down a complex and intertwined set of skills labelled in my brain as \"common sense\" and distinguished each part so that I could work on them separately. Sessions on motivation, cognition, and what habits to build to not fool yourself were particularly helpful. This camp was also the first example that I've seen of people taking current cognitive science and other research, decoding it, and showing people what's been documented to work so that they can use it too. It feels to me now as though the coolest parts of the sequences have been given specific exercises and habits to build off of. This camp, and the people in it, have changed my path for the better.&rdquo;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \">--David Jones /&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/TheDave\">TheDave</a></p>\n<p>You can also read the&nbsp;<a href=\"https://docs.google.com/spreadsheet/pub?key=0AnoM_ZsIBBwEdGNicUMzRkNJNzRKLVpEb2RxZzU3V0E&amp;single=true&amp;gid=0&amp;output=html\">full testimonials from everyone who chose to give one</a>.</p>\n<p><big><strong><a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dEctaFJONTk1UVdfdE9sSEpiQTFLLWc6MA\">Apply now</a></strong></big></p>\n<p>(You can totally fill out the application in just 10 minutes, so you might want to fill in the blanks <em>right now</em> -- we'd like to announce the first acceptances (for May) in the next week)</p>\n<hr />\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\">[1] More exactly, we provide a bed in a shared room at a house or retreat center rented by SIAI.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\">[2] Sometimes people say they&rsquo;re &ldquo;afraid of wasting our time&rdquo; by sending in an application. &nbsp;In a word, no. &nbsp;If you&rsquo;re interested in us, we&rsquo;re interested in you. &nbsp;It takes just seconds to read someone&rsquo;s form, and our experience shows that many of our highest-value people have been the ones who hesitated to apply.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\">[3] Okay, fine, this isn&rsquo;t really a frequently asked question. &nbsp;But seriously, we&rsquo;ll be covering a lot that isn&rsquo;t in the sequences -- and the flesh-and-blood experience of meeting other aspiring rationalists is hard to duplicate.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><strong>ETA: &nbsp;CMR is still looking for good teachers and curriculum designers. &nbsp;If you're interested, please especially consider coming to a minicamp; we're hoping to find some good hires there.</strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><strong>ETA2: &nbsp;We will probably have answers to all applicants within about two weeks (i.e., by April 16 or so), with answers to the May folks probably earlier than the others. &nbsp;<em>If for some reason you need your application processed *faster* than this, please shoot me an email</em>: annasalamon at gmail.</strong></p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DQHWBcKeiLnyh9za9": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fkhbBE2ZTSytvsy9x", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 38, "extendedScore": null, "score": 8.745290561340615e-07, "legacy": true, "legacyId": "14588", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p class=\"p1\"><em>\u201c</em><em>I do not say this lightly... but if you're looking for superpowers, this is the place to start.\u201d</em></p>\n<p class=\"p1\"><em>--Michael Curzi, summer 2011 minicamp participant</em></p>\n<p><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; \">Who:</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; \">&nbsp;</span><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; \">You and a class full of other aspiring rationalists and world-optimizers, from around the world.</span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; \"><em>What:</em>&nbsp;<strong>Two 3-day weekend minicamps</strong> and <strong>one 8-day minicamp</strong>, filled with hands-on activities for applying rationality to your life, your goals, and the making of a better world. &nbsp;(See details in the FAQ.)</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; \"><em>When and where:</em>&nbsp;We're running three camps, so that we can do this for three sets of participants: May 11-13 and June 22-24 for the 3-day camps, and July 21-28 for the eight-day camp, all in the San Francisco Bay Area.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; \"><em>Why:</em>&nbsp;Because you\u2019re a social primate, and the best way to jump into a new way of thinking, make friends, and accomplish your goals is often to spend time with other primates who are doing just that.&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; \"><em><a id=\"more\" style=\"color: #6a8a6b; text-decoration: underline; \"></a></em><em>Other reasons:</em></p>\n<ul style=\"padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; \">\n<li><span style=\"background-color: rgba(255, 255, 255, 0.917969); color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Hang out and explore the Bay Area with two dozen other people like you who are smart, interesting, and passionate about rationality</span></li>\n<li><span style=\"background-color: rgba(255, 255, 255, 0.917969); color: #222222; font-family: arial, sans-serif; font-size: 13px;\">Attend bonus sessions about style, body language, and confidence-building.</span></li>\n<li style=\"font-family: Arial, Helvetica, sans-serif; line-height: 21px;\">Get help charting out career paths; and, entirely optionally for those interested, connect with folks at the Singularity Institute about optimal philanthropy.</li>\n</ul>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; \"><em>Instructors:</em></p>\n<table border=\"0\" frame=\"void\">\n<tbody>\n<tr>\n<td align=\"center\"><img style=\"vertical-align: text-bottom; margin-left: 5px; margin-right: 5px;\" src=\"http://images.lesswrong.com/t3_b98_3.png\" alt=\"\" width=\"127\" height=\"160\"></td>\n<td align=\"center\"><img style=\"vertical-align: text-bottom; margin-left: 5px; margin-right: 5px;\" src=\"http://images.lesswrong.com/t3_b98_2.png\" alt=\"\" width=\"135\" height=\"160\"></td>\n<td align=\"center\"><img style=\"margin-left: 5px; margin-right: 5px; vertical-align: text-bottom;\" src=\"http://4.bp.blogspot.com/_kQZxysNQd6c/S7nN98VxLyI/AAAAAAAAAc0/Zt72L0uzN8k/S220/Snapshot+2010-04-05+01-18-59.jpg\" alt=\"\" width=\"135\" height=\"160\"></td>\n</tr>\n<tr>\n<td align=\"center\"><a href=\"http://yudkowsky.net\">Eliezer Yudkowsky</a></td>\n<td align=\"center\"><a href=\"http://annasalamon.com/\">Anna Salamon</a></td>\n<td align=\"center\"><a href=\"http://measureofdoubt.com/\">Julia Galef</a></td>\n</tr>\n<tr>\n<td align=\"center\"><img style=\"vertical-align: text-bottom; margin-left: 5px; margin-right: 5px;\" src=\"http://images.lesswrong.com/t3_b98_0.png\" alt=\"\" width=\"135\" height=\"160\"></td>\n<td align=\"center\"><img style=\"vertical-align: text-bottom; margin-left: 5px; margin-right: 5px;\" src=\"http://images.lesswrong.com/t3_b98_4.png?v=90266b244d15d2085ea71c485e4f2f65\" alt=\"\" width=\"135\" height=\"160\"></td>\n<td align=\"center\"><img style=\"vertical-align: text-bottom; margin-left: 5px; margin-right: 5px;\" src=\"https://sites.google.com/site/placetoholdthingsforvalentine/home/Val2.JPG\" alt=\"\" width=\"135\" height=\"160\"></td>\n</tr>\n<tr>\n<td align=\"center\"><a href=\"http://math.berkeley.edu/~critch/mphd/\">Andrew Critch</a></td>\n<td align=\"center\"><a href=\"http://lukeprog.com/\">Luke Muehlhauser</a></td>\n<td align=\"center\">Michael Smith</td>\n</tr>\n</tbody>\n</table>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><em>Cost:</em>&nbsp; $650 for the three-day programs; $1500 for the week-long program. &nbsp;This includes lodging[1], meals, and tuition. &nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\">(Note that this *still* isn't quite enough to make running minicamps sustainable in the long-run; a lodging + meals at retreat centers start at around $90 per person per night, the \"three-day camps\" include four nights, and these workshops take a staff of about 5 full-time people for over a month each prior to each workshop, most of us at $3k/month, counting curriculum development time (plus miscellaneous expenses). &nbsp;<span style=\"line-height: 21px;\">We are trying to strike a compromise between \"charge enough that we can run more camps\" and staying affordable, especially for our start-up phase; costs will probably go up in following years.)</span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><span style=\"line-height: 21px;\">Three days (or a week) isn\u2019t long enough to learn rationality, but it's long enough to learn how to learn rationality, and to get some momentum toward doing so.</span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\">Come meet us, and see what you can do.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><a style=\"color: #8a8a8b;\" href=\"https://docs.google.com/spreadsheet/viewform?formkey=dEctaFJONTk1UVdfdE9sSEpiQTFLLWc6MA\"><strong>Apply now.</strong></a></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><strong id=\"Frequently_Asked_Questions_\"><a id=\"more\"></a>Frequently Asked Questions:</strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><em>1. &nbsp;I\u2019m older. &nbsp;Should I still apply?</em></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; padding-left: 30px;\">Yes! &nbsp;We're aiming for a more diverse crowd and would love to add your wider set of experiences and skills.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><em>2. &nbsp;I\u2019d like to come, but I\u2019m not sure you\u2019ll accept me. &nbsp;Should I still apply?</em></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; padding-left: 30px;\">Absolutely! &nbsp; You can fill out our form in as little 10 minutes.&nbsp; What\u2019s the harm?[2]</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><em>3. &nbsp;I\u2019d like to come, but I can\u2019t afford it. &nbsp;Should I still apply?</em></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; padding-left: 30px;\">Yes, you should definitely apply. &nbsp;A limited number of scholarships will probably be available this time, and more may be available later.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; padding-left: 30px;\">(There's also an option on the application form if you want to apply but can't make any of the times - this just says that you want to be part of future minicamps and makes sure we have your application details.)</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><em>4. &nbsp;</em><em style=\"line-height: 21px;\">What will we do, exactly?</em></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; padding-left: 30px;\">We're still working out the details. &nbsp;In our current model:</p>\n<ul style=\"font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; padding: 0px;\">\n<li><strong>Daily schedule:</strong>&nbsp;Every day, you'll have five hours of core workshop sessions (mostly exercises, divided into morning and evening sessions), meals shared with other participants, and shared activities such as soccer, poker, karaoke, and trips to bay area sites.</li>\n<li><strong>Rationality:</strong>&nbsp;You'll practice many specific techniques (e.g. Fermi calculations, applying Bayes' theorem and cognitive biases to daily life, seeing how using fungibility can boost your goal achievement);&nbsp;develop a map of your rationality strengths and gaps;&nbsp;and learn how to continue learning rationality after the program.&nbsp;</li>\n<li><strong>Social effectiveness:</strong>&nbsp; Reading and using body language; developing a fashion sense; improving social courage; and understanding why social reality is important.</li>\n<li><strong style=\"line-height: 21px;\">Individual meetings:</strong><span style=\"line-height: 21px;\">&nbsp; You'll be able to schedule one-on-one appointments to discuss career paths you may want to take (we can help with statistics on earnings in different professions, and strategy for getting in); how to start a LW meet-up or similar community;&nbsp;and, optionally for those interested, how to get involved in existential risks-reducing research and action.</span></li>\n</ul>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><em style=\"line-height: 21px;\">5. &nbsp;I\u2019m new to all this. &nbsp;Will it make sense?</em></p>\n<div>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; padding-left: 30px;\">If you\u2019ve read at least fifteen posts from&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\">the</a>&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Reductionism_(sequence)\">core</a>&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\">sequences</a>, yes it will. &nbsp;If you haven\u2019t: why not read them now?</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; padding-left: 30px;\">We\u2019ll also aim for an atmosphere in which everyone is free to make mistakes and to try things, and in which people are receptive to a wide range of skill levels.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><em>6. &nbsp;I\u2019ve already read the Sequences seventeen times, and also I\u2019m a self-made billionaire with three PhDs. &nbsp;Will I learn anything new?</em><em>[3]</em></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify; padding-left: 30px;\">We hope so. &nbsp;We\u2019re covering a good range of material, with much more of a focus on &nbsp;<em>practice</em>&nbsp; and &nbsp;<em>exercise</em>&nbsp; than in the Sequences, incorporating new lessons learned since the LW material was written, and with some instructors who've developed their own takes on rationality.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><em>7. &nbsp;What evidence is there that I'll be glad I went?</em></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\">After last year's minicamp, participants completed an anonymous exit survey. &nbsp;(With the instructions: \"We're asking you these questions to learn how to run camps; please be honest; it'll help us more if you're accurate than if you're positive.\") &nbsp;Here are their answers to the most relevant questions:</p>\n<ul style=\"line-height: 21px; padding: 0px;\">\n<li>In answer to \u201cZero to ten, are you glad you came?\u201d, the median answer was 10 (mean was 9.3).</li>\n<li>In answer to \u201cZero to ten, will your life go significantly differently because you came to mini-camp?\u201d the median answer was 7.5&nbsp;(the mean was 6.9) [This was the response that was most positively surprising to me.].</li>\n<li>In answer to \u201cZero to ten, has your epistemic rationality improved?\u201d, the median answer was 7 (mean 6.9).</li>\n<li>In answer to \u201cZero to ten, are you more motivated to learn epistemic rationality, than you were when you came?\u201d, the median answer was 8.5 (mean 8.1).</li>\n<li>In answer to \u201cZero to ten, have you become more skilled at modifying your emotions and dispositions?\u201d, the median answer was 7 (mean 6.3).</li>\n<li>In answer to \u201cZero to ten, are you more motivated to modify your emotions and dispositions, than you were when you came?\u201d, the median answer was 9 (mean 8.3).</li>\n<li><span style=\"line-height: 21px;\">In answer to \u201cZero to ten, have you gained social skills since coming?\u201d, the median answer was 7.5 (mean 7.2).</span></li>\n<li><span style=\"line-height: 21px;\">In answer to \"Zero to ten, did you like spending time with the other participants?\", the median answer was 9 (mean 8.8).</span></li>\n</ul>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\">We also asked participants for testimonials -- statements designed to be shown to others, in case they wanted to recommend such camps. &nbsp;They wrote:</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \"><span style=\"line-height: 21px;\">\u201cThis was an intensely positive experience. This was easily the most powerful change self-modification I've ever made, in all of the social, intellectual, and emotional spheres. I'm now a more powerful person than I was a week ago -- and I can explain exactly how and why this is true.</span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \">At mini-camp, I've learned techniques for effective self-modification -- that is, I have a much deeper understanding of how to change my desires, gather my willpower, channel my time and cognitive resources, and model and handle previously confusing situations. What's more, I have a fairly clear map of how to build these skills henceforth, and how to inculcate them in others. And all this was presented in such a way that any sufficiently analytical folk -- anyone who has understood a few of the LW sequences, say -- can gain in extreme measures.\u201d</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \">--Matt Elder /&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/Fiddlemath\">Fiddlemath</a></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \">\u201cI expected a week of interesting things and some useful tools to take away. What I got was 8 days of constant, deep learning, challenges to my limits that helped me grow. I finally grokked that I can and should optimize myself on every dimension I care about, that practice and reinforcement can make me a better thinker, and that I can change very quickly when I'm not constrained by artificial barriers or stress.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \">I would not recommend doing something like this right before another super-busy week, because I was learning at 100% of capacity and will need a lot of time to unpack all the things I learned and apply them to my life, but I came away with a clear plan for becoming better. It is now a normal and easy thing for me to try things out, test my beliefs, and self-improve. And I'm likely to be much more effective at making the world a better place as well, by prioritizing without fear.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \">The material was all soundly-researched and effectively taught, with extremely helpful supplemental exercises and activities. The instructors were very helpful in and out of session. The other participants were excited, engaged, challenging, and supportive.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \">I look forward to sharing what I've learned with my local Lesswrong meetup and others in the area. If that's even 1/4 as awesome as my time at the Mini-Camp, it will make our lives&nbsp;<em>much</em>&nbsp;better.\u201d</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \">--Ben Hoffman /&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/Benquo\">Benquo</a></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \">\u201cI really can't recommend this camp enough! This workshop broke down a complex and intertwined set of skills labelled in my brain as \"common sense\" and distinguished each part so that I could work on them separately. Sessions on motivation, cognition, and what habits to build to not fool yourself were particularly helpful. This camp was also the first example that I've seen of people taking current cognitive science and other research, decoding it, and showing people what's been documented to work so that they can use it too. It feels to me now as though the coolest parts of the sequences have been given specific exercises and habits to build off of. This camp, and the people in it, have changed my path for the better.\u201d</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; line-height: 21px; background-color: #f7f7f8; padding-left: 30px; \">--David Jones /&nbsp;<a style=\"color: #8a8a8b;\" href=\"/user/TheDave\">TheDave</a></p>\n<p>You can also read the&nbsp;<a href=\"https://docs.google.com/spreadsheet/pub?key=0AnoM_ZsIBBwEdGNicUMzRkNJNzRKLVpEb2RxZzU3V0E&amp;single=true&amp;gid=0&amp;output=html\">full testimonials from everyone who chose to give one</a>.</p>\n<p><big><strong><a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dEctaFJONTk1UVdfdE9sSEpiQTFLLWc6MA\">Apply now</a></strong></big></p>\n<p>(You can totally fill out the application in just 10 minutes, so you might want to fill in the blanks <em>right now</em> -- we'd like to announce the first acceptances (for May) in the next week)</p>\n<hr>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\">[1] More exactly, we provide a bed in a shared room at a house or retreat center rented by SIAI.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\">[2] Sometimes people say they\u2019re \u201cafraid of wasting our time\u201d by sending in an application. &nbsp;In a word, no. &nbsp;If you\u2019re interested in us, we\u2019re interested in you. &nbsp;It takes just seconds to read someone\u2019s form, and our experience shows that many of our highest-value people have been the ones who hesitated to apply.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\">[3] Okay, fine, this isn\u2019t really a frequently asked question. &nbsp;But seriously, we\u2019ll be covering a lot that isn\u2019t in the sequences -- and the flesh-and-blood experience of meeting other aspiring rationalists is hard to duplicate.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><strong id=\"ETA___CMR_is_still_looking_for_good_teachers_and_curriculum_designers___If_you_re_interested__please_especially_consider_coming_to_a_minicamp__we_re_hoping_to_find_some_good_hires_there_\">ETA: &nbsp;CMR is still looking for good teachers and curriculum designers. &nbsp;If you're interested, please especially consider coming to a minicamp; we're hoping to find some good hires there.</strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 21px; text-align: justify;\"><strong id=\"ETA2___We_will_probably_have_answers_to_all_applicants_within_about_two_weeks__i_e___by_April_16_or_so___with_answers_to_the_May_folks_probably_earlier_than_the_others___If_for_some_reason_you_need_your_application_processed__faster__than_this__please_shoot_me_an_email__annasalamon_at_gmail_\">ETA2: &nbsp;We will probably have answers to all applicants within about two weeks (i.e., by April 16 or so), with answers to the May folks probably earlier than the others. &nbsp;<em>If for some reason you need your application processed *faster* than this, please shoot me an email</em>: annasalamon at gmail.</strong></p>\n</div>", "sections": [{"title": "Frequently Asked Questions:", "anchor": "Frequently_Asked_Questions_", "level": 1}, {"title": "ETA: \u00a0CMR is still looking for good teachers and curriculum designers. \u00a0If you're interested, please especially consider coming to a minicamp; we're hoping to find some good hires there.", "anchor": "ETA___CMR_is_still_looking_for_good_teachers_and_curriculum_designers___If_you_re_interested__please_especially_consider_coming_to_a_minicamp__we_re_hoping_to_find_some_good_hires_there_", "level": 1}, {"title": "ETA2: \u00a0We will probably have answers to all applicants within about two weeks (i.e., by April 16 or so), with answers to the May folks probably earlier than the others. \u00a0If for some reason you need your application processed *faster* than this, please shoot me an email: annasalamon at gmail.", "anchor": "ETA2___We_will_probably_have_answers_to_all_applicants_within_about_two_weeks__i_e___by_April_16_or_so___with_answers_to_the_May_folks_probably_earlier_than_the_others___If_for_some_reason_you_need_your_application_processed__faster__than_this__please_shoot_me_an_email__annasalamon_at_gmail_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "238 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 243, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-29T23:17:40.608Z", "modifiedAt": null, "url": null, "title": "Doing \"Nothing\"", "slug": "doing-nothing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:53.480Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Voltairina", "createdAt": "2012-02-24T04:00:28.314Z", "isAdmin": false, "displayName": "Voltairina"}, "userId": "a6hK33SK4uawjaL9h", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yHKJGird3HJHYevMu/doing-nothing", "pageUrlRelative": "/posts/yHKJGird3HJHYevMu/doing-nothing", "linkUrl": "https://www.lesswrong.com/posts/yHKJGird3HJHYevMu/doing-nothing", "postedAtFormatted": "Thursday, March 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Doing%20%22Nothing%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoing%20%22Nothing%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyHKJGird3HJHYevMu%2Fdoing-nothing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Doing%20%22Nothing%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyHKJGird3HJHYevMu%2Fdoing-nothing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyHKJGird3HJHYevMu%2Fdoing-nothing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 309, "htmlBody": "<p>It might be a useful habit to remember, whenever you're making a choice about some situation, that \"doing nothing\" is never actually an available option. Even if you avoid doing the task you're considering, you're still making some kind of choice about how you spend your time, and you're still doing something relative to that task. For example, if the task is \"paint the barn\" the alternative is \"leave the bare barn exposed to the elements\", not \"store the barn in some impermeable stasis field and return to paint it later\". Being able to clearly articulate what that \"nothing\" slot entails, its consequences and rewards, might be a helpful way to motivate yourself to make better choices.</p>\n<p>I am working on internalising this, because if I don't think about it, a part of me tends to just think that I'm doing the equivalent of sticking the task in an atemporal stasis field instead of leaving it unattended. If I don't exercise, I don't stay \"the same amount fit\". I get <em>weaker </em>(or, as&nbsp;<span class=\"comment-author\"><strong><span class=\"author\"><a id=\"author_t1_66qo\" href=\"/user/aelephant\">aelephant</a></span></strong></span> points out, I could be getting stronger, during a recovery period - in which case \"doing nothing\" (as far as exercise) is the <em>better</em> option, after evaluation) . If I don't study, I don't stay \"the same amount knowledgeable\". <em>I forget</em>. Sure, there are things which remain effectively \"in stasis\" - Olympus Mons will probably stay about the same whether I climb it in ten years (somehow) or a hundred years - but I won't be the same by then. Or things that are so transient and commonplace that they might as well be in stasis - If I'm thinking of going somewhere, I might think, \"I might miss catching this taxi cab, but I miss cabs all the time, there are always more cabs, and I can catch another one\". But subjectively static opportunities are rare.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dqx5k65wjFfaiJ9sQ": 1, "KoXbd2HmbdRfqLngk": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yHKJGird3HJHYevMu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 40, "extendedScore": null, "score": 8.3e-05, "legacy": true, "legacyId": "14638", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-30T00:18:24.379Z", "modifiedAt": null, "url": null, "title": "Australian Green Party leader Bob Brown talks about global democracy, X-risk, and immortality", "slug": "australian-green-party-leader-bob-brown-talks-about-global", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:36.996Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Broggly", "createdAt": "2010-10-25T09:59:27.695Z", "isAdmin": false, "displayName": "Broggly"}, "userId": "7sD6mwQNz4cZFuaEN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vQ2GCTBRYu4hRBL6g/australian-green-party-leader-bob-brown-talks-about-global", "pageUrlRelative": "/posts/vQ2GCTBRYu4hRBL6g/australian-green-party-leader-bob-brown-talks-about-global", "linkUrl": "https://www.lesswrong.com/posts/vQ2GCTBRYu4hRBL6g/australian-green-party-leader-bob-brown-talks-about-global", "postedAtFormatted": "Friday, March 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Australian%20Green%20Party%20leader%20Bob%20Brown%20talks%20about%20global%20democracy%2C%20X-risk%2C%20and%20immortality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAustralian%20Green%20Party%20leader%20Bob%20Brown%20talks%20about%20global%20democracy%2C%20X-risk%2C%20and%20immortality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvQ2GCTBRYu4hRBL6g%2Faustralian-green-party-leader-bob-brown-talks-about-global%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Australian%20Green%20Party%20leader%20Bob%20Brown%20talks%20about%20global%20democracy%2C%20X-risk%2C%20and%20immortality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvQ2GCTBRYu4hRBL6g%2Faustralian-green-party-leader-bob-brown-talks-about-global", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvQ2GCTBRYu4hRBL6g%2Faustralian-green-party-leader-bob-brown-talks-about-global", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 279, "htmlBody": "<p><a href=\"http://greensmps.org.au/content/news-stories/bob-brown-delivers-3rd-annual-green-oration\">It's interesting to see someone with actual political power (with balance of power in the Senate and being part of the Lower House coalition) talking about the Great Filter.</a></p>\n<blockquote>\n<p>However, recent astronomy tells us that there are trillions of other  planets circling Sunlike stars in the immensity of the Universe,  millions of them friendly to life. So why has no one from elsewhere in  the Cosmos contacted us?</p>\n<p>Surely some people-like animals have evolved elsewhere. Surely we are  not, in this crowded reality of countless other similar planets, the  only thinking beings to have turned up. Most unlikely! So why isn't life  out there contacting us? Why aren't the intergalactic phones ringing?</p>\n<p>Here is one sobering possibility for our isolation: maybe life has  often evolved to intelligence on other planets with biospheres and every  time that intelligence, when it became able to alter its environment,  did so with catastrophic consequences. Maybe we have had many  predecessors in the Cosmos but all have brought about their own  downfall.</p>\n<p>That's why they are not communicating with Earth. They have extincted  themselves. They have come and gone. And now it's our turn.</p>\n</blockquote>\n<blockquote>\n<p>An Earth parliament for all. But what would be its commission? Here are four goals:</p>\n<p>Economy.</p>\n<p>Equality.</p>\n<p>Ecology.</p>\n<p>Eternity.</p>\n<p>... Eternity is for as long as we could be. It means  beyond our own experience. It also means 'forever', if there is no  inevitable end to life. Let's take the idea of eternity and make it our  own business.</p>\n<p>...The pursuit of eternity is no longer the prerogative of the gods: it is the business of us all, here and now.</p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vQ2GCTBRYu4hRBL6g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 24, "extendedScore": null, "score": 6.3e-05, "legacy": true, "legacyId": "14639", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-30T01:08:43.154Z", "modifiedAt": null, "url": null, "title": "New front page", "slug": "new-front-page", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:31.063Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vQTGuuiDt28t9GZqc/new-front-page", "pageUrlRelative": "/posts/vQTGuuiDt28t9GZqc/new-front-page", "linkUrl": "https://www.lesswrong.com/posts/vQTGuuiDt28t9GZqc/new-front-page", "postedAtFormatted": "Friday, March 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20front%20page&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20front%20page%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvQTGuuiDt28t9GZqc%2Fnew-front-page%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20front%20page%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvQTGuuiDt28t9GZqc%2Fnew-front-page", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvQTGuuiDt28t9GZqc%2Fnew-front-page", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 8, "htmlBody": "<p>The new front page <a href=\"/\">is up</a>.</p>\n<p>Previous discussion <a href=\"/lw/at9/what_if_the_front_page/\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vQTGuuiDt28t9GZqc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 19, "extendedScore": null, "score": 8.746367697127868e-07, "legacy": true, "legacyId": "14648", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["G4W6erJW6vPdzdLSp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-30T04:35:26.443Z", "modifiedAt": null, "url": null, "title": "Meetup : Monday Madison Meetup", "slug": "meetup-monday-madison-meetup-4", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hikJSjWPD3yTS2vki/meetup-monday-madison-meetup-4", "pageUrlRelative": "/posts/hikJSjWPD3yTS2vki/meetup-monday-madison-meetup-4", "linkUrl": "https://www.lesswrong.com/posts/hikJSjWPD3yTS2vki/meetup-monday-madison-meetup-4", "postedAtFormatted": "Friday, March 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Monday%20Madison%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Monday%20Madison%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhikJSjWPD3yTS2vki%2Fmeetup-monday-madison-meetup-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Monday%20Madison%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhikJSjWPD3yTS2vki%2Fmeetup-monday-madison-meetup-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhikJSjWPD3yTS2vki%2Fmeetup-monday-madison-meetup-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 123, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8e'>Monday Madison Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 April 2012 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1831 Monroe St., Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Two suggestions, both which we can do on Monday:</p>\n\n<ol>\n<li><p>I'm pretty sure there are major life hacks that folks are using that we didn't get around to describing. Email the list, or email me, or mention at the meetup that you'd like to talk about one, and I'll make sure that you actually get a chance to describe it. ;)</p></li>\n<li><p>Let's play Zendo! (<a href=\"http://www.koryheath.com/games/zendo/\" rel=\"nofollow\">http://www.koryheath.com/games/zendo/</a>) Because it's an awesome game, and because it teaches relinquishment of false hypotheses better than anything else I know.</p></li>\n</ol>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8e'>Monday Madison Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hikJSjWPD3yTS2vki", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.747224556500253e-07, "legacy": true, "legacyId": "14658", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Monday_Madison_Meetup\">Discussion article for the meetup : <a href=\"/meetups/8e\">Monday Madison Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 April 2012 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1831 Monroe St., Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Two suggestions, both which we can do on Monday:</p>\n\n<ol>\n<li><p>I'm pretty sure there are major life hacks that folks are using that we didn't get around to describing. Email the list, or email me, or mention at the meetup that you'd like to talk about one, and I'll make sure that you actually get a chance to describe it. ;)</p></li>\n<li><p>Let's play Zendo! (<a href=\"http://www.koryheath.com/games/zendo/\" rel=\"nofollow\">http://www.koryheath.com/games/zendo/</a>) Because it's an awesome game, and because it teaches relinquishment of false hypotheses better than anything else I know.</p></li>\n</ol>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Monday_Madison_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/8e\">Monday Madison Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Monday Madison Meetup", "anchor": "Discussion_article_for_the_meetup___Monday_Madison_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Monday Madison Meetup", "anchor": "Discussion_article_for_the_meetup___Monday_Madison_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-30T04:39:57.466Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Configurations and Amplitude", "slug": "seq-rerun-configurations-and-amplitude", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:38.739Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qhWtA4Gi5NByyiFXm/seq-rerun-configurations-and-amplitude", "pageUrlRelative": "/posts/qhWtA4Gi5NByyiFXm/seq-rerun-configurations-and-amplitude", "linkUrl": "https://www.lesswrong.com/posts/qhWtA4Gi5NByyiFXm/seq-rerun-configurations-and-amplitude", "postedAtFormatted": "Friday, March 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Configurations%20and%20Amplitude&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Configurations%20and%20Amplitude%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqhWtA4Gi5NByyiFXm%2Fseq-rerun-configurations-and-amplitude%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Configurations%20and%20Amplitude%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqhWtA4Gi5NByyiFXm%2Fseq-rerun-configurations-and-amplitude", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqhWtA4Gi5NByyiFXm%2Fseq-rerun-configurations-and-amplitude", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<p>Today's post, <a href=\"/lw/pd/configurations_and_amplitude/\">Configurations and Amplitude</a> was originally published on 10 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A preliminary glimpse at the stuff reality is made of. The classic split-photon experiment with half-silvered mirrors. Alternative pathways the photon can take, can cancel each other out. The mysterious measuring tool that tells us the relative squared moduli.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/b9h/seq_rerun_quantum_explanations/\">Quantum Explanations</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qhWtA4Gi5NByyiFXm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.747243281141377e-07, "legacy": true, "legacyId": "14659", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5vZD32EynD9n94dhr", "WwJMN5rMYtg93XE4v", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-30T11:28:40.050Z", "modifiedAt": null, "url": null, "title": "Examine your assumptions", "slug": "examine-your-assumptions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:58.815Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kyZgEKzZZtJQTCSG2/examine-your-assumptions", "pageUrlRelative": "/posts/kyZgEKzZZtJQTCSG2/examine-your-assumptions", "linkUrl": "https://www.lesswrong.com/posts/kyZgEKzZZtJQTCSG2/examine-your-assumptions", "postedAtFormatted": "Friday, March 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Examine%20your%20assumptions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExamine%20your%20assumptions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkyZgEKzZZtJQTCSG2%2Fexamine-your-assumptions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Examine%20your%20assumptions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkyZgEKzZZtJQTCSG2%2Fexamine-your-assumptions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkyZgEKzZZtJQTCSG2%2Fexamine-your-assumptions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 467, "htmlBody": "<p>There's a story you've probably heard:</p>\n<p style=\"padding-left: 60px;\">During World War II, the British RAF's&nbsp;Bomber Command wanted a survey done on the effectiveness of their aircraft armouring. &nbsp;This was carried out by&nbsp;inspected all bombers returning from bombing raids over Germany over a particular period. All damage inflicted by German air defences was noted and the recommendation was given that armour be added in the most heavily damaged areas.</p>\n<p style=\"padding-left: 60px;\"><img src=\"http://digitalroam.typepad.com/photos/uncategorized/walds_planes.jpg\" alt=\"\" width=\"320\" height=\"218\" /></p>\n<p style=\"padding-left: 60px;\">However a new group, run by Patrick Blackett, the Operational Research Section, analysed the survey report, and came to a different conclusion. &nbsp; Blackett suggested that, instead,&nbsp;the armour be placed in the areas which were completely untouched by damage in the bombers which returned. &nbsp; He reasoned that the survey was biased, since it only included aircraft that returned to Britain. The untouched areas of returning aircraft were probably vital areas, which, if hit, would result in the loss of the aircraft.</p>\n<p>&nbsp;</p>\n<p>It is a useful fable, but in the context presented it seemed unlikely, given the attitudes of Bomber Harris. &nbsp;So I went looking for further information, and found the story of BC-ORS written by Freeman Dyson:</p>\n<p>\"A Failure of Intelligence\" (<a href=\"http://www.technologyreview.com/printer_friendly_article.aspx?id=17724\">Part 1</a>) (<a href=\"http://www.technologyreview.com/printer_friendly_article.aspx?id=17847\">Part 2</a>)</p>\n<p>which is a great read, but fails to mention any such incident.</p>\n<p>What I did find, however, on further searching, was the work of&nbsp;Abraham Wald. &nbsp; Wald was a Jewish mathematician from Romania who in 1943 published a series of 8 memoranda via the Statistical Research Group at Columbia University while working for the National Defense Research Committee in America. &nbsp;These were republished collectively in 1980 as \"<span style=\"font-family: 'Times New Roman'; font-size: medium;\">'A Method of Estimating Plane Vulnerability Based on Damage of Survivors.</span>\" by the Center for Naval Analyses, and are still in use today.</p>\n<p>In 1984 Mangel and Samaniego published a fairly accessible summary of Wald's work in the Journal of the American Statistical Association (Vol 79, Issue 286, June)</p>\n<p>\"<a href=\"http://people.ucsc.edu/~msmangel/Wald.pdf\">Abraham Wald's Work on Aircraft Survivability</a>\"</p>\n<p>&nbsp;</p>\n<p>So it seems that Wald is the one who should get the credit for being the first to try to compensate for the evidential problem. &nbsp;Tragically he himself died in an airplane crash, just a few years later (in 1950, aged 48).</p>\n<p>The 'bible' on this topic, Robert Ball's \"The Fundamentals of Aircraft Combat Survivability Analysis and Design\" confirms the problem is a real one, and mentioned the F-4 as an example. &nbsp;When they looked at the F-4s which survived combat, there were no holes in the narrowest part of the tail, just forward of the horizontal stabilizers. They figured out that all of the hydraulic lines for the elevators and rudder were tightly clustered in there, so that a single hit could damage all of them at once, leaving the plane uncontrollable. The solution in that case was, rather than increasing the armour, to spread the redundant lines out to reduce the chances of losing all of them to a single hit.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "ec2WRPdGJiWiYmece": 1, "MAp6Ft8b3s7kJdrQ9": 1, "5f5c37ee1b5cdee568cfb0d6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kyZgEKzZZtJQTCSG2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 47, "extendedScore": null, "score": 8.748937822876642e-07, "legacy": true, "legacyId": "14683", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-30T12:43:13.758Z", "modifiedAt": null, "url": null, "title": "Collaborative project: New rationality materials page", "slug": "collaborative-project-new-rationality-materials-page", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:54.457Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "oFp6JLn8z9uxgdPp8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hXbwei7TsajfYk67K/collaborative-project-new-rationality-materials-page", "pageUrlRelative": "/posts/hXbwei7TsajfYk67K/collaborative-project-new-rationality-materials-page", "linkUrl": "https://www.lesswrong.com/posts/hXbwei7TsajfYk67K/collaborative-project-new-rationality-materials-page", "postedAtFormatted": "Friday, March 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Collaborative%20project%3A%20New%20rationality%20materials%20page&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACollaborative%20project%3A%20New%20rationality%20materials%20page%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhXbwei7TsajfYk67K%2Fcollaborative-project-new-rationality-materials-page%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Collaborative%20project%3A%20New%20rationality%20materials%20page%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhXbwei7TsajfYk67K%2Fcollaborative-project-new-rationality-materials-page", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhXbwei7TsajfYk67K%2Fcollaborative-project-new-rationality-materials-page", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 195, "htmlBody": "<p>As you are probably aware, <a href=\"/r/discussion/lw/baw/new_front_page/\">we have a new front page</a> featuring a graphic of a brain. One of the links on the front page, \"<span class=\"rationalityMaterials\">A source of edited  <a title=\"Less Wrong meetup group resources\" href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">rationality materials</a>,\" links to the Less Wrong meetup group resources page. A number of users have <a href=\"/r/discussion/lw/baw/new_front_page/\">suggested</a> that this isn't the best page to show off to new readers, and lukeprog has <a href=\"/r/discussion/lw/baw/new_front_page/66od\">requisitioned</a> a new page </span>to replace it.</p>\n<p>I've <a href=\"/r/discussion/lw/baw/new_front_page/66sc\">volunteered</a> to create this new page, but I'd like it to be a collaborative community project.</p>\n<p>I'd like this new page to contain a few introductory paragraphs about  Less Wrong followed by an index of some of our best content. At the moment, though, this project is still in the brainstorming phase,  so this is just a tentative plan. I'd like to hear your thoughts about what the page should contain, including its content, layout, and  organization. You can help out by editing the wiki page or by leaving suggestions and feedback in the comments below. <strong>Your input is always welcome</strong>, even if it's just \"This is a terrible idea\" or \"I think this post should be in there.\"</p>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Rationality_materials\">Link to the wiki page. </a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hXbwei7TsajfYk67K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 8.749247025194314e-07, "legacy": true, "legacyId": "14684", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vQTGuuiDt28t9GZqc"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-30T16:08:01.631Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Berkeley, Budapest, Houston, Madison, Ohio/Washington DC", "slug": "weekly-lw-meetups-austin-berkeley-budapest-houston-madison", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:33.137Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kToDpemfSDqvrJpQc/weekly-lw-meetups-austin-berkeley-budapest-houston-madison", "pageUrlRelative": "/posts/kToDpemfSDqvrJpQc/weekly-lw-meetups-austin-berkeley-budapest-houston-madison", "linkUrl": "https://www.lesswrong.com/posts/kToDpemfSDqvrJpQc/weekly-lw-meetups-austin-berkeley-budapest-houston-madison", "postedAtFormatted": "Friday, March 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Berkeley%2C%20Budapest%2C%20Houston%2C%20Madison%2C%20Ohio%2FWashington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Berkeley%2C%20Budapest%2C%20Houston%2C%20Madison%2C%20Ohio%2FWashington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkToDpemfSDqvrJpQc%2Fweekly-lw-meetups-austin-berkeley-budapest-houston-madison%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Berkeley%2C%20Budapest%2C%20Houston%2C%20Madison%2C%20Ohio%2FWashington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkToDpemfSDqvrJpQc%2Fweekly-lw-meetups-austin-berkeley-budapest-houston-madison", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkToDpemfSDqvrJpQc%2Fweekly-lw-meetups-austin-berkeley-budapest-houston-madison", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 463, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/6k\">[Ohio/Washington DC] Interest in Reason Rally meetup?:&nbsp;<span class=\"date\">24 March 2012 04:14PM</span></a></li>\n<li><a href=\"/meetups/83\">Houston Meetup:&nbsp;<span class=\"date\">25 March 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/7z\">First meetup in Budapest:&nbsp;<span class=\"date\">25 March 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/87\">Brussels meetup:&nbsp;<span class=\"date\">14 April 2012 11:00AM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/7x\">Austin, TX:&nbsp;<span class=\"date\">24 March 2012 01:30PM</span></a></li>\n<li><a href=\"/meetups/85\">Monthly Bay Area meetup:&nbsp;<span class=\"date\">24 March 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/84\">Madison Monday Meetup:&nbsp;<span class=\"date\">26 March 2012 06:30PM</span></a></li>\n<li><a href=\"/meetups/81\">Small weekly Berkeley meetup:&nbsp;<span class=\"date\">28 March 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/86\">Melbourne, practical rationality:&nbsp;<span class=\"date\">06 April 2012 08:00PM</span></a></li>\n</ul>\n<ul>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>, </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>\n<p><strong>Also:</strong> Nickolai Leschov is collecting information on all LW meetups, and is working with Luke Muehlhauser to provide helpful materials and maybe even some coaching. If someone from your meetup isn't currently in touch with him, shoot him an email for great justice. More information <a href=\"/r/discussion/lw/aw5/weekly_lw_meetups_atlanta_brussels_fort_collins/6319\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kToDpemfSDqvrJpQc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.750096406990174e-07, "legacy": true, "legacyId": "14410", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-30T16:45:29.150Z", "modifiedAt": null, "url": null, "title": "Meetup : Second Budapest Meetup", "slug": "meetup-second-budapest-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:37.045Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "pGkuD3jTixcf4NWhc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BJeaaXLfcyTbGeBF7/meetup-second-budapest-meetup", "pageUrlRelative": "/posts/BJeaaXLfcyTbGeBF7/meetup-second-budapest-meetup", "linkUrl": "https://www.lesswrong.com/posts/BJeaaXLfcyTbGeBF7/meetup-second-budapest-meetup", "postedAtFormatted": "Friday, March 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Second%20Budapest%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Second%20Budapest%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBJeaaXLfcyTbGeBF7%2Fmeetup-second-budapest-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Second%20Budapest%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBJeaaXLfcyTbGeBF7%2Fmeetup-second-budapest-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBJeaaXLfcyTbGeBF7%2Fmeetup-second-budapest-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8f'>Second Budapest Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">31 March 2012 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Szent Istvan ter 4-5, Budapest</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meeting at California Coffee Company Basilica (coffee shop), Szent Istvan ter 4-5. <a href=\"http://www.californiacoffeeco.net/?page_id=50&amp;lang=en\" rel=\"nofollow\">link</a>.</p>\n\n<p>Please come and bring friends. If you have questions, contact <a href=\"http://lesswrong.com/user/katyusha\">katyusha</a>.</p>\n\n<p>BudLW mailing list: <a href=\"http://groups.google.com/group/budlesswrong\">link</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8f'>Second Budapest Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BJeaaXLfcyTbGeBF7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 8.750251779210533e-07, "legacy": true, "legacyId": "14686", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Second_Budapest_Meetup\">Discussion article for the meetup : <a href=\"/meetups/8f\">Second Budapest Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">31 March 2012 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Szent Istvan ter 4-5, Budapest</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meeting at California Coffee Company Basilica (coffee shop), Szent Istvan ter 4-5. <a href=\"http://www.californiacoffeeco.net/?page_id=50&amp;lang=en\" rel=\"nofollow\">link</a>.</p>\n\n<p>Please come and bring friends. If you have questions, contact <a href=\"http://lesswrong.com/user/katyusha\">katyusha</a>.</p>\n\n<p>BudLW mailing list: <a href=\"http://groups.google.com/group/budlesswrong\">link</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Second_Budapest_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/8f\">Second Budapest Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Second Budapest Meetup", "anchor": "Discussion_article_for_the_meetup___Second_Budapest_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Second Budapest Meetup", "anchor": "Discussion_article_for_the_meetup___Second_Budapest_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-30T17:55:25.558Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge, MA first-Sundays meetup", "slug": "meetup-cambridge-ma-first-sundays-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cMefMF8hg6XMiG9Ga/meetup-cambridge-ma-first-sundays-meetup-0", "pageUrlRelative": "/posts/cMefMF8hg6XMiG9Ga/meetup-cambridge-ma-first-sundays-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/cMefMF8hg6XMiG9Ga/meetup-cambridge-ma-first-sundays-meetup-0", "postedAtFormatted": "Friday, March 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%2C%20MA%20first-Sundays%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%2C%20MA%20first-Sundays%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcMefMF8hg6XMiG9Ga%2Fmeetup-cambridge-ma-first-sundays-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%2C%20MA%20first-Sundays%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcMefMF8hg6XMiG9Ga%2Fmeetup-cambridge-ma-first-sundays-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcMefMF8hg6XMiG9Ga%2Fmeetup-cambridge-ma-first-sundays-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 93, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8g'>Cambridge, MA first-Sundays meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 April 2012 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames St, Cambridge, MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're meeting at our usual place at MIT this Sunday, April 1st at 2pm. Since this is taking place on All Fools' Day, pranking the group is encouraged.</p>\n\n<p>Meetups happen on the first and third Sundays of every month, regardless of whether an announcement is posted. You can get email reminders of meetups, and notifications of irregularly-scheduled meetups, by joining the <a href=\"http://www.meetup.com/Cambridge-Less-Wrong-Meetup/events/55785172/\" rel=\"nofollow\">meetup.com group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8g'>Cambridge, MA first-Sundays meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cMefMF8hg6XMiG9Ga", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 2, "extendedScore": null, "score": 8.75054189224266e-07, "legacy": true, "legacyId": "14687", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA_first_Sundays_meetup\">Discussion article for the meetup : <a href=\"/meetups/8g\">Cambridge, MA first-Sundays meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 April 2012 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames St, Cambridge, MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're meeting at our usual place at MIT this Sunday, April 1st at 2pm. Since this is taking place on All Fools' Day, pranking the group is encouraged.</p>\n\n<p>Meetups happen on the first and third Sundays of every month, regardless of whether an announcement is posted. You can get email reminders of meetups, and notifications of irregularly-scheduled meetups, by joining the <a href=\"http://www.meetup.com/Cambridge-Less-Wrong-Meetup/events/55785172/\" rel=\"nofollow\">meetup.com group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA_first_Sundays_meetup1\">Discussion article for the meetup : <a href=\"/meetups/8g\">Cambridge, MA first-Sundays meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge, MA first-Sundays meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA_first_Sundays_meetup", "level": 1}, {"title": "Discussion article for the meetup : Cambridge, MA first-Sundays meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA_first_Sundays_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-30T19:55:44.917Z", "modifiedAt": null, "url": null, "title": "Pascal's mugging and Bayes", "slug": "pascal-s-mugging-and-bayes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:38.425Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dmytry", "createdAt": "2009-12-03T17:11:53.492Z", "isAdmin": false, "displayName": "Dmytry"}, "userId": "AjtmA2qtA8sdiMbru", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/S9txLY3662w3Kt8r5/pascal-s-mugging-and-bayes", "pageUrlRelative": "/posts/S9txLY3662w3Kt8r5/pascal-s-mugging-and-bayes", "linkUrl": "https://www.lesswrong.com/posts/S9txLY3662w3Kt8r5/pascal-s-mugging-and-bayes", "postedAtFormatted": "Friday, March 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pascal's%20mugging%20and%20Bayes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APascal's%20mugging%20and%20Bayes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS9txLY3662w3Kt8r5%2Fpascal-s-mugging-and-bayes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pascal's%20mugging%20and%20Bayes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS9txLY3662w3Kt8r5%2Fpascal-s-mugging-and-bayes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS9txLY3662w3Kt8r5%2Fpascal-s-mugging-and-bayes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 534, "htmlBody": "<p>Suppose that your prior probability that giving $1000 to a stranger will save precisely N beings is P(1000$ saves N beings)=f(N) , where f is some sort of probability distribution.</p>\n<p>When the stranger makes a claim that he will torture N beings unless you give him the $1000 , the probability has to be increased to</p>\n<p>P(1000$ saves N beings | asking for $1000 to save N beings) = f(N) * P(Asking for $1000 to save N beings | 1000$ saves N beings) / P(asking for $1000 to save N beings)</p>\n<p>The probability is increased by factor of P(Asking for $1000 to save N beings | 1000$ saves N beings) / P(asking for $1000 to save N beings) &lt;= 1/ P(asking for $1000 to save N beings)</p>\n<p>&nbsp;If you are attending philosophical events, and being pascal-mugged by a philosopher, the 1/P(asking for $1000 to save N beings) can be less than 100 . Being asked then only raises the probability by at most factor of 100 over your f(N). If there was only one person in the world who came up with Pascal's mugging, the factor is at most a few billions.</p>\n<p>edit: Note (it may not be very clear from the post) that if your f(N) is not small enough, not only should you be Pascal-mugged, you should also give money to random stranger when he did not even Pascal-mug you - unless the utility of the mugging is very close to 1000$.</p>\n<p>I think it is fairly clear that it is reasonable to have f(N) that decreases monotonously with N, and it has to sum to 1 which implies that it has to fall off faster than 1/N . So the f(3^^^3) is much much smaller than 1/(3^^^3) . If one is not to do that, one is not only prone to being Pascal-mugged, one should run around screaming 'take my money and please don't torture 3^^^3 beings' at random people.</p>\n<p>[Of course there is still a problem if one is to assign prior probability to N via Kolmogorov's complexity, but it seems to me that it doesn't make much sense to do so as such f won't be monotonously decreasing]</p>\n<p>Other issue is the claim of 'more than 3^^^3 beings', but any reasonable f(N) seem to eat up that sum as well.</p>\n<p>This highlight a practically important problem with use of probabilistic reasoning in decision making. A proposition may be pulled out of immensely huge space of similar propositions, which should give it appropriately small prior; but we typically don't know of the competing propositions, especially when it was transmitted from person to person, and substitute 'do we trust that person' in place of original statement. One needs to be very careful when trying to be rational and abandon intuitions, as it is very difficult to transform word problems into mathematical problems - and this operation itself relies on intuitions - and thus one could easily make a gross mistake that one's intuitions do correctly veto, providing only a very vague hint along the lines of \"anyone can make this claim\" .</p>\n<p>While typing this up I found a post that <a href=\"/lw/745/why_we_cant_take_expected_value_estimates/\">goes in greater detail on the issue</a>.</p>\n<p>(This sort of outgrew the reply I wanted to post in the other thread)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "S9txLY3662w3Kt8r5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 3, "extendedScore": null, "score": 8.751041033024556e-07, "legacy": true, "legacyId": "14688", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RdpqsQ6xbHzyckW9m"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-30T21:19:49.029Z", "modifiedAt": null, "url": null, "title": "Setting up LW meetups in unlikely places: Positive Data Point", "slug": "setting-up-lw-meetups-in-unlikely-places-positive-data-point", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:52.708Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "pGkuD3jTixcf4NWhc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4YMFSdxWSK9JgQHDv/setting-up-lw-meetups-in-unlikely-places-positive-data-point", "pageUrlRelative": "/posts/4YMFSdxWSK9JgQHDv/setting-up-lw-meetups-in-unlikely-places-positive-data-point", "linkUrl": "https://www.lesswrong.com/posts/4YMFSdxWSK9JgQHDv/setting-up-lw-meetups-in-unlikely-places-positive-data-point", "postedAtFormatted": "Friday, March 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Setting%20up%20LW%20meetups%20in%20unlikely%20places%3A%20Positive%20Data%20Point&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASetting%20up%20LW%20meetups%20in%20unlikely%20places%3A%20Positive%20Data%20Point%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4YMFSdxWSK9JgQHDv%2Fsetting-up-lw-meetups-in-unlikely-places-positive-data-point%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Setting%20up%20LW%20meetups%20in%20unlikely%20places%3A%20Positive%20Data%20Point%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4YMFSdxWSK9JgQHDv%2Fsetting-up-lw-meetups-in-unlikely-places-positive-data-point", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4YMFSdxWSK9JgQHDv%2Fsetting-up-lw-meetups-in-unlikely-places-positive-data-point", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1037, "htmlBody": "<p>Meeting fellow LessWrongians in meat space is a great opportunity to participate in interesting discussions and to make new friends. But there aren't that many places in the world (hopefully, <em>yet</em>) where regularly active meetup groups exist. Here is a story of how I realised that setting up LW meetup groups is <em>much</em> easier than I thought; and an idea of an approach to help build more LW communities in real life.</p>\n<p>When I co-organised the LW meetup group in Cambridge, there was already a group of friends irregularly discussing LW related topics. Strangely, it took us some time before we actually realised that we should announce a meetup on the LW website. Once we did that, our group exploded in numbers and we have had <em>regular</em> meetups almost every week.</p>\n<p>Of course, Cambridge, UK is a place where we <em>expected</em>&nbsp;to be successful in forming a meetup group. It is small and the concentration of usual target audience of LW is extremely high. I thought we were lucky with the location that creating a regular meetup group proved to be so easy.</p>\n<p>Then I had an idea of an experiment. I was travelling to Budapest last week for 3 days to visit my family and I thought that I would simply try to organise a meetup there. In the worst case, I would spend a couple of hours in a cafe reading a book. My guesstimate was that 3-4 of my friends (whom I reminded several times) and maybe 1-3 people I don't actually know would turn up.</p>\n<p>I was surprised to find that 14 people attended the meetup, two of them travelling all the way from Bratislava to Budapest. We spent almost 4 hours in a fantastic discussion, a mailing list was created, and a second meetup is happening tomorrow. My experiment produced a result I didn't expect.</p>\n<p>One data point is not sufficient to draw conclusions, but this result suggests that further experiments should be tried. It may just be that many cities have reached a critical number of active LessWrongians and regular meetups can start happening. Which is trivially of positive net effect.</p>\n<p>Therefore, I would encourage people to consider getting out there and trying to set up meetup groups in their areas. But since this requires individuals actually willing to assume the role of organisers, this may not be as easy as it sounds. Fortunately, there is a document currently in development that aims to provide some help with this:&nbsp;<a href=\"/r/discussion/lw/bak/draft_how_to_run_a_successful_less_wrong_meetup/\">http://lesswrong.com/r/discussion/lw/bak/draft_how_to_run_a_successful_less_wrong_meetup/</a>. But there might be something else that can be done - a pioneering approach.</p>\n<p>If you have some experience of attending/organising LW meetups, next time you are in a different city (even if only for a couple of days!) try to devote a couple of hours to organising a meetup there. Be a \"pioneer\". Here is what I learnt from the meetup in Budapest:</p>\n<p>&nbsp;</p>\n<ul>\n<li>Announce the meetup on the website well in advance.</li>\n<li>Choose a weekend evening, a public place in the centre - a cafe or equivalent, with some food and drinks works well.</li>\n<li>Bring some friends with you if there are any in the area, even if they are not very actively LessWrongian.</li>\n<li>Arrive earlier than the announced time - some people may turn up early.</li>\n<li>Get a sign saying \"Less Wrong\" and put it on the table.</li>\n<li>Get people to do some introductions first.</li>\n<li>It is possible that some people turning up might not be speaking the local language so switching to English may be necessary.</li>\n<li>People don't know what to expect from a LW meetup, so the organiser has to feel confident leading the discussion in the beginning. It probably will take off when people relax into it. Taking turns in answering some basic questions can lead to interesting discussion. Examples of questions: How did you become interested in Less Wrong? Which particular aspects of the range of LW topics you are most interested in? How does being a LessWrongian translate into your everyday life?&nbsp;</li>\n<li>If the group becomes too large, divide it into two. Optimal size is probably between 4-7. Move between groups, to encourage active discussion and participation (some groups may get stuck not knowing what to talk about). Rotate some people between groups from time to time.</li>\n<li>Circulate a piece of paper to get people's email addresses, create a mailing list and sign them up to it.</li>\n<li>Identifying one or two very active and keen members and talk to them about helping you with organising some further events.</li>\n<li>Even if you don't expect to be back in the area in the near future, help with choosing a time, a venue, discussion topics and activity ideas for the next meetup through the mailing list. Announce it on the website. Get the chosen organisers to help you and gradually let them do it.</li>\n<li>Refer the people on the mailing list to&nbsp;<a href=\"/r/discussion/lw/bak/draft_how_to_run_a_successful_less_wrong_meetup/\">http://lesswrong.com/r/discussion/lw/bak/draft_how_to_run_a_successful_less_wrong_meetup/</a>.</li>\n<li>Hope that the group lives on.</li>\n</ul>\n<div>Are you travelling any time soon? Consider setting up a LW meetup group - it's easier than I thought!&nbsp;Having done this in Budapest, and judging from it being successful, I would encourage others to try setting up meetup groups in this way too. And please provide feedback on how it went! I am going to be in Rome next (in three weeks), so I will try something similar again.</div>\n<div><br /></div>\n<div><strong>UPDATE</strong>: The second meetup was also a success. A short summary by Katya Morgunova (<a href=\"/user/katyusha\">katyusha</a>):</div>\n<div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969);\"><em>\"We met at the same caff&eacute;e. Initially, there were just four of us, later on five. We had a fascinating discussion, the starting point of which was an article previously posted to the mailing list by one of the members. The topics of the resulting talk varied from why people in Uzbekistan buy corner reflectors to what the difference between English and Dutch auction is.</em></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969);\"><em>By the way, the language was Hungarian all the way through, unlike that of the first meetup.</em></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969);\"><em>Before some of us had to leave we discussed our future meetup-related plans, and agreed that we should have random conversations at the next few occasions, then it's a good idea to commence with the rationality training.\"</em></div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"T57Qd9J3AfxmwhQtY": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4YMFSdxWSK9JgQHDv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 30, "extendedScore": null, "score": 8.751389808569857e-07, "legacy": true, "legacyId": "14690", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["T2fcyjay3GtkvGn7F"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-31T00:28:58.826Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Joint Configurations", "slug": "seq-rerun-joint-configurations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:38.406Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CBasheGcr8SWbn6Mi/seq-rerun-joint-configurations", "pageUrlRelative": "/posts/CBasheGcr8SWbn6Mi/seq-rerun-joint-configurations", "linkUrl": "https://www.lesswrong.com/posts/CBasheGcr8SWbn6Mi/seq-rerun-joint-configurations", "postedAtFormatted": "Saturday, March 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Joint%20Configurations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Joint%20Configurations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCBasheGcr8SWbn6Mi%2Fseq-rerun-joint-configurations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Joint%20Configurations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCBasheGcr8SWbn6Mi%2Fseq-rerun-joint-configurations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCBasheGcr8SWbn6Mi%2Fseq-rerun-joint-configurations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 232, "htmlBody": "<p>Today's post, <a href=\"/lw/pe/joint_configurations/\">Joint Configurations</a> was originally published on 11 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The laws of physics are inherently over mathematical entities, configurations, that involve multiple particles. A basic, ontologically existent entity, according to our current understanding of quantum mechanics, does not look like a photon - it looks like a configuration of the universe with \"A photon here, a photon there.\" Amplitude flows between these configurations can cancel or add; this gives us a way to detect which configurations are distinct. It is an experimentally testable fact that \"Photon 1 here, photon 2 there\" is the same configuration as \"Photon 2 here, photon 1 there\".</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bb7/seq_rerun_configurations_and_amplitude/\">http://lesswrong.com/lw/bb7/seq_rerun_configurations_and_amplitude/</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CBasheGcr8SWbn6Mi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 8.752174681613542e-07, "legacy": true, "legacyId": "14695", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ybusFwDqiZgQa6NCq", "qhWtA4Gi5NByyiFXm", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-31T09:18:32.819Z", "modifiedAt": null, "url": null, "title": "Learning the basics of probability & beliefs", "slug": "learning-the-basics-of-probability-and-beliefs", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:38.103Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tomme", "createdAt": "2012-03-14T19:51:35.247Z", "isAdmin": false, "displayName": "tomme"}, "userId": "uyGXufxNcB7WRQ63C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sXMbCGFC95ZoJADN8/learning-the-basics-of-probability-and-beliefs", "pageUrlRelative": "/posts/sXMbCGFC95ZoJADN8/learning-the-basics-of-probability-and-beliefs", "linkUrl": "https://www.lesswrong.com/posts/sXMbCGFC95ZoJADN8/learning-the-basics-of-probability-and-beliefs", "postedAtFormatted": "Saturday, March 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Learning%20the%20basics%20of%20probability%20%26%20beliefs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALearning%20the%20basics%20of%20probability%20%26%20beliefs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsXMbCGFC95ZoJADN8%2Flearning-the-basics-of-probability-and-beliefs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Learning%20the%20basics%20of%20probability%20%26%20beliefs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsXMbCGFC95ZoJADN8%2Flearning-the-basics-of-probability-and-beliefs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsXMbCGFC95ZoJADN8%2Flearning-the-basics-of-probability-and-beliefs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 40, "htmlBody": "<p>Let's say that I believe that the sky is green.</p>\n<p>1) How can I know whether this belief is true?</p>\n<p>2) How can I assign a probability to it to test its degree of truthfulness?</p>\n<p>3) How can I update this belief?</p>\n<p>Thank you.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sXMbCGFC95ZoJADN8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 6, "extendedScore": null, "score": 8.754372608210571e-07, "legacy": true, "legacyId": "14726", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-31T10:07:17.663Z", "modifiedAt": null, "url": null, "title": "[draft] Concepts are Difficult, and Unfriendliness is the Default: A Scary Idea Summary", "slug": "draft-concepts-are-difficult-and-unfriendliness-is-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:05.327Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Fnm8u3ScB5YTbXHku/draft-concepts-are-difficult-and-unfriendliness-is-the", "pageUrlRelative": "/posts/Fnm8u3ScB5YTbXHku/draft-concepts-are-difficult-and-unfriendliness-is-the", "linkUrl": "https://www.lesswrong.com/posts/Fnm8u3ScB5YTbXHku/draft-concepts-are-difficult-and-unfriendliness-is-the", "postedAtFormatted": "Saturday, March 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Bdraft%5D%20Concepts%20are%20Difficult%2C%20and%20Unfriendliness%20is%20the%20Default%3A%20A%20Scary%20Idea%20Summary&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Bdraft%5D%20Concepts%20are%20Difficult%2C%20and%20Unfriendliness%20is%20the%20Default%3A%20A%20Scary%20Idea%20Summary%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFnm8u3ScB5YTbXHku%2Fdraft-concepts-are-difficult-and-unfriendliness-is-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Bdraft%5D%20Concepts%20are%20Difficult%2C%20and%20Unfriendliness%20is%20the%20Default%3A%20A%20Scary%20Idea%20Summary%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFnm8u3ScB5YTbXHku%2Fdraft-concepts-are-difficult-and-unfriendliness-is-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFnm8u3ScB5YTbXHku%2Fdraft-concepts-are-difficult-and-unfriendliness-is-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 952, "htmlBody": "<p>Here's my draft document <a href=\"https://docs.google.com/document/d/1Id212TsETLp0pIehQ7eJRdHqHSjDOjf-SDxhlufPlHQ/edit\">Concepts are Difficult, and Unfriendliness is the Default</a>. (Google Docs, commenting enabled.) Despite the name, it's still informal and would need a lot more references, but it could be written up to a proper paper if people felt that the reasoning was solid.</p>\n<p>Here's my introduction:</p>\n<blockquote>\n<p><span id=\"internal-source-marker_0.5917619377522723\" style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">In the \"</span><a href=\"/r/discussion/lw/aw7/muehlhausergoertzel_dialogue_part_1\"><span style=\"font-size: 16px; font-family: Times New Roman; color: #1155cc; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">Muehlhauser-Goertzel Dialogue, Part 1</span></a><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">\", Ben Goertzel writes:</span></p>\n<p style=\"margin-left: 36pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">[Anna Salamon] gave the familiar SIAI argument that, if one picks a mind at random from &ldquo;mind space&rdquo;, the odds that it will be Friendly to humans are effectively zero.</span></p>\n<br />\n<p style=\"margin-left: 36pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">I made the familiar counter-argument that this is irrelevant, because nobody is advocating building a random mind. Rather, what some of us are suggesting is to build a mind with a Friendly-looking goal system, and a cognitive architecture that&rsquo;s roughly human-like in nature but with a non-human-like propensity to choose its actions rationally based on its goals, and then raise this AGI mind in a caring way and integrate it into society. Arguments against the Friendliness of random minds are irrelevant as critiques of this sort of suggestion.</span></p>\n<br />\n<p style=\"margin-left: 36pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">[...] Over all these years, the SIAI community maintains the Scary Idea in its collective mind, and also maintains a great devotion to the idea of rationality, but yet fails to produce anything resembling a rational argument for the Scary Idea -- instead repetitiously trotting out irrelevant statements about random minds!!</span></p>\n<br /><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Ben has a valid complaint here. Therefore, I'll attempt to formalize the arguments for the following conclusion:</span><br /><br />\n<p style=\"margin-left: 36pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Even if</span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\"> an AGI is </span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\">explicitly</span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\"> built to have a Friendly-looking goal system, </span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\">and</span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\"> a cognitive architecture that&rsquo;s roughly human-like in nature but with a non-human-like propensity to choose its actions rationally based on its goals, </span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\">and</span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\"> this AGI mind is raised in caring way in an attempt to integrate it into society, there is </span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\">still </span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">a very large chance of creating a mind that is unFriendly.</span></p>\n<br /><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">First, I'll outline my argument, and then expand upon each specific piece in detail.</span><br /><br /><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: bold; font-style: italic; font-variant: normal; vertical-align: baseline; text-decoration: underline;\">The premises in outline</span><br /><br /><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">0. There will eventually be a situation where the AGI's goals and behaviors are no longer under our control.</span><br /><br /><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">1. Whether or not the AGI will eventually come to understand what we wanted it to do is irrelevant, if that understanding does not guide its actions in &rdquo;the right way&rdquo;.</span><br /><br /><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">2. Providing an AGI with the kind of understanding that'd guide its actions in &rdquo;the right way&rdquo; requires some way of defining our intentions.</span><br /><br /><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">3. In addition to defining what counts as our intentions, we also need to define the concepts that make up those intentions.</span><br /><br /><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">4. Any difference between the way we understand concepts and the way that they are defined by the AGI is something that the AGI may exploit, with likely catastrophic results.</span><br /><br /><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">5. Common-sense concepts are complicated and allow for many degrees of freedom: fully satisfactory definitions for most concepts do not exist.</span><br /><br /><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">6. Even if an AGI seemed to learn our concepts, without human inductive biases it would most likely mislearn them.</span><br /><br /><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: bold; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">7. AGI concepts are likely to be opaque and hard to understand, making proper verification impossible.</span></blockquote>\n<p>And here's my conclusion:</p>\n<blockquote>\n<p><span id=\"internal-source-marker_0.5917619377522723\" style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Above, I have argued that an AGI will only be Friendly if its goals are the kinds of goals that we would want it to have, and it will only have the kinds of goals that we would want it to have if the concepts that it bases its goals on are sufficiently similar to the concepts that we use. Even subtle differences in the concepts will quickly lead to drastic differences &ndash; even an AGI with most of its ontology basically correct, but with a differing definition regarding the concept of &rdquo;time&rdquo;, might end up destroying humanity. I have also argued that human behavioral data severly underconstrains the actual models that could be generated about human concepts, that humans do not understand the concepts they use themselves, and that an AGI developing concepts that are subtly different from those of humans is therefore unavoidable. Furthermore, AGI concepts are themselves likely to be opaque in that they cannot simply be read off the AGI, but have to be inferred in the same way that an AGI tries to infer human concepts, so humans cannot even reliably know whether an AGI that </span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\">seems</span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\"> Friendly really </span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\">is</span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\"> Friendly. The most likely scenario is that it is not, but there is no safe way for the humans to test this.</span><br /><br /><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Presuming that one accepts this chain of reasoning, it seems like</span></p>\n<p style=\"margin-left: 36pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\">Even if</span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\"> an AGI is </span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\">explicitly </span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">built to have a Friendly-looking goal system, </span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\">and</span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\"> a cognitive architecture that&rsquo;s roughly human-like in nature but with a non-human-like propensity to choose its actions rationally based on its goals, </span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\">and</span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\"> this AGI mind is raised in caring way in an attempt to integrate it into society, there is </span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: italic; font-variant: normal; text-decoration: none; vertical-align: baseline;\">still</span><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\"> a very large chance of creating a mind that is unFriendly.</span></p>\n<br /><span style=\"font-size: 16px; font-family: Times New Roman; color: #000000; background-color: transparent; font-weight: normal; font-style: normal; font-variant: normal; text-decoration: none; vertical-align: baseline;\">would be a safe conclusion to accept.</span></blockquote>\n<p>For the actual argumentation defending the various premises, see the linked document. I have a feeling that there are still several conceptual distinctions that I should be making but am not, but I figured that the easiest way to find the problems would be to have people tell me what points they find unclear or disagreeable.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Fnm8u3ScB5YTbXHku", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 12, "extendedScore": null, "score": 8.754574979376995e-07, "legacy": true, "legacyId": "14729", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TpNRpncLBAzddBnRB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-31T14:34:02.926Z", "modifiedAt": null, "url": null, "title": "AI Risk & Opportunity: A Timeline of Early Ideas and Arguments", "slug": "ai-risk-and-opportunity-a-timeline-of-early-ideas-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:30.170Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qdq2SKyMi8vf7Snxq/ai-risk-and-opportunity-a-timeline-of-early-ideas-and", "pageUrlRelative": "/posts/Qdq2SKyMi8vf7Snxq/ai-risk-and-opportunity-a-timeline-of-early-ideas-and", "linkUrl": "https://www.lesswrong.com/posts/Qdq2SKyMi8vf7Snxq/ai-risk-and-opportunity-a-timeline-of-early-ideas-and", "postedAtFormatted": "Saturday, March 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20Risk%20%26%20Opportunity%3A%20A%20Timeline%20of%20Early%20Ideas%20and%20Arguments&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20Risk%20%26%20Opportunity%3A%20A%20Timeline%20of%20Early%20Ideas%20and%20Arguments%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQdq2SKyMi8vf7Snxq%2Fai-risk-and-opportunity-a-timeline-of-early-ideas-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20Risk%20%26%20Opportunity%3A%20A%20Timeline%20of%20Early%20Ideas%20and%20Arguments%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQdq2SKyMi8vf7Snxq%2Fai-risk-and-opportunity-a-timeline-of-early-ideas-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQdq2SKyMi8vf7Snxq%2Fai-risk-and-opportunity-a-timeline-of-early-ideas-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2588, "htmlBody": "<p><small>Part of the series <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">AI Risk and Opportunity: A Strategic Analysis</a>.</small></p>\n<p>(You can leave anonymous feedback on posts in this series <strong><a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dDZ6d0RvM19qVkduX2pjNng4ZklHZXc6MQ\">here</a></strong>. I alone will read the comments, and may use them to improve past and forthcoming posts in this series.)</p>\n<p>Building on <a href=\"/r/discussion/lw/b0v/ai_risk_and_opportunity_humanitys_efforts_so_far/\">the previous post on AI risk history</a>, this post provides an incomplete timeline (up to 1993) of significant <em>novel</em> ideas and arguments related to AI as a potential catastrophic risk. I do not include ideas and arguments concerning only, for example, the possibility of AI (<a href=\"http://www.csee.umbc.edu/courses/471/papers/turing.pdf\">Turing 1950</a>) or attempts to predict its arrival (<a href=\"/www.nickbostrom.com/superintelligence.html\">Bostrom 1998</a>).</p>\n<p>As is usually the case, we find that when we look closely at a cluster of ideas, it turns out these ideas did not appear all at once in the minds of a Few Great Men. Instead, they grew and mutated and gave birth to new ideas gradually as they passed from mind to mind over the course of many decades.</p>\n<p>&nbsp;</p>\n<p><strong>1863: Machine intelligence as an existential risk to humanity; relinquishment of machine technology recommended</strong>. Samuel Butler in <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Butler-Darwin-Among-the-Machines.pdf\">Darwin among the machines</a> worries that as we build increasingly sophisticated and autonomous machines, they will achieve greater capability than humans and replace humans as the dominant agents on the planet:</p>\n<blockquote>\n<p>...we are ourselves creating our own successors; we are daily adding to the beauty and delicacy of their physical organisation; we are daily giving them greater power and supplying by all sorts of ingenious contrivances that self-regulating, self-acting power which will be to them what intellect has been to the human race. In the course of ages we shall find ourselves the inferior race... the time will come when the machines will hold the real supremacy over the world and its inhabitants...</p>\n<p>Our opinion is that war to the death should be instantly proclaimed against them. Every machine of every sort should be destroyed by the well-wisher of his species. Let there be no exceptions made, no quarter shown...</p>\n</blockquote>\n<p>(See also <a href=\"http://www.gutenberg.org/ebooks/1906\">Butler 1872</a>; <a href=\"http://www.gutenberg.org/ebooks/27462\">Campbell 1932</a>.)</p>\n<p><strong>1921: Robots as an existential risk</strong>. The Czech play <em><a href=\"http://www.gutenberg.org/ebooks/13083\">R.U.R.</a></em> by Karel Capek tells the story of robots which grow in power and intelligence and destroy the entire human race (except for a single survivor).</p>\n<p><strong>1947: Fragility &amp; complexity of human values (in the context of machine goal systems); perverse instantiation</strong>. Jack Williamson's novelette <em><a href=\"http://www.amazon.com/Folded-Searching-Collected-Stories-Williamson/dp/1893887375/\">With Folded Hands</a></em> (1947) tells the story of a race of machines that, in order to follow the Prime Directive: \"to serve and obey and guard men from harm.\" To obey this rule, the machines interfere with every aspect of human life, and humans who resist are lobotomized. Due to the fragility and complexity of human values (<a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Yudkowsky 2008</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">Muehlhauser and Helm 2012</a>), the machines' rules of behavior had unintended consequences, manifesting a \"perverse instantiation\" in the language of Bostrom (forthcoming).</p>\n<p>(Also see <a href=\"http://www.amazon.com/Robot-Visions-Isaac-Asimov/dp/0451450647/\">Asimov 1950</a>, <a href=\"http://www.amazon.com/The-Naked-Sun-Isaac-Asimov/dp/0553293397/\">1957</a>, <a href=\"http://www.amazon.com/The-Robots-Dawn-Isaac-Asimov/dp/0553299492/\">1983</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Versenyi-Can-Robots-Be-Moral.pdf\">Versenyi 1974</a>;&nbsp;<a href=\"http://web.media.mit.edu/~minsky/papers/TrueNames.Afterword.html\">Minsky 1984</a>; <a href=\"http://intelligence.org/upload/CFAI.html\">Yudkowsky 2001</a>, <a href=\"http://intelligence.org/upload/complex-value-systems.pdf\">2011</a>.)</p>\n<p><strong>1948-1949: Precursor idea to intelligence explosion</strong>. Von Neumann (<a href=\"http://www.pensamientocomplejo.com.ar/docs/files/von%20neuman%20-%20central%20and%20logical%20theory%20of%20automata.pdf\">1948</a>) wrote:</p>\n<blockquote>\n<p><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:RelyOnVML /> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:Garamond; mso-ascii-font-family:Garamond; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Garamond; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--></p>\n<p class=\"MsoNormal\">...&ldquo;complication\" on its lower levels is probably degenerative, that is, that every automaton that can produce other automata will only be able to produce less complicated ones. There is, however, a certain minimum level where this degenerative characteristic ceases to be universal. At this point automata which can reproduce themselves, or even construct higher entities, become possible.</p>\n<!--EndFragment-->\n<p>&nbsp;</p>\n</blockquote>\n<p>Von Nuemann (1949) came very close to articulating the idea of intelligence explosion:</p>\n<blockquote>\n<p>There is thus this completely decisive property of complexity, that there exists a critical size below which the process of synthesis is degenerative, but above which the phenomenon of synthesis, if properly arranged, can become explosive, in other words, where syntheses of automata can proceed in such a manner that each automaton will produce other automata which are more complex and of higher potentialities than itself.</p>\n</blockquote>\n<p><strong>1951: Potentially rapid transition from machine intelligence to machine takeover</strong>. Turing (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Turing-Intelligent-Machinery-a-heretical-theory.pdf\">1951</a>) described ways that intelligent computers might learn and improve their capabilities, concluding that:</p>\n<blockquote>\n<p>...it seems probable that once the machine thinking method has started, it would not take long to outstrip our feeble powers... At some stage therefore we should have to expect the machines to take control...</p>\n</blockquote>\n<p><strong>1959: Intelligence explosion; the need for human-friendly goals for machine superintelligence</strong>. Good (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Speculations-on-perceptrons-and-other-automata.pdf\">1959</a>) describes what he later (<a href=\"http://www.web-e.stat.vt.edu/dept/web-e/tech_reports/TechReport05-3.pdf\">1965</a>) called an \"intelligence explosion,\" a particular mechanism for rapid transition from artificial general intelligence to dangerous machine takeover:</p>\n<blockquote>\n<p>Once a machine is designed that is good enough&hellip; it can be put to work designing an even better machine. At this point an \"explosion\" will clearly occur; all the problems of science and technology will be handed over to machines and it will no longer be necessary for people to work. Whether this will lead to a Utopia or to the extermination of the human race will depend on how the problem is handled by the machines. The important thing will be to give them the aim of serving human beings.</p>\n</blockquote>\n<p>(Also see Good <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/11/Good-The-social-implications-of-artificial-intelligence.pdf\">1962</a>,&nbsp;<a href=\"http://www.web-e.stat.vt.edu/dept/web-e/tech_reports/TechReport05-3.pdf\">1965</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Some-future-social-repurcussions-of-computers.pdf\">1970</a>; <a href=\"http://www.amazon.com/Fire-Upon-Deep-Zones-Thought/dp/0765329824/\">Vinge 1992</a>, <a href=\"http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html\">1993</a>; <a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Yudkowsky 2008</a>.)</p>\n<p><strong>1966: A military arms race for machine superintelligence could accelerate machine takeover; convergence toward a singleton is likely</strong>. Dennis Feltham Jones' 1966 novel <em><a href=\"http://www.amazon.com/Colossus-D-F-Jones/dp/0425076482/\">Colossus</a></em>&nbsp;depicted what may be a particularly likely scenario: two world superpowers (the USA and USSR) are in an arms race to develop superintelligent computers, one of which self-improves enough to take control of the planet.</p>\n<p>In the same year, <a href=\"http://www.amazon.com/Other-Worlds-Than-Cecil-Maxwell/dp/0800861256/\">Cade (1966)</a> argued the same thing:</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p style=\"padding-left: 30px;\">political leaders on Earth will slowly come to realize... that intelligent machines having superhuman thinking ability can be built. The construction of such machines, even taking into account all the latest developments in computer technology, would call for a major national effort. It is only to be expected that any nation which did put forth the financial and physical effort needed to build and programme such a machine, would also attempt to utilize it to its maximum capacity, which implies that it would be used to make major decisions of national policy. Here is where the awful dilemma arises. Any restriction to the range of data supplied to the machine would limit its ability to make effective political and economic decisions, yet if no such restrictions are placed upon the machine's command of information, then the entire control of the nation would virtually be surrendered to the judgment of the robot.</p>\n<p style=\"padding-left: 30px;\">On the other hand, any major nation which was led by a superior, unemotional intelligence of any kind, would quickly rise to a position of world domination. This by itself is sufficient to guarantee that, sooner or later, the effort to build such an intelligence will be made &mdash; if not in the Western world, then elsewhere, where people are more accustomed to iron dictatorships.</p>\n<p style=\"padding-left: 30px;\">...It seems that, in the forseeable future, the major nations of the world will have to face the alternative of surrendering national control to mechanical ministers, or being dominated by other nations which have already done this.&nbsp;Such a process will eventually lead to the domination of the whole Earth by a dictatorship of an unparalleled type &mdash; a single supreme central authority.</p>\n<p>&nbsp;</p>\n<p>(This last paragraph also argues for convergence toward what Bostrom later called a \"singleton.\")</p>\n<p>(Also see <a href=\"http://www.amazon.com/Have-No-Mouth-Harlan-Ellison/dp/0441363954/\">Ellison 1967</a>.)</p>\n<p><strong>1970: Proposal for an association that analyzes the implications of machine superintelligence; naive control solutions like \"switch off the power\" may not work because the superintelligence will outsmart us, thus we must focus on its <em>motivations</em>; possibility of \"pointless\" optimization by machine superintelligence</strong>. <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Some-future-social-repurcussions-of-computers.pdf\">Good (1970)</a> argues:</p>\n<blockquote>\n<p>Even if the chance that the ultraintelligent machine will be available [soon] is small, the repercussions would be so enormous, good or bad, that it is not too early to entertain the possibility. In any case by 1980 I hope that the implications and the safeguards will have been thoroughly discussed, and this is my main reason for airing the matter: an association for considering it should be started.</p>\n</blockquote>\n<p>(Also see <a href=\"http://www.nickbostrom.com/old/predict.html\">Bostrom 1997</a>.)</p>\n<p>On the idea that naive control solutions like \"switch off the power\" may not work because the superintelligence will find a way to outsmart us, and thus we must focus our efforts on the superintelligence's <em>motivations</em>, Good writes:</p>\n<blockquote>\n<p>Some people have suggested that in order to prevent the [ultraintelligent machine] from taking over we should be ready to switch of its power supply. But it is not as simple as that because the machine could recommend the appointment of its own operators, it could recommend that they be paid well and it could select older men who would not be worried about losing their jobs. Then it could replace its operators by robots in order to make sure that it is not switched off. Next it could have the neo-Luddites ridiculed by calling them Ludditeniks, and if necessary it would later have them imprisoned or executed. This shows how careful we must be to keep our eye on the \"motivation\" of the machines, if possible, just as we should with politicians.</p>\n</blockquote>\n<p>(Also see <a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Yudkowsky 2008</a>.)</p>\n<p>Good also outlines one possibility for \"pointless\" goal-optimization by machine superintelligence:</p>\n<blockquote>\n<p>If the machines took over and men became redundant and ultimately extinct, the society of machines would continue in a complex and interesting manner, but it would all apparently be pointless because there would be no one there to be interested. If machines cannot be conscious there would be only a zombie world. This would perhaps not be as bad as in many human societies where most people have lived in misery and degradation while a few have lived in pomp and luxury. It seems to me that the utility of such societies has been negative (while in the condition described) whereas the utility of a zombie society would be zero and hence preferable.</p>\n</blockquote>\n<p>(Also see <a href=\"http://www.nickbostrom.com/fut/evolution.html\">Bostrom 2004</a>; <a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Yudkowsky 2008</a>.)</p>\n<p><strong>1974: We can't much predict what will happen after the creation of machine superintelligence</strong>. Julius Lukasiewicz (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/09/Lukasiewicz-The-Ignorance-Explosion.pdf\">1974</a>) writes:</p>\n<blockquote>\n<p>The survival of man may depend on the early construction of an ultraintelligent machine-or the ultraintelligent machine may take over and render the human race redundant or develop another form of life. The prospect that a merely intelligent man could ever attempt to predict the impact of an ultraintelligent device is of course unlikely but the temptation to speculate seems irresistible.</p>\n</blockquote>\n<p>(Also see <a href=\"http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html\">Vinge 1993</a>.)</p>\n<p><strong>1977: Self-improving AI could stealthily take over the internet; convergent instrumental goals in AI; the treacherous turn</strong>. Though the concept of a self-propagating computer worm was introduced by John Brunner's <a href=\"http://www.amazon.com/The-Shockwave-Rider-John-Brunner/dp/0345467175\"><em>The Shockwave Rider</em></a> (1975), Thomas J. Ryan's novel <a href=\"http://www.amazon.com/Adolescence-P-1-Thomas-J-Ryan/dp/0671559702/\"><em>The Adolescence of P-1</em></a> (1977) tells the story of an intelligent worm that at first is merely able to learn to hack novel computer systems and use them to propagate itself, but later (1) has novel insights on how to improve its own intelligence, (2) develops convergent instrumental subgoals (see <a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">Bostrom 2012</a>) for self-preservation and resource acquisition, and (3) learns the ability to fake its own death so that it can grow its powers in secret and later engage in a \"treacherous turn\" (see Bostrom forthcoming) against humans.</p>\n<p><strong>1982: To design ethical machine superintelligence, we may need to design superintelligence first and then ask it to solve philosophical problems (e.g. including ethics).</strong></p>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Ethical-Machines.pdf\">Good (1982)</a> writes:</p>\n<blockquote>\n<p>Unfortunately, after 2500 years, the philosophical problems are nowhere near solution. Do we need to solve these philosophical problems before we can design an adequate ethical machine, or is there another approach? One approach that cannot be ruled out is first to produce an ultra-intelligent machine and then ask it to solve philosophical problems.</p>\n</blockquote>\n<p><strong>1988: Even though AI poses an existential threat, we may need to rush toward it so we can use it to mitigate other existential threats</strong>. Moravec (<a href=\"http://www.amazon.com/Mind-Children-Future-Robot-Intelligence/dp/0674576187/\">1988</a>, p. 100-101) writes:</p>\n<blockquote>\n<p>...intelligent machines... threaten our existence... Machines merely as clever as human beings will have enormous advantages in competitive situations... So why rush headlong into an era of intelligent machines? The answer, I believe, is that we have very little choice, if our culture is to remain viable... The universe is one random event after another. Sooner or later an unstoppable virus deadly to humans will evolve, or a major asteroid will collide with the earth, or the sun will expand, or we will be invaded from the stars, or a black hole will swallow the galaxy. The bigger, more diverse, and competent a culture is, the better it can detect and deal with external dangers. The larger events happen less frequently. By growing rapidly enough, a culture has a finite chance of surviving forever.</p>\n</blockquote>\n<p><strong>1993: Physical confinement is unlikely to constrain superintelligences, for superintelligences will outsmart us</strong>. Vinge (1993) <a href=\"http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html\">writes</a>:</p>\n<blockquote>\n<p>I argue that confinement [of superintelligent machines] is intrinsically impractical. For the case of physical confinement: Imagine yourself confined to your house with only limited data access to the outside, to your masters. If those masters thought at a rate &mdash; say &mdash; one million times slower than you, there is little doubt that over a period of years (your time) you could come up with \"helpful advice\" that would incidentally set you free...</p>\n</blockquote>\n<p><strong>After 1993.</strong> The <a href=\"http://lists.extropy.org/pipermail/extropy-chat/\">extropians mailing list</a> was launched in 1991, and was home to hundreds of discussions in which many important new ideas were proposed &mdash; ideas later developed in the public writings of Bostrom, Yudkowsky, Goertzel, and others. Unfortunately, the discussions from before 1998 were private, by agreement among subscribers. The early years of the archive cannot be made public without getting permission from everyone involved &mdash; a nearly impossible task. I have, however, collected all posts I could find from 1998 onward and uploaded them <a href=\"http://dl.dropbox.com/u/163098/extropians%20mailing%20list%20%281998-2012%29%2C%20somewhat%20fragmentary%29.zip\">here</a>&nbsp;(link fixed 04-03-2012).</p>\n<p>I will end this post here. Perhaps in a future post I will extend the timeline past 1993, when interest in the subject became greater and thus the number of new ideas generated per decade rapidly increased.</p>\n<h4><strong>References</strong></h4>\n<ul>\n<li><small>Asimov (1950). <a href=\"http://www.amazon.com/Robot-Visions-Isaac-Asimov/dp/0451450647/\">The Evitable Conflict</a></small></li>\n<li><small>Asimov (1957). <a href=\"http://www.amazon.com/The-Naked-Sun-Isaac-Asimov/dp/0553293397/\">The Naked Sun</a></small></li>\n<li><small>Asimov (1983). <a href=\"http://www.amazon.com/The-Robots-Dawn-Isaac-Asimov/dp/0553299492/\">The Robots of Dawn</a></small></li>\n<li><small>Bostrom (1997). <a href=\"http://www.nickbostrom.com/old/predict.html\">Predictions from Philosophy? How philosophers could make themselves useful</a></small></li>\n<li><small>Bostrom (1998). <a href=\"/www.nickbostrom.com/superintelligence.html\">How Long before Superintelligence?</a></small></li>\n<li><small>Bostrom (2004). <a href=\"http://www.nickbostrom.com/fut/evolution.html\">The Future of Human Evolution</a></small></li>\n<li><small>Bostrom (2012). <a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents</a></small></li>\n<li><small>Bostrom (forthcoming). <em>Superintelligence</em>.</small></li>\n<li><small>Brunner (1975). <em><a href=\"http://www.amazon.com/The-Shockwave-Rider-John-Brunner/dp/0345467175\">The Shockwave Rider</a></em></small></li>\n<li><small>Butler (1863). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Butler-Darwin-Among-the-Machines.pdf\">Darwin among the machines</a></small></li>\n<li><small>Butler (1872). <em><a href=\"http://www.gutenberg.org/ebooks/1906\">Erewhon</a></em>.</small></li>\n<li><small>Campbell (1932). <a href=\"http://www.gutenberg.org/ebooks/27462\">The Last Evolution</a></small></li>\n<li><small>Capek (1921). <em><a href=\"http://www.gutenberg.org/ebooks/13083\">R.U.R.</a></em></small></li>\n<li><small>Ellison (1967). <a href=\"http://www.amazon.com/Have-No-Mouth-Harlan-Ellison/dp/0441363954/\">I Have No Mouth, and I Must Scream</a></small></li>\n<li><small>Good (1959). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Speculations-on-perceptrons-and-other-automata.pdf\">Speculations on perceptrons and other automata</a></small></li>\n<li><small>Good (1962). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/11/Good-The-social-implications-of-artificial-intelligence.pdf\">The social implications of artificial intelligence</a></small></li>\n<li><small>Good (1965). <a href=\"http://www.web-e.stat.vt.edu/dept/web-e/tech_reports/TechReport05-3.pdf\">Speculations Concerning the First Ultraintelligent Machine</a></small></li>\n<li><small>Good (1970). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Some-future-social-repurcussions-of-computers.pdf\">Some future social repercussions of computers</a></small></li>\n<li><small>Jones (1966). <em><a href=\"http://www.amazon.com/Colossus-D-F-Jones/dp/0425076482/\">Colossus</a></em>.</small></li>\n<li><small>Lukasiewicz (1974). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/09/Lukasiewicz-The-Ignorance-Explosion.pdf\">The Ignorance Explosion</a>.</small></li>\n<li><small>Minsky (1984). <a href=\"http://web.media.mit.edu/~minsky/papers/TrueNames.Afterword.html\">Afterward to Vinge's 'True Names'</a>.</small> </li>\n<li><small>Moravec (1988). <em><a href=\"http://www.amazon.com/Mind-Children-Future-Robot-Intelligence/dp/0674576187/\">Mind Children: The Future of Robot and Human Intelligence</a></em>.</small></li>\n<li><small>Muehlhauser &amp; Helm (2012). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">The Singularity and Machine Ethics</a></small></li>\n<li><small>Ryan (1977). <a href=\"http://www.amazon.com/Adolescence-P-1-Thomas-J-Ryan/dp/0671559702/\">The Adolescence of P-1</a></small></li>\n<li><small>Turing (1950). <a href=\"http://www.csee.umbc.edu/courses/471/papers/turing.pdf\">Computing Machinery and Intelligence</a></small></li>\n<li><small>Turing (1951). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Turing-Intelligent-Machinery-a-heretical-theory.pdf\">Intelligent machinery, a heretical theory</a></small></li>\n<li><small>Versenyi (1974). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Versenyi-Can-Robots-Be-Moral.pdf\">Can robots be moral?</a></small></li>\n<li><small>Vinge (1992). <em><a href=\"http://www.amazon.com/Fire-Upon-Deep-Zones-Thought/dp/0765329824/\">A Fire Upon The Deep</a></em>.</small></li>\n<li><small>Vinge (1993). <a href=\"http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html\">The Coming Technological Singularity</a>.</small></li>\n<li><small>Von Neumann (1948). <a href=\"http://www.pensamientocomplejo.com.ar/docs/files/von%20neuman%20-%20central%20and%20logical%20theory%20of%20automata.pdf\">The general and logical theory of automata</a>.</small></li>\n<li><small>Von Neumann (1949).&nbsp;Theory and Organization of Complicated Automata. (Five lectures delivered at the University of Illinois in December, 1949. Reprinted in <em><a href=\"http://www.amazon.com/Neumann-Computers-Computing-Charles-Institute/dp/026222030X/\">Papers of John Von Neumann on Computers and Computing Theory</a></em>.)</small></li>\n<li><small>Williamson (1947). <em><a href=\"http://www.amazon.com/Folded-Searching-Collected-Stories-Williamson/dp/1893887375/\">With Folded Hands</a></em>.</small></li>\n<li><small>Yudkowsky (2001). <em><a href=\"http://intelligence.org/upload/CFAI.html\">Creating Friendly AI</a></em>.</small></li>\n<li><small>Yudkowsky (2008). <a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Artificial Intelligence as a Positive and Negative Factor in Global Risk</a></small></li>\n<li><small>Yudkowsky (2011). <a href=\"http://intelligence.org/upload/complex-value-systems.pdf\">Complex value systems are required to realize valuable futures</a></small></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qdq2SKyMi8vf7Snxq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 17, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "14730", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Part of the series <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">AI Risk and Opportunity: A Strategic Analysis</a>.</small></p>\n<p>(You can leave anonymous feedback on posts in this series <strong><a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dDZ6d0RvM19qVkduX2pjNng4ZklHZXc6MQ\">here</a></strong>. I alone will read the comments, and may use them to improve past and forthcoming posts in this series.)</p>\n<p>Building on <a href=\"/r/discussion/lw/b0v/ai_risk_and_opportunity_humanitys_efforts_so_far/\">the previous post on AI risk history</a>, this post provides an incomplete timeline (up to 1993) of significant <em>novel</em> ideas and arguments related to AI as a potential catastrophic risk. I do not include ideas and arguments concerning only, for example, the possibility of AI (<a href=\"http://www.csee.umbc.edu/courses/471/papers/turing.pdf\">Turing 1950</a>) or attempts to predict its arrival (<a href=\"/www.nickbostrom.com/superintelligence.html\">Bostrom 1998</a>).</p>\n<p>As is usually the case, we find that when we look closely at a cluster of ideas, it turns out these ideas did not appear all at once in the minds of a Few Great Men. Instead, they grew and mutated and gave birth to new ideas gradually as they passed from mind to mind over the course of many decades.</p>\n<p>&nbsp;</p>\n<p><strong>1863: Machine intelligence as an existential risk to humanity; relinquishment of machine technology recommended</strong>. Samuel Butler in <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Butler-Darwin-Among-the-Machines.pdf\">Darwin among the machines</a> worries that as we build increasingly sophisticated and autonomous machines, they will achieve greater capability than humans and replace humans as the dominant agents on the planet:</p>\n<blockquote>\n<p>...we are ourselves creating our own successors; we are daily adding to the beauty and delicacy of their physical organisation; we are daily giving them greater power and supplying by all sorts of ingenious contrivances that self-regulating, self-acting power which will be to them what intellect has been to the human race. In the course of ages we shall find ourselves the inferior race... the time will come when the machines will hold the real supremacy over the world and its inhabitants...</p>\n<p>Our opinion is that war to the death should be instantly proclaimed against them. Every machine of every sort should be destroyed by the well-wisher of his species. Let there be no exceptions made, no quarter shown...</p>\n</blockquote>\n<p>(See also <a href=\"http://www.gutenberg.org/ebooks/1906\">Butler 1872</a>; <a href=\"http://www.gutenberg.org/ebooks/27462\">Campbell 1932</a>.)</p>\n<p><strong>1921: Robots as an existential risk</strong>. The Czech play <em><a href=\"http://www.gutenberg.org/ebooks/13083\">R.U.R.</a></em> by Karel Capek tells the story of robots which grow in power and intelligence and destroy the entire human race (except for a single survivor).</p>\n<p><strong>1947: Fragility &amp; complexity of human values (in the context of machine goal systems); perverse instantiation</strong>. Jack Williamson's novelette <em><a href=\"http://www.amazon.com/Folded-Searching-Collected-Stories-Williamson/dp/1893887375/\">With Folded Hands</a></em> (1947) tells the story of a race of machines that, in order to follow the Prime Directive: \"to serve and obey and guard men from harm.\" To obey this rule, the machines interfere with every aspect of human life, and humans who resist are lobotomized. Due to the fragility and complexity of human values (<a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Yudkowsky 2008</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">Muehlhauser and Helm 2012</a>), the machines' rules of behavior had unintended consequences, manifesting a \"perverse instantiation\" in the language of Bostrom (forthcoming).</p>\n<p>(Also see <a href=\"http://www.amazon.com/Robot-Visions-Isaac-Asimov/dp/0451450647/\">Asimov 1950</a>, <a href=\"http://www.amazon.com/The-Naked-Sun-Isaac-Asimov/dp/0553293397/\">1957</a>, <a href=\"http://www.amazon.com/The-Robots-Dawn-Isaac-Asimov/dp/0553299492/\">1983</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Versenyi-Can-Robots-Be-Moral.pdf\">Versenyi 1974</a>;&nbsp;<a href=\"http://web.media.mit.edu/~minsky/papers/TrueNames.Afterword.html\">Minsky 1984</a>; <a href=\"http://intelligence.org/upload/CFAI.html\">Yudkowsky 2001</a>, <a href=\"http://intelligence.org/upload/complex-value-systems.pdf\">2011</a>.)</p>\n<p><strong>1948-1949: Precursor idea to intelligence explosion</strong>. Von Neumann (<a href=\"http://www.pensamientocomplejo.com.ar/docs/files/von%20neuman%20-%20central%20and%20logical%20theory%20of%20automata.pdf\">1948</a>) wrote:</p>\n<blockquote>\n<p><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:RelyOnVML /> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--> <!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>JA</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"276\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:Garamond; mso-ascii-font-family:Garamond; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Garamond; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--></p>\n<p class=\"MsoNormal\">...\u201ccomplication\" on its lower levels is probably degenerative, that is, that every automaton that can produce other automata will only be able to produce less complicated ones. There is, however, a certain minimum level where this degenerative characteristic ceases to be universal. At this point automata which can reproduce themselves, or even construct higher entities, become possible.</p>\n<!--EndFragment-->\n<p>&nbsp;</p>\n</blockquote>\n<p>Von Nuemann (1949) came very close to articulating the idea of intelligence explosion:</p>\n<blockquote>\n<p>There is thus this completely decisive property of complexity, that there exists a critical size below which the process of synthesis is degenerative, but above which the phenomenon of synthesis, if properly arranged, can become explosive, in other words, where syntheses of automata can proceed in such a manner that each automaton will produce other automata which are more complex and of higher potentialities than itself.</p>\n</blockquote>\n<p><strong>1951: Potentially rapid transition from machine intelligence to machine takeover</strong>. Turing (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Turing-Intelligent-Machinery-a-heretical-theory.pdf\">1951</a>) described ways that intelligent computers might learn and improve their capabilities, concluding that:</p>\n<blockquote>\n<p>...it seems probable that once the machine thinking method has started, it would not take long to outstrip our feeble powers... At some stage therefore we should have to expect the machines to take control...</p>\n</blockquote>\n<p><strong>1959: Intelligence explosion; the need for human-friendly goals for machine superintelligence</strong>. Good (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Speculations-on-perceptrons-and-other-automata.pdf\">1959</a>) describes what he later (<a href=\"http://www.web-e.stat.vt.edu/dept/web-e/tech_reports/TechReport05-3.pdf\">1965</a>) called an \"intelligence explosion,\" a particular mechanism for rapid transition from artificial general intelligence to dangerous machine takeover:</p>\n<blockquote>\n<p>Once a machine is designed that is good enough\u2026 it can be put to work designing an even better machine. At this point an \"explosion\" will clearly occur; all the problems of science and technology will be handed over to machines and it will no longer be necessary for people to work. Whether this will lead to a Utopia or to the extermination of the human race will depend on how the problem is handled by the machines. The important thing will be to give them the aim of serving human beings.</p>\n</blockquote>\n<p>(Also see Good <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/11/Good-The-social-implications-of-artificial-intelligence.pdf\">1962</a>,&nbsp;<a href=\"http://www.web-e.stat.vt.edu/dept/web-e/tech_reports/TechReport05-3.pdf\">1965</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Some-future-social-repurcussions-of-computers.pdf\">1970</a>; <a href=\"http://www.amazon.com/Fire-Upon-Deep-Zones-Thought/dp/0765329824/\">Vinge 1992</a>, <a href=\"http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html\">1993</a>; <a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Yudkowsky 2008</a>.)</p>\n<p><strong>1966: A military arms race for machine superintelligence could accelerate machine takeover; convergence toward a singleton is likely</strong>. Dennis Feltham Jones' 1966 novel <em><a href=\"http://www.amazon.com/Colossus-D-F-Jones/dp/0425076482/\">Colossus</a></em>&nbsp;depicted what may be a particularly likely scenario: two world superpowers (the USA and USSR) are in an arms race to develop superintelligent computers, one of which self-improves enough to take control of the planet.</p>\n<p>In the same year, <a href=\"http://www.amazon.com/Other-Worlds-Than-Cecil-Maxwell/dp/0800861256/\">Cade (1966)</a> argued the same thing:</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p style=\"padding-left: 30px;\">political leaders on Earth will slowly come to realize... that intelligent machines having superhuman thinking ability can be built. The construction of such machines, even taking into account all the latest developments in computer technology, would call for a major national effort. It is only to be expected that any nation which did put forth the financial and physical effort needed to build and programme such a machine, would also attempt to utilize it to its maximum capacity, which implies that it would be used to make major decisions of national policy. Here is where the awful dilemma arises. Any restriction to the range of data supplied to the machine would limit its ability to make effective political and economic decisions, yet if no such restrictions are placed upon the machine's command of information, then the entire control of the nation would virtually be surrendered to the judgment of the robot.</p>\n<p style=\"padding-left: 30px;\">On the other hand, any major nation which was led by a superior, unemotional intelligence of any kind, would quickly rise to a position of world domination. This by itself is sufficient to guarantee that, sooner or later, the effort to build such an intelligence will be made \u2014 if not in the Western world, then elsewhere, where people are more accustomed to iron dictatorships.</p>\n<p style=\"padding-left: 30px;\">...It seems that, in the forseeable future, the major nations of the world will have to face the alternative of surrendering national control to mechanical ministers, or being dominated by other nations which have already done this.&nbsp;Such a process will eventually lead to the domination of the whole Earth by a dictatorship of an unparalleled type \u2014 a single supreme central authority.</p>\n<p>&nbsp;</p>\n<p>(This last paragraph also argues for convergence toward what Bostrom later called a \"singleton.\")</p>\n<p>(Also see <a href=\"http://www.amazon.com/Have-No-Mouth-Harlan-Ellison/dp/0441363954/\">Ellison 1967</a>.)</p>\n<p><strong>1970: Proposal for an association that analyzes the implications of machine superintelligence; naive control solutions like \"switch off the power\" may not work because the superintelligence will outsmart us, thus we must focus on its <em>motivations</em>; possibility of \"pointless\" optimization by machine superintelligence</strong>. <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Some-future-social-repurcussions-of-computers.pdf\">Good (1970)</a> argues:</p>\n<blockquote>\n<p>Even if the chance that the ultraintelligent machine will be available [soon] is small, the repercussions would be so enormous, good or bad, that it is not too early to entertain the possibility. In any case by 1980 I hope that the implications and the safeguards will have been thoroughly discussed, and this is my main reason for airing the matter: an association for considering it should be started.</p>\n</blockquote>\n<p>(Also see <a href=\"http://www.nickbostrom.com/old/predict.html\">Bostrom 1997</a>.)</p>\n<p>On the idea that naive control solutions like \"switch off the power\" may not work because the superintelligence will find a way to outsmart us, and thus we must focus our efforts on the superintelligence's <em>motivations</em>, Good writes:</p>\n<blockquote>\n<p>Some people have suggested that in order to prevent the [ultraintelligent machine] from taking over we should be ready to switch of its power supply. But it is not as simple as that because the machine could recommend the appointment of its own operators, it could recommend that they be paid well and it could select older men who would not be worried about losing their jobs. Then it could replace its operators by robots in order to make sure that it is not switched off. Next it could have the neo-Luddites ridiculed by calling them Ludditeniks, and if necessary it would later have them imprisoned or executed. This shows how careful we must be to keep our eye on the \"motivation\" of the machines, if possible, just as we should with politicians.</p>\n</blockquote>\n<p>(Also see <a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Yudkowsky 2008</a>.)</p>\n<p>Good also outlines one possibility for \"pointless\" goal-optimization by machine superintelligence:</p>\n<blockquote>\n<p>If the machines took over and men became redundant and ultimately extinct, the society of machines would continue in a complex and interesting manner, but it would all apparently be pointless because there would be no one there to be interested. If machines cannot be conscious there would be only a zombie world. This would perhaps not be as bad as in many human societies where most people have lived in misery and degradation while a few have lived in pomp and luxury. It seems to me that the utility of such societies has been negative (while in the condition described) whereas the utility of a zombie society would be zero and hence preferable.</p>\n</blockquote>\n<p>(Also see <a href=\"http://www.nickbostrom.com/fut/evolution.html\">Bostrom 2004</a>; <a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Yudkowsky 2008</a>.)</p>\n<p><strong>1974: We can't much predict what will happen after the creation of machine superintelligence</strong>. Julius Lukasiewicz (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/09/Lukasiewicz-The-Ignorance-Explosion.pdf\">1974</a>) writes:</p>\n<blockquote>\n<p>The survival of man may depend on the early construction of an ultraintelligent machine-or the ultraintelligent machine may take over and render the human race redundant or develop another form of life. The prospect that a merely intelligent man could ever attempt to predict the impact of an ultraintelligent device is of course unlikely but the temptation to speculate seems irresistible.</p>\n</blockquote>\n<p>(Also see <a href=\"http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html\">Vinge 1993</a>.)</p>\n<p><strong>1977: Self-improving AI could stealthily take over the internet; convergent instrumental goals in AI; the treacherous turn</strong>. Though the concept of a self-propagating computer worm was introduced by John Brunner's <a href=\"http://www.amazon.com/The-Shockwave-Rider-John-Brunner/dp/0345467175\"><em>The Shockwave Rider</em></a> (1975), Thomas J. Ryan's novel <a href=\"http://www.amazon.com/Adolescence-P-1-Thomas-J-Ryan/dp/0671559702/\"><em>The Adolescence of P-1</em></a> (1977) tells the story of an intelligent worm that at first is merely able to learn to hack novel computer systems and use them to propagate itself, but later (1) has novel insights on how to improve its own intelligence, (2) develops convergent instrumental subgoals (see <a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">Bostrom 2012</a>) for self-preservation and resource acquisition, and (3) learns the ability to fake its own death so that it can grow its powers in secret and later engage in a \"treacherous turn\" (see Bostrom forthcoming) against humans.</p>\n<p><strong id=\"1982__To_design_ethical_machine_superintelligence__we_may_need_to_design_superintelligence_first_and_then_ask_it_to_solve_philosophical_problems__e_g__including_ethics__\">1982: To design ethical machine superintelligence, we may need to design superintelligence first and then ask it to solve philosophical problems (e.g. including ethics).</strong></p>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Ethical-Machines.pdf\">Good (1982)</a> writes:</p>\n<blockquote>\n<p>Unfortunately, after 2500 years, the philosophical problems are nowhere near solution. Do we need to solve these philosophical problems before we can design an adequate ethical machine, or is there another approach? One approach that cannot be ruled out is first to produce an ultra-intelligent machine and then ask it to solve philosophical problems.</p>\n</blockquote>\n<p><strong>1988: Even though AI poses an existential threat, we may need to rush toward it so we can use it to mitigate other existential threats</strong>. Moravec (<a href=\"http://www.amazon.com/Mind-Children-Future-Robot-Intelligence/dp/0674576187/\">1988</a>, p. 100-101) writes:</p>\n<blockquote>\n<p>...intelligent machines... threaten our existence... Machines merely as clever as human beings will have enormous advantages in competitive situations... So why rush headlong into an era of intelligent machines? The answer, I believe, is that we have very little choice, if our culture is to remain viable... The universe is one random event after another. Sooner or later an unstoppable virus deadly to humans will evolve, or a major asteroid will collide with the earth, or the sun will expand, or we will be invaded from the stars, or a black hole will swallow the galaxy. The bigger, more diverse, and competent a culture is, the better it can detect and deal with external dangers. The larger events happen less frequently. By growing rapidly enough, a culture has a finite chance of surviving forever.</p>\n</blockquote>\n<p><strong>1993: Physical confinement is unlikely to constrain superintelligences, for superintelligences will outsmart us</strong>. Vinge (1993) <a href=\"http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html\">writes</a>:</p>\n<blockquote>\n<p>I argue that confinement [of superintelligent machines] is intrinsically impractical. For the case of physical confinement: Imagine yourself confined to your house with only limited data access to the outside, to your masters. If those masters thought at a rate \u2014 say \u2014 one million times slower than you, there is little doubt that over a period of years (your time) you could come up with \"helpful advice\" that would incidentally set you free...</p>\n</blockquote>\n<p><strong>After 1993.</strong> The <a href=\"http://lists.extropy.org/pipermail/extropy-chat/\">extropians mailing list</a> was launched in 1991, and was home to hundreds of discussions in which many important new ideas were proposed \u2014 ideas later developed in the public writings of Bostrom, Yudkowsky, Goertzel, and others. Unfortunately, the discussions from before 1998 were private, by agreement among subscribers. The early years of the archive cannot be made public without getting permission from everyone involved \u2014 a nearly impossible task. I have, however, collected all posts I could find from 1998 onward and uploaded them <a href=\"http://dl.dropbox.com/u/163098/extropians%20mailing%20list%20%281998-2012%29%2C%20somewhat%20fragmentary%29.zip\">here</a>&nbsp;(link fixed 04-03-2012).</p>\n<p>I will end this post here. Perhaps in a future post I will extend the timeline past 1993, when interest in the subject became greater and thus the number of new ideas generated per decade rapidly increased.</p>\n<h4 id=\"References\"><strong>References</strong></h4>\n<ul>\n<li><small>Asimov (1950). <a href=\"http://www.amazon.com/Robot-Visions-Isaac-Asimov/dp/0451450647/\">The Evitable Conflict</a></small></li>\n<li><small>Asimov (1957). <a href=\"http://www.amazon.com/The-Naked-Sun-Isaac-Asimov/dp/0553293397/\">The Naked Sun</a></small></li>\n<li><small>Asimov (1983). <a href=\"http://www.amazon.com/The-Robots-Dawn-Isaac-Asimov/dp/0553299492/\">The Robots of Dawn</a></small></li>\n<li><small>Bostrom (1997). <a href=\"http://www.nickbostrom.com/old/predict.html\">Predictions from Philosophy? How philosophers could make themselves useful</a></small></li>\n<li><small>Bostrom (1998). <a href=\"/www.nickbostrom.com/superintelligence.html\">How Long before Superintelligence?</a></small></li>\n<li><small>Bostrom (2004). <a href=\"http://www.nickbostrom.com/fut/evolution.html\">The Future of Human Evolution</a></small></li>\n<li><small>Bostrom (2012). <a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents</a></small></li>\n<li><small>Bostrom (forthcoming). <em>Superintelligence</em>.</small></li>\n<li><small>Brunner (1975). <em><a href=\"http://www.amazon.com/The-Shockwave-Rider-John-Brunner/dp/0345467175\">The Shockwave Rider</a></em></small></li>\n<li><small>Butler (1863). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Butler-Darwin-Among-the-Machines.pdf\">Darwin among the machines</a></small></li>\n<li><small>Butler (1872). <em><a href=\"http://www.gutenberg.org/ebooks/1906\">Erewhon</a></em>.</small></li>\n<li><small>Campbell (1932). <a href=\"http://www.gutenberg.org/ebooks/27462\">The Last Evolution</a></small></li>\n<li><small>Capek (1921). <em><a href=\"http://www.gutenberg.org/ebooks/13083\">R.U.R.</a></em></small></li>\n<li><small>Ellison (1967). <a href=\"http://www.amazon.com/Have-No-Mouth-Harlan-Ellison/dp/0441363954/\">I Have No Mouth, and I Must Scream</a></small></li>\n<li><small>Good (1959). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Speculations-on-perceptrons-and-other-automata.pdf\">Speculations on perceptrons and other automata</a></small></li>\n<li><small>Good (1962). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/11/Good-The-social-implications-of-artificial-intelligence.pdf\">The social implications of artificial intelligence</a></small></li>\n<li><small>Good (1965). <a href=\"http://www.web-e.stat.vt.edu/dept/web-e/tech_reports/TechReport05-3.pdf\">Speculations Concerning the First Ultraintelligent Machine</a></small></li>\n<li><small>Good (1970). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Good-Some-future-social-repurcussions-of-computers.pdf\">Some future social repercussions of computers</a></small></li>\n<li><small>Jones (1966). <em><a href=\"http://www.amazon.com/Colossus-D-F-Jones/dp/0425076482/\">Colossus</a></em>.</small></li>\n<li><small>Lukasiewicz (1974). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/09/Lukasiewicz-The-Ignorance-Explosion.pdf\">The Ignorance Explosion</a>.</small></li>\n<li><small>Minsky (1984). <a href=\"http://web.media.mit.edu/~minsky/papers/TrueNames.Afterword.html\">Afterward to Vinge's 'True Names'</a>.</small> </li>\n<li><small>Moravec (1988). <em><a href=\"http://www.amazon.com/Mind-Children-Future-Robot-Intelligence/dp/0674576187/\">Mind Children: The Future of Robot and Human Intelligence</a></em>.</small></li>\n<li><small>Muehlhauser &amp; Helm (2012). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">The Singularity and Machine Ethics</a></small></li>\n<li><small>Ryan (1977). <a href=\"http://www.amazon.com/Adolescence-P-1-Thomas-J-Ryan/dp/0671559702/\">The Adolescence of P-1</a></small></li>\n<li><small>Turing (1950). <a href=\"http://www.csee.umbc.edu/courses/471/papers/turing.pdf\">Computing Machinery and Intelligence</a></small></li>\n<li><small>Turing (1951). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Turing-Intelligent-Machinery-a-heretical-theory.pdf\">Intelligent machinery, a heretical theory</a></small></li>\n<li><small>Versenyi (1974). <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/03/Versenyi-Can-Robots-Be-Moral.pdf\">Can robots be moral?</a></small></li>\n<li><small>Vinge (1992). <em><a href=\"http://www.amazon.com/Fire-Upon-Deep-Zones-Thought/dp/0765329824/\">A Fire Upon The Deep</a></em>.</small></li>\n<li><small>Vinge (1993). <a href=\"http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html\">The Coming Technological Singularity</a>.</small></li>\n<li><small>Von Neumann (1948). <a href=\"http://www.pensamientocomplejo.com.ar/docs/files/von%20neuman%20-%20central%20and%20logical%20theory%20of%20automata.pdf\">The general and logical theory of automata</a>.</small></li>\n<li><small>Von Neumann (1949).&nbsp;Theory and Organization of Complicated Automata. (Five lectures delivered at the University of Illinois in December, 1949. Reprinted in <em><a href=\"http://www.amazon.com/Neumann-Computers-Computing-Charles-Institute/dp/026222030X/\">Papers of John Von Neumann on Computers and Computing Theory</a></em>.)</small></li>\n<li><small>Williamson (1947). <em><a href=\"http://www.amazon.com/Folded-Searching-Collected-Stories-Williamson/dp/1893887375/\">With Folded Hands</a></em>.</small></li>\n<li><small>Yudkowsky (2001). <em><a href=\"http://intelligence.org/upload/CFAI.html\">Creating Friendly AI</a></em>.</small></li>\n<li><small>Yudkowsky (2008). <a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Artificial Intelligence as a Positive and Negative Factor in Global Risk</a></small></li>\n<li><small>Yudkowsky (2011). <a href=\"http://intelligence.org/upload/complex-value-systems.pdf\">Complex value systems are required to realize valuable futures</a></small></li>\n</ul>", "sections": [{"title": "1982: To design ethical machine superintelligence, we may need to design superintelligence first and then ask it to solve philosophical problems (e.g. including ethics).", "anchor": "1982__To_design_ethical_machine_superintelligence__we_may_need_to_design_superintelligence_first_and_then_ask_it_to_solve_philosophical_problems__e_g__including_ethics__", "level": 2}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "29 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["i2XoqtYEykc4XWp9B", "i4susk4W3ieR5K92u"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-03-31T21:07:52.231Z", "modifiedAt": null, "url": null, "title": "Meetup : Seattle: Decision Theory", "slug": "meetup-seattle-decision-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:37.960Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fft3sK8v73gCn4Jvo/meetup-seattle-decision-theory", "pageUrlRelative": "/posts/fft3sK8v73gCn4Jvo/meetup-seattle-decision-theory", "linkUrl": "https://www.lesswrong.com/posts/fft3sK8v73gCn4Jvo/meetup-seattle-decision-theory", "postedAtFormatted": "Saturday, March 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Seattle%3A%20Decision%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Seattle%3A%20Decision%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffft3sK8v73gCn4Jvo%2Fmeetup-seattle-decision-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Seattle%3A%20Decision%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffft3sK8v73gCn4Jvo%2Fmeetup-seattle-decision-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffft3sK8v73gCn4Jvo%2Fmeetup-seattle-decision-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8h'>Seattle: Decision Theory</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 April 2012 04:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">5523 University Way NE #501 Seattle, WA 98115</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's past time we had another meetup. As I mentioned earlier, it's past time for another meetup and, I'd like to have a discussion about decision theory. <a href=\"http://lesswrong.com/lw/aq9/decision_theories_a_less_wrong_primer/\">This</a> recent post on decision theory is good background material. I'll be going over the material in that post and talking about some of the decision theory puzzles which standard decision theories don't do very well on and thus motivate trying to find new, better theories. Guy also that he would bring some puzzles. After that, we'll have a discussion about the puzzles. Ben will be hosting this week. The address is 5523 University Way NE, #501, There is a little buzzer next to the door so if you press \"501\" someone can let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8h'>Seattle: Decision Theory</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fft3sK8v73gCn4Jvo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 8.757318139600774e-07, "legacy": true, "legacyId": "14732", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Seattle__Decision_Theory\">Discussion article for the meetup : <a href=\"/meetups/8h\">Seattle: Decision Theory</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 April 2012 04:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">5523 University Way NE #501 Seattle, WA 98115</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It's past time we had another meetup. As I mentioned earlier, it's past time for another meetup and, I'd like to have a discussion about decision theory. <a href=\"http://lesswrong.com/lw/aq9/decision_theories_a_less_wrong_primer/\">This</a> recent post on decision theory is good background material. I'll be going over the material in that post and talking about some of the decision theory puzzles which standard decision theories don't do very well on and thus motivate trying to find new, better theories. Guy also that he would bring some puzzles. After that, we'll have a discussion about the puzzles. Ben will be hosting this week. The address is 5523 University Way NE, #501, There is a little buzzer next to the door so if you press \"501\" someone can let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Seattle__Decision_Theory1\">Discussion article for the meetup : <a href=\"/meetups/8h\">Seattle: Decision Theory</a></h2>", "sections": [{"title": "Discussion article for the meetup : Seattle: Decision Theory", "anchor": "Discussion_article_for_the_meetup___Seattle__Decision_Theory", "level": 1}, {"title": "Discussion article for the meetup : Seattle: Decision Theory", "anchor": "Discussion_article_for_the_meetup___Seattle__Decision_Theory1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["af9MjBqF2hgu3EN6r"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-01T04:24:34.672Z", "modifiedAt": null, "url": null, "title": "Open Thread, April 1-15, 2012", "slug": "open-thread-april-1-15-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:23.059Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JfR6MRFZr6hPMBgcx/open-thread-april-1-15-2012", "pageUrlRelative": "/posts/JfR6MRFZr6hPMBgcx/open-thread-april-1-15-2012", "linkUrl": "https://www.lesswrong.com/posts/JfR6MRFZr6hPMBgcx/open-thread-april-1-15-2012", "postedAtFormatted": "Sunday, April 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20April%201-15%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20April%201-15%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJfR6MRFZr6hPMBgcx%2Fopen-thread-april-1-15-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20April%201-15%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJfR6MRFZr6hPMBgcx%2Fopen-thread-april-1-15-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJfR6MRFZr6hPMBgcx%2Fopen-thread-april-1-15-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JfR6MRFZr6hPMBgcx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 8.759132479914482e-07, "legacy": true, "legacyId": "14735", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 154, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-01T06:43:13.264Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Distinct Configurations", "slug": "seq-rerun-distinct-configurations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:38.255Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RNG7G4dDmYSvfowcJ/seq-rerun-distinct-configurations", "pageUrlRelative": "/posts/RNG7G4dDmYSvfowcJ/seq-rerun-distinct-configurations", "linkUrl": "https://www.lesswrong.com/posts/RNG7G4dDmYSvfowcJ/seq-rerun-distinct-configurations", "postedAtFormatted": "Sunday, April 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Distinct%20Configurations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Distinct%20Configurations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRNG7G4dDmYSvfowcJ%2Fseq-rerun-distinct-configurations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Distinct%20Configurations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRNG7G4dDmYSvfowcJ%2Fseq-rerun-distinct-configurations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRNG7G4dDmYSvfowcJ%2Fseq-rerun-distinct-configurations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 252, "htmlBody": "<p>Today's post, <a href=\"/lw/pf/distinct_configurations/\">Distinct Configurations</a> was originally published on 12 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Since configurations are over the combined state of all the elements in a system, adding a sensor that detects whether a particle went one way or the other, becomes a new element of the system that can make configurations \"distinct\" instead of \"identical\". This confused the living daylights out of early quantum experimenters, because it meant that things behaved differently when they tried to \"measure\" them. But it's not only measuring instruments that do the trick - any sensitive physical element will do - and the distinctness of configurations is a physical fact, not a fact about our knowledge. There is no need to suppose that the universe cares what we think.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bc7/seq_rerun_joint_configurations/\">Joint Configurations</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RNG7G4dDmYSvfowcJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.759708623922854e-07, "legacy": true, "legacyId": "14741", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KbeHkLNY5ETJ3TN3W", "CBasheGcr8SWbn6Mi", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-01T16:55:54.694Z", "modifiedAt": null, "url": null, "title": "April 2012 Media Thread", "slug": "april-2012-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:55.896Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NacEfLMiXc8neLrDZ/april-2012-media-thread", "pageUrlRelative": "/posts/NacEfLMiXc8neLrDZ/april-2012-media-thread", "linkUrl": "https://www.lesswrong.com/posts/NacEfLMiXc8neLrDZ/april-2012-media-thread", "postedAtFormatted": "Sunday, April 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20April%202012%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AApril%202012%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNacEfLMiXc8neLrDZ%2Fapril-2012-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=April%202012%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNacEfLMiXc8neLrDZ%2Fapril-2012-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNacEfLMiXc8neLrDZ%2Fapril-2012-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 111, "htmlBody": "<p>Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing!</p>\n<p>Rules:</p>\n<ul>\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a <a href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees, which&nbsp;<a href=\"/lw/94z/january_2012_media_thread/5kos\">I was apparently too dumb to do</a>.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NacEfLMiXc8neLrDZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 7, "extendedScore": null, "score": 8.76225551915032e-07, "legacy": true, "legacyId": "14750", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-01T18:40:21.179Z", "modifiedAt": null, "url": null, "title": "Characterizing the superintelligence which we are concerned about", "slug": "characterizing-the-superintelligence-which-we-are-concerned", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:00.364Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaFox", "createdAt": "2009-03-05T06:55:09.368Z", "isAdmin": false, "displayName": "JoshuaFox"}, "userId": "5yNJS8bxEYhgFD9XJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6GHusokYXPGNwpvEF/characterizing-the-superintelligence-which-we-are-concerned", "pageUrlRelative": "/posts/6GHusokYXPGNwpvEF/characterizing-the-superintelligence-which-we-are-concerned", "linkUrl": "https://www.lesswrong.com/posts/6GHusokYXPGNwpvEF/characterizing-the-superintelligence-which-we-are-concerned", "postedAtFormatted": "Sunday, April 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Characterizing%20the%20superintelligence%20which%20we%20are%20concerned%20about&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACharacterizing%20the%20superintelligence%20which%20we%20are%20concerned%20about%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6GHusokYXPGNwpvEF%2Fcharacterizing-the-superintelligence-which-we-are-concerned%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Characterizing%20the%20superintelligence%20which%20we%20are%20concerned%20about%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6GHusokYXPGNwpvEF%2Fcharacterizing-the-superintelligence-which-we-are-concerned", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6GHusokYXPGNwpvEF%2Fcharacterizing-the-superintelligence-which-we-are-concerned", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1764, "htmlBody": "<p><strong id=\"internal-source-marker_0.0799084932077676\" style=\"font-family: 'Times New Roman'; font-size: medium;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">What is this &ldquo;superintelligence&rdquo; we are concerned about? In writing articles on FAI topics, I took the easy way out and defined the focus of attention as an AI that can far outdo humans in all areas. But this just a useful shortcut, not what we are really talking about. </span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">In this essay, I will try to better rcharacterize the topic of interest. </span><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"><br class=\"kix-line-break\" />Some possibilities that have been brought up include intelligences </span> </strong></p>\n<ul style=\"margin-top: 0pt; margin-bottom: 0pt;\">\n<strong id=\"internal-source-marker_0.0799084932077676\" style=\"font-family: 'Times New Roman'; font-size: medium;\">\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline;\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">which are human-like, </span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline;\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">which are conscious, </span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline;\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">which can outperform humans in some or all areas,</span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline;\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">which can self-improve,</span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline;\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">or which meet a semi-formal or formal definition of intelligence or of above-human intelligence.</span></li>\n</strong> \n</ul>\n<p><strong id=\"internal-source-marker_0.0799084932077676\" style=\"font-family: 'Times New Roman'; font-size: medium;\"> <br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">All these are important features in possible future AIs which we should be thinking about.But what really counts is whether an AI can </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">outwit </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">us when its goals are pitted against ours.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">1. </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Human-like</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> intelligence. We are humans, we care about human welfare; and humans are the primary intelligence which cooperates and competes with us; so human intelligence is our primary model. &nbsp;Machines that &ldquo;think like humans&rdquo; are an intuitive focus on discussions of AI; Turing took this as the basis for his practical test for intelligence</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Future AIs might have exactly this type of intelligence, particularly if they are emulated brains, what Robin Hanson calls &ldquo;ems.&rdquo; </span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">If human-like AI is the only AI to come, then not much will have happened: We already have seven billion humans, and a few more will simply extend economic trends. If, as Hanson describes, the ems need fewer resources than humans, then we can expect extreme economic impact. If such AI has certain differences from us humans, like the ability to self-improve, then it will fall under the other categories, as described below.<a id=\"more\"></a></span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">J. Storrs Hall (</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Beyond AI: Creating the conscience of the machine</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, 2007) </span><a href=\"http://lifeboat.com/ex/kinds.of.minds\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">explores</span></a><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> different types of intelligence in relation to the human model: &ldquo;hypohuman&rdquo;, &ldquo;epihuman,&rdquo; &ldquo;allohuman,&rdquo; &ldquo;parahuman,&rdquo; and &ldquo;hyperhuman.&rdquo; Those distinctions are important, but in this essay I am focusing specifically on the specific level of intelligence which can be expected to end the human era, rather than on the relation of such intelligence to human intelligence.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">2. More narrowly than full human emulation, we can consider machines that are </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">conscious</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, feel emotions, or have some other human mental characteristic. The first half of </span><a href=\"http://www.imprint.co.uk/singularity.pdf\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Chalmers&rsquo; seminal 2010 article</span></a><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> brought some essential FAI ideas into the academic sphere; but the second half was devoted to the question of AI consciousness, as if this were the most important thing about AI.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Future AIs may eventually have consciousness or not, whatever consciousness is. But in any case, a machine which is is about to create a cure for aging, or turn us the solar system into computronium is very relevant to our future, whether or not it is conscious; and likewise for emotions and other human mental features.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">3. Performing better than humans in </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">some but not all areas</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. We already have that, in arithmetic, information retrieval, chess, and other areas, and though this has benefited humanity, this is not what we are trying to describe here. </span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">We might imagine an roughly human-level intelligence which is better than humans in a broad range of abilities, and worse in a variety of other areas, an &ldquo;alien intelligence.&rdquo; Just as humans today can harm and help us, such an intelligence could potentially be significant to our futures. But what we are interested here is not just an intelligence that is good in a variety of goals, if this has no effect on us.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">4. </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Outperforming humans in all areas</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">: a machine that can surpass--or far surpass, in IJ Good&rsquo;s original formulation--&rdquo;all the intellectual activities of any man however clever.&rdquo; This is convenient as a </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">sufficient </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">condition in considering superintelligences which can radically benefit or harm the human future. (E.g., this </span><a href=\"http://www.nickbostrom.com/ethics/ai.html\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">see this article by Bostrom</span></a><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">) </span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">But it is not a </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">necessary </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">condition. Imagine a machine that can far surpass any human in all areas except for cooking fine cuisine: It can still excel in creating cancer cures, conquering the earth, or turning us all into paper clips. This would qualify as the entity that can rework our futures for better or worse, even though it does not &ldquo;surpass all the intellectual activities of any man.&rdquo;</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">5. </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Self-improving</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. Even an infrahuman AI, if it were capable of self-improving &nbsp;and motivated to do so (for reasons well-discussed by Omohundro and others), might &nbsp;improve itself to human levels and ultimately to superintelligence. Yudkowsky, in his early article &ldquo;</span><a href=\"http://yudkowsky.net/obsolete/singularity.html\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Staring into the Singularity</span></a><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">,&rdquo; devotes the abstract and the article&rsquo;s key point, to AI self-improvement.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">But what really interests us in self-improvement is actually the resulting superintelligence. If the AI were to self-improve but stop at an infrahuman level, that would not be the AI which we are considering. On the other hand, if human engineers construct an AI which from the first has a greatly superhuman level of intelligence, but which is utterly incapable of self-improving, that </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">would </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">be one of those entities we are considering here, one capable of great benefit or harm.<br class=\"kix-line-break\" /></span><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">6. </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Optimization power.</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> In trying to broaden the idea of intelligence beyond the human model, terms such as &ldquo;flexible,&rdquo; or &ldquo;general&rdquo; intelligence are used. </span><a href=\"http://www.vetta.org/shane/intelligence.html\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Legg and Hutter</span></a><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> captured the idea with a definition summarizing the work of about 70 authors: &ldquo;Intelligence measures an agent&rsquo;s ability to achieve goals in a wide range of environments.&rdquo; To distinguish this from the human model, we sometimes speak of &ldquo;optimization power&rdquo; instead of &ldquo;intelligence.&rdquo;</span><br /><br /><a href=\"http://www.goertzel.org/papers/DefinitionOfIntelligence.pdf\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Goertzel</span></a><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> suggests that we adds to this definition a consideration of efficiency: An agent that can work quickly with limited resources is smarter than one that can do the same thing slowly with more resources. Goertzel also asks that we consider an agent&rsquo;s ability in the real-world environment which we humans have to deal with, and not across all possible environments, the great majority of which are completely irrelevant to anything the agent will be faced with.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">However, these definitions are too loose to help us identify the entities that we are most interested in, if we care about radical changes to the human future?</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">7. Legg and Hutter further formalize the definition mentioned above with a </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">formal metric</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, the </span><a href=\"http://www.vetta.org/documents/Machine_Super_Intelligence.pdf\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Universal Intelligence Measure</span></a><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. We could use this metric to precisely specify entities which are smarter than the humans. &nbsp;If the average human (or the smartest one, or the collective of all humans) has an intelligence measure of, say, 3.1, you could say that we are interested any AI with intelligence 3.1 or higher.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">However, the Universal Intelligence Measure is incomputable, and Goertzel&rsquo;s modifications add even more complications. Until useful approximations are found, these formal measures are not applicable to our problem--and even then, we will only know which agents are smarter than humans according to a certain metric, which is not necessarily what we are looking for here.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">8. Able and motivated to </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">outwit </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">humans. This is what we really care about. If the agent is able to achieve its goals </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">against our will</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, to out-trick us, to outthink us when we are trying to stop it, we could be in big trouble, if its goals are not exactly compatible with ours.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Some entities today can achieve their goals at the expense of humans. If you get on an automated monorail in the airport, and then realize that you on the train to Terminal 2, when you need to be in Terminal 1, the train will accomplish its goal (i.e., what it is designed for), and it will get you to Terminal 2, even though you don&rsquo;t want it to. Biological viruses may well achieve their &ldquo;goals&rdquo; of reproduction at the expense of humans, and if it turns out that a pandemic is the greatest threat to the human species, we should fight to prevent that; maybe even redirecting resources from Friendly AI research. </span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">But these entities can&rsquo;t be said to &ldquo;outwit&rdquo; us: They are not achieving complex goals in complex environments, as in the definitions of intelligence mentioned above.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Though fully general intelligence is useful for outwitting humans, it is not necessary. If a machine has the goal of maximizing the number of paperclips in the world, and uses trickiness and guile to overcome our best efforts to stop it, ultimately converting all the matter in the solar system to paper clips, but has no ability to understand how to prepare cheesecake or why Benny Hill is funny, then it has still outwitted us to our detriment.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">There is another type of agent that we are interested in, a Friendly one which does only what we want, one which can cure aging, hunger, and every other affliction.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Such a helpful AI also need not be fully general. Already today, computers help scientists cure diseases and do other good things. If we develop better and better specialized AI, we could go a long way even without artificial general intelligence.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">But a Friendly artificial </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">general </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">intelligence could be even more helpful than a narrow one, by finding creative new ideas for giving us what we want. This is the Friendly AI which SI would like to build.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">An AI which harms us and one which benefits us need not differ in their architecture, just in goals. A full, general intelligence could be very effective at outwitting us and also at helping us, depending on what it is trying to do.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">All the concepts in 1-7 above are relevant to the question we are asking. Human intelligence is our moral concern, and our main existing model of intelligence, and consciousness is one of its central features. Superintelligence, per IJ Good&rsquo;s definition, is a sufficient condition (together with a non-Friendly goal system) for destroying humanity. Self-improvement is the most likely way to reach that point. Definitions and formal metrics help pin down more precisely what we mean by intelligence.</span><br /><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">But the AI to be concerned about is one which can </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">outwit</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> us in achieving its goals, and secondarily, one which is as good as possible in helping us achieve our goals.</span></strong></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6GHusokYXPGNwpvEF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 14, "extendedScore": null, "score": 8.762689802841932e-07, "legacy": true, "legacyId": "14751", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"What_is_this__superintelligence__we_are_concerned_about__In_writing_articles_on_FAI_topics__I_took_the_easy_way_out_and_defined_the_focus_of_attention_as_an_AI_that_can_far_outdo_humans_in_all_areas__But_this_just_a_useful_shortcut__not_what_we_are_really_talking_about__In_this_essay__I_will_try_to_better_rcharacterize_the_topic_of_interest__Some_possibilities_that_have_been_brought_up_include_intelligences__\" style=\"font-family: 'Times New Roman'; font-size: medium;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">What is this \u201csuperintelligence\u201d we are concerned about? In writing articles on FAI topics, I took the easy way out and defined the focus of attention as an AI that can far outdo humans in all areas. But this just a useful shortcut, not what we are really talking about. </span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">In this essay, I will try to better rcharacterize the topic of interest. </span><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"><br class=\"kix-line-break\">Some possibilities that have been brought up include intelligences </span> </strong></p>\n<ul style=\"margin-top: 0pt; margin-bottom: 0pt;\">\n<strong id=\"internal-source-marker_0.0799084932077676\" style=\"font-family: 'Times New Roman'; font-size: medium;\">\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline;\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">which are human-like, </span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline;\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">which are conscious, </span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline;\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">which can outperform humans in some or all areas,</span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline;\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">which can self-improve,</span></li>\n<li style=\"list-style-type: disc; font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline;\"><span style=\"background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">or which meet a semi-formal or formal definition of intelligence or of above-human intelligence.</span></li>\n</strong> \n</ul>\n<p><strong id=\"All_these_are_important_features_in_possible_future_AIs_which_we_should_be_thinking_about_But_what_really_counts_is_whether_an_AI_can_outwit_us_when_its_goals_are_pitted_against_ours_1__Human_like_intelligence__We_are_humans__we_care_about_human_welfare__and_humans_are_the_primary_intelligence_which_cooperates_and_competes_with_us__so_human_intelligence_is_our_primary_model___Machines_that__think_like_humans__are_an_intuitive_focus_on_discussions_of_AI__Turing_took_this_as_the_basis_for_his_practical_test_for_intelligenceFuture_AIs_might_have_exactly_this_type_of_intelligence__particularly_if_they_are_emulated_brains__what_Robin_Hanson_calls__ems___If_human_like_AI_is_the_only_AI_to_come__then_not_much_will_have_happened__We_already_have_seven_billion_humans__and_a_few_more_will_simply_extend_economic_trends__If__as_Hanson_describes__the_ems_need_fewer_resources_than_humans__then_we_can_expect_extreme_economic_impact__If_such_AI_has_certain_differences_from_us_humans__like_the_ability_to_self_improve__then_it_will_fall_under_the_other_categories__as_described_below_J__Storrs_Hall__Beyond_AI__Creating_the_conscience_of_the_machine__2007__explores_different_types_of_intelligence_in_relation_to_the_human_model___hypohuman____epihuman____allohuman____parahuman___and__hyperhuman___Those_distinctions_are_important__but_in_this_essay_I_am_focusing_specifically_on_the_specific_level_of_intelligence_which_can_be_expected_to_end_the_human_era__rather_than_on_the_relation_of_such_intelligence_to_human_intelligence_2__More_narrowly_than_full_human_emulation__we_can_consider_machines_that_are_conscious__feel_emotions__or_have_some_other_human_mental_characteristic__The_first_half_of_Chalmers__seminal_2010_article_brought_some_essential_FAI_ideas_into_the_academic_sphere__but_the_second_half_was_devoted_to_the_question_of_AI_consciousness__as_if_this_were_the_most_important_thing_about_AI_Future_AIs_may_eventually_have_consciousness_or_not__whatever_consciousness_is__But_in_any_case__a_machine_which_is_is_about_to_create_a_cure_for_aging__or_turn_us_the_solar_system_into_computronium_is_very_relevant_to_our_future__whether_or_not_it_is_conscious__and_likewise_for_emotions_and_other_human_mental_features_3__Performing_better_than_humans_in_some_but_not_all_areas__We_already_have_that__in_arithmetic__information_retrieval__chess__and_other_areas__and_though_this_has_benefited_humanity__this_is_not_what_we_are_trying_to_describe_here__We_might_imagine_an_roughly_human_level_intelligence_which_is_better_than_humans_in_a_broad_range_of_abilities__and_worse_in_a_variety_of_other_areas__an__alien_intelligence___Just_as_humans_today_can_harm_and_help_us__such_an_intelligence_could_potentially_be_significant_to_our_futures__But_what_we_are_interested_here_is_not_just_an_intelligence_that_is_good_in_a_variety_of_goals__if_this_has_no_effect_on_us_4__Outperforming_humans_in_all_areas__a_machine_that_can_surpass__or_far_surpass__in_IJ_Good_s_original_formulation___all_the_intellectual_activities_of_any_man_however_clever___This_is_convenient_as_a_sufficient_condition_in_considering_superintelligences_which_can_radically_benefit_or_harm_the_human_future___E_g___this_see_this_article_by_Bostrom__But_it_is_not_a_necessary_condition__Imagine_a_machine_that_can_far_surpass_any_human_in_all_areas_except_for_cooking_fine_cuisine__It_can_still_excel_in_creating_cancer_cures__conquering_the_earth__or_turning_us_all_into_paper_clips__This_would_qualify_as_the_entity_that_can_rework_our_futures_for_better_or_worse__even_though_it_does_not__surpass_all_the_intellectual_activities_of_any_man__5__Self_improving__Even_an_infrahuman_AI__if_it_were_capable_of_self_improving__and_motivated_to_do_so__for_reasons_well_discussed_by_Omohundro_and_others___might__improve_itself_to_human_levels_and_ultimately_to_superintelligence__Yudkowsky__in_his_early_article__Staring_into_the_Singularity___devotes_the_abstract_and_the_article_s_key_point__to_AI_self_improvement_But_what_really_interests_us_in_self_improvement_is_actually_the_resulting_superintelligence__If_the_AI_were_to_self_improve_but_stop_at_an_infrahuman_level__that_would_not_be_the_AI_which_we_are_considering__On_the_other_hand__if_human_engineers_construct_an_AI_which_from_the_first_has_a_greatly_superhuman_level_of_intelligence__but_which_is_utterly_incapable_of_self_improving__that_would_be_one_of_those_entities_we_are_considering_here__one_capable_of_great_benefit_or_harm_6__Optimization_power__In_trying_to_broaden_the_idea_of_intelligence_beyond_the_human_model__terms_such_as__flexible___or__general__intelligence_are_used__Legg_and_Hutter_captured_the_idea_with_a_definition_summarizing_the_work_of_about_70_authors___Intelligence_measures_an_agent_s_ability_to_achieve_goals_in_a_wide_range_of_environments___To_distinguish_this_from_the_human_model__we_sometimes_speak_of__optimization_power__instead_of__intelligence__Goertzel_suggests_that_we_adds_to_this_definition_a_consideration_of_efficiency__An_agent_that_can_work_quickly_with_limited_resources_is_smarter_than_one_that_can_do_the_same_thing_slowly_with_more_resources__Goertzel_also_asks_that_we_consider_an_agent_s_ability_in_the_real_world_environment_which_we_humans_have_to_deal_with__and_not_across_all_possible_environments__the_great_majority_of_which_are_completely_irrelevant_to_anything_the_agent_will_be_faced_with_However__these_definitions_are_too_loose_to_help_us_identify_the_entities_that_we_are_most_interested_in__if_we_care_about_radical_changes_to_the_human_future_7__Legg_and_Hutter_further_formalize_the_definition_mentioned_above_with_a_formal_metric__the_Universal_Intelligence_Measure__We_could_use_this_metric_to_precisely_specify_entities_which_are_smarter_than_the_humans___If_the_average_human__or_the_smartest_one__or_the_collective_of_all_humans__has_an_intelligence_measure_of__say__3_1__you_could_say_that_we_are_interested_any_AI_with_intelligence_3_1_or_higher_However__the_Universal_Intelligence_Measure_is_incomputable__and_Goertzel_s_modifications_add_even_more_complications__Until_useful_approximations_are_found__these_formal_measures_are_not_applicable_to_our_problem__and_even_then__we_will_only_know_which_agents_are_smarter_than_humans_according_to_a_certain_metric__which_is_not_necessarily_what_we_are_looking_for_here_8__Able_and_motivated_to_outwit_humans__This_is_what_we_really_care_about__If_the_agent_is_able_to_achieve_its_goals_against_our_will__to_out_trick_us__to_outthink_us_when_we_are_trying_to_stop_it__we_could_be_in_big_trouble__if_its_goals_are_not_exactly_compatible_with_ours_Some_entities_today_can_achieve_their_goals_at_the_expense_of_humans__If_you_get_on_an_automated_monorail_in_the_airport__and_then_realize_that_you_on_the_train_to_Terminal_2__when_you_need_to_be_in_Terminal_1__the_train_will_accomplish_its_goal__i_e___what_it_is_designed_for___and_it_will_get_you_to_Terminal_2__even_though_you_don_t_want_it_to__Biological_viruses_may_well_achieve_their__goals__of_reproduction_at_the_expense_of_humans__and_if_it_turns_out_that_a_pandemic_is_the_greatest_threat_to_the_human_species__we_should_fight_to_prevent_that__maybe_even_redirecting_resources_from_Friendly_AI_research__But_these_entities_can_t_be_said_to__outwit__us__They_are_not_achieving_complex_goals_in_complex_environments__as_in_the_definitions_of_intelligence_mentioned_above_Though_fully_general_intelligence_is_useful_for_outwitting_humans__it_is_not_necessary__If_a_machine_has_the_goal_of_maximizing_the_number_of_paperclips_in_the_world__and_uses_trickiness_and_guile_to_overcome_our_best_efforts_to_stop_it__ultimately_converting_all_the_matter_in_the_solar_system_to_paper_clips__but_has_no_ability_to_understand_how_to_prepare_cheesecake_or_why_Benny_Hill_is_funny__then_it_has_still_outwitted_us_to_our_detriment_There_is_another_type_of_agent_that_we_are_interested_in__a_Friendly_one_which_does_only_what_we_want__one_which_can_cure_aging__hunger__and_every_other_affliction_Such_a_helpful_AI_also_need_not_be_fully_general__Already_today__computers_help_scientists_cure_diseases_and_do_other_good_things__If_we_develop_better_and_better_specialized_AI__we_could_go_a_long_way_even_without_artificial_general_intelligence_But_a_Friendly_artificial_general_intelligence_could_be_even_more_helpful_than_a_narrow_one__by_finding_creative_new_ideas_for_giving_us_what_we_want__This_is_the_Friendly_AI_which_SI_would_like_to_build_An_AI_which_harms_us_and_one_which_benefits_us_need_not_differ_in_their_architecture__just_in_goals__A_full__general_intelligence_could_be_very_effective_at_outwitting_us_and_also_at_helping_us__depending_on_what_it_is_trying_to_do_All_the_concepts_in_1_7_above_are_relevant_to_the_question_we_are_asking__Human_intelligence_is_our_moral_concern__and_our_main_existing_model_of_intelligence__and_consciousness_is_one_of_its_central_features__Superintelligence__per_IJ_Good_s_definition__is_a_sufficient_condition__together_with_a_non_Friendly_goal_system__for_destroying_humanity__Self_improvement_is_the_most_likely_way_to_reach_that_point__Definitions_and_formal_metrics_help_pin_down_more_precisely_what_we_mean_by_intelligence_But_the_AI_to_be_concerned_about_is_one_which_can_outwit_us_in_achieving_its_goals__and_secondarily__one_which_is_as_good_as_possible_in_helping_us_achieve_our_goals_\" style=\"font-family: 'Times New Roman'; font-size: medium;\"> <br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">All these are important features in possible future AIs which we should be thinking about.But what really counts is whether an AI can </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">outwit </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">us when its goals are pitted against ours.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">1. </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Human-like</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> intelligence. We are humans, we care about human welfare; and humans are the primary intelligence which cooperates and competes with us; so human intelligence is our primary model. &nbsp;Machines that \u201cthink like humans\u201d are an intuitive focus on discussions of AI; Turing took this as the basis for his practical test for intelligence</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Future AIs might have exactly this type of intelligence, particularly if they are emulated brains, what Robin Hanson calls \u201cems.\u201d </span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">If human-like AI is the only AI to come, then not much will have happened: We already have seven billion humans, and a few more will simply extend economic trends. If, as Hanson describes, the ems need fewer resources than humans, then we can expect extreme economic impact. If such AI has certain differences from us humans, like the ability to self-improve, then it will fall under the other categories, as described below.<a id=\"more\"></a></span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">J. Storrs Hall (</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">Beyond AI: Creating the conscience of the machine</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, 2007) </span><a href=\"http://lifeboat.com/ex/kinds.of.minds\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">explores</span></a><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> different types of intelligence in relation to the human model: \u201chypohuman\u201d, \u201cepihuman,\u201d \u201callohuman,\u201d \u201cparahuman,\u201d and \u201chyperhuman.\u201d Those distinctions are important, but in this essay I am focusing specifically on the specific level of intelligence which can be expected to end the human era, rather than on the relation of such intelligence to human intelligence.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">2. More narrowly than full human emulation, we can consider machines that are </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">conscious</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, feel emotions, or have some other human mental characteristic. The first half of </span><a href=\"http://www.imprint.co.uk/singularity.pdf\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Chalmers\u2019 seminal 2010 article</span></a><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> brought some essential FAI ideas into the academic sphere; but the second half was devoted to the question of AI consciousness, as if this were the most important thing about AI.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Future AIs may eventually have consciousness or not, whatever consciousness is. But in any case, a machine which is is about to create a cure for aging, or turn us the solar system into computronium is very relevant to our future, whether or not it is conscious; and likewise for emotions and other human mental features.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">3. Performing better than humans in </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">some but not all areas</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. We already have that, in arithmetic, information retrieval, chess, and other areas, and though this has benefited humanity, this is not what we are trying to describe here. </span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">We might imagine an roughly human-level intelligence which is better than humans in a broad range of abilities, and worse in a variety of other areas, an \u201calien intelligence.\u201d Just as humans today can harm and help us, such an intelligence could potentially be significant to our futures. But what we are interested here is not just an intelligence that is good in a variety of goals, if this has no effect on us.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">4. </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Outperforming humans in all areas</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">: a machine that can surpass--or far surpass, in IJ Good\u2019s original formulation--\u201dall the intellectual activities of any man however clever.\u201d This is convenient as a </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">sufficient </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">condition in considering superintelligences which can radically benefit or harm the human future. (E.g., this </span><a href=\"http://www.nickbostrom.com/ethics/ai.html\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">see this article by Bostrom</span></a><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">) </span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">But it is not a </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">necessary </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">condition. Imagine a machine that can far surpass any human in all areas except for cooking fine cuisine: It can still excel in creating cancer cures, conquering the earth, or turning us all into paper clips. This would qualify as the entity that can rework our futures for better or worse, even though it does not \u201csurpass all the intellectual activities of any man.\u201d</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">5. </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Self-improving</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. Even an infrahuman AI, if it were capable of self-improving &nbsp;and motivated to do so (for reasons well-discussed by Omohundro and others), might &nbsp;improve itself to human levels and ultimately to superintelligence. Yudkowsky, in his early article \u201c</span><a href=\"http://yudkowsky.net/obsolete/singularity.html\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Staring into the Singularity</span></a><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">,\u201d devotes the abstract and the article\u2019s key point, to AI self-improvement.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">But what really interests us in self-improvement is actually the resulting superintelligence. If the AI were to self-improve but stop at an infrahuman level, that would not be the AI which we are considering. On the other hand, if human engineers construct an AI which from the first has a greatly superhuman level of intelligence, but which is utterly incapable of self-improving, that </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">would </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">be one of those entities we are considering here, one capable of great benefit or harm.<br class=\"kix-line-break\"></span><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">6. </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">Optimization power.</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> In trying to broaden the idea of intelligence beyond the human model, terms such as \u201cflexible,\u201d or \u201cgeneral\u201d intelligence are used. </span><a href=\"http://www.vetta.org/shane/intelligence.html\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Legg and Hutter</span></a><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> captured the idea with a definition summarizing the work of about 70 authors: \u201cIntelligence measures an agent\u2019s ability to achieve goals in a wide range of environments.\u201d To distinguish this from the human model, we sometimes speak of \u201coptimization power\u201d instead of \u201cintelligence.\u201d</span><br><br><a href=\"http://www.goertzel.org/papers/DefinitionOfIntelligence.pdf\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Goertzel</span></a><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> suggests that we adds to this definition a consideration of efficiency: An agent that can work quickly with limited resources is smarter than one that can do the same thing slowly with more resources. Goertzel also asks that we consider an agent\u2019s ability in the real-world environment which we humans have to deal with, and not across all possible environments, the great majority of which are completely irrelevant to anything the agent will be faced with.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">However, these definitions are too loose to help us identify the entities that we are most interested in, if we care about radical changes to the human future?</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">7. Legg and Hutter further formalize the definition mentioned above with a </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">formal metric</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, the </span><a href=\"http://www.vetta.org/documents/Machine_Super_Intelligence.pdf\"><span style=\"font-size: 15px; font-family: Arial; color: #1155cc; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Universal Intelligence Measure</span></a><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">. We could use this metric to precisely specify entities which are smarter than the humans. &nbsp;If the average human (or the smartest one, or the collective of all humans) has an intelligence measure of, say, 3.1, you could say that we are interested any AI with intelligence 3.1 or higher.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">However, the Universal Intelligence Measure is incomputable, and Goertzel\u2019s modifications add even more complications. Until useful approximations are found, these formal measures are not applicable to our problem--and even then, we will only know which agents are smarter than humans according to a certain metric, which is not necessarily what we are looking for here.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">8. Able and motivated to </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">outwit </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">humans. This is what we really care about. If the agent is able to achieve its goals </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">against our will</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">, to out-trick us, to outthink us when we are trying to stop it, we could be in big trouble, if its goals are not exactly compatible with ours.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Some entities today can achieve their goals at the expense of humans. If you get on an automated monorail in the airport, and then realize that you on the train to Terminal 2, when you need to be in Terminal 1, the train will accomplish its goal (i.e., what it is designed for), and it will get you to Terminal 2, even though you don\u2019t want it to. Biological viruses may well achieve their \u201cgoals\u201d of reproduction at the expense of humans, and if it turns out that a pandemic is the greatest threat to the human species, we should fight to prevent that; maybe even redirecting resources from Friendly AI research. </span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">But these entities can\u2019t be said to \u201coutwit\u201d us: They are not achieving complex goals in complex environments, as in the definitions of intelligence mentioned above.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Though fully general intelligence is useful for outwitting humans, it is not necessary. If a machine has the goal of maximizing the number of paperclips in the world, and uses trickiness and guile to overcome our best efforts to stop it, ultimately converting all the matter in the solar system to paper clips, but has no ability to understand how to prepare cheesecake or why Benny Hill is funny, then it has still outwitted us to our detriment.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">There is another type of agent that we are interested in, a Friendly one which does only what we want, one which can cure aging, hunger, and every other affliction.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">Such a helpful AI also need not be fully general. Already today, computers help scientists cure diseases and do other good things. If we develop better and better specialized AI, we could go a long way even without artificial general intelligence.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">But a Friendly artificial </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; font-style: italic; vertical-align: baseline; white-space: pre-wrap;\">general </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">intelligence could be even more helpful than a narrow one, by finding creative new ideas for giving us what we want. This is the Friendly AI which SI would like to build.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">An AI which harms us and one which benefits us need not differ in their architecture, just in goals. A full, general intelligence could be very effective at outwitting us and also at helping us, depending on what it is trying to do.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">All the concepts in 1-7 above are relevant to the question we are asking. Human intelligence is our moral concern, and our main existing model of intelligence, and consciousness is one of its central features. Superintelligence, per IJ Good\u2019s definition, is a sufficient condition (together with a non-Friendly goal system) for destroying humanity. Self-improvement is the most likely way to reach that point. Definitions and formal metrics help pin down more precisely what we mean by intelligence.</span><br><br><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\">But the AI to be concerned about is one which can </span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline; white-space: pre-wrap;\">outwit</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; font-weight: normal; vertical-align: baseline; white-space: pre-wrap;\"> us in achieving its goals, and secondarily, one which is as good as possible in helping us achieve our goals.</span></strong></p>\n<p>&nbsp;</p>", "sections": [{"title": "What is this \u201csuperintelligence\u201d we are concerned about? In writing articles on FAI topics, I took the easy way out and defined the focus of attention as an AI that can far outdo humans in all areas. But this just a useful shortcut, not what we are really talking about. In this essay, I will try to better rcharacterize the topic of interest. Some possibilities that have been brought up include intelligences  ", "anchor": "What_is_this__superintelligence__we_are_concerned_about__In_writing_articles_on_FAI_topics__I_took_the_easy_way_out_and_defined_the_focus_of_attention_as_an_AI_that_can_far_outdo_humans_in_all_areas__But_this_just_a_useful_shortcut__not_what_we_are_really_talking_about__In_this_essay__I_will_try_to_better_rcharacterize_the_topic_of_interest__Some_possibilities_that_have_been_brought_up_include_intelligences__", "level": 1}, {"title": "All these are important features in possible future AIs which we should be thinking about.But what really counts is whether an AI can outwit us when its goals are pitted against ours.1. Human-like intelligence. We are humans, we care about human welfare; and humans are the primary intelligence which cooperates and competes with us; so human intelligence is our primary model. \u00a0Machines that \u201cthink like humans\u201d are an intuitive focus on discussions of AI; Turing took this as the basis for his practical test for intelligenceFuture AIs might have exactly this type of intelligence, particularly if they are emulated brains, what Robin Hanson calls \u201cems.\u201d If human-like AI is the only AI to come, then not much will have happened: We already have seven billion humans, and a few more will simply extend economic trends. If, as Hanson describes, the ems need fewer resources than humans, then we can expect extreme economic impact. If such AI has certain differences from us humans, like the ability to self-improve, then it will fall under the other categories, as described below.J. Storrs Hall (Beyond AI: Creating the conscience of the machine, 2007) explores different types of intelligence in relation to the human model: \u201chypohuman\u201d, \u201cepihuman,\u201d \u201callohuman,\u201d \u201cparahuman,\u201d and \u201chyperhuman.\u201d Those distinctions are important, but in this essay I am focusing specifically on the specific level of intelligence which can be expected to end the human era, rather than on the relation of such intelligence to human intelligence.2. More narrowly than full human emulation, we can consider machines that are conscious, feel emotions, or have some other human mental characteristic. The first half of Chalmers\u2019 seminal 2010 article brought some essential FAI ideas into the academic sphere; but the second half was devoted to the question of AI consciousness, as if this were the most important thing about AI.Future AIs may eventually have consciousness or not, whatever consciousness is. But in any case, a machine which is is about to create a cure for aging, or turn us the solar system into computronium is very relevant to our future, whether or not it is conscious; and likewise for emotions and other human mental features.3. Performing better than humans in some but not all areas. We already have that, in arithmetic, information retrieval, chess, and other areas, and though this has benefited humanity, this is not what we are trying to describe here. We might imagine an roughly human-level intelligence which is better than humans in a broad range of abilities, and worse in a variety of other areas, an \u201calien intelligence.\u201d Just as humans today can harm and help us, such an intelligence could potentially be significant to our futures. But what we are interested here is not just an intelligence that is good in a variety of goals, if this has no effect on us.4. Outperforming humans in all areas: a machine that can surpass--or far surpass, in IJ Good\u2019s original formulation--\u201dall the intellectual activities of any man however clever.\u201d This is convenient as a sufficient condition in considering superintelligences which can radically benefit or harm the human future. (E.g., this see this article by Bostrom) But it is not a necessary condition. Imagine a machine that can far surpass any human in all areas except for cooking fine cuisine: It can still excel in creating cancer cures, conquering the earth, or turning us all into paper clips. This would qualify as the entity that can rework our futures for better or worse, even though it does not \u201csurpass all the intellectual activities of any man.\u201d5. Self-improving. Even an infrahuman AI, if it were capable of self-improving \u00a0and motivated to do so (for reasons well-discussed by Omohundro and others), might \u00a0improve itself to human levels and ultimately to superintelligence. Yudkowsky, in his early article \u201cStaring into the Singularity,\u201d devotes the abstract and the article\u2019s key point, to AI self-improvement.But what really interests us in self-improvement is actually the resulting superintelligence. If the AI were to self-improve but stop at an infrahuman level, that would not be the AI which we are considering. On the other hand, if human engineers construct an AI which from the first has a greatly superhuman level of intelligence, but which is utterly incapable of self-improving, that would be one of those entities we are considering here, one capable of great benefit or harm.6. Optimization power. In trying to broaden the idea of intelligence beyond the human model, terms such as \u201cflexible,\u201d or \u201cgeneral\u201d intelligence are used. Legg and Hutter captured the idea with a definition summarizing the work of about 70 authors: \u201cIntelligence measures an agent\u2019s ability to achieve goals in a wide range of environments.\u201d To distinguish this from the human model, we sometimes speak of \u201coptimization power\u201d instead of \u201cintelligence.\u201dGoertzel suggests that we adds to this definition a consideration of efficiency: An agent that can work quickly with limited resources is smarter than one that can do the same thing slowly with more resources. Goertzel also asks that we consider an agent\u2019s ability in the real-world environment which we humans have to deal with, and not across all possible environments, the great majority of which are completely irrelevant to anything the agent will be faced with.However, these definitions are too loose to help us identify the entities that we are most interested in, if we care about radical changes to the human future?7. Legg and Hutter further formalize the definition mentioned above with a formal metric, the Universal Intelligence Measure. We could use this metric to precisely specify entities which are smarter than the humans. \u00a0If the average human (or the smartest one, or the collective of all humans) has an intelligence measure of, say, 3.1, you could say that we are interested any AI with intelligence 3.1 or higher.However, the Universal Intelligence Measure is incomputable, and Goertzel\u2019s modifications add even more complications. Until useful approximations are found, these formal measures are not applicable to our problem--and even then, we will only know which agents are smarter than humans according to a certain metric, which is not necessarily what we are looking for here.8. Able and motivated to outwit humans. This is what we really care about. If the agent is able to achieve its goals against our will, to out-trick us, to outthink us when we are trying to stop it, we could be in big trouble, if its goals are not exactly compatible with ours.Some entities today can achieve their goals at the expense of humans. If you get on an automated monorail in the airport, and then realize that you on the train to Terminal 2, when you need to be in Terminal 1, the train will accomplish its goal (i.e., what it is designed for), and it will get you to Terminal 2, even though you don\u2019t want it to. Biological viruses may well achieve their \u201cgoals\u201d of reproduction at the expense of humans, and if it turns out that a pandemic is the greatest threat to the human species, we should fight to prevent that; maybe even redirecting resources from Friendly AI research. But these entities can\u2019t be said to \u201coutwit\u201d us: They are not achieving complex goals in complex environments, as in the definitions of intelligence mentioned above.Though fully general intelligence is useful for outwitting humans, it is not necessary. If a machine has the goal of maximizing the number of paperclips in the world, and uses trickiness and guile to overcome our best efforts to stop it, ultimately converting all the matter in the solar system to paper clips, but has no ability to understand how to prepare cheesecake or why Benny Hill is funny, then it has still outwitted us to our detriment.There is another type of agent that we are interested in, a Friendly one which does only what we want, one which can cure aging, hunger, and every other affliction.Such a helpful AI also need not be fully general. Already today, computers help scientists cure diseases and do other good things. If we develop better and better specialized AI, we could go a long way even without artificial general intelligence.But a Friendly artificial general intelligence could be even more helpful than a narrow one, by finding creative new ideas for giving us what we want. This is the Friendly AI which SI would like to build.An AI which harms us and one which benefits us need not differ in their architecture, just in goals. A full, general intelligence could be very effective at outwitting us and also at helping us, depending on what it is trying to do.All the concepts in 1-7 above are relevant to the question we are asking. Human intelligence is our moral concern, and our main existing model of intelligence, and consciousness is one of its central features. Superintelligence, per IJ Good\u2019s definition, is a sufficient condition (together with a non-Friendly goal system) for destroying humanity. Self-improvement is the most likely way to reach that point. Definitions and formal metrics help pin down more precisely what we mean by intelligence.But the AI to be concerned about is one which can outwit us in achieving its goals, and secondarily, one which is as good as possible in helping us achieve our goals.", "anchor": "All_these_are_important_features_in_possible_future_AIs_which_we_should_be_thinking_about_But_what_really_counts_is_whether_an_AI_can_outwit_us_when_its_goals_are_pitted_against_ours_1__Human_like_intelligence__We_are_humans__we_care_about_human_welfare__and_humans_are_the_primary_intelligence_which_cooperates_and_competes_with_us__so_human_intelligence_is_our_primary_model___Machines_that__think_like_humans__are_an_intuitive_focus_on_discussions_of_AI__Turing_took_this_as_the_basis_for_his_practical_test_for_intelligenceFuture_AIs_might_have_exactly_this_type_of_intelligence__particularly_if_they_are_emulated_brains__what_Robin_Hanson_calls__ems___If_human_like_AI_is_the_only_AI_to_come__then_not_much_will_have_happened__We_already_have_seven_billion_humans__and_a_few_more_will_simply_extend_economic_trends__If__as_Hanson_describes__the_ems_need_fewer_resources_than_humans__then_we_can_expect_extreme_economic_impact__If_such_AI_has_certain_differences_from_us_humans__like_the_ability_to_self_improve__then_it_will_fall_under_the_other_categories__as_described_below_J__Storrs_Hall__Beyond_AI__Creating_the_conscience_of_the_machine__2007__explores_different_types_of_intelligence_in_relation_to_the_human_model___hypohuman____epihuman____allohuman____parahuman___and__hyperhuman___Those_distinctions_are_important__but_in_this_essay_I_am_focusing_specifically_on_the_specific_level_of_intelligence_which_can_be_expected_to_end_the_human_era__rather_than_on_the_relation_of_such_intelligence_to_human_intelligence_2__More_narrowly_than_full_human_emulation__we_can_consider_machines_that_are_conscious__feel_emotions__or_have_some_other_human_mental_characteristic__The_first_half_of_Chalmers__seminal_2010_article_brought_some_essential_FAI_ideas_into_the_academic_sphere__but_the_second_half_was_devoted_to_the_question_of_AI_consciousness__as_if_this_were_the_most_important_thing_about_AI_Future_AIs_may_eventually_have_consciousness_or_not__whatever_consciousness_is__But_in_any_case__a_machine_which_is_is_about_to_create_a_cure_for_aging__or_turn_us_the_solar_system_into_computronium_is_very_relevant_to_our_future__whether_or_not_it_is_conscious__and_likewise_for_emotions_and_other_human_mental_features_3__Performing_better_than_humans_in_some_but_not_all_areas__We_already_have_that__in_arithmetic__information_retrieval__chess__and_other_areas__and_though_this_has_benefited_humanity__this_is_not_what_we_are_trying_to_describe_here__We_might_imagine_an_roughly_human_level_intelligence_which_is_better_than_humans_in_a_broad_range_of_abilities__and_worse_in_a_variety_of_other_areas__an__alien_intelligence___Just_as_humans_today_can_harm_and_help_us__such_an_intelligence_could_potentially_be_significant_to_our_futures__But_what_we_are_interested_here_is_not_just_an_intelligence_that_is_good_in_a_variety_of_goals__if_this_has_no_effect_on_us_4__Outperforming_humans_in_all_areas__a_machine_that_can_surpass__or_far_surpass__in_IJ_Good_s_original_formulation___all_the_intellectual_activities_of_any_man_however_clever___This_is_convenient_as_a_sufficient_condition_in_considering_superintelligences_which_can_radically_benefit_or_harm_the_human_future___E_g___this_see_this_article_by_Bostrom__But_it_is_not_a_necessary_condition__Imagine_a_machine_that_can_far_surpass_any_human_in_all_areas_except_for_cooking_fine_cuisine__It_can_still_excel_in_creating_cancer_cures__conquering_the_earth__or_turning_us_all_into_paper_clips__This_would_qualify_as_the_entity_that_can_rework_our_futures_for_better_or_worse__even_though_it_does_not__surpass_all_the_intellectual_activities_of_any_man__5__Self_improving__Even_an_infrahuman_AI__if_it_were_capable_of_self_improving__and_motivated_to_do_so__for_reasons_well_discussed_by_Omohundro_and_others___might__improve_itself_to_human_levels_and_ultimately_to_superintelligence__Yudkowsky__in_his_early_article__Staring_into_the_Singularity___devotes_the_abstract_and_the_article_s_key_point__to_AI_self_improvement_But_what_really_interests_us_in_self_improvement_is_actually_the_resulting_superintelligence__If_the_AI_were_to_self_improve_but_stop_at_an_infrahuman_level__that_would_not_be_the_AI_which_we_are_considering__On_the_other_hand__if_human_engineers_construct_an_AI_which_from_the_first_has_a_greatly_superhuman_level_of_intelligence__but_which_is_utterly_incapable_of_self_improving__that_would_be_one_of_those_entities_we_are_considering_here__one_capable_of_great_benefit_or_harm_6__Optimization_power__In_trying_to_broaden_the_idea_of_intelligence_beyond_the_human_model__terms_such_as__flexible___or__general__intelligence_are_used__Legg_and_Hutter_captured_the_idea_with_a_definition_summarizing_the_work_of_about_70_authors___Intelligence_measures_an_agent_s_ability_to_achieve_goals_in_a_wide_range_of_environments___To_distinguish_this_from_the_human_model__we_sometimes_speak_of__optimization_power__instead_of__intelligence__Goertzel_suggests_that_we_adds_to_this_definition_a_consideration_of_efficiency__An_agent_that_can_work_quickly_with_limited_resources_is_smarter_than_one_that_can_do_the_same_thing_slowly_with_more_resources__Goertzel_also_asks_that_we_consider_an_agent_s_ability_in_the_real_world_environment_which_we_humans_have_to_deal_with__and_not_across_all_possible_environments__the_great_majority_of_which_are_completely_irrelevant_to_anything_the_agent_will_be_faced_with_However__these_definitions_are_too_loose_to_help_us_identify_the_entities_that_we_are_most_interested_in__if_we_care_about_radical_changes_to_the_human_future_7__Legg_and_Hutter_further_formalize_the_definition_mentioned_above_with_a_formal_metric__the_Universal_Intelligence_Measure__We_could_use_this_metric_to_precisely_specify_entities_which_are_smarter_than_the_humans___If_the_average_human__or_the_smartest_one__or_the_collective_of_all_humans__has_an_intelligence_measure_of__say__3_1__you_could_say_that_we_are_interested_any_AI_with_intelligence_3_1_or_higher_However__the_Universal_Intelligence_Measure_is_incomputable__and_Goertzel_s_modifications_add_even_more_complications__Until_useful_approximations_are_found__these_formal_measures_are_not_applicable_to_our_problem__and_even_then__we_will_only_know_which_agents_are_smarter_than_humans_according_to_a_certain_metric__which_is_not_necessarily_what_we_are_looking_for_here_8__Able_and_motivated_to_outwit_humans__This_is_what_we_really_care_about__If_the_agent_is_able_to_achieve_its_goals_against_our_will__to_out_trick_us__to_outthink_us_when_we_are_trying_to_stop_it__we_could_be_in_big_trouble__if_its_goals_are_not_exactly_compatible_with_ours_Some_entities_today_can_achieve_their_goals_at_the_expense_of_humans__If_you_get_on_an_automated_monorail_in_the_airport__and_then_realize_that_you_on_the_train_to_Terminal_2__when_you_need_to_be_in_Terminal_1__the_train_will_accomplish_its_goal__i_e___what_it_is_designed_for___and_it_will_get_you_to_Terminal_2__even_though_you_don_t_want_it_to__Biological_viruses_may_well_achieve_their__goals__of_reproduction_at_the_expense_of_humans__and_if_it_turns_out_that_a_pandemic_is_the_greatest_threat_to_the_human_species__we_should_fight_to_prevent_that__maybe_even_redirecting_resources_from_Friendly_AI_research__But_these_entities_can_t_be_said_to__outwit__us__They_are_not_achieving_complex_goals_in_complex_environments__as_in_the_definitions_of_intelligence_mentioned_above_Though_fully_general_intelligence_is_useful_for_outwitting_humans__it_is_not_necessary__If_a_machine_has_the_goal_of_maximizing_the_number_of_paperclips_in_the_world__and_uses_trickiness_and_guile_to_overcome_our_best_efforts_to_stop_it__ultimately_converting_all_the_matter_in_the_solar_system_to_paper_clips__but_has_no_ability_to_understand_how_to_prepare_cheesecake_or_why_Benny_Hill_is_funny__then_it_has_still_outwitted_us_to_our_detriment_There_is_another_type_of_agent_that_we_are_interested_in__a_Friendly_one_which_does_only_what_we_want__one_which_can_cure_aging__hunger__and_every_other_affliction_Such_a_helpful_AI_also_need_not_be_fully_general__Already_today__computers_help_scientists_cure_diseases_and_do_other_good_things__If_we_develop_better_and_better_specialized_AI__we_could_go_a_long_way_even_without_artificial_general_intelligence_But_a_Friendly_artificial_general_intelligence_could_be_even_more_helpful_than_a_narrow_one__by_finding_creative_new_ideas_for_giving_us_what_we_want__This_is_the_Friendly_AI_which_SI_would_like_to_build_An_AI_which_harms_us_and_one_which_benefits_us_need_not_differ_in_their_architecture__just_in_goals__A_full__general_intelligence_could_be_very_effective_at_outwitting_us_and_also_at_helping_us__depending_on_what_it_is_trying_to_do_All_the_concepts_in_1_7_above_are_relevant_to_the_question_we_are_asking__Human_intelligence_is_our_moral_concern__and_our_main_existing_model_of_intelligence__and_consciousness_is_one_of_its_central_features__Superintelligence__per_IJ_Good_s_definition__is_a_sufficient_condition__together_with_a_non_Friendly_goal_system__for_destroying_humanity__Self_improvement_is_the_most_likely_way_to_reach_that_point__Definitions_and_formal_metrics_help_pin_down_more_precisely_what_we_mean_by_intelligence_But_the_AI_to_be_concerned_about_is_one_which_can_outwit_us_in_achieving_its_goals__and_secondarily__one_which_is_as_good_as_possible_in_helping_us_achieve_our_goals_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-01T18:40:52.001Z", "modifiedAt": null, "url": null, "title": "What are you working on? April 2012", "slug": "what-are-you-working-on-april-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:31.307Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qLjzA83GPHqh7vCnZ/what-are-you-working-on-april-2012", "pageUrlRelative": "/posts/qLjzA83GPHqh7vCnZ/what-are-you-working-on-april-2012", "linkUrl": "https://www.lesswrong.com/posts/qLjzA83GPHqh7vCnZ/what-are-you-working-on-april-2012", "postedAtFormatted": "Sunday, April 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20you%20working%20on%3F%20April%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20you%20working%20on%3F%20April%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqLjzA83GPHqh7vCnZ%2Fwhat-are-you-working-on-april-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20you%20working%20on%3F%20April%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqLjzA83GPHqh7vCnZ%2Fwhat-are-you-working-on-april-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqLjzA83GPHqh7vCnZ%2Fwhat-are-you-working-on-april-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p>This is the bimonthly 'What are you working On?' thread. Previous threads are <a href=\"/r/discussion/tag/waywo\">here</a>. So here's the question:</p>\n<p style=\"padding-left: 60px;\"><em>What are you working on?&nbsp;</em></p>\n<p>Here are some guidelines:</p>\n<ul>\n<li>Focus on projects that you have recently made progress on, not projects that you're thinking about doing but haven't started.</li>\n<li>Why this project and not others? Mention reasons why you're doing the project and/or why others should contribute to your project (if applicable).</li>\n<li>Talk about your goals for the project.</li>\n<li>Any kind of project is fair game: personal improvement, research project, art project, whatever.</li>\n<li>Link to your work if it's linkable.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qLjzA83GPHqh7vCnZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 4, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "14752", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 61, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-01T19:19:21.414Z", "modifiedAt": null, "url": null, "title": "AI Risk & Opportunity: Questions We Want Answered", "slug": "ai-risk-and-opportunity-questions-we-want-answered", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:34.145Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3w7XHLf8AYRvtoN8b/ai-risk-and-opportunity-questions-we-want-answered", "pageUrlRelative": "/posts/3w7XHLf8AYRvtoN8b/ai-risk-and-opportunity-questions-we-want-answered", "linkUrl": "https://www.lesswrong.com/posts/3w7XHLf8AYRvtoN8b/ai-risk-and-opportunity-questions-we-want-answered", "postedAtFormatted": "Sunday, April 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20Risk%20%26%20Opportunity%3A%20Questions%20We%20Want%20Answered&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20Risk%20%26%20Opportunity%3A%20Questions%20We%20Want%20Answered%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3w7XHLf8AYRvtoN8b%2Fai-risk-and-opportunity-questions-we-want-answered%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20Risk%20%26%20Opportunity%3A%20Questions%20We%20Want%20Answered%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3w7XHLf8AYRvtoN8b%2Fai-risk-and-opportunity-questions-we-want-answered", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3w7XHLf8AYRvtoN8b%2Fai-risk-and-opportunity-questions-we-want-answered", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 868, "htmlBody": "<p><small>Part of the series <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">AI Risk and Opportunity: A Strategic Analysis</a>.</small></p>\n<p>(You can leave anonymous feedback on posts in this series <strong><a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dDZ6d0RvM19qVkduX2pjNng4ZklHZXc6MQ\">here</a></strong>. I alone will read the comments, and may use them to improve past and forthcoming posts in this series.)</p>\n<p>This post provides a list of questions about AI risk strategy &mdash; questions we want answered. Please suggest additional questions (a paragraph of explanation is preferred but not necessary); I may add them to the list. You can submit questions anonymously <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dDZ6d0RvM19qVkduX2pjNng4ZklHZXc6MQ\">here</a>.</p>\n<p>Also, please <strong>identify which 3-5 of these questions you think are low-hanging fruit for productive strategic analysis on Less Wrong</strong>.</p>\n<p>The list is in no particular order, but question numbers will remain unchanged (so that you can reliably refer to questions by their number):</p>\n<ol>\n<li>\n<p><strong>What methods can we use to predict technological development?</strong> We <a href=\"/lw/9ao/longterm_technological_forecasting/\">don't yet have</a> reliable methods for long-term technological forecasting. But not all methods have been examined yet. Perhaps technology futures have a good track record. Perhaps we could look at historical technological predictions and see if there is any pattern in the data suggesting that certain character traits and contexts lend themselves to accurate technological predictions. Perhaps there are creative solutions we haven't thought of yet.</p>\n</li>\n<li>\n<p><strong>Which kinds of differential technological development should we encourage, and how?</strong> Should we <a href=\"http://intelligence.org/upload/Singularity%20Summit%202011%20Workshop%20Report.pdf\">\"push\" on WBE, or not?</a> Are some kinds of AI research risk-reducing, and other kinds risk-increasing? How can we achieve such effects, if they are desired?</p>\n</li>\n<li>\n<p><strong>Which open problems are safe to discuss, and which are potentially dangerous?</strong> AI risk research may itself produce risk in some cases, in the form of information hazards (<a href=\"http://www.nickbostrom.com/information-hazards.pdf\">Bostrom 2011</a>). Is it safe to discuss decision theories? Acausal trade? Certain kinds of strategic questions, for example involving government intervention?</p>\n</li>\n<li>\n<p><strong>What can we do to reduce the risk of an AI arms race?</strong></p>\n</li>\n<li>\n<p><strong>What can we do to raise the \"sanity waterline,\" and how much will this help?</strong></p>\n</li>\n<li>\n<p><strong>What can we do to attract more funding, support, and research to x-risk reduction and to the specific sub-problems of successful Singularity navigation?</strong></p>\n</li>\n<li>\n<p><strong>Which interventions should we prioritize?</strong></p>\n</li>\n<li>\n<p><strong>How should x-risk reducers and AI safety researchers interact with governments and corporations?</strong> Does Drexler's interaction with the U.S. government regarding molecular nanotechnology provide any lessons for how AI risk researchers should act?</p>\n</li>\n<li>\n<p><strong>How can optimal philanthropists get the most x-risk reduction for their philanthropic buck?</strong></p>\n</li>\n<li>\n<p><strong>How does AI risk compare to other existential risks?</strong></p>\n</li>\n<li>\n<p><strong>Which problems do we need to solve, and which ones can we have an AI solve?</strong></p>\n</li>\n<li>\n<p><strong>How can we develop microeconomic models of WBEs and self-improving systems?</strong></p>\n</li>\n<li>\n<p><strong>How can we be sure a Friendly AI development team will be altruistic?</strong></p>\n</li>\n<li>\n<p><strong>How hard is it to create Friendly AI?</strong></p>\n</li>\n<li>\n<p><strong>What is the strength of feedback from neuroscience to AI rather than brain emulation?</strong></p>\n</li>\n<li>\n<p><strong>Is there a safe way to do uploads, where they don't turn into neuromorphic AI?</strong></p>\n</li>\n<li>\n<p><strong>How much must we spend on security when developing a Friendly AI team?</strong></p>\n</li>\n<li>\n<p><strong>What's the best way to recruit talent toward working on AI risks?</strong></p>\n</li>\n<li>\n<p><strong>How difficult is stabilizing the world so we can work on Friendly AI slowly?</strong></p>\n</li>\n<li>\n<p><strong>How hard will a takeoff be?</strong> To what degree is \"intelligence\" (as <a href=\"/lw/vb/efficient_crossdomain_optimization/\">efficient cross-domain optimization</a>) a matter of content vs. algorithms? How much does takeoff depend on slow, real-world experiments?</p>\n</li>\n<li>\n<p><strong>What is the value of strategy vs. object-level progress toward a positive Singularity?</strong></p>\n</li>\n<li>\n<p><strong>What different kinds of Oracle AI are there, and are any of them both safe and feasible?</strong></p>\n</li>\n<li>\n<p><strong>How much should we be worried about \"metacomputational hazards\"?</strong> E.g. should we worry about <a href=\"/lw/x4/nonperson_predicates/\">nonperson predicates</a>? Oracle AIs engaging in <a href=\"/lw/5bu/is_it_possible_to_build_a_safe_oracle_ai/3zbo\">self-fulfilling prophecies</a>? <a href=\"http://ordinaryideas.wordpress.com/2011/12/15/hazards/\">Acausal hijacking</a>?</p>\n</li>\n<li>\n<p><strong>What improvements can we make to the way we go about answering strategy questions?</strong> Wei Dai's <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/5yvd\">notes</a> on this question: \"For example, should we differentiate between \"strategic insights\" (such as Carl Shulman's insight that WBE-based Singletons may be feasible) and \"keeping track of the big picture\" (forming the overall strategy and updating it based on new insights and evidence), and aim to have people specialize in each, so that people deciding strategy won't be tempted to overweigh their own insights? Another example: is there a better way to combine probability estimates from multiple people?\"</p>\n</li>\n<li>\n<p><strong>How do people in other fields answer strategy questions?</strong> Wei Dai's <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/5yvd\">notes</a> on this question: \"Is there such a thing as a science or art of strategy that we can copy from (and perhaps improve upon with ideas from x-rationality)?\"</p>\n</li>\n</ol>\n<p>[more questions to come, as they are posted to the comments section]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3w7XHLf8AYRvtoN8b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 12, "extendedScore": null, "score": 8.762851996792137e-07, "legacy": true, "legacyId": "14753", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Part of the series <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">AI Risk and Opportunity: A Strategic Analysis</a>.</small></p>\n<p>(You can leave anonymous feedback on posts in this series <strong><a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dDZ6d0RvM19qVkduX2pjNng4ZklHZXc6MQ\">here</a></strong>. I alone will read the comments, and may use them to improve past and forthcoming posts in this series.)</p>\n<p>This post provides a list of questions about AI risk strategy \u2014 questions we want answered. Please suggest additional questions (a paragraph of explanation is preferred but not necessary); I may add them to the list. You can submit questions anonymously <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dDZ6d0RvM19qVkduX2pjNng4ZklHZXc6MQ\">here</a>.</p>\n<p>Also, please <strong>identify which 3-5 of these questions you think are low-hanging fruit for productive strategic analysis on Less Wrong</strong>.</p>\n<p>The list is in no particular order, but question numbers will remain unchanged (so that you can reliably refer to questions by their number):</p>\n<ol>\n<li>\n<p><strong>What methods can we use to predict technological development?</strong> We <a href=\"/lw/9ao/longterm_technological_forecasting/\">don't yet have</a> reliable methods for long-term technological forecasting. But not all methods have been examined yet. Perhaps technology futures have a good track record. Perhaps we could look at historical technological predictions and see if there is any pattern in the data suggesting that certain character traits and contexts lend themselves to accurate technological predictions. Perhaps there are creative solutions we haven't thought of yet.</p>\n</li>\n<li>\n<p><strong>Which kinds of differential technological development should we encourage, and how?</strong> Should we <a href=\"http://intelligence.org/upload/Singularity%20Summit%202011%20Workshop%20Report.pdf\">\"push\" on WBE, or not?</a> Are some kinds of AI research risk-reducing, and other kinds risk-increasing? How can we achieve such effects, if they are desired?</p>\n</li>\n<li>\n<p><strong>Which open problems are safe to discuss, and which are potentially dangerous?</strong> AI risk research may itself produce risk in some cases, in the form of information hazards (<a href=\"http://www.nickbostrom.com/information-hazards.pdf\">Bostrom 2011</a>). Is it safe to discuss decision theories? Acausal trade? Certain kinds of strategic questions, for example involving government intervention?</p>\n</li>\n<li>\n<p><strong id=\"What_can_we_do_to_reduce_the_risk_of_an_AI_arms_race_\">What can we do to reduce the risk of an AI arms race?</strong></p>\n</li>\n<li>\n<p><strong id=\"What_can_we_do_to_raise_the__sanity_waterline___and_how_much_will_this_help_\">What can we do to raise the \"sanity waterline,\" and how much will this help?</strong></p>\n</li>\n<li>\n<p><strong id=\"What_can_we_do_to_attract_more_funding__support__and_research_to_x_risk_reduction_and_to_the_specific_sub_problems_of_successful_Singularity_navigation_\">What can we do to attract more funding, support, and research to x-risk reduction and to the specific sub-problems of successful Singularity navigation?</strong></p>\n</li>\n<li>\n<p><strong id=\"Which_interventions_should_we_prioritize_\">Which interventions should we prioritize?</strong></p>\n</li>\n<li>\n<p><strong>How should x-risk reducers and AI safety researchers interact with governments and corporations?</strong> Does Drexler's interaction with the U.S. government regarding molecular nanotechnology provide any lessons for how AI risk researchers should act?</p>\n</li>\n<li>\n<p><strong id=\"How_can_optimal_philanthropists_get_the_most_x_risk_reduction_for_their_philanthropic_buck_\">How can optimal philanthropists get the most x-risk reduction for their philanthropic buck?</strong></p>\n</li>\n<li>\n<p><strong id=\"How_does_AI_risk_compare_to_other_existential_risks_\">How does AI risk compare to other existential risks?</strong></p>\n</li>\n<li>\n<p><strong id=\"Which_problems_do_we_need_to_solve__and_which_ones_can_we_have_an_AI_solve_\">Which problems do we need to solve, and which ones can we have an AI solve?</strong></p>\n</li>\n<li>\n<p><strong id=\"How_can_we_develop_microeconomic_models_of_WBEs_and_self_improving_systems_\">How can we develop microeconomic models of WBEs and self-improving systems?</strong></p>\n</li>\n<li>\n<p><strong id=\"How_can_we_be_sure_a_Friendly_AI_development_team_will_be_altruistic_\">How can we be sure a Friendly AI development team will be altruistic?</strong></p>\n</li>\n<li>\n<p><strong id=\"How_hard_is_it_to_create_Friendly_AI_\">How hard is it to create Friendly AI?</strong></p>\n</li>\n<li>\n<p><strong id=\"What_is_the_strength_of_feedback_from_neuroscience_to_AI_rather_than_brain_emulation_\">What is the strength of feedback from neuroscience to AI rather than brain emulation?</strong></p>\n</li>\n<li>\n<p><strong id=\"Is_there_a_safe_way_to_do_uploads__where_they_don_t_turn_into_neuromorphic_AI_\">Is there a safe way to do uploads, where they don't turn into neuromorphic AI?</strong></p>\n</li>\n<li>\n<p><strong id=\"How_much_must_we_spend_on_security_when_developing_a_Friendly_AI_team_\">How much must we spend on security when developing a Friendly AI team?</strong></p>\n</li>\n<li>\n<p><strong id=\"What_s_the_best_way_to_recruit_talent_toward_working_on_AI_risks_\">What's the best way to recruit talent toward working on AI risks?</strong></p>\n</li>\n<li>\n<p><strong id=\"How_difficult_is_stabilizing_the_world_so_we_can_work_on_Friendly_AI_slowly_\">How difficult is stabilizing the world so we can work on Friendly AI slowly?</strong></p>\n</li>\n<li>\n<p><strong>How hard will a takeoff be?</strong> To what degree is \"intelligence\" (as <a href=\"/lw/vb/efficient_crossdomain_optimization/\">efficient cross-domain optimization</a>) a matter of content vs. algorithms? How much does takeoff depend on slow, real-world experiments?</p>\n</li>\n<li>\n<p><strong id=\"What_is_the_value_of_strategy_vs__object_level_progress_toward_a_positive_Singularity_\">What is the value of strategy vs. object-level progress toward a positive Singularity?</strong></p>\n</li>\n<li>\n<p><strong id=\"What_different_kinds_of_Oracle_AI_are_there__and_are_any_of_them_both_safe_and_feasible_\">What different kinds of Oracle AI are there, and are any of them both safe and feasible?</strong></p>\n</li>\n<li>\n<p><strong>How much should we be worried about \"metacomputational hazards\"?</strong> E.g. should we worry about <a href=\"/lw/x4/nonperson_predicates/\">nonperson predicates</a>? Oracle AIs engaging in <a href=\"/lw/5bu/is_it_possible_to_build_a_safe_oracle_ai/3zbo\">self-fulfilling prophecies</a>? <a href=\"http://ordinaryideas.wordpress.com/2011/12/15/hazards/\">Acausal hijacking</a>?</p>\n</li>\n<li>\n<p><strong>What improvements can we make to the way we go about answering strategy questions?</strong> Wei Dai's <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/5yvd\">notes</a> on this question: \"For example, should we differentiate between \"strategic insights\" (such as Carl Shulman's insight that WBE-based Singletons may be feasible) and \"keeping track of the big picture\" (forming the overall strategy and updating it based on new insights and evidence), and aim to have people specialize in each, so that people deciding strategy won't be tempted to overweigh their own insights? Another example: is there a better way to combine probability estimates from multiple people?\"</p>\n</li>\n<li>\n<p><strong>How do people in other fields answer strategy questions?</strong> Wei Dai's <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/5yvd\">notes</a> on this question: \"Is there such a thing as a science or art of strategy that we can copy from (and perhaps improve upon with ideas from x-rationality)?\"</p>\n</li>\n</ol>\n<p>[more questions to come, as they are posted to the comments section]</p>", "sections": [{"title": "What can we do to reduce the risk of an AI arms race?", "anchor": "What_can_we_do_to_reduce_the_risk_of_an_AI_arms_race_", "level": 1}, {"title": "What can we do to raise the \"sanity waterline,\" and how much will this help?", "anchor": "What_can_we_do_to_raise_the__sanity_waterline___and_how_much_will_this_help_", "level": 1}, {"title": "What can we do to attract more funding, support, and research to x-risk reduction and to the specific sub-problems of successful Singularity navigation?", "anchor": "What_can_we_do_to_attract_more_funding__support__and_research_to_x_risk_reduction_and_to_the_specific_sub_problems_of_successful_Singularity_navigation_", "level": 1}, {"title": "Which interventions should we prioritize?", "anchor": "Which_interventions_should_we_prioritize_", "level": 1}, {"title": "How can optimal philanthropists get the most x-risk reduction for their philanthropic buck?", "anchor": "How_can_optimal_philanthropists_get_the_most_x_risk_reduction_for_their_philanthropic_buck_", "level": 1}, {"title": "How does AI risk compare to other existential risks?", "anchor": "How_does_AI_risk_compare_to_other_existential_risks_", "level": 1}, {"title": "Which problems do we need to solve, and which ones can we have an AI solve?", "anchor": "Which_problems_do_we_need_to_solve__and_which_ones_can_we_have_an_AI_solve_", "level": 1}, {"title": "How can we develop microeconomic models of WBEs and self-improving systems?", "anchor": "How_can_we_develop_microeconomic_models_of_WBEs_and_self_improving_systems_", "level": 1}, {"title": "How can we be sure a Friendly AI development team will be altruistic?", "anchor": "How_can_we_be_sure_a_Friendly_AI_development_team_will_be_altruistic_", "level": 1}, {"title": "How hard is it to create Friendly AI?", "anchor": "How_hard_is_it_to_create_Friendly_AI_", "level": 1}, {"title": "What is the strength of feedback from neuroscience to AI rather than brain emulation?", "anchor": "What_is_the_strength_of_feedback_from_neuroscience_to_AI_rather_than_brain_emulation_", "level": 1}, {"title": "Is there a safe way to do uploads, where they don't turn into neuromorphic AI?", "anchor": "Is_there_a_safe_way_to_do_uploads__where_they_don_t_turn_into_neuromorphic_AI_", "level": 1}, {"title": "How much must we spend on security when developing a Friendly AI team?", "anchor": "How_much_must_we_spend_on_security_when_developing_a_Friendly_AI_team_", "level": 1}, {"title": "What's the best way to recruit talent toward working on AI risks?", "anchor": "What_s_the_best_way_to_recruit_talent_toward_working_on_AI_risks_", "level": 1}, {"title": "How difficult is stabilizing the world so we can work on Friendly AI slowly?", "anchor": "How_difficult_is_stabilizing_the_world_so_we_can_work_on_Friendly_AI_slowly_", "level": 1}, {"title": "What is the value of strategy vs. object-level progress toward a positive Singularity?", "anchor": "What_is_the_value_of_strategy_vs__object_level_progress_toward_a_positive_Singularity_", "level": 1}, {"title": "What different kinds of Oracle AI are there, and are any of them both safe and feasible?", "anchor": "What_different_kinds_of_Oracle_AI_are_there__and_are_any_of_them_both_safe_and_feasible_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "20 comments"}], "headingsCount": 19}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["i2XoqtYEykc4XWp9B", "eXdGr2LeHYEgMh9qX", "yLeEPFnnB9wE7KLx2", "wqDRRx9RqwKLzWt7R"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-01T20:08:16.568Z", "modifiedAt": null, "url": null, "title": "Meetup : Graz Meetup", "slug": "meetup-graz-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:59.696Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "2xWKexdAKndQD34ps", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/StjqxGEmnKzu9FpjS/meetup-graz-meetup", "pageUrlRelative": "/posts/StjqxGEmnKzu9FpjS/meetup-graz-meetup", "linkUrl": "https://www.lesswrong.com/posts/StjqxGEmnKzu9FpjS/meetup-graz-meetup", "postedAtFormatted": "Sunday, April 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Graz%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Graz%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FStjqxGEmnKzu9FpjS%2Fmeetup-graz-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Graz%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FStjqxGEmnKzu9FpjS%2Fmeetup-graz-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FStjqxGEmnKzu9FpjS%2Fmeetup-graz-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 45, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8i'>Graz Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 April 2012 11:21:03PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Graz, Steiermark, Austria</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet up. Please upvote the comment post asking for attendance, and the subcomments for date, time, location, and any other suggestions.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8i'>Graz Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "StjqxGEmnKzu9FpjS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "14754", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Graz_Meetup\">Discussion article for the meetup : <a href=\"/meetups/8i\">Graz Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 April 2012 11:21:03PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Graz, Steiermark, Austria</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet up. Please upvote the comment post asking for attendance, and the subcomments for date, time, location, and any other suggestions.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Graz_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/8i\">Graz Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Graz Meetup", "anchor": "Discussion_article_for_the_meetup___Graz_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Graz Meetup", "anchor": "Discussion_article_for_the_meetup___Graz_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "15 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-01T21:12:11.269Z", "modifiedAt": null, "url": null, "title": "What is life?", "slug": "what-is-life", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:57.574Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XyaZZPLsAAWHEctPX/what-is-life", "pageUrlRelative": "/posts/XyaZZPLsAAWHEctPX/what-is-life", "linkUrl": "https://www.lesswrong.com/posts/XyaZZPLsAAWHEctPX/what-is-life", "postedAtFormatted": "Sunday, April 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20life%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20life%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXyaZZPLsAAWHEctPX%2Fwhat-is-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20life%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXyaZZPLsAAWHEctPX%2Fwhat-is-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXyaZZPLsAAWHEctPX%2Fwhat-is-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 460, "htmlBody": "<p>(This post is part of the attempt to do a write up after every Cambridge_UK meetup, of something raised at the meetup.)</p>\n<p>&nbsp;</p>\n<h2>What is life?</h2>\n<p>&nbsp;</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">Eliezer, in \"<a href=\"/lw/w0/the_first_world_takeover/\">The First World Takeover</a>\", talks about the search for objects with increasing numbers of bits of functional complexity. &nbsp;He framed the difference between life, and the previous non-living universe, as being the introduction of self-replicators who could search for self-replicators with an improved ability to search.</span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">In terms of organisms that share genes among a population via sexual reproduction, we have to consider this search mechanism (evolution by natural selection) to be a property of a population rather than of a single individual.</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">And, indeed, when we take into account the <a href=\"http://wiki.lesswrong.com/wiki/Speed_limit_and_complexity_bound_for_evolution\">speed limit and complexity bound for evolution</a>, what we're looking for is a population who search faster than the natural decay rate imposed by copying fidelity, genome size and how many harmful mutations get dropped each generation by the fraction of the population that don't reproduce.</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">And that is relative to the environment the population happens to find themselves situated in. &nbsp;Radiation affects mutation rate. &nbsp;Harshness of the environment (including competitors) determines whether the population is viable (replacement rate at least equals the death rate) or whether they will spiral down into extinction.</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">So maybe, rather than asking whether an individual or system of individuals counts as a life form, perhaps a more well defined question would be to ask whether they count as a viable searcher relative to a specific environment.</span></span></p>\n<p>&nbsp;</p>\n<h2>Why does that distinction matter?</h2>\n<p>&nbsp;</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">Are prions living? &nbsp;How about viruses? &nbsp; Crystals?</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">We know that <a href=\"http://rendell-attic.org/gol/tm.htm\">a turing complete machine can be implemented in Conway's Game of Life</a>&nbsp;and that, given a very specific environment, <a href=\"http://dl.acm.org/citation.cfm?id=2104449\">crystals can not only self-replicate, but also pass on information to their 'off-spring'</a>. &nbsp; So it would make sense to say that a system of crystals might be constructed that would be 'alive' relative to a specific environment that permitted the system to evolve - to 'search' not just for a variant replicator that might be improved in some limited way (such as replicating faster), but search unlimited bits of search space, including for an improved searcher.</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">Whereas a virus or prion would not count, unless in a particular environment the information they pass from generation to generation is able to alter the 'search' mechanism (the machinery that replicates them) in a way that can improve how it 'searches'.</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">The topic was raised at the meet-up, by the way, over the question of under what circumstances it would make sense to count as being alive, self-replicators made of <a href=\"http://www.mendeley.com/research/chemically-engineered-ribosomes-new-frontier-synthetic-biology/\">synthetic RNA</a>. &nbsp;A definitive answer wasn't reached. &nbsp; Any opinions?</span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">(Please note, I'm not an expert in these areas, just writing up the report, so any mistakes are mine, not those of the meet-up participants who generated the specific ideas.)</span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XyaZZPLsAAWHEctPX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 11, "extendedScore": null, "score": 8.763321223498948e-07, "legacy": true, "legacyId": "14755", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>(This post is part of the attempt to do a write up after every Cambridge_UK meetup, of something raised at the meetup.)</p>\n<p>&nbsp;</p>\n<h2 id=\"What_is_life_\">What is life?</h2>\n<p>&nbsp;</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">Eliezer, in \"<a href=\"/lw/w0/the_first_world_takeover/\">The First World Takeover</a>\", talks about the search for objects with increasing numbers of bits of functional complexity. &nbsp;He framed the difference between life, and the previous non-living universe, as being the introduction of self-replicators who could search for self-replicators with an improved ability to search.</span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">In terms of organisms that share genes among a population via sexual reproduction, we have to consider this search mechanism (evolution by natural selection) to be a property of a population rather than of a single individual.</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">And, indeed, when we take into account the <a href=\"http://wiki.lesswrong.com/wiki/Speed_limit_and_complexity_bound_for_evolution\">speed limit and complexity bound for evolution</a>, what we're looking for is a population who search faster than the natural decay rate imposed by copying fidelity, genome size and how many harmful mutations get dropped each generation by the fraction of the population that don't reproduce.</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">And that is relative to the environment the population happens to find themselves situated in. &nbsp;Radiation affects mutation rate. &nbsp;Harshness of the environment (including competitors) determines whether the population is viable (replacement rate at least equals the death rate) or whether they will spiral down into extinction.</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">So maybe, rather than asking whether an individual or system of individuals counts as a life form, perhaps a more well defined question would be to ask whether they count as a viable searcher relative to a specific environment.</span></span></p>\n<p>&nbsp;</p>\n<h2 id=\"Why_does_that_distinction_matter_\">Why does that distinction matter?</h2>\n<p>&nbsp;</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">Are prions living? &nbsp;How about viruses? &nbsp; Crystals?</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">We know that <a href=\"http://rendell-attic.org/gol/tm.htm\">a turing complete machine can be implemented in Conway's Game of Life</a>&nbsp;and that, given a very specific environment, <a href=\"http://dl.acm.org/citation.cfm?id=2104449\">crystals can not only self-replicate, but also pass on information to their 'off-spring'</a>. &nbsp; So it would make sense to say that a system of crystals might be constructed that would be 'alive' relative to a specific environment that permitted the system to evolve - to 'search' not just for a variant replicator that might be improved in some limited way (such as replicating faster), but search unlimited bits of search space, including for an improved searcher.</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">Whereas a virus or prion would not count, unless in a particular environment the information they pass from generation to generation is able to alter the 'search' mechanism (the machinery that replicates them) in a way that can improve how it 'searches'.</span></span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">The topic was raised at the meet-up, by the way, over the question of under what circumstances it would make sense to count as being alive, self-replicators made of <a href=\"http://www.mendeley.com/research/chemically-engineered-ribosomes-new-frontier-synthetic-biology/\">synthetic RNA</a>. &nbsp;A definitive answer wasn't reached. &nbsp; Any opinions?</span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">(Please note, I'm not an expert in these areas, just writing up the report, so any mistakes are mine, not those of the meet-up participants who generated the specific ideas.)</span></span></p>", "sections": [{"title": "What is life?", "anchor": "What_is_life_", "level": 1}, {"title": "Why does that distinction matter?", "anchor": "Why_does_that_distinction_matter_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "44 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["spKYZgoh3RmhxMqyu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-01T21:40:35.508Z", "modifiedAt": null, "url": null, "title": "Meetup : Rome LessWrong Meetup", "slug": "meetup-rome-lesswrong-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:03.691Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "pGkuD3jTixcf4NWhc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QjMvqKysBjBggGtfx/meetup-rome-lesswrong-meetup", "pageUrlRelative": "/posts/QjMvqKysBjBggGtfx/meetup-rome-lesswrong-meetup", "linkUrl": "https://www.lesswrong.com/posts/QjMvqKysBjBggGtfx/meetup-rome-lesswrong-meetup", "postedAtFormatted": "Sunday, April 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Rome%20LessWrong%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Rome%20LessWrong%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQjMvqKysBjBggGtfx%2Fmeetup-rome-lesswrong-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Rome%20LessWrong%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQjMvqKysBjBggGtfx%2Fmeetup-rome-lesswrong-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQjMvqKysBjBggGtfx%2Fmeetup-rome-lesswrong-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8j'>Rome LessWrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 April 2012 08:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Via della Gatta, 1/a, rome, italy</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Myself, along with two other LessWrongians, will be visiting Rome. I previously described setting up a meetup group in Budapest. <a href=\"http://lesswrong.com/lw/bc2/setting_up_lw_meetups_in_unlikely_places_positive/\" rel=\"nofollow\">http://lesswrong.com/lw/bc2/setting_up_lw_meetups_in_unlikely_places_positive/</a> I will be attempting the same in Rome. Do come and bring friends if you are in the area! Suggestions for a great central cafe/equivalent to host the meetup are very welcome - I will update the exact location once it is settled.</p>\n\n<p>UPDATE: Caff\u00e9 Doria as the meeting place.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8j'>Rome LessWrong Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QjMvqKysBjBggGtfx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.763439353215005e-07, "legacy": true, "legacyId": "14756", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Rome_LessWrong_Meetup\">Discussion article for the meetup : <a href=\"/meetups/8j\">Rome LessWrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 April 2012 08:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Via della Gatta, 1/a, rome, italy</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Myself, along with two other LessWrongians, will be visiting Rome. I previously described setting up a meetup group in Budapest. <a href=\"http://lesswrong.com/lw/bc2/setting_up_lw_meetups_in_unlikely_places_positive/\" rel=\"nofollow\">http://lesswrong.com/lw/bc2/setting_up_lw_meetups_in_unlikely_places_positive/</a> I will be attempting the same in Rome. Do come and bring friends if you are in the area! Suggestions for a great central cafe/equivalent to host the meetup are very welcome - I will update the exact location once it is settled.</p>\n\n<p>UPDATE: Caff\u00e9 Doria as the meeting place.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Rome_LessWrong_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/8j\">Rome LessWrong Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Rome LessWrong Meetup", "anchor": "Discussion_article_for_the_meetup___Rome_LessWrong_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Rome LessWrong Meetup", "anchor": "Discussion_article_for_the_meetup___Rome_LessWrong_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4YMFSdxWSK9JgQHDv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-02T02:10:20.559Z", "modifiedAt": null, "url": null, "title": "Fictional Bias", "slug": "fictional-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:55.101Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "thomblake", "createdAt": "2009-02-27T15:35:08.282Z", "isAdmin": false, "displayName": "thomblake"}, "userId": "zCHE6bXWKB6kfJsJS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pDTEB7qjXiPxfe5Ri/fictional-bias", "pageUrlRelative": "/posts/pDTEB7qjXiPxfe5Ri/fictional-bias", "linkUrl": "https://www.lesswrong.com/posts/pDTEB7qjXiPxfe5Ri/fictional-bias", "postedAtFormatted": "Monday, April 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fictional%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFictional%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpDTEB7qjXiPxfe5Ri%2Ffictional-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fictional%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpDTEB7qjXiPxfe5Ri%2Ffictional-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpDTEB7qjXiPxfe5Ri%2Ffictional-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 510, "htmlBody": "<p>As rationalists, we are trained to maintain constant vigilance against common errors in our own thinking. &nbsp;Still, we must be especially careful of biases that are unusually common amongst our kind.</p>\n<p>Consider the following scenario: Frodo Baggins is buying pants. &nbsp;Which of these is he most likely to buy:</p>\n<p>(a) 32/30</p>\n<p>(b) 48/32</p>\n<p>(c) 30/20</p>\n<p><a id=\"more\"></a></p>\n<p>If you're like me, your answer is (c). &nbsp;Frodo, as we know, is about 4' tall, so his inseam is much more likely 20'' than 30''.</p>\n<p>But like me, you'd be wrong. &nbsp;Since there aren't *actually* any hobbits, all we know is that we're talking about a person named Frodo Baggins, who is male. &nbsp;And the most common pants size is 32/30, so the correct answer, given our actual state of knowledge about the real world, is (a).</p>\n<p>This is what researchers call Fictional Bias, and there is evidence that it affects virtually every domain of decision-making. &nbsp;The mistake is using information from fictional sources in real contexts. &nbsp;It is the more-pernicious cousin of <a href=\"http://wiki.lesswrong.com/wiki/Generalization_from_fictional_evidence\">generalizing from fictional evidence</a>&nbsp;- instead of merely generalizing, we treat real objects and persons as though they are the specific fictional entities they resemble.</p>\n<p>We're of course familiar with particularly egregious examples of people confusing fiction with reality. &nbsp;For example in the 1930's, there were multiple cases where someone was killed for having the same name as the serial killer of children from the movie M. &nbsp;But these could be written off as merely disturbed individuals. &nbsp;But as it turns out, we're affected by this bias in our daily lives.</p>\n<p>Examples abound in the literature, though the name \"fictional bias\" is not always used. &nbsp;A 1984 study by&nbsp;<span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px;\">Dr. Sidney Zweibel and&nbsp;</span><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px;\">Dr. Emilio Lizardo</span><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px;\">&nbsp;asked subjects to trust someone with an unusual name. &nbsp;Subjects were 70% less likely to trust when the person had the same name as a fictional villain. &nbsp;</span><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19px;\">A 1989 study by Dr.&nbsp;</span></span><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px;\">Wayne Szalinski established that subjects were 89% more likely to agree to take an experimental drug, when it was named after a fictional drug (for example&nbsp;</span><span style=\"background-color: #f9f9f9; font-family: sans-serif; font-size: 13px; line-height: 19px;\"><em>Ephemerol</em>).</span></p>\n<p><span style=\"background-color: #f9f9f9; font-family: sans-serif; font-size: 13px; line-height: 19px;\">The moral? &nbsp;It stands to reason that we need to be careful where our intuitive inferences come from. &nbsp;For a group that consumes so much science fiction and fantasy, we must be especially on our guard. &nbsp;(Also, this post was an April Fools prank; the effect may or may not be real, and all citations are either irrelevant or fictional.) &nbsp;It might even behoove us to discourage the reading of fiction amongst aspiring rationalists.</span></p>\n<hr />\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Kahneman, D. and Frederick, S. 2002. Representativeness revisited: Attribute substitution in intuitive judgment. Pp 49-81 in Gilovich, T., Griffin, D. and Kahneman, D., eds.&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Heuristics and Biases: The Psychology of Intuitive Judgment</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">. Cambridge University Press, Cambridge.</span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Szalinski, W. 1989. &nbsp;Recollection of fictional medication. <em>Journal of Cognitive Minification</em>, <strong>67</strong>: 173-186.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Tversky, A. and Kahneman, D. 1982. Judgments of and by representativeness. Pp 84-98 in Kahneman, D., Slovic, P., and Tversky, A., eds.&nbsp;<em>Judgment under uncertainty: Heuristics and biases.</em>&nbsp;New York: Cambridge University Press.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Tversky, A. and Kahneman, D. 1983. Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment.&nbsp;<em>Psychological Review</em>,&nbsp;<strong>90:</strong>&nbsp;293-315.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">Zweibel, S. and Lizardo, E. 1985. Fictional bias in interpersonal trust. &nbsp;<em>Cross-Dimensional Neurosurgery</em>, <strong>45</strong>: 307-324.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pDTEB7qjXiPxfe5Ri", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 79, "baseScore": -2, "extendedScore": null, "score": 8.764559431196935e-07, "legacy": true, "legacyId": "14734", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-02T02:47:45.987Z", "modifiedAt": null, "url": null, "title": "April Fools - Harry Potter and the Methods of Rationality Joke Chapter", "slug": "april-fools-harry-potter-and-the-methods-of-rationality-joke", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:30.009Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shokwave", "createdAt": "2010-10-12T12:55:00.568Z", "isAdmin": false, "displayName": "shokwave"}, "userId": "jtjgXtj7FepKrQPGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dGwr3gJJuEvxwptHy/april-fools-harry-potter-and-the-methods-of-rationality-joke", "pageUrlRelative": "/posts/dGwr3gJJuEvxwptHy/april-fools-harry-potter-and-the-methods-of-rationality-joke", "linkUrl": "https://www.lesswrong.com/posts/dGwr3gJJuEvxwptHy/april-fools-harry-potter-and-the-methods-of-rationality-joke", "postedAtFormatted": "Monday, April 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20April%20Fools%20-%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20Joke%20Chapter&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AApril%20Fools%20-%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20Joke%20Chapter%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdGwr3gJJuEvxwptHy%2Fapril-fools-harry-potter-and-the-methods-of-rationality-joke%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=April%20Fools%20-%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20Joke%20Chapter%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdGwr3gJJuEvxwptHy%2Fapril-fools-harry-potter-and-the-methods-of-rationality-joke", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdGwr3gJJuEvxwptHy%2Fapril-fools-harry-potter-and-the-methods-of-rationality-joke", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<p><strong>This was an april fools joke.</strong></p>\n<p>&nbsp;</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; \">This is a new thread to discuss <del>Eliezer Yudkowsky&rsquo;s</del> my chapter of&nbsp;<em style=\"font-style: italic; \"><span style=\"color: #8a8a8b;\"><span style=\"text-decoration: underline;\"><a href=\"http://www.freefanfic.net/s/5782108/82/Harry_Potter_and_the_Methods_of_Rationality/\">Harry Potter and the Methods of Rationality</a></span></span></em>&nbsp;and anything related to it. This thread is intended for discussing the fake April Fools chapter, which is now published.&nbsp;I suggest refraining from reading comments here <strong style=\"font-weight: bold; \">until you <a href=\"http://www.freefanfic.net/s/5782108/82/Harry_Potter_and_the_Methods_of_Rationality/\">read chapter 82</a>.</strong>&nbsp;After you've read chapter 82, I suggest all discussion of this chapter to be kept here, with links to comments in the previous thread.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hNFdS3rRiYgqqD8aM": 2, "etDohXtBrXd8WqCtR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dGwr3gJJuEvxwptHy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 29, "extendedScore": null, "score": 6.3e-05, "legacy": true, "legacyId": "14768", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-02T03:50:14.620Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Where Philosophy Meets Science", "slug": "seq-rerun-where-philosophy-meets-science", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:53.442Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v9qjNWR852y3Hy8yY/seq-rerun-where-philosophy-meets-science", "pageUrlRelative": "/posts/v9qjNWR852y3Hy8yY/seq-rerun-where-philosophy-meets-science", "linkUrl": "https://www.lesswrong.com/posts/v9qjNWR852y3Hy8yY/seq-rerun-where-philosophy-meets-science", "postedAtFormatted": "Monday, April 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Where%20Philosophy%20Meets%20Science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Where%20Philosophy%20Meets%20Science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv9qjNWR852y3Hy8yY%2Fseq-rerun-where-philosophy-meets-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Where%20Philosophy%20Meets%20Science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv9qjNWR852y3Hy8yY%2Fseq-rerun-where-philosophy-meets-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv9qjNWR852y3Hy8yY%2Fseq-rerun-where-philosophy-meets-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<p>Today's post, <a href=\"/lw/pg/where_philosophy_meets_science/\">Where Philosophy Meets Science</a> was originally published on 12 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>In retrospect, supposing that quantum physics had anything to do with consciousness was a big mistake. Could philosophers have told the physicists so? But we don't usually see philosophers sponsoring major advances in physics; why not?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bdh/seq_rerun_distinct_configurations/\">Distinct Configurations</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v9qjNWR852y3Hy8yY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 8.764976960802532e-07, "legacy": true, "legacyId": "14769", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Bh9cdfMjATrTdLrGH", "RNG7G4dDmYSvfowcJ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-02T11:46:22.896Z", "modifiedAt": null, "url": null, "title": "Werewolf, Cambridge UK Less Wrong Meetup April 1st 2012", "slug": "werewolf-cambridge-uk-less-wrong-meetup-april-1st-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:53.296Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Clarity1992", "createdAt": "2010-12-20T15:00:17.085Z", "isAdmin": false, "displayName": "Clarity1992"}, "userId": "YyKn2drJ3MKKXiam7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cdzwDTu4LDCq3LSoG/werewolf-cambridge-uk-less-wrong-meetup-april-1st-2012", "pageUrlRelative": "/posts/cdzwDTu4LDCq3LSoG/werewolf-cambridge-uk-less-wrong-meetup-april-1st-2012", "linkUrl": "https://www.lesswrong.com/posts/cdzwDTu4LDCq3LSoG/werewolf-cambridge-uk-less-wrong-meetup-april-1st-2012", "postedAtFormatted": "Monday, April 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Werewolf%2C%20Cambridge%20UK%20Less%20Wrong%20Meetup%20April%201st%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWerewolf%2C%20Cambridge%20UK%20Less%20Wrong%20Meetup%20April%201st%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcdzwDTu4LDCq3LSoG%2Fwerewolf-cambridge-uk-less-wrong-meetup-april-1st-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Werewolf%2C%20Cambridge%20UK%20Less%20Wrong%20Meetup%20April%201st%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcdzwDTu4LDCq3LSoG%2Fwerewolf-cambridge-uk-less-wrong-meetup-april-1st-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcdzwDTu4LDCq3LSoG%2Fwerewolf-cambridge-uk-less-wrong-meetup-april-1st-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1082, "htmlBody": "<p style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\"><em><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">There is already </span></span><a style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\" title=\"What is life?\" href=\"/r/discussion/lw/bdv/what_is_life/\">a post related to this meetup</a><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">&nbsp;but it&nbsp;concerns a discussion which took place after I had left so I will write about the games of Werewolf. Please post your thoughts too and correct any inaccuracies.</span></span></span></span></em></span></span><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\"><strong><br /></strong></span></span></p>\r\n<p style=\"text-align: justify;\"><strong>Thoughts:</strong></p>\r\n<ul>\r\n<li><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; \">Most people said that this was very good fun and I suspect those that didn't still really enjoyed it.<br /><br /></span></li>\r\n<li><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; \">Each game lasted about 20 minutes.<br /><br /></span></li>\r\n<li><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; \"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; line-height: normal;\"><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">I was late and observed the first game. I remember&nbsp;<span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; line-height: normal;\">Ai was given a werewolf card but she didn't realise so the game was played with her as a villager.<br /><br /></span></span></span></span></li>\r\n<li><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; \"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; line-height: normal;\"><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; line-height: normal;\">W</span></span></span></span><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">hen Douglas suggested people give reasons for lynching Thomas one that stood out was \"he talks too much\". This seems to go with Douglas' later observation that the game is all about information, whether that is obtained by careful choice of sheriff/lynching to maximise what is learned next round or by picking up on what people have said, how they have said it, and how much they have said. Personally I played it very much on instinct and watching for tells, letting others do the logical reasoning (!).<br /><br /></span></li>\r\n<li><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; \"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; line-height: normal; \">Jon left after game one. There was some discussion about whether he was coming back. \"His body language seemed dismissive like 'nah, I'm not into this'\", \"Really? I didn't get that impression!\", \"I disagree with your analysis. Past evidence of Jon leaving suggests he will return\", \"I think he would have said goodbye if he wasn't coming back. Since he didn't I assume he is returning\". I found it interesting how we applied rationality principles to this.<br /><br /></span></span></li>\r\n<li><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; \">Generally the sheriff/lynching discussions would begin with sincere considerations of outcome trees then as soon as anyone said \"but that's what you'd say if you were a werewolf!\" or \"she seemed a little quick to agree with that!\" or \"he's swallowing a lot while talking!\" it switched to accusations and double bluffs.<br /><br /></span></li>\r\n<li><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; \">There were quite a few pieces of reasoning relating to proximity to people. e.g. \"I'm sure I heard movement next to me 'last night'\". My immediate instinct was that this is outside of the rules and unsporting, but obviously that isn't the case with this game!<br /><br /></span></li>\r\n<li><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; \">Something I found especially inspired was Alexey (as a werewolf) in game two claiming to be the seer after Thomas (the actual seer) had already told everyone that he himself was. Alexey argued that he had withheld the information to see who would try to pretend to be the seer and then he would know who one of the werewolves was. Most people weren't convinced but it was very entertaining.<br /><br /></span></li>\r\n<li><span style=\"line-height: 19px; font-family: Arial, Helvetica, sans-serif; \">We decided, on Alexey's suggestion, that a coin toss is acceptable to decide a tied vote. Jonathan remarked that British coins land on heads 53 times out of 100. Does anyone have a link for that?<br /><br /></span></li>\r\n<li><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; \">Douglas did a great job giving the game some life with the storytelling style of delivery. I don't know what the proper term for this is, or whether you're traditionally supposed to play werewolves that way (I suspect you are), but it was cool. As was Thomas' replication of it when he was GM.<br /><br /></span></li>\r\n<li><span style=\"text-align: -webkit-auto; \">Ramana spent the most time dead and made the point that it's very different watching from the outside compared to playing. He said you can perceive much better what people are trying to do and who is gullible.<br /><br /></span></li>\r\n<li><span style=\"text-align: -webkit-auto; \">Douglas explained that for the villagers it is always best to lynch someone because otherwise the next day you'll just be in the exact same position with one less villagers' vote against the same number of werewolves' votes. This seems definitely true, but oddly&nbsp;</span><span style=\"text-align: -webkit-auto; \">counter-intuitive</span><span style=\"text-align: -webkit-auto; \">&nbsp;given that you're more likely to lynch a villager by mistake, the more of them you have.<br /><br /></span></li>\r\n<li><span style=\"text-align: -webkit-auto; \">Between games three and four there was a false start because someone had forgotten they had a werewolf card and then suddenly and noisily realised they were supposed to have their eyes open. Oops!<br /><br /></span></li>\r\n<li>I hadn't&nbsp;played before but was familiar with the concept and had been meaning to try it with friends for a long time. If you're in a similar position, then bump it up your priority list. It's awesome!</li>\r\n</ul>\r\n<div>\r\n<p>--------------------------------------------------------------------------------------------------------------------------------------</p>\r\n<p>&nbsp;</p>\r\n<p><a id=\"more\"></a></p>\r\n<p style=\"text-align: justify;\"><strong>Games:<br />1&nbsp;</strong><em>GM: {Douglas}, Villagers: {Ramana, Jon, Jonathan, Alexey, Ai}, Werewolf: {Thomas}<br /></em>Jon mauled. No-one lynched. Jonathan mauled. Thomas lynched.&nbsp;Villagers win.</p>\r\n<p style=\"text-align: justify;\"><strong>2 (Seer added)&nbsp;</strong><em>GM: {Douglas}, Villagers: {James, Jonathan, Alexey}, Werewolves: {Thomas, Ai}, Seer {Ramana}<br /></em>Ramana mauled. Jonathan made Sheriff. No-one lynched. Alexey mauled. Ai lynched. Jonathan mauled. James lynched.&nbsp;Werewolves win.</p>\r\n<p style=\"text-align: justify;\"><strong>3 (Sheriff's deputy and wills added)&nbsp;</strong><em>GM: {Douglas}, Villagers: {James, Jonathan, Ai}, Werewolves: {Ramana, Alexey}, Seer: {Thomas}<br /></em>Jonathan mauled. James made Sheriff. Ramana lynched. Thomas mauled. Alexey lynched.&nbsp;Villagers win.</p>\r\n<p style=\"text-align: justify;\"><strong>4&nbsp;</strong><em>GM: {Thomas}, Villagers: {Douglas, Jonathan, Alexey}, Werewolves {James, Ai}; Seer {Ramana}.<br /></em>Ramana mauled. Alexey made Sheriff. Douglas lynched. Alexey mauled. Sheriff passed to Ai. Jonathan lynched.&nbsp;Werewolves win.</p>\r\n</div>\r\n<p>--------------------------------------------------------------------------------------------------------------------------------------</p>\r\n<p style=\"text-align: -webkit-auto;\"><em style=\"text-align: justify;\">This post was partly motivated by this comment in the google group:</em></p>\r\n<blockquote>\r\n<p style=\"text-align: justify;\"><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; text-align: -webkit-auto; background-color: rgba(255, 255, 255, 0.917969); \">So, I'd observe that</span><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; text-align: -webkit-auto; background-color: rgba(255, 255, 255, 0.917969); \" /><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; text-align: -webkit-auto; background-color: rgba(255, 255, 255, 0.917969); \">&nbsp;&nbsp; \"we tried a social meeting, we played werewolf [also called Mafia], it went well/badly etc.\"</span><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; text-align: -webkit-auto; background-color: rgba(255, 255, 255, 0.917969); \" /><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; text-align: -webkit-auto; background-color: rgba(255, 255, 255, 0.917969); \">is far more useful data to the community that</span><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; text-align: -webkit-auto; background-color: rgba(255, 255, 255, 0.917969); \" /><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; text-align: -webkit-auto; background-color: rgba(255, 255, 255, 0.917969); \">&nbsp;&nbsp; \"we failed to call taboo on a contentious term, and thus displayed massive confusion\"</span><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; text-align: -webkit-auto; background-color: rgba(255, 255, 255, 0.917969); \" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; text-align: -webkit-auto; background-color: rgba(255, 255, 255, 0.917969); \" /><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; text-align: -webkit-auto; background-color: rgba(255, 255, 255, 0.917969); \">From a community building perspective, the former is useful and the latter is not. From an informational perspective, the first is null and the latter is a net negative, in that it looks like it might contain content but does not.</span></p>\r\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cdzwDTu4LDCq3LSoG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 8.766958211187528e-07, "legacy": true, "legacyId": "14787", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XyaZZPLsAAWHEctPX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-02T15:53:15.967Z", "modifiedAt": null, "url": null, "title": "Generic Modafinil sales begin? ", "slug": "generic-modafinil-sales-begin", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:01.021Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/g6kKTtNevCwGxa5px/generic-modafinil-sales-begin", "pageUrlRelative": "/posts/g6kKTtNevCwGxa5px/generic-modafinil-sales-begin", "linkUrl": "https://www.lesswrong.com/posts/g6kKTtNevCwGxa5px/generic-modafinil-sales-begin", "postedAtFormatted": "Monday, April 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Generic%20Modafinil%20sales%20begin%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGeneric%20Modafinil%20sales%20begin%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg6kKTtNevCwGxa5px%2Fgeneric-modafinil-sales-begin%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Generic%20Modafinil%20sales%20begin%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg6kKTtNevCwGxa5px%2Fgeneric-modafinil-sales-begin", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg6kKTtNevCwGxa5px%2Fgeneric-modafinil-sales-begin", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<p>Due to <a href=\"http://en.wikipedia.org/wiki/Modafinil#Patent_protection_and_antitrust_litigation\">agreements with the patent holder</a>, Cephalon, that were made in 2005-2006 several generics&nbsp;manufacturers are now allowed to sell generic <a href=\"http://en.wikipedia.org/wiki/Modafinil\">Modafinil</a>. I have found <a href=\"http://www.thepharmaletter.com/file/112273/teva-debuts-generic-of-provigil-avapro-and-avalide-ranbaxy-crestor-copy-oked-in-canada.html\">confirmation</a>&nbsp;that the manufacturer Teva has begun selling generic Modafinil, though I haven't seen news about other manufacturers. However, the article also, says that Cephalon is a subsidiary of Teva, so perhaps this won't have an effect? The Modafinil patent is set to expire in April 2015.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "g6kKTtNevCwGxa5px", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 22, "extendedScore": null, "score": 8.767985832069279e-07, "legacy": true, "legacyId": "14788", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-02T18:27:20.780Z", "modifiedAt": null, "url": null, "title": "Cryonics on LessWrong vs at LessWrong meetups", "slug": "cryonics-on-lesswrong-vs-at-lesswrong-meetups", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:06.685Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SEAQ4mqkTiW7EedEy/cryonics-on-lesswrong-vs-at-lesswrong-meetups", "pageUrlRelative": "/posts/SEAQ4mqkTiW7EedEy/cryonics-on-lesswrong-vs-at-lesswrong-meetups", "linkUrl": "https://www.lesswrong.com/posts/SEAQ4mqkTiW7EedEy/cryonics-on-lesswrong-vs-at-lesswrong-meetups", "postedAtFormatted": "Monday, April 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cryonics%20on%20LessWrong%20vs%20at%20LessWrong%20meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACryonics%20on%20LessWrong%20vs%20at%20LessWrong%20meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEAQ4mqkTiW7EedEy%2Fcryonics-on-lesswrong-vs-at-lesswrong-meetups%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cryonics%20on%20LessWrong%20vs%20at%20LessWrong%20meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEAQ4mqkTiW7EedEy%2Fcryonics-on-lesswrong-vs-at-lesswrong-meetups", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEAQ4mqkTiW7EedEy%2Fcryonics-on-lesswrong-vs-at-lesswrong-meetups", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 68, "htmlBody": "<p>When I've brought up cryonics on LessWrong [1][2], most commenters have said I'm being too pessimistic.&nbsp; When I brought it up yesterday at the Cambridge MA meetup, most people thought I was too optimistic.&nbsp; (I think it could work, but there are enough things that could go wrong that it's ~1000:1 against.)&nbsp; What makes the groups so different on this?</p>\n<p>[1] <a href=\"/lw/b93/brain_preservation\">Brain Preservation</a></p>\n<p>[2] <a href=\"/lw/7sj/how_likely_is_cryonics_to_work/\">How Likely is Cryonics to Work</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 2, "izp6eeJJEg9v5zcur": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SEAQ4mqkTiW7EedEy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 8.768627276259546e-07, "legacy": true, "legacyId": "14790", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nk9928vPqoeAMrTh6", "NEpZGLNMGc447ez34"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-02T21:21:38.043Z", "modifiedAt": null, "url": null, "title": "Partly-baked ideas", "slug": "partly-baked-ideas", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:50.561Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZqcFPCXKJRnuQMZhy/partly-baked-ideas", "pageUrlRelative": "/posts/ZqcFPCXKJRnuQMZhy/partly-baked-ideas", "linkUrl": "https://www.lesswrong.com/posts/ZqcFPCXKJRnuQMZhy/partly-baked-ideas", "postedAtFormatted": "Monday, April 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Partly-baked%20ideas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APartly-baked%20ideas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZqcFPCXKJRnuQMZhy%2Fpartly-baked-ideas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Partly-baked%20ideas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZqcFPCXKJRnuQMZhy%2Fpartly-baked-ideas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZqcFPCXKJRnuQMZhy%2Fpartly-baked-ideas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 249, "htmlBody": "<p>I.J. Good, from the opening of 1962's&nbsp;<a style=\"font-style: italic;\" href=\"http://www.amazon.com/The-Scientist-Speculates-anthology-partly-baked/dp/B000AMMQ5A/\">The Scientist Speculates</a>&nbsp;(a collection of partly-baked ideas):</p>\n<blockquote>\n<p>A partly-baked idea or PBI is either a speculation, a question of some novelty, a suggestion for a novel experiment, a stimulating analogy, or (rarely) a classification. It has a bakedness of <em>p</em>&nbsp;that is less than unity, or even negative. The bakedness of an idea should be judged by its potential value, the chance that it can be completely baked, its originality, interest, stimulation, conciseness, lucidity, and liveliness. It is often better to be stimulating and wrong than boring and right.</p>\n<p>A very rough guide to the maximum length that a PBI should have is given by the formula</p>\n<p>10^(9<em>px</em>/2) words</p>\n<p>where <em>x</em>, the importance of the topic, is between 0 and 1. For example, the maximum length for a negatively-baked idea is less than one word. An idea can compensate in importance what it lacks in bakedness, and conversely. The formula is applicable to each sentence and to each paragraph, as well as to the whole of a contribution. For the non-specialist, the formula makes sense even when <em>px</em>&nbsp;= 1, but in this anthology <em>px</em>&nbsp;rarely exceeds 7/9.</p>\n<p>A possible justification for the exponential or antilogarithmic form is that if an idea is developed to a certain length <em>d</em>, then the size of the expository tree increases roughly exponentially with <em>d</em>, if the multifurcation of the tree is the same at every level.</p>\n</blockquote>\n<p>(Note that I changed the formatting a bit for readability.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZqcFPCXKJRnuQMZhy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 8.769352945545943e-07, "legacy": true, "legacyId": "14791", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-03T00:42:04.135Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes April 2012", "slug": "rationality-quotes-april-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:23.932Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oscar_Cunningham", "createdAt": "2009-09-18T13:28:22.764Z", "isAdmin": false, "displayName": "Oscar_Cunningham"}, "userId": "G2SZuAiaBaNPg9rBt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EbQWLTuxwJpJFdebn/rationality-quotes-april-2012", "pageUrlRelative": "/posts/EbQWLTuxwJpJFdebn/rationality-quotes-april-2012", "linkUrl": "https://www.lesswrong.com/posts/EbQWLTuxwJpJFdebn/rationality-quotes-april-2012", "postedAtFormatted": "Tuesday, April 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20April%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20April%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEbQWLTuxwJpJFdebn%2Frationality-quotes-april-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20April%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEbQWLTuxwJpJFdebn%2Frationality-quotes-april-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEbQWLTuxwJpJFdebn%2Frationality-quotes-april-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Here's the new thread for posting quotes, with the usual rules:</span></p>\n<ul>\n<li><span style=\"font-family: Arial,Helvetica,sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Please post all quotes separately, so that they can be voted up/down separately. &nbsp;(If they are strongly related, reply to your own comments. &nbsp;If strongly ordered, then go ahead and post them together.)</span></li>\n<li><span style=\"font-family: Arial,Helvetica,sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Do not quote yourself</span></li>\n<li><span style=\"font-family: Arial,Helvetica,sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Do not quote comments/posts on LW/OB</span></li>\n<li><span style=\"font-family: Arial,Helvetica,sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">No more than 5 quotes per person per monthly thread, please.</span></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EbQWLTuxwJpJFdebn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 8.770185678574707e-07, "legacy": true, "legacyId": "14748", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 867, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-03T01:42:15.905Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Can You Prove Two Particles Are Identical?", "slug": "seq-rerun-can-you-prove-two-particles-are-identical", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:55.220Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y7nY9A9DA4jNnsWth/seq-rerun-can-you-prove-two-particles-are-identical", "pageUrlRelative": "/posts/y7nY9A9DA4jNnsWth/seq-rerun-can-you-prove-two-particles-are-identical", "linkUrl": "https://www.lesswrong.com/posts/y7nY9A9DA4jNnsWth/seq-rerun-can-you-prove-two-particles-are-identical", "postedAtFormatted": "Tuesday, April 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Can%20You%20Prove%20Two%20Particles%20Are%20Identical%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Can%20You%20Prove%20Two%20Particles%20Are%20Identical%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy7nY9A9DA4jNnsWth%2Fseq-rerun-can-you-prove-two-particles-are-identical%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Can%20You%20Prove%20Two%20Particles%20Are%20Identical%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy7nY9A9DA4jNnsWth%2Fseq-rerun-can-you-prove-two-particles-are-identical", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy7nY9A9DA4jNnsWth%2Fseq-rerun-can-you-prove-two-particles-are-identical", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 213, "htmlBody": "<p>Today's post, <a href=\"/lw/ph/can_you_prove_two_particles_are_identical/\">Can You Prove Two Particles Are Identical?</a> was originally published on 14 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>You wouldn't think that it would be possible to do an experiment that told you that two particles are completely identical - not just to the limit of experimental precision, but perfectly. You could even give a precise-sounding philosophical argument for why it was not possible - but the argument would have a deeply buried assumption. Quantum physics violates this deep assumption, making the experiment easy.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/be9/seq_rerun_where_philosophy_meets_science/\">Where Philosophy Meets Science</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y7nY9A9DA4jNnsWth", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 8.770438314510792e-07, "legacy": true, "legacyId": "14794", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Bp8vnEciPA5TXSy6f", "v9qjNWR852y3Hy8yY", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-03T03:08:48.937Z", "modifiedAt": null, "url": null, "title": "John Danaher on 'The Superintelligent Will'", "slug": "john-danaher-on-the-superintelligent-will", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:51.087Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DDxntjTYjRbZF9xTd/john-danaher-on-the-superintelligent-will", "pageUrlRelative": "/posts/DDxntjTYjRbZF9xTd/john-danaher-on-the-superintelligent-will", "linkUrl": "https://www.lesswrong.com/posts/DDxntjTYjRbZF9xTd/john-danaher-on-the-superintelligent-will", "postedAtFormatted": "Tuesday, April 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20John%20Danaher%20on%20'The%20Superintelligent%20Will'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJohn%20Danaher%20on%20'The%20Superintelligent%20Will'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDDxntjTYjRbZF9xTd%2Fjohn-danaher-on-the-superintelligent-will%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=John%20Danaher%20on%20'The%20Superintelligent%20Will'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDDxntjTYjRbZF9xTd%2Fjohn-danaher-on-the-superintelligent-will", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDDxntjTYjRbZF9xTd%2Fjohn-danaher-on-the-superintelligent-will", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 228, "htmlBody": "<p>Philosopher <a href=\"https://sites.google.com/site/johndanaher84/\">John Danaher</a> has written <a href=\"http://philosophicaldisquisitions.blogspot.com/2012/04/bostrom-on-superintelligence-and.html\">an explication and critique</a> of Bostrom's \"orthogonality thesis\" from \"<a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">The Superintelligent Will</a>.\" To quote the conclusion:</p>\n<blockquote>\n<p>&nbsp;</p>\n<p>Summing up, in this post I&rsquo;ve considered Bostrom&rsquo;s discussion of the orthogonality thesis. According to this thesis, any level of intelligence is, within certain weak constraints, compatible with any type of final goal. If true, the thesis might provide support for those who think it possible to create a benign superintelligence. But, as I have pointed out, Bostrom&rsquo;s defence of the orthogonality thesis is lacking in certain respects, particularly in his somewhat opaque and cavalier dismissal of normatively thick theories of rationality.</p>\n<p>As it happens, none of this may affect what Bostrom has to say about unfriendly superintelligences. His defence of that argument relies on the convergence thesis, not the orthogonality thesis. If the orthogonality thesis turns out to be false, then all that happens is that the kind of convergence Bostrom alludes to simply occurs at a higher level in the AI&rsquo;s goal architecture.&nbsp;</p>\n<p>What might, however, be significant is whether the higher-level convergence is a convergence towards certain moral beliefs or a convergence toward nihilistic beliefs. If it is the former, then friendliness might be necessitated, not simply possible. If it is the latter, then all bets are off. A nihilistic agent could do pretty anything since, no goals would be rationally entailed.</p>\n<p>&nbsp;</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"BXL4riEJvJJHoydjG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DDxntjTYjRbZF9xTd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 8.770798796135228e-07, "legacy": true, "legacyId": "14801", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-03T04:15:09.856Z", "modifiedAt": null, "url": null, "title": "LessWrong downtime 2012-03-26, and site speed", "slug": "lesswrong-downtime-2012-03-26-and-site-speed", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:53.693Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LSeSdq4SE5dDu5atX/lesswrong-downtime-2012-03-26-and-site-speed", "pageUrlRelative": "/posts/LSeSdq4SE5dDu5atX/lesswrong-downtime-2012-03-26-and-site-speed", "linkUrl": "https://www.lesswrong.com/posts/LSeSdq4SE5dDu5atX/lesswrong-downtime-2012-03-26-and-site-speed", "postedAtFormatted": "Tuesday, April 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20downtime%202012-03-26%2C%20and%20site%20speed&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20downtime%202012-03-26%2C%20and%20site%20speed%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLSeSdq4SE5dDu5atX%2Flesswrong-downtime-2012-03-26-and-site-speed%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20downtime%202012-03-26%2C%20and%20site%20speed%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLSeSdq4SE5dDu5atX%2Flesswrong-downtime-2012-03-26-and-site-speed", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLSeSdq4SE5dDu5atX%2Flesswrong-downtime-2012-03-26-and-site-speed", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 230, "htmlBody": "<p>Our investigation into last week's LW downtime is complete:&nbsp;<a style=\"font-family: arial;\" href=\"https://docs.google.com/document/d/1IXYAjoQQgzDx6xAyefIYCRgKrDlsldzfT-TORTt1JiQ/view?pli=1\">here</a>&nbsp;(Google Docs).</p>\n<p><strong>Executive summary:</strong></p>\n<p>We failed to update our&nbsp;<a href=\"http://aws.amazon.com/\">AWS</a>&nbsp;configuration after changes at Amazon, which caused a cycle of servers being spawned then killed before they could properly boot. Our automated testing should have notified us of this failure immediately, but included a predictable failure mode (identified by us last year but not fixed).&nbsp;We became aware of the downtime when I checked my email and worked on it until it was resolved.</p>\n<p>I personally feel very bad about our multiple failures leading to this incident.</p>\n<p>ref. the last time I did this to you:&nbsp;http://lesswrong.com/lw/29v/lesswrong_downtime_20100511_and_other_recent/</p>\n<p><strong>Actions:</strong></p>\n<ol>\n<li>We have reconfigured AWS and the tools we use to communicate with it to avoid this failure in the future.</li>\n<li>Improvements to our automated site testing system (Nagios) are underway (expected to be live before 2012-04-13 - these tests will detect greater-than-X-failures-from-Y-trials, rather than the current detect zero-successes-from-Z-trials).</li>\n<li>We have changed our staffing in part in recognition that some systems (including this one) had been allowed to fall out of date, and allocated a developer to review our system administration project planning.</li>\n</ol><ol> </ol>\n<p>&nbsp;</p>\n<p><strong>Further actions - site speed:</strong></p>\n<p>We're unhappy with the site's speed. We plan on spending some time next week doing what we can to improve it.</p>\n<p>&nbsp;</p>\n<p>(If you upvote this post, please downvote my \"Karma sink\" comment below - I would prefer not to earn karma from an event like this.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "zcvsZQWJBFK6SxK4K": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LSeSdq4SE5dDu5atX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 62, "extendedScore": null, "score": 0.000148, "legacy": true, "legacyId": "14802", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 62, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Our investigation into last week's LW downtime is complete:&nbsp;<a style=\"font-family: arial;\" href=\"https://docs.google.com/document/d/1IXYAjoQQgzDx6xAyefIYCRgKrDlsldzfT-TORTt1JiQ/view?pli=1\">here</a>&nbsp;(Google Docs).</p>\n<p><strong id=\"Executive_summary_\">Executive summary:</strong></p>\n<p>We failed to update our&nbsp;<a href=\"http://aws.amazon.com/\">AWS</a>&nbsp;configuration after changes at Amazon, which caused a cycle of servers being spawned then killed before they could properly boot. Our automated testing should have notified us of this failure immediately, but included a predictable failure mode (identified by us last year but not fixed).&nbsp;We became aware of the downtime when I checked my email and worked on it until it was resolved.</p>\n<p>I personally feel very bad about our multiple failures leading to this incident.</p>\n<p>ref. the last time I did this to you:&nbsp;http://lesswrong.com/lw/29v/lesswrong_downtime_20100511_and_other_recent/</p>\n<p><strong id=\"Actions_\">Actions:</strong></p>\n<ol>\n<li>We have reconfigured AWS and the tools we use to communicate with it to avoid this failure in the future.</li>\n<li>Improvements to our automated site testing system (Nagios) are underway (expected to be live before 2012-04-13 - these tests will detect greater-than-X-failures-from-Y-trials, rather than the current detect zero-successes-from-Z-trials).</li>\n<li>We have changed our staffing in part in recognition that some systems (including this one) had been allowed to fall out of date, and allocated a developer to review our system administration project planning.</li>\n</ol><ol> </ol>\n<p>&nbsp;</p>\n<p><strong id=\"Further_actions___site_speed_\">Further actions - site speed:</strong></p>\n<p>We're unhappy with the site's speed. We plan on spending some time next week doing what we can to improve it.</p>\n<p>&nbsp;</p>\n<p>(If you upvote this post, please downvote my \"Karma sink\" comment below - I would prefer not to earn karma from an event like this.)</p>", "sections": [{"title": "Executive summary:", "anchor": "Executive_summary_", "level": 1}, {"title": "Actions:", "anchor": "Actions_", "level": 1}, {"title": "Further actions - site speed:", "anchor": "Further_actions___site_speed_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-03T04:40:22.254Z", "modifiedAt": null, "url": null, "title": "Advice for an isolated Rationalist?", "slug": "advice-for-an-isolated-rationalist", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:29.497Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Goobahman", "createdAt": "2011-01-13T05:09:28.962Z", "isAdmin": false, "displayName": "Goobahman"}, "userId": "cidN68rGuy4wwnvFp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FwcFW8FefRcqDFBAA/advice-for-an-isolated-rationalist", "pageUrlRelative": "/posts/FwcFW8FefRcqDFBAA/advice-for-an-isolated-rationalist", "linkUrl": "https://www.lesswrong.com/posts/FwcFW8FefRcqDFBAA/advice-for-an-isolated-rationalist", "postedAtFormatted": "Tuesday, April 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Advice%20for%20an%20isolated%20Rationalist%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdvice%20for%20an%20isolated%20Rationalist%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFwcFW8FefRcqDFBAA%2Fadvice-for-an-isolated-rationalist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Advice%20for%20an%20isolated%20Rationalist%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFwcFW8FefRcqDFBAA%2Fadvice-for-an-isolated-rationalist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFwcFW8FefRcqDFBAA%2Fadvice-for-an-isolated-rationalist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 130, "htmlBody": "<p>Hello fellow readers.<br /><br />I've been enjoying LW for a while now, and I can confidently say that many of the ideas on in this community have done much to better my life.</p>\n<p>However, I live in some isolation from like-minded individuals. I lack social groups that aspire to the same values of rationality that I have come to treasure. My nearest meet-up is Melbourne, but that takes approximately a hour and a half to get to, and would require more time and money than I can reasonably afford at the moment.<br />I find it difficult to immerse myself and live out many of these ideas when I do not have the social support to back me. <br /><br />Anyone have any tips for managing rationalist isolation?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FwcFW8FefRcqDFBAA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 19, "extendedScore": null, "score": 8.771180151829697e-07, "legacy": true, "legacyId": "14803", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-03T06:11:40.184Z", "modifiedAt": "2021-05-01T22:44:07.613Z", "url": null, "title": "SotW: Be Specific", "slug": "sotw-be-specific", "viewCount": null, "lastCommentedAt": "2015-07-29T19:25:05.301Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NgtYDP3ZtLJaM248W/sotw-be-specific", "pageUrlRelative": "/posts/NgtYDP3ZtLJaM248W/sotw-be-specific", "linkUrl": "https://www.lesswrong.com/posts/NgtYDP3ZtLJaM248W/sotw-be-specific", "postedAtFormatted": "Tuesday, April 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SotW%3A%20Be%20Specific&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASotW%3A%20Be%20Specific%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNgtYDP3ZtLJaM248W%2Fsotw-be-specific%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SotW%3A%20Be%20Specific%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNgtYDP3ZtLJaM248W%2Fsotw-be-specific", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNgtYDP3ZtLJaM248W%2Fsotw-be-specific", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3618, "htmlBody": "<p><em>(The&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/CFAR_Exercise_Prize\">Exercise Prize</a>&nbsp;series of&nbsp;<a href=\"/tag/exprize/\">posts</a>&nbsp;is the Center for Applied Rationality asking for help inventing exercises that can teach cognitive skills. &nbsp;The difficulty is&nbsp;</em><em>coming up with exercises interesting enough, with a high enough hedonic return, that people actually do them and remember them; this often involves standing up and performing actions, or interacting with other people, not just working alone with an exercise booklet and a pencil. &nbsp;</em><em>We offer prizes of $50 for any suggestion we decide to test, and $500 for any suggestion we decide to adopt. &nbsp;This prize also extends to LW meetup activities and good ideas for verifying that a skill has been acquired. &nbsp;<a href=\"http://wiki.lesswrong.com/wiki/CFAR_Exercise_Prize\">See&nbsp;here&nbsp;for details</a>.)</em></p>\n<hr />\n<p><em><strong>Exercise Prize: &nbsp;Be Specific</strong></em></p>\n<p>During YCombinator's Startup School 2011, Paul Graham and Harj Tagger did \"office hours\" onstage. &nbsp;One pair of entrepreneurs were doing a matchmaking (dating) startup, and Paul and Harj were trying to figure out what their startup <em>did,</em>&nbsp;exactly - for example, what their startup could do that the existing low-tech solution couldn't. &nbsp;(<a href=\"http://www.youtube.com/watch?v=K9m9vPAlb_0#t=30m18s\">Video.</a>)</p>\n<blockquote>\n<p>Harj: &nbsp;Low-tech like, you know, just like word of mouth, telling someone \"hey, you should like, meet up with my friend\" or \"we're getting drinks, why don't you come along?\" Like, what can the software do that's specifically better than that?</p>\n<p>Entrepreneur: &nbsp;I think that our software specifically is providing the better connections for people, um...</p>\n<p>Paul:<span style=\"white-space: pre; \"> </span>Providing the better connections for people...?</p>\n<p>Entrepreneur: &nbsp;I mean, one way you can think about it, I don't know if this is the right answer, but... there's a lot of things that are happening in real life that they're trying to mimic online, maybe that's not the correct way to... &nbsp;Look at it like this: to give them an online tool to also do this, like they're already doing in real life, maybe they could reach, uh expand their reach through the online website.</p>\n</blockquote>\n<p>This had been happening with <em>most</em>&nbsp;of the startups Paul and Harj were interrogating - <a href=\"http://www.youtube.com/watch?v=K9m9vPAlb_0#t=3m10s\">they just <em>could not</em> seem to provide a customer use-case</a>&nbsp;-&nbsp;and I couldn't&nbsp;<em>stand&nbsp;</em>it any more; which is why at this point I whispered audibly enough for a few nearby people to hear, \"Be specific! &nbsp;Be specific!\"</p>\n<p>A moment later, on stage:</p>\n<blockquote>\n<p>Paul: &nbsp;Hm. &nbsp;Not very specific.</p>\n</blockquote>\n<p>I got some strange looks from the people sitting next to me.</p>\n<p>I hope this provides some background for my guess that around half of Paul Graham's advantage is based on years of incubator experience, and the other half is unusual rationality skills of the sort that the Center for Modern Rationality is trying to figure out how to teach.&nbsp; Obviously this is only a very rough conjecture. &nbsp;But you can see the basis for the hope that - after a fair amount more work - we'll be able to offer a 2-day course for YCombinator entrepreneurs that eliminates 50% of the overhead from their conversations with Paul Graham.</p>\n<p>(Also, note how this post starts off with a specific example - an instance of the <em>concrete-abstract</em>&nbsp;writing pattern in which you state the example first and the generalization afterward. &nbsp;This is one of the most common bits of nonfiction writing advice I dispense: &nbsp;\"Open with the concrete example, not the abstract explanation!\")</p>\n<p><a id=\"more\"></a></p>\n<p><strong>Theoretical background:</strong></p>\n<p>S. I. Hayakawa once gave this illustration of the \"ladder of abstraction\", and in particular, the difference between going <em>up </em>or <em>down:</em></p>\n<blockquote>\n<p>\"What is meant by the word red?\"<br />\"It's a color.\"<br />\"What's a color?\"<br />\"Why, it's a quality things have.\"<br />\"What's a quality?\"</p>\n</blockquote>\n<p>vs.</p>\n<blockquote>\n<p>\"What is meant by the word red?\"<br />\"Well, the next time you see some cars stopped at an intersection, look at the traffic light facing them. &nbsp;Also, you might go to the fire department and see how their trucks are painted.\"</p>\n</blockquote>\n<p>\"Red is a color\" is moving <em>up </em>the ladder; \"color\" is a supercategory of red. &nbsp;All things which are red, have colors; but not all things which have colors, are red. &nbsp;And similarly, if you look at a specific firetruck, that firetruck is a red thing, but there are also many other red things which are not that firetruck.</p>\n<p>What is true of one apple may not be true of another apple; suppose apple<sub>1</sub> weighs 100 grams and is slightly green in some places, and apple<sub>2</sub> weighs 200 grams and is entirely dark-red. &nbsp;You can say more truths about apple<sub>2</sub>, like \"apple<sub>2</sub> is dark red\", then you can say that is true of <em>all</em>&nbsp;apples. &nbsp;(For more on this point see&nbsp;<a href=\"/lw/ic/the_virtue_of_narrowness/\">The Virtue of Narrowness</a>.)</p>\n<p>Thus, it may be easier to mentally picture \"a firetruck\" than \"something red\" - \"firetruck\" describes a narrower section of <a href=\"/lw/nl/the_cluster_structure_of_thingspace\">Thingspace</a>, so you're less likely to get lost along the way.</p>\n<p>S. I. Hayakawa called this the ladder of abstraction. &nbsp;I'm not sure if understanding the following section will really help with the skill of Being Specific, or help anyone construct exercises for the skill of being specific. &nbsp;But a better theoretical understanding does sometimes prove useful. &nbsp;So I will now digress to explain that abstraction isn't really a ladder, but a <em>lattice.</em></p>\n<p><em></em>Let's illustrate this using a classic example from the field of machine learning. &nbsp;Suppose that Days have three properties:</p>\n<ul>\n<li>Weather: {Sunny, Cloudy, Rainy}</li>\n<li>Temperature: {Cool, Hot}</li>\n<li>Timing: {Weekday, Weekend}</li>\n</ul>\n<p>And suppose that we've been given some examples of Days on which it was good, or alternatively bad, to play tennis. &nbsp;For example, the Day {Sunny, Cool, Weekend} was good for playing tennis, but the day {Rainy, Hot, Weekday} was bad for playing tennis. &nbsp;A classic task in machine learning is to induct, from a set of pre-classified examples like these, a <em>rule</em>&nbsp;describing when it is good to play tennis.</p>\n<p>Any proposed rule which can classify all days as good or bad is a <em>concept,&nbsp;</em>in the lingo of machine learning. &nbsp;\"Sunny Days\" is a concept; likewise \"Sunny Cool Days\", and \"Days which are either Cool or Sunny\". &nbsp;Each of these is a concept which classifies all 12 possible days either positively or negatively - instances or non-instances of the concept.</p>\n<p>There are 2<sup>12</sup>&nbsp;possible concepts over the 12 possible Days. &nbsp;Why so many? &nbsp;Because - for example - there's a concept which only includes the two Days {Sunny+Cool+Weekday} and {Cloudy+Cool+Weekend}}, but classifies all other Days as noninstances. &nbsp;This is a way of classifying all Days into instances or noninstances, hence a possible concept. &nbsp;It's not a <em>compact</em>&nbsp;concept, but it's a concept.&nbsp;&nbsp;Each Day can be classified either positively or negatively - one binary decision per Day - so 2<sup>12</sup>&nbsp;possible concepts. &nbsp;(That's why induction is a difficult problem in machine learning.)</p>\n<p>The concept \"Sunny\" is a&nbsp;superconcept&nbsp;of \"Sunny and Cool\"; it lies above it in the lattice of abstraction, since all days which are \"Sunny and Cool\" are \"Sunny\". &nbsp;\"Sunny or Hot\" is a supercategory of \"Sunny\". &nbsp;\"Weekend\" is neither a superconcept nor a subconcept of \"Sunny\".</p>\n<p>Concepts form a directed lattice from <em>most general</em>&nbsp;to <em>most specific,</em> with \"all Days\" at the top (every Day classified as an instance) and \"no Days\" at the bottom (the concept which classifies every Day as a noninstance).</p>\n<p>If you now go back to the problem of telling someone what \"red\" means, when you say \"red is a color\", then, even if the listener does&nbsp;happen to know what \"color\" means, you're still moving <em>upward in the lattice of abstraction.</em>&nbsp; When you said \"color\", you were talking about a concept that included all red things, but also many other things that were not red.</p>\n<p>\"Our software is providing the better connections for people\" - the entrepreneur who said that might have had something specific in mind, or they might have just been bluffing or succumbing to wishful thinking. &nbsp;But they described it using an abstract statement so broad that it included Facebook, or Western Union back when they were sending telegrams. &nbsp;They might - though this is somewhat optimistic - they might have known themselves what they had in mind; <em>they</em>&nbsp;didn't think of Facebook; so they didn't realize how many other possibilities fit their words. &nbsp;This is a classic manifestation of the <a href=\"http://wiki.lesswrong.com/wiki/Illusion_of_transparency\">Illusion of Transparency</a>, and it's why we have to keep telling people to navigate the lattice <em>downward.</em></p>\n<p>The skill of Being Specific is the skill of understanding how to <em>navigate the lattice of abstraction.</em>&nbsp; You can see why this would be a key element of cognition on a par with Bayes's Theorem or <a href=\"/lw/b4f/sotw_check_consequentialism/\">consequentialism</a>.</p>\n<p>And this is true in practice as well as theory. &nbsp;When I'm talking to anyone outside the local LW community, I find that a very large amount of my conversation involves repeatedly asking them to be more specific - and if you think that's just me being annoying, watch Paul Graham in the video.</p>\n<hr />\n<p>A closely related skill is <strong>concreteness,</strong>&nbsp;which has to do with <em>nearness-to-sensory-experience</em>&nbsp;or <em>actionability.</em></p>\n<p><em></em>According to David Allen's \"Getting Things Done\", for your brain to stop thinking about an unfinished task, you must (1) know and trust that an external system will remind you to perform that task when it is time to perform it, and (2) have chosen the <em>next action taken</em>&nbsp;at a sufficiently concrete level that your brain is no longer trying to plan it out in the background. &nbsp;\"Contact Luke about dispersing prize awards\" is not a sufficiently concrete to-do; it leaves open the question of whether to phone or email, and what exactly to say. &nbsp;\"Read through the comments, gather the LessWrong usernames of everyone who made a suggestion we tried or adopted, and email the list to Luke\" is an action item I know how to perform straightforwardly, without my brain trying to plan it in the background. &nbsp;When you have a <em>trustworthy</em>&nbsp;external system to remind you of what to do, at the time you need to do it - so that the back of your mind isn't worrying about remembering to check the to-do list - and <em>all </em>to-do items have been concretized to the point of being executable without further background planning - then you have, in GTD parlance, \"gotten to zero\", a state of pure mental blissfulness in which your brain is not worrying about <em>anything </em>except what you're doing <em>right now.</em></p>\n<p><em></em>Similarly, <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">for a statement like \"Wulky Wilkinsen is a post-utopian\" or \"Earth gravity pulls at 9.8 meters per second squared\" to be <em>falsifiable,</em></a>&nbsp;it must be <em>concretized </em>- rendered near-to-experience - to a sufficient degree that you can potentially <em>see </em>something and say \"Oh, guess the hypothesis was wrong\"; you must be able to have an experience which the concretized statement <em>constrains,</em>&nbsp;and which falsifies the theory if the experience is out-of-bounds.</p>\n<p>Theoretically: &nbsp;If you imagine the universe as a huge directed graph of causes and effects - the Great Web of Causality - then \"concreteness\" is being near enough in the Web to either your <em>sensory inputs</em>&nbsp;or <em>motor outputs</em>&nbsp;that you can directly see the prediction unfold, or directly implement the plan, without much further thought.</p>\n<p>\"Be Specific\" and \"Be Concrete\" could easily end up being the same unit - they're closely related - and we're happy to entertain exercises for Being Concrete, as well as Being Specific. &nbsp;Visualizing what your customer literally <em>sees</em>&nbsp;or <em>does</em>&nbsp;after navigating to your site, would've been a good first step toward being able to answer many of Paul Graham's questions.</p>\n<hr />\n<p><em>A possible success criterion:</em></p>\n<p>One question that we spent a lot of time discussing at CMR, was translating our sense of \"specific enough\" or \"concrete enough\" into a describable criterion. &nbsp;(Instead of just a wordless intuition for when something is \"too abstract\".)</p>\n<p>There was an exchange in Paul Graham's office hours that went like this, while interviewing a startup that did metrics - analyzing pageviews, roughly - and the entrepreneur was having great trouble describing what they did that MixPanel didn't. &nbsp;It went on for a while. &nbsp;It was painful to watch.</p>\n<blockquote>\n<p>Paul: &nbsp;I don't get what the difference is. &nbsp;I <em>still </em>don't get what the difference is. &nbsp;What's the difference between you and MixPanel?</p>\n<p>Entrepreneur: &nbsp;The difference is - when you have to supplement - they're a view company and we're a platform. &nbsp;That's what it comes down to. &nbsp;They're like a view, a reporting company. &nbsp;If you need something they don't have, a feature -&nbsp;</p>\n<p>Harj: &nbsp;So what's an example of somewhere you'd use your thing over MixPanel? &nbsp;Can you give a use-case?</p>\n<p>Entrepreneur: &nbsp;Yeah, I mean, we had revenue on day zero. There's a good reason for um...&nbsp;it's a start up, it's a series A company in the daily deals space. &nbsp;One we've signed a social game company to -</p>\n<p>Harj: &nbsp;And why do they prefer your thing?</p>\n<p>Paul: &nbsp;That wasn't what Harj was asking.</p>\n</blockquote>\n<p>The problem (from the perspective of our present discussion) is that the Entrepreneur did not understand that Paul and Harj were repeatedly asking him to move downward on the ladder of abstraction. &nbsp;When the Entrepreneur said \"We had revenue on day zero\", he was trying to offer&nbsp;<em>confirmation&nbsp;</em>of the abstract statement \"We can do things MixPanel can't\", but Paul and Harj still had no idea what his startup&nbsp;<em>actually did.</em>[1]</p>\n<p>A quick bit of theoretical background: &nbsp;There's an important difference, in the field of mathematical logic, between <em>models</em>&nbsp;and <em>axioms.</em>&nbsp; An axiom is something like \"All kittens are cute\", i.e. \"All x: kitten(x)-&gt;cute(x)\". &nbsp;A <em>model</em>&nbsp;is a particular universe of objects that includes {Obj #19834, kitten: T, cute: T, color: grey} and {Obj #19835, kitten: F, cute: F, color: striped}, and so on.</p>\n<p>Correspondingly, in logical inference, there's a distinction between <em>model-checking</em>&nbsp;and <em>deduction.</em>&nbsp; Suppose you want to know whether it's true that all positive integers less than 5, when multiplied by 7, are less than 50. &nbsp;If you prove the general truth that all integers less than 5, times 7, are less than 35, by manipulating the axioms of multiplication and inequality, that's deduction. &nbsp;If you notice that the only positive integers less than 5 are just {1, 2, 3, 4} and enumerate their products {7, 14, 21, 28}, which are all less than 50, that's model-checking.</p>\n<p>My hypothesis about what it means to be \"specific enough\" or \"concrete enough\" is that the picture painted is detailed enough to use in <em>model-checking</em> whatever points are being debated. &nbsp;Paul and Harj don't want to&nbsp;<em>trust</em>&nbsp;you when you state the abstract generalization, \"We're better than MixPanel\". &nbsp;They aren't even content with deducing support for this generalization from the further generalization, \"We already have customers.\" &nbsp;They want a&nbsp;<em>picture</em>&nbsp;of something&nbsp;you do that MixPanel doesn't, which is detailed enough that they can <em>model-check</em>&nbsp;whether you have a competitive advantage.</p>\n<p>Not to mention that Paul Graham is probably thinking about a number of other questions:</p>\n<ul>\n<li>How much would I pay for this product?</li>\n<li>Is this startup exciting enough that I would tweet about using it?</li>\n<li>How much resources will it take to develop these features further?</li>\n</ul>\n<p>Paul Graham doesn't want you to say, \"$50, yes, and twenty engineer-months\". &nbsp;He wants a sufficiently specific picture of (a customer using) your product that he can arrive at his own answers by model-checking.</p>\n<p>If Paul Graham is reading this, he's welcome to contradict my interpretation of what was going on in that particular session - but it did seem like a very nice concrete illustration.</p>\n<p>That's my guess for what often constitutes \"specific enough\" - though I'm not sure that's the <em>only </em>thing that ever determines specific-enoughness.</p>\n<p>[1]: &nbsp;The&nbsp;strange part was, near the end of that session, it started to look like this might be an interesting startup; that the Entrepreneur wasn't just bluffing. &nbsp;Their actual use-case was to let customers easily roll their own code to measure, e.g., the page-viewing behavior of only customers who'd bought more than $200 worth of stuff, which allegedly MixPanel wouldn't let you do. &nbsp;Which would've been a perfectly good answer if the Entrepreneur had&nbsp;given it <em>at the start of the session,</em>&nbsp;instead of the whole session being about Paul and Harj trying to get at that information.</p>\n<hr />\n<p><strong>Five-second-level skill:</strong></p>\n<p>The 5SL skill for this problem requires:</p>\n<ul>\n<li>Trigger: &nbsp;Recognizing when your words or thoughts are&nbsp;<em>too abstract.</em></li>\n<li>Action: &nbsp;Moving downward in the abstraction lattice, or moving nearer to sense input or motor output; being able to render your thoughts more specific or more concrete.</li>\n</ul>\n<p>Both of these are targetable for exercises.</p>\n<hr />\n<p><strong>Pain points &amp; Pluses:</strong></p>\n<p>&bull; You want Paul Graham to believe your startup is better than MixPanel. &nbsp;So you say, \"My startup is better than MixPanel\" - just produce the pure abstract conclusion you want Paul Graham to arrive at. &nbsp;You keep trying to convince Paul Graham of this statement, saying that you have customers or that you have venture capital, but never actually move <em>downward</em>&nbsp;to the level where Paul Graham could arrive at this conclusion by model-checking.</p>\n<p>&bull;&nbsp;You want to describe what your software does, so you say it makes connections between people. &nbsp;<em>You</em>&nbsp;have something specific in mind, but the words coming out of your mouth are so general that - although <em>you're </em>not thinking of those other cases - they could apply equally well to Facebook or telegraph lines. &nbsp;Paul Graham has no idea at all what you're trying to describe and is giving you blank looks.</p>\n<p>&bull; The worse version - and the reason why Paul Graham doesn't just trust you, even if he thinks you're honest - is the case where you <em>yourself</em>&nbsp;want to believe your startup is better than Facebook, but you can't think of any <em>specific</em>&nbsp;thing your startup does better than Facebook, so you think of other abstract generalizations that seem to support the conclusion, like \"We have smarter people\" or \"We got more funding earlier.\" &nbsp;Where fuzzy thinking is motivated, overly abstract thinking is motivated.</p>\n<p>&bull;&nbsp;Abstract words can also avoid <em>emotion</em>. &nbsp;<a href=\"http://www.k-1.com/Orwell/index.cgi/work/essays/language.html\">George Orwell</a>: &nbsp;\"Defenceless villages are bombarded from the air, the inhabitants driven out into the countryside, the cattle machine-gunned, the huts set on fire with incendiary bullets: this is called&nbsp;<em>pacification</em>.\" &nbsp;Or contrast \"Humanity is awful, it'd be better for the planet if we all died\" to \"Everyone including my little sister is awful, we'd be better off if everyone died including her.\" &nbsp;To feel&nbsp;sympathy, we need enough concrete detail that our emotions can model-check the picture and be activated.</p>\n<p>&bull;&nbsp;Cognitive-behavioral therapy is the big <em>experimentally supported</em>&nbsp;version of therapy, for anyone not aware of this, bearing very little resemblance to anything Freudian.&nbsp;&nbsp;CBT talks about using requests for specific details to interrupt thoughts looping around vague but affectively laden centers, like \"I am a good husband\", \"I am a bad husband\", or \"my roommate is a slob\". &nbsp;How are you a good husband? &nbsp;How are you a bad husband? &nbsp;Which specific feature of your roommate are you objecting to? &nbsp;<a href=\"/lw/nu/taboo_your_words/\">Taboo</a> the emotionally valent <em>word</em>&nbsp;at the center, like \"slob\", and replace it with something that's specific enough to be testable, or concrete enough to be acted upon.</p>\n<p>&bull;&bull;&nbsp;Contrast also \"It bothers me when you leave soda cans on the table\" vs. \"You're such a slob, stop being such a slob.\" &nbsp;Or contrast:&nbsp;&nbsp;\"I'm upset\" -&gt; \"I'm upset because I think the other person is looking down on me\" -&gt; \"I'm upset because the person's tone of voice sounds like people who looked down on me in high school\". &nbsp;This is related to the incredibly important skill, <em>search for the historical causes of your thoughts, rather than their justifications.</em></p>\n<p>&bull;&nbsp;Focusing on the specific details of a concrete example, instead of repeating a word or arguing about a category, can interrupt <a href=\"/lw/ny/sneaking_in_connotations/\">Sneaking in Connotations</a> and <a href=\"/lw/nz/arguing_by_definition/\">Arguing By Definition</a>.</p>\n<p>&bull;&nbsp;All the failures of&nbsp;<em>concreteness&nbsp;</em>warned against in the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\">Mysterious Answers</a>&nbsp;sequence, where you go on and on about how Wulky Wilkinsen is a post-utopian without ever once asking or imagining how the world ought to look, and what you yourself should experience, if that were true or alternatively false.</p>\n<p>&bull; Visualizing specific examples often improves quality of thought in general - we're often smarter&nbsp;when we're using both model-checking and deduction, visualizing a picture of what we're supposed to be reasoning about, constantly checking our deductive steps against some specific model those deductions are supposed to be true about. &nbsp;Saith Richard Feynman:</p>\n<blockquote>\n<p>I had a scheme, which I still use today when somebody is explaining something that I'm trying to understand: I keep making up examples. For instance, the mathematicians would come in with a terrific theorem, and they're all excited. As they're telling me the conditions of the theorem, I construct something which fits all the conditions. You know, you have a set (one ball) - disjoint (two halls). Then the balls turn colors, grow hairs, or whatever, in my head as they put more conditions on. Finally they state the&nbsp;theorem, which is some dumb thing about the ball which isn't true for my hairy green ball thing, so I say, \"False!\"</p>\n<p>&nbsp;If it's true, they get all excited, and I let them go on for a while. Then I point out my counterexample.</p>\n<p>\"Oh. We forgot to tell you that it's Class 2 Hausdorff homomorphic.\"</p>\n<p>\"Well, then,\" I say, \"It's trivial! It's trivial!\"</p>\n</blockquote>\n<p>&bull;&nbsp;Being specific helps notice and call bluffs, should you be mischievously inclined.</p>\n<p style=\"padding-left: 30px;\">\"Beware, demon!\" he intoned hollowly. &nbsp;\"I am not without defenses.\"<br />\"Oh yeah? &nbsp;Name three.\"<br /><span style=\"white-space: pre;\"> </span>-- Robert Asprin, Another Fine Myth</p>\n<p style=\"padding-left: 30px;\">Wannabe executive:&nbsp;&nbsp;\"I will improve communications between employees and management.\"<br />Me:&nbsp; \"Can you give me a specific example of how you would do that?\"</p>\n<hr />\n<p>Known exercises for this skill:</p>\n<ul>\n<li><a href=\"/lw/nu/taboo_your_words/\">Rationalist Taboo</a></li>\n</ul>\n<p>In our previous&nbsp;<a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/\">Rationality Camps</a>, Anna found that her attempt to teach a unit on \"Being Specific\" didn't seem to work. &nbsp;Her central exercise was picking a category and asking people to name examples.</p>\n<p>This isn't to say that the Camps were unsuccessful at teaching the skill. &nbsp;Attendees picked it up, not from the explicit unit, but from all the instructors having to repeatedly ask the attendees to be more specific, and then having to ask them again, while being specific themselves, until the attendees picked up the rhythm by example and feedback.</p>\n<p>Given our present teaching technology, this skill seems&nbsp;<em>transmissible</em>&nbsp;from master to apprentice, but not yet&nbsp;<em>replicable</em>&nbsp;by exercises. &nbsp;That's why we're turning it over to you.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "FtT2T9bRbECCGYxrL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NgtYDP3ZtLJaM248W", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 50, "baseScore": 74, "extendedScore": null, "score": 0.000154, "legacy": true, "legacyId": "14691", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 74, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>(The&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/CFAR_Exercise_Prize\">Exercise Prize</a>&nbsp;series of&nbsp;<a href=\"/tag/exprize/\">posts</a>&nbsp;is the Center for Applied Rationality asking for help inventing exercises that can teach cognitive skills. &nbsp;The difficulty is&nbsp;</em><em>coming up with exercises interesting enough, with a high enough hedonic return, that people actually do them and remember them; this often involves standing up and performing actions, or interacting with other people, not just working alone with an exercise booklet and a pencil. &nbsp;</em><em>We offer prizes of $50 for any suggestion we decide to test, and $500 for any suggestion we decide to adopt. &nbsp;This prize also extends to LW meetup activities and good ideas for verifying that a skill has been acquired. &nbsp;<a href=\"http://wiki.lesswrong.com/wiki/CFAR_Exercise_Prize\">See&nbsp;here&nbsp;for details</a>.)</em></p>\n<hr>\n<p><em><strong>Exercise Prize: &nbsp;Be Specific</strong></em></p>\n<p>During YCombinator's Startup School 2011, Paul Graham and Harj Tagger did \"office hours\" onstage. &nbsp;One pair of entrepreneurs were doing a matchmaking (dating) startup, and Paul and Harj were trying to figure out what their startup <em>did,</em>&nbsp;exactly - for example, what their startup could do that the existing low-tech solution couldn't. &nbsp;(<a href=\"http://www.youtube.com/watch?v=K9m9vPAlb_0#t=30m18s\">Video.</a>)</p>\n<blockquote>\n<p>Harj: &nbsp;Low-tech like, you know, just like word of mouth, telling someone \"hey, you should like, meet up with my friend\" or \"we're getting drinks, why don't you come along?\" Like, what can the software do that's specifically better than that?</p>\n<p>Entrepreneur: &nbsp;I think that our software specifically is providing the better connections for people, um...</p>\n<p>Paul:<span style=\"white-space: pre; \"> </span>Providing the better connections for people...?</p>\n<p>Entrepreneur: &nbsp;I mean, one way you can think about it, I don't know if this is the right answer, but... there's a lot of things that are happening in real life that they're trying to mimic online, maybe that's not the correct way to... &nbsp;Look at it like this: to give them an online tool to also do this, like they're already doing in real life, maybe they could reach, uh expand their reach through the online website.</p>\n</blockquote>\n<p>This had been happening with <em>most</em>&nbsp;of the startups Paul and Harj were interrogating - <a href=\"http://www.youtube.com/watch?v=K9m9vPAlb_0#t=3m10s\">they just <em>could not</em> seem to provide a customer use-case</a>&nbsp;-&nbsp;and I couldn't&nbsp;<em>stand&nbsp;</em>it any more; which is why at this point I whispered audibly enough for a few nearby people to hear, \"Be specific! &nbsp;Be specific!\"</p>\n<p>A moment later, on stage:</p>\n<blockquote>\n<p>Paul: &nbsp;Hm. &nbsp;Not very specific.</p>\n</blockquote>\n<p>I got some strange looks from the people sitting next to me.</p>\n<p>I hope this provides some background for my guess that around half of Paul Graham's advantage is based on years of incubator experience, and the other half is unusual rationality skills of the sort that the Center for Modern Rationality is trying to figure out how to teach.&nbsp; Obviously this is only a very rough conjecture. &nbsp;But you can see the basis for the hope that - after a fair amount more work - we'll be able to offer a 2-day course for YCombinator entrepreneurs that eliminates 50% of the overhead from their conversations with Paul Graham.</p>\n<p>(Also, note how this post starts off with a specific example - an instance of the <em>concrete-abstract</em>&nbsp;writing pattern in which you state the example first and the generalization afterward. &nbsp;This is one of the most common bits of nonfiction writing advice I dispense: &nbsp;\"Open with the concrete example, not the abstract explanation!\")</p>\n<p><a id=\"more\"></a></p>\n<p><strong id=\"Theoretical_background_\">Theoretical background:</strong></p>\n<p>S. I. Hayakawa once gave this illustration of the \"ladder of abstraction\", and in particular, the difference between going <em>up </em>or <em>down:</em></p>\n<blockquote>\n<p>\"What is meant by the word red?\"<br>\"It's a color.\"<br>\"What's a color?\"<br>\"Why, it's a quality things have.\"<br>\"What's a quality?\"</p>\n</blockquote>\n<p>vs.</p>\n<blockquote>\n<p>\"What is meant by the word red?\"<br>\"Well, the next time you see some cars stopped at an intersection, look at the traffic light facing them. &nbsp;Also, you might go to the fire department and see how their trucks are painted.\"</p>\n</blockquote>\n<p>\"Red is a color\" is moving <em>up </em>the ladder; \"color\" is a supercategory of red. &nbsp;All things which are red, have colors; but not all things which have colors, are red. &nbsp;And similarly, if you look at a specific firetruck, that firetruck is a red thing, but there are also many other red things which are not that firetruck.</p>\n<p>What is true of one apple may not be true of another apple; suppose apple<sub>1</sub> weighs 100 grams and is slightly green in some places, and apple<sub>2</sub> weighs 200 grams and is entirely dark-red. &nbsp;You can say more truths about apple<sub>2</sub>, like \"apple<sub>2</sub> is dark red\", then you can say that is true of <em>all</em>&nbsp;apples. &nbsp;(For more on this point see&nbsp;<a href=\"/lw/ic/the_virtue_of_narrowness/\">The Virtue of Narrowness</a>.)</p>\n<p>Thus, it may be easier to mentally picture \"a firetruck\" than \"something red\" - \"firetruck\" describes a narrower section of <a href=\"/lw/nl/the_cluster_structure_of_thingspace\">Thingspace</a>, so you're less likely to get lost along the way.</p>\n<p>S. I. Hayakawa called this the ladder of abstraction. &nbsp;I'm not sure if understanding the following section will really help with the skill of Being Specific, or help anyone construct exercises for the skill of being specific. &nbsp;But a better theoretical understanding does sometimes prove useful. &nbsp;So I will now digress to explain that abstraction isn't really a ladder, but a <em>lattice.</em></p>\n<p><em></em>Let's illustrate this using a classic example from the field of machine learning. &nbsp;Suppose that Days have three properties:</p>\n<ul>\n<li>Weather: {Sunny, Cloudy, Rainy}</li>\n<li>Temperature: {Cool, Hot}</li>\n<li>Timing: {Weekday, Weekend}</li>\n</ul>\n<p>And suppose that we've been given some examples of Days on which it was good, or alternatively bad, to play tennis. &nbsp;For example, the Day {Sunny, Cool, Weekend} was good for playing tennis, but the day {Rainy, Hot, Weekday} was bad for playing tennis. &nbsp;A classic task in machine learning is to induct, from a set of pre-classified examples like these, a <em>rule</em>&nbsp;describing when it is good to play tennis.</p>\n<p>Any proposed rule which can classify all days as good or bad is a <em>concept,&nbsp;</em>in the lingo of machine learning. &nbsp;\"Sunny Days\" is a concept; likewise \"Sunny Cool Days\", and \"Days which are either Cool or Sunny\". &nbsp;Each of these is a concept which classifies all 12 possible days either positively or negatively - instances or non-instances of the concept.</p>\n<p>There are 2<sup>12</sup>&nbsp;possible concepts over the 12 possible Days. &nbsp;Why so many? &nbsp;Because - for example - there's a concept which only includes the two Days {Sunny+Cool+Weekday} and {Cloudy+Cool+Weekend}}, but classifies all other Days as noninstances. &nbsp;This is a way of classifying all Days into instances or noninstances, hence a possible concept. &nbsp;It's not a <em>compact</em>&nbsp;concept, but it's a concept.&nbsp;&nbsp;Each Day can be classified either positively or negatively - one binary decision per Day - so 2<sup>12</sup>&nbsp;possible concepts. &nbsp;(That's why induction is a difficult problem in machine learning.)</p>\n<p>The concept \"Sunny\" is a&nbsp;superconcept&nbsp;of \"Sunny and Cool\"; it lies above it in the lattice of abstraction, since all days which are \"Sunny and Cool\" are \"Sunny\". &nbsp;\"Sunny or Hot\" is a supercategory of \"Sunny\". &nbsp;\"Weekend\" is neither a superconcept nor a subconcept of \"Sunny\".</p>\n<p>Concepts form a directed lattice from <em>most general</em>&nbsp;to <em>most specific,</em> with \"all Days\" at the top (every Day classified as an instance) and \"no Days\" at the bottom (the concept which classifies every Day as a noninstance).</p>\n<p>If you now go back to the problem of telling someone what \"red\" means, when you say \"red is a color\", then, even if the listener does&nbsp;happen to know what \"color\" means, you're still moving <em>upward in the lattice of abstraction.</em>&nbsp; When you said \"color\", you were talking about a concept that included all red things, but also many other things that were not red.</p>\n<p>\"Our software is providing the better connections for people\" - the entrepreneur who said that might have had something specific in mind, or they might have just been bluffing or succumbing to wishful thinking. &nbsp;But they described it using an abstract statement so broad that it included Facebook, or Western Union back when they were sending telegrams. &nbsp;They might - though this is somewhat optimistic - they might have known themselves what they had in mind; <em>they</em>&nbsp;didn't think of Facebook; so they didn't realize how many other possibilities fit their words. &nbsp;This is a classic manifestation of the <a href=\"http://wiki.lesswrong.com/wiki/Illusion_of_transparency\">Illusion of Transparency</a>, and it's why we have to keep telling people to navigate the lattice <em>downward.</em></p>\n<p>The skill of Being Specific is the skill of understanding how to <em>navigate the lattice of abstraction.</em>&nbsp; You can see why this would be a key element of cognition on a par with Bayes's Theorem or <a href=\"/lw/b4f/sotw_check_consequentialism/\">consequentialism</a>.</p>\n<p>And this is true in practice as well as theory. &nbsp;When I'm talking to anyone outside the local LW community, I find that a very large amount of my conversation involves repeatedly asking them to be more specific - and if you think that's just me being annoying, watch Paul Graham in the video.</p>\n<hr>\n<p>A closely related skill is <strong>concreteness,</strong>&nbsp;which has to do with <em>nearness-to-sensory-experience</em>&nbsp;or <em>actionability.</em></p>\n<p><em></em>According to David Allen's \"Getting Things Done\", for your brain to stop thinking about an unfinished task, you must (1) know and trust that an external system will remind you to perform that task when it is time to perform it, and (2) have chosen the <em>next action taken</em>&nbsp;at a sufficiently concrete level that your brain is no longer trying to plan it out in the background. &nbsp;\"Contact Luke about dispersing prize awards\" is not a sufficiently concrete to-do; it leaves open the question of whether to phone or email, and what exactly to say. &nbsp;\"Read through the comments, gather the LessWrong usernames of everyone who made a suggestion we tried or adopted, and email the list to Luke\" is an action item I know how to perform straightforwardly, without my brain trying to plan it in the background. &nbsp;When you have a <em>trustworthy</em>&nbsp;external system to remind you of what to do, at the time you need to do it - so that the back of your mind isn't worrying about remembering to check the to-do list - and <em>all </em>to-do items have been concretized to the point of being executable without further background planning - then you have, in GTD parlance, \"gotten to zero\", a state of pure mental blissfulness in which your brain is not worrying about <em>anything </em>except what you're doing <em>right now.</em></p>\n<p><em></em>Similarly, <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">for a statement like \"Wulky Wilkinsen is a post-utopian\" or \"Earth gravity pulls at 9.8 meters per second squared\" to be <em>falsifiable,</em></a>&nbsp;it must be <em>concretized </em>- rendered near-to-experience - to a sufficient degree that you can potentially <em>see </em>something and say \"Oh, guess the hypothesis was wrong\"; you must be able to have an experience which the concretized statement <em>constrains,</em>&nbsp;and which falsifies the theory if the experience is out-of-bounds.</p>\n<p>Theoretically: &nbsp;If you imagine the universe as a huge directed graph of causes and effects - the Great Web of Causality - then \"concreteness\" is being near enough in the Web to either your <em>sensory inputs</em>&nbsp;or <em>motor outputs</em>&nbsp;that you can directly see the prediction unfold, or directly implement the plan, without much further thought.</p>\n<p>\"Be Specific\" and \"Be Concrete\" could easily end up being the same unit - they're closely related - and we're happy to entertain exercises for Being Concrete, as well as Being Specific. &nbsp;Visualizing what your customer literally <em>sees</em>&nbsp;or <em>does</em>&nbsp;after navigating to your site, would've been a good first step toward being able to answer many of Paul Graham's questions.</p>\n<hr>\n<p><em>A possible success criterion:</em></p>\n<p>One question that we spent a lot of time discussing at CMR, was translating our sense of \"specific enough\" or \"concrete enough\" into a describable criterion. &nbsp;(Instead of just a wordless intuition for when something is \"too abstract\".)</p>\n<p>There was an exchange in Paul Graham's office hours that went like this, while interviewing a startup that did metrics - analyzing pageviews, roughly - and the entrepreneur was having great trouble describing what they did that MixPanel didn't. &nbsp;It went on for a while. &nbsp;It was painful to watch.</p>\n<blockquote>\n<p>Paul: &nbsp;I don't get what the difference is. &nbsp;I <em>still </em>don't get what the difference is. &nbsp;What's the difference between you and MixPanel?</p>\n<p>Entrepreneur: &nbsp;The difference is - when you have to supplement - they're a view company and we're a platform. &nbsp;That's what it comes down to. &nbsp;They're like a view, a reporting company. &nbsp;If you need something they don't have, a feature -&nbsp;</p>\n<p>Harj: &nbsp;So what's an example of somewhere you'd use your thing over MixPanel? &nbsp;Can you give a use-case?</p>\n<p>Entrepreneur: &nbsp;Yeah, I mean, we had revenue on day zero. There's a good reason for um...&nbsp;it's a start up, it's a series A company in the daily deals space. &nbsp;One we've signed a social game company to -</p>\n<p>Harj: &nbsp;And why do they prefer your thing?</p>\n<p>Paul: &nbsp;That wasn't what Harj was asking.</p>\n</blockquote>\n<p>The problem (from the perspective of our present discussion) is that the Entrepreneur did not understand that Paul and Harj were repeatedly asking him to move downward on the ladder of abstraction. &nbsp;When the Entrepreneur said \"We had revenue on day zero\", he was trying to offer&nbsp;<em>confirmation&nbsp;</em>of the abstract statement \"We can do things MixPanel can't\", but Paul and Harj still had no idea what his startup&nbsp;<em>actually did.</em>[1]</p>\n<p>A quick bit of theoretical background: &nbsp;There's an important difference, in the field of mathematical logic, between <em>models</em>&nbsp;and <em>axioms.</em>&nbsp; An axiom is something like \"All kittens are cute\", i.e. \"All x: kitten(x)-&gt;cute(x)\". &nbsp;A <em>model</em>&nbsp;is a particular universe of objects that includes {Obj #19834, kitten: T, cute: T, color: grey} and {Obj #19835, kitten: F, cute: F, color: striped}, and so on.</p>\n<p>Correspondingly, in logical inference, there's a distinction between <em>model-checking</em>&nbsp;and <em>deduction.</em>&nbsp; Suppose you want to know whether it's true that all positive integers less than 5, when multiplied by 7, are less than 50. &nbsp;If you prove the general truth that all integers less than 5, times 7, are less than 35, by manipulating the axioms of multiplication and inequality, that's deduction. &nbsp;If you notice that the only positive integers less than 5 are just {1, 2, 3, 4} and enumerate their products {7, 14, 21, 28}, which are all less than 50, that's model-checking.</p>\n<p>My hypothesis about what it means to be \"specific enough\" or \"concrete enough\" is that the picture painted is detailed enough to use in <em>model-checking</em> whatever points are being debated. &nbsp;Paul and Harj don't want to&nbsp;<em>trust</em>&nbsp;you when you state the abstract generalization, \"We're better than MixPanel\". &nbsp;They aren't even content with deducing support for this generalization from the further generalization, \"We already have customers.\" &nbsp;They want a&nbsp;<em>picture</em>&nbsp;of something&nbsp;you do that MixPanel doesn't, which is detailed enough that they can <em>model-check</em>&nbsp;whether you have a competitive advantage.</p>\n<p>Not to mention that Paul Graham is probably thinking about a number of other questions:</p>\n<ul>\n<li>How much would I pay for this product?</li>\n<li>Is this startup exciting enough that I would tweet about using it?</li>\n<li>How much resources will it take to develop these features further?</li>\n</ul>\n<p>Paul Graham doesn't want you to say, \"$50, yes, and twenty engineer-months\". &nbsp;He wants a sufficiently specific picture of (a customer using) your product that he can arrive at his own answers by model-checking.</p>\n<p>If Paul Graham is reading this, he's welcome to contradict my interpretation of what was going on in that particular session - but it did seem like a very nice concrete illustration.</p>\n<p>That's my guess for what often constitutes \"specific enough\" - though I'm not sure that's the <em>only </em>thing that ever determines specific-enoughness.</p>\n<p>[1]: &nbsp;The&nbsp;strange part was, near the end of that session, it started to look like this might be an interesting startup; that the Entrepreneur wasn't just bluffing. &nbsp;Their actual use-case was to let customers easily roll their own code to measure, e.g., the page-viewing behavior of only customers who'd bought more than $200 worth of stuff, which allegedly MixPanel wouldn't let you do. &nbsp;Which would've been a perfectly good answer if the Entrepreneur had&nbsp;given it <em>at the start of the session,</em>&nbsp;instead of the whole session being about Paul and Harj trying to get at that information.</p>\n<hr>\n<p><strong id=\"Five_second_level_skill_\">Five-second-level skill:</strong></p>\n<p>The 5SL skill for this problem requires:</p>\n<ul>\n<li>Trigger: &nbsp;Recognizing when your words or thoughts are&nbsp;<em>too abstract.</em></li>\n<li>Action: &nbsp;Moving downward in the abstraction lattice, or moving nearer to sense input or motor output; being able to render your thoughts more specific or more concrete.</li>\n</ul>\n<p>Both of these are targetable for exercises.</p>\n<hr>\n<p><strong id=\"Pain_points___Pluses_\">Pain points &amp; Pluses:</strong></p>\n<p>\u2022 You want Paul Graham to believe your startup is better than MixPanel. &nbsp;So you say, \"My startup is better than MixPanel\" - just produce the pure abstract conclusion you want Paul Graham to arrive at. &nbsp;You keep trying to convince Paul Graham of this statement, saying that you have customers or that you have venture capital, but never actually move <em>downward</em>&nbsp;to the level where Paul Graham could arrive at this conclusion by model-checking.</p>\n<p>\u2022&nbsp;You want to describe what your software does, so you say it makes connections between people. &nbsp;<em>You</em>&nbsp;have something specific in mind, but the words coming out of your mouth are so general that - although <em>you're </em>not thinking of those other cases - they could apply equally well to Facebook or telegraph lines. &nbsp;Paul Graham has no idea at all what you're trying to describe and is giving you blank looks.</p>\n<p>\u2022 The worse version - and the reason why Paul Graham doesn't just trust you, even if he thinks you're honest - is the case where you <em>yourself</em>&nbsp;want to believe your startup is better than Facebook, but you can't think of any <em>specific</em>&nbsp;thing your startup does better than Facebook, so you think of other abstract generalizations that seem to support the conclusion, like \"We have smarter people\" or \"We got more funding earlier.\" &nbsp;Where fuzzy thinking is motivated, overly abstract thinking is motivated.</p>\n<p>\u2022&nbsp;Abstract words can also avoid <em>emotion</em>. &nbsp;<a href=\"http://www.k-1.com/Orwell/index.cgi/work/essays/language.html\">George Orwell</a>: &nbsp;\"Defenceless villages are bombarded from the air, the inhabitants driven out into the countryside, the cattle machine-gunned, the huts set on fire with incendiary bullets: this is called&nbsp;<em>pacification</em>.\" &nbsp;Or contrast \"Humanity is awful, it'd be better for the planet if we all died\" to \"Everyone including my little sister is awful, we'd be better off if everyone died including her.\" &nbsp;To feel&nbsp;sympathy, we need enough concrete detail that our emotions can model-check the picture and be activated.</p>\n<p>\u2022&nbsp;Cognitive-behavioral therapy is the big <em>experimentally supported</em>&nbsp;version of therapy, for anyone not aware of this, bearing very little resemblance to anything Freudian.&nbsp;&nbsp;CBT talks about using requests for specific details to interrupt thoughts looping around vague but affectively laden centers, like \"I am a good husband\", \"I am a bad husband\", or \"my roommate is a slob\". &nbsp;How are you a good husband? &nbsp;How are you a bad husband? &nbsp;Which specific feature of your roommate are you objecting to? &nbsp;<a href=\"/lw/nu/taboo_your_words/\">Taboo</a> the emotionally valent <em>word</em>&nbsp;at the center, like \"slob\", and replace it with something that's specific enough to be testable, or concrete enough to be acted upon.</p>\n<p>\u2022\u2022&nbsp;Contrast also \"It bothers me when you leave soda cans on the table\" vs. \"You're such a slob, stop being such a slob.\" &nbsp;Or contrast:&nbsp;&nbsp;\"I'm upset\" -&gt; \"I'm upset because I think the other person is looking down on me\" -&gt; \"I'm upset because the person's tone of voice sounds like people who looked down on me in high school\". &nbsp;This is related to the incredibly important skill, <em>search for the historical causes of your thoughts, rather than their justifications.</em></p>\n<p>\u2022&nbsp;Focusing on the specific details of a concrete example, instead of repeating a word or arguing about a category, can interrupt <a href=\"/lw/ny/sneaking_in_connotations/\">Sneaking in Connotations</a> and <a href=\"/lw/nz/arguing_by_definition/\">Arguing By Definition</a>.</p>\n<p>\u2022&nbsp;All the failures of&nbsp;<em>concreteness&nbsp;</em>warned against in the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\">Mysterious Answers</a>&nbsp;sequence, where you go on and on about how Wulky Wilkinsen is a post-utopian without ever once asking or imagining how the world ought to look, and what you yourself should experience, if that were true or alternatively false.</p>\n<p>\u2022 Visualizing specific examples often improves quality of thought in general - we're often smarter&nbsp;when we're using both model-checking and deduction, visualizing a picture of what we're supposed to be reasoning about, constantly checking our deductive steps against some specific model those deductions are supposed to be true about. &nbsp;Saith Richard Feynman:</p>\n<blockquote>\n<p>I had a scheme, which I still use today when somebody is explaining something that I'm trying to understand: I keep making up examples. For instance, the mathematicians would come in with a terrific theorem, and they're all excited. As they're telling me the conditions of the theorem, I construct something which fits all the conditions. You know, you have a set (one ball) - disjoint (two halls). Then the balls turn colors, grow hairs, or whatever, in my head as they put more conditions on. Finally they state the&nbsp;theorem, which is some dumb thing about the ball which isn't true for my hairy green ball thing, so I say, \"False!\"</p>\n<p>&nbsp;If it's true, they get all excited, and I let them go on for a while. Then I point out my counterexample.</p>\n<p>\"Oh. We forgot to tell you that it's Class 2 Hausdorff homomorphic.\"</p>\n<p>\"Well, then,\" I say, \"It's trivial! It's trivial!\"</p>\n</blockquote>\n<p>\u2022&nbsp;Being specific helps notice and call bluffs, should you be mischievously inclined.</p>\n<p style=\"padding-left: 30px;\">\"Beware, demon!\" he intoned hollowly. &nbsp;\"I am not without defenses.\"<br>\"Oh yeah? &nbsp;Name three.\"<br><span style=\"white-space: pre;\"> </span>-- Robert Asprin, Another Fine Myth</p>\n<p style=\"padding-left: 30px;\">Wannabe executive:&nbsp;&nbsp;\"I will improve communications between employees and management.\"<br>Me:&nbsp; \"Can you give me a specific example of how you would do that?\"</p>\n<hr>\n<p>Known exercises for this skill:</p>\n<ul>\n<li><a href=\"/lw/nu/taboo_your_words/\">Rationalist Taboo</a></li>\n</ul>\n<p>In our previous&nbsp;<a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/\">Rationality Camps</a>, Anna found that her attempt to teach a unit on \"Being Specific\" didn't seem to work. &nbsp;Her central exercise was picking a category and asking people to name examples.</p>\n<p>This isn't to say that the Camps were unsuccessful at teaching the skill. &nbsp;Attendees picked it up, not from the explicit unit, but from all the instructors having to repeatedly ask the attendees to be more specific, and then having to ask them again, while being specific themselves, until the attendees picked up the rhythm by example and feedback.</p>\n<p>Given our present teaching technology, this skill seems&nbsp;<em>transmissible</em>&nbsp;from master to apprentice, but not yet&nbsp;<em>replicable</em>&nbsp;by exercises. &nbsp;That's why we're turning it over to you.</p>", "sections": [{"title": "Theoretical background:", "anchor": "Theoretical_background_", "level": 1}, {"title": "Five-second-level skill:", "anchor": "Five_second_level_skill_", "level": 1}, {"title": "Pain points & Pluses:", "anchor": "Pain_points___Pluses_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "295 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 306, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yDfxTj9TKYsYiWH5o", "WBw8dDkAWohFjWQSk", "xypbWhzEEw4ZsRK9i", "a7n8GdKiAZRX86T5A", "WBdvyyHLdxZSAMmoz", "yuKaWPRTxZoov4z8K", "cFzC996D7Jjds3vS9", "fkhbBE2ZTSytvsy9x"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-04-03T06:11:40.184Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-03T10:58:21.502Z", "modifiedAt": "2020-09-27T00:25:47.026Z", "url": null, "title": "Evidence for the orthogonality thesis", "slug": "evidence-for-the-orthogonality-thesis", "viewCount": null, "lastCommentedAt": "2017-12-03T15:35:01.644Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CRsYy3xtbMrLjoXZT/evidence-for-the-orthogonality-thesis", "pageUrlRelative": "/posts/CRsYy3xtbMrLjoXZT/evidence-for-the-orthogonality-thesis", "linkUrl": "https://www.lesswrong.com/posts/CRsYy3xtbMrLjoXZT/evidence-for-the-orthogonality-thesis", "postedAtFormatted": "Tuesday, April 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Evidence%20for%20the%20orthogonality%20thesis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEvidence%20for%20the%20orthogonality%20thesis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCRsYy3xtbMrLjoXZT%2Fevidence-for-the-orthogonality-thesis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Evidence%20for%20the%20orthogonality%20thesis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCRsYy3xtbMrLjoXZT%2Fevidence-for-the-orthogonality-thesis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCRsYy3xtbMrLjoXZT%2Fevidence-for-the-orthogonality-thesis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<p>One of the most annoying arguments when discussing AI is the perennial \"But if the AI is so smart, why won't it figure out the right thing to do anyway?\" It's often the ultimate curiosity stopper.</p>\n<p>Nick Bostrom has defined the \"Orthogonality thesis\" as the principle that motivation and intelligence are essentially unrelated: superintelligences can have nearly any type of motivation (at least, nearly any utility function-bases motivation). We're trying to get some rigorous papers out so that when that question comes up, we can point people to standard, and published, arguments. Nick has had a <a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">paper</a> accepted that points out the orthogonality thesis is <em>compatible</em> with a lot of philosophical positions that would seem to contradict it.</p>\n<p>I'm hoping to complement this with a paper laying out the positive arguments in favour of the thesis. So I'm asking you for your strongest arguments for (or against) the orthogonality thesis. Think of trying to convince a conservative philosopher who's caught a bad case of moral realism - what would you say to them?</p>\n<p>Many thanks! Karma and acknowledgements&nbsp;will shower on the best suggestions, and many puppies will be <a href=\"http://images.dogsandpuppies.co.uk/papillon-puppies-for-sale-breed-tips.jpg\">happy</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"BXL4riEJvJJHoydjG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CRsYy3xtbMrLjoXZT", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 14, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "14815", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 293, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-04-03T10:58:21.502Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-03T16:00:10.251Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta", "slug": "meetup-atlanta-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hankx7787", "createdAt": "2011-07-10T22:12:52.395Z", "isAdmin": false, "displayName": "hankx7787"}, "userId": "B4SKuX6dAQMnNqHzH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Zvi8jKff4KefMKbEz/meetup-atlanta-0", "pageUrlRelative": "/posts/Zvi8jKff4KefMKbEz/meetup-atlanta-0", "linkUrl": "https://www.lesswrong.com/posts/Zvi8jKff4KefMKbEz/meetup-atlanta-0", "postedAtFormatted": "Tuesday, April 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZvi8jKff4KefMKbEz%2Fmeetup-atlanta-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZvi8jKff4KefMKbEz%2Fmeetup-atlanta-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZvi8jKff4KefMKbEz%2Fmeetup-atlanta-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/8k'>Atlanta</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 April 2012 06:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2094 North Decatur Road, Decatur, GA 30033-5367</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next Atlanta meetup will be Saturday, April 7th at 6:30pm at Chocolate Coffee in Decatur:</p>\n\n<p><a href=\"http://www.mychocolatecoffee.com/\" rel=\"nofollow\">http://www.mychocolatecoffee.com/</a> <br />\n2094 North Decatur Road, Decatur, GA 30033-5367 <br />\n(404) 982-0790</p>\n\n<p>We will be starting the next sequence, \"A Human's Guide To Words\" with the main post \"37 Ways That Words Can Be Wrong\": <br />\n<a href=\"http://lesswrong.com/lw/od/37_ways_that_words_can_be_wrong/\" rel=\"nofollow\">http://lesswrong.com/lw/od/37_ways_that_words_can_be_wrong/</a></p>\n\n<p>We will also be discussing any current articles on LessWrong at the time, as well as continuing meta discussions on improving our meetup.</p>\n\n<p>Please let me know if you have any questions or comments! I hope to see all of you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/8k'>Atlanta</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Zvi8jKff4KefMKbEz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.774012644494865e-07, "legacy": true, "legacyId": "14816", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta\">Discussion article for the meetup : <a href=\"/meetups/8k\">Atlanta</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 April 2012 06:30:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2094 North Decatur Road, Decatur, GA 30033-5367</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next Atlanta meetup will be Saturday, April 7th at 6:30pm at Chocolate Coffee in Decatur:</p>\n\n<p><a href=\"http://www.mychocolatecoffee.com/\" rel=\"nofollow\">http://www.mychocolatecoffee.com/</a> <br>\n2094 North Decatur Road, Decatur, GA 30033-5367 <br>\n(404) 982-0790</p>\n\n<p>We will be starting the next sequence, \"A Human's Guide To Words\" with the main post \"37 Ways That Words Can Be Wrong\": <br>\n<a href=\"http://lesswrong.com/lw/od/37_ways_that_words_can_be_wrong/\" rel=\"nofollow\">http://lesswrong.com/lw/od/37_ways_that_words_can_be_wrong/</a></p>\n\n<p>We will also be discussing any current articles on LessWrong at the time, as well as continuing meta discussions on improving our meetup.</p>\n\n<p>Please let me know if you have any questions or comments! I hope to see all of you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta1\">Discussion article for the meetup : <a href=\"/meetups/8k\">Atlanta</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta", "anchor": "Discussion_article_for_the_meetup___Atlanta", "level": 1}, {"title": "Discussion article for the meetup : Atlanta", "anchor": "Discussion_article_for_the_meetup___Atlanta1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FaJaCgqBKphrDzDSj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-03T18:32:15.520Z", "modifiedAt": null, "url": null, "title": "[LINK] Neil deGrasse Tyson on killer asteroids", "slug": "link-neil-degrasse-tyson-on-killer-asteroids", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:21.731Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gmCKNQ9qSdt22yNaA/link-neil-degrasse-tyson-on-killer-asteroids", "pageUrlRelative": "/posts/gmCKNQ9qSdt22yNaA/link-neil-degrasse-tyson-on-killer-asteroids", "linkUrl": "https://www.lesswrong.com/posts/gmCKNQ9qSdt22yNaA/link-neil-degrasse-tyson-on-killer-asteroids", "postedAtFormatted": "Tuesday, April 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Neil%20deGrasse%20Tyson%20on%20killer%20asteroids&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Neil%20deGrasse%20Tyson%20on%20killer%20asteroids%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgmCKNQ9qSdt22yNaA%2Flink-neil-degrasse-tyson-on-killer-asteroids%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Neil%20deGrasse%20Tyson%20on%20killer%20asteroids%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgmCKNQ9qSdt22yNaA%2Flink-neil-degrasse-tyson-on-killer-asteroids", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgmCKNQ9qSdt22yNaA%2Flink-neil-degrasse-tyson-on-killer-asteroids", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 86, "htmlBody": "<p>LessWrong is not big on discussion of non-AI existential risks. But Neil deGrasse Tyson notes killer asteroids not just as a generic problem, but as a specific one, naming <a href=\"http://www.wired.com/wiredscience/2012/04/opinion-tyson-killer-asteroids/\">Apophis as an imminent hazard</a>.</p>\n<p>So treat this as your exercise for today: what are the numbers, what is the risk, what are the costs, what actions are appropriate? Assume your answers need to work in the context of a society that's responded to the notion of anthropogenic climate change with almost nothing but <a href=\"/lw/gt/a_fable_of_science_and_politics/\">blue vs. green politics</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gmCKNQ9qSdt22yNaA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 1.4e-05, "legacy": true, "legacyId": "14817", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6hfGNLf4Hg5DXqJCF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-03T21:45:14.024Z", "modifiedAt": null, "url": null, "title": "The efficiency of prizes", "slug": "the-efficiency-of-prizes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:56.335Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/85N4EF2a8MCfbZzoM/the-efficiency-of-prizes", "pageUrlRelative": "/posts/85N4EF2a8MCfbZzoM/the-efficiency-of-prizes", "linkUrl": "https://www.lesswrong.com/posts/85N4EF2a8MCfbZzoM/the-efficiency-of-prizes", "postedAtFormatted": "Tuesday, April 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20efficiency%20of%20prizes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20efficiency%20of%20prizes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F85N4EF2a8MCfbZzoM%2Fthe-efficiency-of-prizes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20efficiency%20of%20prizes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F85N4EF2a8MCfbZzoM%2Fthe-efficiency-of-prizes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F85N4EF2a8MCfbZzoM%2Fthe-efficiency-of-prizes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 533, "htmlBody": "<p>\n<p>Several commenters on <a href=\"/lw/bc3/sotw_be_specific/\">SoTW: Be Specific</a> suggest that prizes may not be a good idea because they might be counterproductive. As a counterpoint, <a href=\"http://tony-barrett.com/\">Tony Barrett</a> of <a href=\"http://www.bmsis.org/gcri\">GCRi</a> pointed me to <a href=\"http://opinionator.blogs.nytimes.com/2012/02/29/prizes-with-an-eye-toward-the-future/\">this piece</a> at <em>NYT</em>&nbsp;blogs:</p>\n<blockquote>\n<p>While awards for past achievement can spur innovation &mdash; a significant amount of American investigative journalism might not take place without prizes like the Pulitzer &mdash; forward-looking prizes for specific challenges are far more efficient. They are extraordinarily cost-effective. Look at Harrison and his clock: the &pound;20,000 he won was enormous for him, but it was a tiny sum compared to what the British government might have spent to solve the longitude problem by giving grants to various astronomers or cartographers. The fact that prizes pay only for success also makes them attractive to those who finance them, whether they are taxpayers, shareholders or charitable donors.</p>\n<p>More important, prizes work where other methods do not. A lot of problems aren&rsquo;t new &mdash; someone has already solved them or has solved something similar. By casting a very wide net, prizes find these people.</p>\n<p>Winners tend not to be the people you expect. The principal scientific adviser to the Board of Longitude &mdash; someone to be reckoned with, as it was Isaac Newton &mdash; had said that the only solutions possible for the longitude problem would come from astronomy. No one thought the answer would be found in a new kind of clock. If the British had spent money on grants to the usual suspects, it would have been wasted. The conventional wisdom is that if you want to solve a molecular biology problem you ask only molecular biologists. Big mistake.</p>\n<p>At Harvard Business School, Lakhani led a <a href=\"http://www.hbs.edu/research/pdf/07-050.pdf\">study</a> of hundreds of scientific problems posted on InnoCentive. These were problems that the laboratories of science-driven companies had mostly failed to solve, which is why they turned to InnoCentive. They found that InnoCentive&rsquo;s network solved nearly 30 percent of them.</p>\n<p>What made for success? InnoCentive asks solvers to check boxes indicating the different scientific fields that interest them. The more diverse the interests of the base of solvers, the more likely the problem was to be solved. The study also found that expertise in the field of the problem actually hurt a solver&rsquo;s chances. &ldquo;The further the problem from the solver&rsquo;s expertise, the more likely they are to solve it,&rdquo; Lakhini and his co-authors concluded. If the problem fell completely outside a solver&rsquo;s expertise, that raised his or her chance of success by 10 percent. In addition to being a technical outsider, being a social outsider also helped &mdash; women did significantly better than men, perhaps because they tended to be more marginalized in the scientific community. Alph Bingham, one of InnoCentive&rsquo;s founders, told McKinsey that &ldquo;you wouldn&rsquo;t hire&rdquo; a significant percentage of successful solvers based on their credentials.</p>\n<p>This seems bizarre, but it is consistent with scientific history, which shows that innovation occurs when knowledge from one scientific discipline is applied to another. &ldquo;Innovation happens when someone comes in from a different perspective and breaks a problem open,&rdquo; says Lakhani. &ldquo;But rarely do we have mechanisms in place so this happens systematically.&rdquo; Prizes provide that mechanism.</p>\n</blockquote>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sPpZRaxpNNJjw55eu": 1, "ipJwbLxhR83ZksN6Z": 1, "aHjTRDkGypPqbXWpN": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "85N4EF2a8MCfbZzoM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 37, "extendedScore": null, "score": 8.77545102323004e-07, "legacy": true, "legacyId": "14818", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NgtYDP3ZtLJaM248W"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-03T23:36:37.551Z", "modifiedAt": null, "url": null, "title": "Chess Analyst \"solves\" King's Gambit", "slug": "chess-analyst-solves-king-s-gambit", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:56.977Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "N6W7sAzCo3fGauM7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FvhBwKKkaaab8eLLA/chess-analyst-solves-king-s-gambit", "pageUrlRelative": "/posts/FvhBwKKkaaab8eLLA/chess-analyst-solves-king-s-gambit", "linkUrl": "https://www.lesswrong.com/posts/FvhBwKKkaaab8eLLA/chess-analyst-solves-king-s-gambit", "postedAtFormatted": "Tuesday, April 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Chess%20Analyst%20%22solves%22%20King's%20Gambit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChess%20Analyst%20%22solves%22%20King's%20Gambit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFvhBwKKkaaab8eLLA%2Fchess-analyst-solves-king-s-gambit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Chess%20Analyst%20%22solves%22%20King's%20Gambit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFvhBwKKkaaab8eLLA%2Fchess-analyst-solves-king-s-gambit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFvhBwKKkaaab8eLLA%2Fchess-analyst-solves-king-s-gambit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p><strong>Edit: </strong>it was unfortunately a prank. I definitely checked the date of the article (which is dated Apr. 2), before posting on it. Kind of mean to make an April Fool's prank after April Fool's. I didn't realize I'd have a chance to <a href=\"http://suitdummy.blogspot.com/2012/03/i-idiot.html\">practice what I preach</a> so soon.</p>\n<p><a href=\"/lw/i9/the_importance_of_saying_oops/\">I guess I need to just say oops.</a></p>\n<p>&nbsp;</p>\n<p><strong>Original Post:</strong></p>\n<p>Chess analyst Vasik Rajlich had some <a href=\"http://www.chessbase.com/newsdetail.asp?newsid=8047\">big news today:</a> solving the King&rsquo;s Gambit.</p>\n<p>I know that this doesn&rsquo;t add much new to the complexity theory aspects of games like chess, but I would say it&rsquo;s a beautiful result, very much like the recent improvement on the complexity of matrix multiplication, and it certainly emphasizes the role computation plays as the King&rsquo;s Gambit is a pretty popular, classical opening. By most any human standard it&rsquo;s a respectable opening, and yet we can conclusively say it is unequivocally bad for White assuming two rational players.</p>\n<p>I wrote up a <a href=\"http://suitdummy.blogspot.com/2012/04/goodbye-kings-gambit.html\">short blurb</a> about it at my blog.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FvhBwKKkaaab8eLLA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 6, "extendedScore": null, "score": 8.775915445074788e-07, "legacy": true, "legacyId": "14819", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wCqfCLs8z5Qw4GbKS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-04T02:53:41.790Z", "modifiedAt": null, "url": null, "title": "Harry Potter and the Methods of Rationality discussion thread, part 14, chapter 82", "slug": "harry-potter-and-the-methods-of-rationality-discussion-15", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:34.111Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FAWS", "createdAt": "2010-01-09T18:58:38.832Z", "isAdmin": false, "displayName": "FAWS"}, "userId": "a7Neq3q2DbWrbo6B6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pBTcCB5uJTzADdMm4/harry-potter-and-the-methods-of-rationality-discussion-15", "pageUrlRelative": "/posts/pBTcCB5uJTzADdMm4/harry-potter-and-the-methods-of-rationality-discussion-15", "linkUrl": "https://www.lesswrong.com/posts/pBTcCB5uJTzADdMm4/harry-potter-and-the-methods-of-rationality-discussion-15", "postedAtFormatted": "Wednesday, April 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2014%2C%20chapter%2082&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2014%2C%20chapter%2082%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpBTcCB5uJTzADdMm4%2Fharry-potter-and-the-methods-of-rationality-discussion-15%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harry%20Potter%20and%20the%20Methods%20of%20Rationality%20discussion%20thread%2C%20part%2014%2C%20chapter%2082%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpBTcCB5uJTzADdMm4%2Fharry-potter-and-the-methods-of-rationality-discussion-15", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpBTcCB5uJTzADdMm4%2Fharry-potter-and-the-methods-of-rationality-discussion-15", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 275, "htmlBody": "<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong>The new discussion thread (part 15) is <a href=\"/r/discussion/lw/bmx/harry_potter_and_the_methods_of_rationality/\">here</a>.&nbsp;</strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong><br /></strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is a new thread to discuss Eliezer Yudkowsky&rsquo;s&nbsp;<em><a style=\"color: #8a8a8b;\" href=\"http://www.fanfiction.net/s/5782108/1/\">Harry Potter and the Methods of Rationality</a></em>&nbsp;and anything related to it. This thread is intended for discussing <a href=\"http://www.fanfiction.net/s/5782108/82/\">chapter</a> <a href=\"http://hpmor.com/chapter/82\">82</a>.&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/b7s/harry_potter_and_the_methods_of_rationality/\">The previous thread</a>&nbsp;passed 1000 comments as of the time of this writing, and so has long passed 500. C<strong>omment in&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/b7s/harry_potter_and_the_methods_of_rationality/\">the 13th thread</a>&nbsp;until you read chapter 82.</strong>&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">There is now a site dedicated to the story at&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/\">hpmor.com</a>, which is now the place to go to find the&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://hpmor.com/notes/\">authors notes</a>&nbsp;and all sorts of other goodies. AdeleneDawner has kept an&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://www.evernote.com/pub/adelenedawner/Eliezer\">archive of Author&rsquo;s Notes</a>. (This goes up to the notes for chapter 76, and is now not updating. The authors notes from chapter 77 onwards are on hpmor.com.)&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">The first 5 discussion threads are on the main page under the&nbsp;<a style=\"color: #8a8a8b;\" href=\"/tag/harry_potter/\">harry_potter tag</a>.&nbsp; Threads 6 and on (including this one) are in the&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/tag/harry_potter/\">discussion section</a>&nbsp;using its separate tag system.&nbsp; Also:&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2ab/harry_potter_and_the_methods_of_rationality\">1</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2ie/harry_potter_and_the_methods_of_rationality\">2</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2nm/harry_potter_and_the_methods_of_rationality\">3</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality\">4</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/30g/harry_potter_and_the_methods_of_rationality\">5</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/364/harry_potter_and_the_methods_of_rationality/\">6</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/3rb/harry_potter_and_the_methods_of_rationality/\">7</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/797/harry_potter_and_the_methods_of_rationality/\">8</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/7jd/harry_potter_and_the_methods_of_rationality\">9</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/ams/harry_potter_and_the_methods_of_rationality\">10</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/axe/harry_potter_and_the_methods_of_rationality/\">11</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/b5s/harry_potter_and_the_methods_of_rationality/\">12</a>,&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/b7s/harry_potter_and_the_methods_of_rationality/\">13</a>.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">As a reminder, it&rsquo;s often useful to start your comment by indicating which chapter you are commenting on.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><strong>Spoiler Warning</strong>: this thread is full of spoilers. With few exceptions, spoilers for MOR and canon are fair game to post, without warning or rot13.&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/2tr/harry_potter_and_the_methods_of_rationality/2v1l\">More specifically</a>:</p>\n<blockquote style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">You do not need to rot13 anything about HP:MoR or the original Harry Potter series unless you are posting insider information from Eliezer Yudkowsky which is not supposed to be publicly available (which includes public statements by Eliezer that have been retracted).</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">If there is evidence for X in MOR and/or canon then it&rsquo;s fine to post about X without rot13, even if you also have heard privately from Eliezer that X is true. But you should not post that &ldquo;Eliezer said X is true&rdquo; unless you use rot13.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pBTcCB5uJTzADdMm4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 8.776735253741387e-07, "legacy": true, "legacyId": "14820", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 794, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XN4WDRSPFo9iGEuk3", "xK6Pswbozev6pv4A6", "59rDBidWmmJTXL4Np", "xexS9nyzwRgP9sowp", "LzQcmBwAJBGyzrt6Z", "qKzeJvFWyPh5H2hwj", "nnnd4KRQxs6DYcehD", "y2Hszb4Dsm5FggnDC", "6ae2kq3JmKvL4YPgk", "zvXfBqp6TSriNkmbg", "WQ7XMjqvuRRj8nkpu", "LKFR5pBA3bBkERDxL", "8yEdpDpGgvDWHeodM", "K4JBpAxhvstdnNbeg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-04T04:32:07.652Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Classical Configuration Spaces", "slug": "seq-rerun-classical-configuration-spaces", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8494ns9GH3urHJnDY/seq-rerun-classical-configuration-spaces", "pageUrlRelative": "/posts/8494ns9GH3urHJnDY/seq-rerun-classical-configuration-spaces", "linkUrl": "https://www.lesswrong.com/posts/8494ns9GH3urHJnDY/seq-rerun-classical-configuration-spaces", "postedAtFormatted": "Wednesday, April 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Classical%20Configuration%20Spaces&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Classical%20Configuration%20Spaces%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8494ns9GH3urHJnDY%2Fseq-rerun-classical-configuration-spaces%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Classical%20Configuration%20Spaces%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8494ns9GH3urHJnDY%2Fseq-rerun-classical-configuration-spaces", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8494ns9GH3urHJnDY%2Fseq-rerun-classical-configuration-spaces", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<p>Today's post, <a href=\"/lw/pi/classical_configuration_spaces/\">Classical Configuration Spaces</a> was originally published on 15 April 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Classical_Configuration_Spaces\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>How to visualize the state of a system of two 1-dimensional particles, as a single point in 2-dimensional space. Understanding configuration spaces in classical physics is a useful first step, before trying to imagine quantum configuration spaces.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/bey/seq_rerun_can_you_prove_two_particles_are/\">Can You Prove Two Particles Are Identical?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8494ns9GH3urHJnDY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.777147677152548e-07, "legacy": true, "legacyId": "14822", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KAHt3t7a6KH4kfX4L", "y7nY9A9DA4jNnsWth", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-04-04T06:33:32.097Z", "modifiedAt": null, "url": null, "title": "Rationality anecdotes for the homepage?", "slug": "rationality-anecdotes-for-the-homepage", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:51.365Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MwPCcmwxWfwtH7yLh/rationality-anecdotes-for-the-homepage", "pageUrlRelative": "/posts/MwPCcmwxWfwtH7yLh/rationality-anecdotes-for-the-homepage", "linkUrl": "https://www.lesswrong.com/posts/MwPCcmwxWfwtH7yLh/rationality-anecdotes-for-the-homepage", "postedAtFormatted": "Wednesday, April 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20anecdotes%20for%20the%20homepage%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20anecdotes%20for%20the%20homepage%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMwPCcmwxWfwtH7yLh%2Frationality-anecdotes-for-the-homepage%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20anecdotes%20for%20the%20homepage%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMwPCcmwxWfwtH7yLh%2Frationality-anecdotes-for-the-homepage", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMwPCcmwxWfwtH7yLh%2Frationality-anecdotes-for-the-homepage", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 238, "htmlBody": "<p>In the comments for <a href=\"/lw/7e5/the_cognitive_science_of_rationality/\">The Cognitive Science of Rationality</a>, Spurlock said</p>\n<blockquote>\n<p>The beginning of this post (the list of concrete, powerful, real/realistic, and avoidable cases of irrationality in action), is probably the best introduction to x-rationality I've read yet. I can easily imagine it hooking lots of potential readers that our previous attempts at introduction (our home page, the \"welcome to LW\" posts, etc) wouldn't.</p>\n<p>In fact, I'd nominate some version of that text as our new home page text, perhaps just changing out the last couple sentences to something that encompasses more of LW in general (rather than cogsci specifically). I mean this as a serious actionable suggestion.</p>\n</blockquote>\n<p>There are couple problems with using the specific anecdotes from the post:</p>\n<ul>\n<li>It would make the beginning of the post seem boring for anyone who had read the homepage.</li>\n<li>There has been discussion on LW that the sunk cost fallacy may not be much of a fallacy in practice, and commenters on the post were also skeptical of the rare disease example.</li>\n</ul>\n<div>But the idea of starting our website off with concrete examples, the way Eliezer <a href=\"/r/lesswrong/lw/bc3/sotw_be_specific/\">recently recommended</a> starting off essays, seems like a good one.</div>\n<div>So what are some quick, concrete, compelling stories about how&nbsp;irrationality sucks/rationality rocks that we could put on the homepage?&nbsp;Bonus points if the story is straight from a study, or is a true story that happened to you or someone you know.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MwPCcmwxWfwtH7yLh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 5, "extendedScore": null, "score": 8.777654030692461e-07, "legacy": true, "legacyId": "14823", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xLm9mgJRPvmPGpo7Q", "NgtYDP3ZtLJaM248W"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}