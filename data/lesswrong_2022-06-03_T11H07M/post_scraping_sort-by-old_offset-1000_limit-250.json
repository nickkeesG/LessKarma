{"results": [{"createdAt": null, "postedAt": "2009-05-13T22:51:50.133Z", "modifiedAt": null, "url": null, "title": "A Parable On Obsolete Ideologies", "slug": "a-parable-on-obsolete-ideologies", "viewCount": null, "lastCommentedAt": "2021-07-02T16:37:42.864Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ltey8BS83qSkd9M3u/a-parable-on-obsolete-ideologies", "pageUrlRelative": "/posts/Ltey8BS83qSkd9M3u/a-parable-on-obsolete-ideologies", "linkUrl": "https://www.lesswrong.com/posts/Ltey8BS83qSkd9M3u/a-parable-on-obsolete-ideologies", "postedAtFormatted": "Wednesday, May 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Parable%20On%20Obsolete%20Ideologies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Parable%20On%20Obsolete%20Ideologies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLtey8BS83qSkd9M3u%2Fa-parable-on-obsolete-ideologies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Parable%20On%20Obsolete%20Ideologies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLtey8BS83qSkd9M3u%2Fa-parable-on-obsolete-ideologies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLtey8BS83qSkd9M3u%2Fa-parable-on-obsolete-ideologies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2004, "htmlBody": "<p><strong>Followup to:</strong>&nbsp; <a href=\"/lw/4i/bhtv_yudkowsky_adam_frank_on_religious_experience/\">Yudkowsky and Frank on Religious Experience</a>, <a href=\"http://bloggingheads.tv/diavlogs/19610\">Yudkowksy and Frank On Religious Experience Pt 2</a><br /><strong>With sincere apologies to:</strong> <a href=\"http://en.wikipedia.org/wiki/Godwin%27s_Law\">Mike Godwin</a></p>\n<p>You are General Eisenhower. It is 1945. The Allies have just triumphantly liberated Berlin. As the remaining leaders of the old regime are being tried and executed, it begins to become apparent just how vile and despicable the Third Reich truly was.<br /><br />In the midst of the chaos, a group of German leaders come to you with a proposal. Nazism, they admit, was completely wrong. Its racist ideology was false and its consequences were horrific. However, in the bleak poverty of post-war Germany, people need to keep united somehow. They need something to believe in. And a whole generation of them have been raised on Nazi ideology and symbolism. Why not take advantage of the national unity Nazism provides while discarding all the racist baggage? \"Make it so,\" you say.<br /><br />The swastikas hanging from every boulevard stay up, but now they represent \"traditional values\" and even \"peace\". Big pictures of Hitler still hang in every government office, not because Hitler was right about racial purity, but because he represents the desire for spiritual purity inside all of us, and the desire to create a better society by any means necessary. It's still acceptable to shout \"KILL ALL THE JEWS AND GYPSIES AND HOMOSEXUALS!\" in public places, but only because everyone realizes that Hitler meant \"Jews\" as a metaphor for \"greed\", \"gypsies\" as a metaphor for \"superstition\", and \"homosexuals\" as a metaphor for \"lust\", and so what he really meant is that you need to kill the greed, lust, and superstition in your own heart. Good Nazis love real, physical Jews! Some Jews even choose to join the Party, inspired by their principled stand against spiritual evil.<br /><br />The Hitler Youth remains, but it's become more or less a German version of the Boy Scouts. The Party infrastructure remains, but only as a group of spiritual advisors helping people fight the untermenschen in their own soul. They suggest that, during times of trouble, people look to Mein Kampf for inspiration. If they open to a sentence like \"The Aryan race shall conquer all in its path\", then they can interpret \"the Aryan race\" to mean \"righteous people\", and the sentence is really just saying that good people can do anything if they set their minds to it. Isn't that lovely?<br /><br />Soon, \"Nazi\" comes to just be a synonym for \"good person\". If anyone's not a member of the Nazi Party, everyone immediately becomes suspicious. Why is she against exterminating greed, lust, and superstition from her soul? Does she really not believe good people can do anything if they set their minds to it? Why does he oppose <a href=\"http://en.wikiquote.org/wiki/Adolf_Hitler#Undated\">caring for your aging parents</a>? We definitely can't trust <em>him</em> with high political office.</p>\n<p><a id=\"more\"></a></p>\n<p>It is four years later. Soon, the occupation will end, and Germany will become an independent country once again. The Soviets have already taken East Germany and turned it Communist. As the de facto ruler of West Germany, its fate is in your hands. You ask your two most trusted subordinates for advice.<br /><br />First, Colonel F gives his suggestion. It is vital that you order the preservation of the Nazi ideology so that Germany remains strong. After all, the Germans will need to stay united as a people in order to survive the inevitable struggle with the Soviets. If Nazism collapsed, then people would lose everything that connects them together, and become dispirited. The beautiful poetry of Mein Kampf speaks to something deep in the soul of every German, and if the Allies try to eradicate that just because they disagree with one outdated interpretation of the text, they will have removed meaning from the lives of millions of people all in the name of some sort of misguided desire to take everything absolutely literally all the time.<br /><br />Your other trusted subordinate, Colonel Y, disagrees. He thinks that Mein Kampf may have some rousing passages, but that there's no <a href=\"/lw/eo/special_status_needs_special_support/\">special reason</a> it has a unique ability to impart meaning to people other than that everyone believes it does. Not only that, but the actual contents of Mein Kampf are repulsive. Sure, if you make an extraordinary effort to gloss over or reinterpret the repulsive passages, you can do it, but this is more trouble than it is worth and might very well leave some lingering mental poison behind. Germany should completely lose all the baggage of Nazism and replace it with a completely democratic society that has no causal linkage whatsoever to its bloody past.<br /><br />Colonel F objects. He hopes you don't just immediately side with Colonel Y just because the question includes the word \"Nazi\". Condemning Nazism is an obvious <a href=\"http://www.overcomingbias.com/2007/09/applause-lights.html\">applause light</a>, but a political decision of this magnitude requires a more carefully thought-out decision. After all, Nazism has been purged of its most objectionable elements, and the Germans really do seem to like it and draw a richer life from it. Colonel Y needs to have a better reason his personal distaste for an ideology because of past history in order to take it away from them.<br /><br />Colonel Y thinks for a moment, then begins speaking. You have noticed, he says, that the new German society also has a lot of normal, \"full-strength\" Nazis around. The \"reformed\" Nazis occasionally denounce these people, and accuse them of misinterpreting Hitler's words, but they don't seem nearly as offended by the \"full-strength\" Nazis as they are by the idea of people who reject Nazism completely.<br /><br />Might the existence of \"reformed\" Nazis, he asks, enable \"full-strength\" Nazis to become more powerful and influential? He thinks it might. It becomes impossible to condemn \"full-strength\" Nazis for worshipping a horrible figure like Hitler, or adoring a horrible book like Mein Kampf, when they're doing the same thing themselves. At worst, they can just say the others are misinterpreting it a little. And it will be very difficult to make this argument, because all evidence suggests that in fact it's the \"full-strength\" Nazis who are following Hitler's original intent and the true meaning of Mein Kampf, and the \"reformed\" Nazis who have reinterpreted it for political reasons. Assuming the idea of not being a Nazi at all remains socially beyond the pale, intellectually honest people will feel a strong pull towards \"full-strength\" Nazism.<br /><br />Even if the \"reformed\" Nazis accept all moderate liberal practices considered reasonable today, he says, their ideology might still cause trouble later. Today, in 1945, mixed race marriage is still considered taboo by most liberal societies, including the United States. The re-interpreters of Mein Kampf have decided that, although \"kill all the Jews\" is clearly metaphorical, \"never mix races\" is meant literally. If other nations began legalizing mixed race marriage in the years to come, Party members will preach to the faithful that it is an abomination, and can even point to the verse in Mein Kampf that said so. It's utterly plausible that a \"reformed\" Nazi Germany may go on forbidding mixed race marriage much longer than surrounding countries. Even if Party leaders eventually bow to pressure and change their interpretation, the Party will always exist as a force opposing racial equality and social justice until the last possible moment.<br /><br />And, he theorizes, there could be even deeper subconscious influences. He explains that people often process ideas and morals <a href=\"/lw/48/the_power_of_positivist_thinking/\">in ways that are only tangentially linked to specific facts and decisions</a>. Instead, we tend to <a href=\"/lw/bk/the_trouble_with_good/\">conflate things into huge, fuzzy concepts and assign \"good\" and \"bad\" tags to them</a>. Saying \"Jews are bad, but this doesn't apply to actual specific Jews\" is the sort of thing the brain isn't very good at. At best, it will end out with the sort of forced politeness <a href=\"/lw/53/the_implicit_association_test/\">a person who's trying very hard not be racist</a> shows around black people. As soon as we assign a good feeling to the broad idea of \"Nazism\", that reflects at least a little on everything Nazism stands for, everything Nazism ever has stood for, and every person who identifies as a Nazi.<br /><br />He has read other essays that discuss <a href=\"/lw/4h/when_truth_isnt_enough/\">the ability of connotations to warp thinking</a>. Imagine you're taught things like \"untermenschen like Jews and Gypsies are people too, and should be treated equally.\" The content of this opinion is perfectly fine. Unfortunately, it creates a category called \"untermenschen\" with a bad connotation and sticks Jews and Gypsies into it. Once you have accepted that Jews and Gypsies comprise a different category, even if that category is \"people who are exactly like the rest of us except for being in this category here\", three-quarters of the damage is already done. Here the Colonel sighs, and reminds you of <a href=\"http://www.overcomingbias.com/2008/02/sneak-connotati.html\">the discrimination faced by wiggins in the modern military</a>.<br /><br />And (he adds) won't someone please think of the children? They're not very good at metaphor, they trust almost anything they hear, and they form a scaffolding of belief that later life can only edit, not demolish and rebuild. If someone was scared of ghosts as a child, <a href=\"/lw/1l/the_mystery_of_the_haunted_rationalist/\">they may not believe in ghosts now, but they're going to have some visceral reaction to them</a>. Imagine telling a child \"We should kill everyone in the lesser races\" five times a day, on the assumption that once they're a teenager they'll understand what a \"figurative\" means and it'll all be okay.<br /><br />He closes by telling you that he's not at all convinced that whatever metaphors the Nazis reinterpret Mein Kampf to mean aren't going to be damaging in themselves. After all, these metaphors will have been invented by Nazis, who are not exactly known for choosing the best moral lessons. What if \"kill all lesser races\" gets reinterpreted to \"have no tolerance for anything that is less than perfect\"? This <em>sounds</em> sort of like a good moral lesson, until people start preaching that it means we should lock up gay people, because homosexuality is an \"imperfection\". That, he says, is the sort of thing that happens when you get your morality from cliched maxims taken by drawing vapid conclusions from despicably evil works of literature.<br /><br />So, the Colonel concludes, if you really want the German people to be peaceful and moral, you really have no choice but to nip this growing \"reformed Nazi\" movement in the bud. Colonel F has made some good points about respecting the Germans' culture, but doing so would make it difficult to eradicate their existing racist ideas, bias their younger generation towards habits of thought that encourage future racism, create a strong regressive tendency in their society, and yoke them to poorly fashioned moral arguments.<br /><br />And, he finishes, he doesn't really think Nazism is that necessary for Germany to survive. Even in some crazy alternate universe where the Allies had immediately cracked down on Nazism as soon as they captured Berlin, yea, even in the absurd case where Germany immediately switched to a completely democratic society that condemned everything remotely associated with Nazism as evil and <a href=\"http://en.wikipedia.org/wiki/Strafgesetzbuch_%C2%A7_86a\">even banned swastikas and pictures of Hitler from even being displayed</a> - even in that universe, Germans would keep a strong cultural identity and find new symbols of their patriotism.<br /><br />Ridiculous, Colonel F objects! In such a universe, the Germans would be left adrift without the anchor of tradition, and immediately be taken over by the Soviets.<br /><br />Colonel Y just smiles enigmatically. You are reminded of the time he first appeared at your command tent, during the middle of an unnatural thunderstorm, with a copy of Hugh Everett's <em>The Theory of the Universal Wave Function</em> tucked under one arm. You shudder, shake your head, and drag yourself back to the present.<br /><br />So, General, what is your decision?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FkzScn5byCs9PxGsA": 3, "NSMKfa8emSbGNXRKD": 3, "gHCNhqxuJq2bZ2akb": 2, "F2XfCTxXLQBGjbm8P": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ltey8BS83qSkd9M3u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 172, "baseScore": 171, "extendedScore": null, "score": 0.000261, "legacy": true, "legacyId": "562", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 171, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 288, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xDroHJ3AzWwJ45ufJ", "jaW5XerRuYRhzjDLE", "azoP7WeKYYfgCozoh", "M2LWXsJxKS626QNEA", "iYJo382hY28K7eCrP", "9hR2RmpJmxT8dyPo4", "mja6jZ6k9gAwki9Nu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-14T06:17:02.672Z", "modifiedAt": null, "url": null, "title": "\"Open-Mindedness\" - the video", "slug": "open-mindedness-the-video", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:37.553Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4qqzCF7MzyPh76nNX/open-mindedness-the-video", "pageUrlRelative": "/posts/4qqzCF7MzyPh76nNX/open-mindedness-the-video", "linkUrl": "https://www.lesswrong.com/posts/4qqzCF7MzyPh76nNX/open-mindedness-the-video", "postedAtFormatted": "Thursday, May 14th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Open-Mindedness%22%20-%20the%20video&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Open-Mindedness%22%20-%20the%20video%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4qqzCF7MzyPh76nNX%2Fopen-mindedness-the-video%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Open-Mindedness%22%20-%20the%20video%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4qqzCF7MzyPh76nNX%2Fopen-mindedness-the-video", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4qqzCF7MzyPh76nNX%2Fopen-mindedness-the-video", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 38, "htmlBody": "<p><a href=\"http://www.dailymotion.com/user/totocacapouet/video/x8uei4_openmindedness_tech\">An interesting little Flash-like video on \"openmindedness\"</a> by someone named QualiaSoup (hopefully ironically).</p>\n<p>Does anyone know how much effort is required to produce this sort of video, perhaps from a script?&nbsp; We need at least another thousand of these.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"moeYqrcakMgXnQNyF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4qqzCF7MzyPh76nNX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 21, "extendedScore": null, "score": 4.941155927478296e-07, "legacy": true, "legacyId": "563", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-14T23:41:06.878Z", "modifiedAt": null, "url": null, "title": "Religion, Mystery, and Warm, Soft Fuzzies", "slug": "religion-mystery-and-warm-soft-fuzzies", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:28.918Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Psychohistorian", "createdAt": "2009-04-05T18:10:22.976Z", "isAdmin": false, "displayName": "Psychohistorian"}, "userId": "mAQgf4N8jv2jTMK5B", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Soae3L98bBooTpzez/religion-mystery-and-warm-soft-fuzzies", "pageUrlRelative": "/posts/Soae3L98bBooTpzez/religion-mystery-and-warm-soft-fuzzies", "linkUrl": "https://www.lesswrong.com/posts/Soae3L98bBooTpzez/religion-mystery-and-warm-soft-fuzzies", "postedAtFormatted": "Thursday, May 14th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Religion%2C%20Mystery%2C%20and%20Warm%2C%20Soft%20Fuzzies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReligion%2C%20Mystery%2C%20and%20Warm%2C%20Soft%20Fuzzies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSoae3L98bBooTpzez%2Freligion-mystery-and-warm-soft-fuzzies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Religion%2C%20Mystery%2C%20and%20Warm%2C%20Soft%20Fuzzies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSoae3L98bBooTpzez%2Freligion-mystery-and-warm-soft-fuzzies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSoae3L98bBooTpzez%2Freligion-mystery-and-warm-soft-fuzzies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 752, "htmlBody": "<p>Reaction to: <a href=\"/lw/4i/bhtv_yudkowsky_adam_frank_on_religious_experience\">Yudkowsky and Frank on Religious Experience</a>, <a href=\"http://bloggingheads.tv/diavlogs/19610\">Yudkowksy and Frank On Religious Experience Pt 2,</a><span style=\"text-decoration: underline;\"> </span><a class=\"reddit-link-title\" href=\"/lw/fm/a_parable_on_obsolete_ideologies\">A Parable On Obsolete Ideologies</a></p>\n<p>Frank's point got rather lost in all this. It seems to be quite simple: there's a warm fuzziness to life that science just doesn't seem to get, and some religious artwork touches on and stimulates this warm fuzziness, and hence is of value.<sup>1 </sup>Moreover, understanding this point seems rather important to being able to spread an ideology.</p>\n<p>The main problem is viewing this warm fuzziness as a \"mystery.\" This warm fuzziness, as an experience, is a <em>reality.</em> It's part of that set of things that doesn't go away no matter what you say or think about them. Women (or men) will still be alluring, food will still be delicious, and Michaelangelo's David will still be beautiful, no matter how well you describe these phenomenon. The view that shattering mysteries reduces their value is very much a result of religion trying to protect itself. EY is probably correct that science will one day destroy this mystery as it has so many others, but because it is an \"experience we can't clearly describe\" rather than an actual \"mystery,\" the experience will remain. The argument is with the <em>description</em>, not the experience; the experience is real, and experiences of its nature are totally desirable.</p>\n<p>The second, sub-point: Frank thinks that certain religious stories and artwork may be of artistic value. The selection of the story of Job is unfortunate, but both speakers value it for the same reason: its truth. One sees it as true (and inspiring) and likes it, the other sees it as false (and insidious) and hates it. I think both agree that if you put it on the shelf next to Tolkien, and rational atheists still buy it and enjoy it, hey, good for Job. And if not, well, throw it out with the rest of the trash.<a id=\"more\"></a></p>\n<p>Frank also has a point about rationality not being the only way to view the world. I think he's once again right, he's just really, tragically bad at expressing his point without borrowing heavily from religion. His point seems to be that rationality isn't the only way to *experience* the world, which is absolutely, 100% right. You don't experience the world through rationality. You experience it through your senses and the qualia of consciousness. Rationality is how you figure out what's going on, or what's going to be going on, or what causes one thing to happen and not another. Appreciating art, or food, or sex, or life is not generally done by applying rationality. Rationality is extremely useful for figuring out how to get these things we like, or even figure out what things we <em>should</em> like, but it doesn't factor into the qualitative experience of those things in most cases. For many people it probably doesn't factor into the enjoyment of <em>anything</em>. If you don't embrace and explain this distinction, you come out looking like Spock.</p>\n<p>This seems to be a key point atheists fail to communicate, because it is logically irrelevant to the truth of their propositions. A lot of people avoid decisions that they believe will destroy everything that makes them happy, and I'm not sure we can blame them. It's important to explain that you can still have all kinds of warm fuzziness, and, even better, you can be really confident it's well-founded <em>and </em>avoid abysmal epistemology, too! Instead, the atheist tries to defeat some weird, religiously-motivated expression of warm fuzziness, and that becomes the debate, and people like their fuzzies.</p>\n<p>We experience warm fuzziness directly,<sup>2</sup> through however our brains work. No amount of science is likely to change that, no matter how well it understands the phenomenon. This is a good thing for science, and it's a good thing for warmth and fuzziness.</p>\n<p>&nbsp;</p>\n<hr />\n<p><strong>1- </strong>I have admittedly not read his book. It's quite possible he's advocating we actually go through religion and make it fit our current sensibilities, then take it as uber-fiction. If that's the case, I have serious problems with it. If that's not the case, and he just thinks that some of it contains truth/beauty/is salvagable as literature, then I have serious problems with the <a href=\"http://en.wikipedia.org/wiki/Reductio_ad_Hitlerum\">argumentum-ad-hitlerum</a> <a href=\"/lw/fm/a_parable_on_obsolete_ideologies\">employed against him</a>, as it seems to burn a straw man.</p>\n<p>&nbsp;</p>\n<p><strong>2</strong> - I'm not saying there's warm fuzziness in the territory and we put it in our map. There's <em>something</em> in the territory that, when we map it out, the mapping causes us to directly experience a feeling of warm fuzziness.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb0e9": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Soae3L98bBooTpzez", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 21, "extendedScore": null, "score": 3.2e-05, "legacy": true, "legacyId": "564", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 121, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xDroHJ3AzWwJ45ufJ", "Ltey8BS83qSkd9M3u"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-15T02:26:25.688Z", "modifiedAt": null, "url": null, "title": "Cheerios: An \"Untested New Drug\"", "slug": "cheerios-an-untested-new-drug", "viewCount": null, "lastCommentedAt": "2014-01-09T16:06:55.451Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qJM5kuxN8j3PwNNx6/cheerios-an-untested-new-drug", "pageUrlRelative": "/posts/qJM5kuxN8j3PwNNx6/cheerios-an-untested-new-drug", "linkUrl": "https://www.lesswrong.com/posts/qJM5kuxN8j3PwNNx6/cheerios-an-untested-new-drug", "postedAtFormatted": "Friday, May 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cheerios%3A%20An%20%22Untested%20New%20Drug%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACheerios%3A%20An%20%22Untested%20New%20Drug%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqJM5kuxN8j3PwNNx6%2Fcheerios-an-untested-new-drug%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cheerios%3A%20An%20%22Untested%20New%20Drug%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqJM5kuxN8j3PwNNx6%2Fcheerios-an-untested-new-drug", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqJM5kuxN8j3PwNNx6%2Fcheerios-an-untested-new-drug", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<p>I found <a href=\"http://www.fda.gov/foi/warning_letters/s7188c.htm\">this letter</a> from the US Food and Drug Administration to General Mills interesting. It appears on the surface that the agency is trying to protect the American public from ungrounded persuasion, yet I can't find anything in the letter claiming that GM has made an unsupported statement.</p>\n<p>Does anyone understand this better than I do?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xHjy88N2uJvGdgzfw": 1, "FkzScn5byCs9PxGsA": 1, "cHoCqtfE9cF7aSs9d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qJM5kuxN8j3PwNNx6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 9, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "565", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2009-05-15T02:26:25.688Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-15T05:04:55.374Z", "modifiedAt": null, "url": null, "title": "Essay-Question Poll: Voting", "slug": "essay-question-poll-voting", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:03.797Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y7TmaJReDrEHQmBce/essay-question-poll-voting", "pageUrlRelative": "/posts/y7TmaJReDrEHQmBce/essay-question-poll-voting", "linkUrl": "https://www.lesswrong.com/posts/y7TmaJReDrEHQmBce/essay-question-poll-voting", "postedAtFormatted": "Friday, May 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Essay-Question%20Poll%3A%20Voting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEssay-Question%20Poll%3A%20Voting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy7TmaJReDrEHQmBce%2Fessay-question-poll-voting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Essay-Question%20Poll%3A%20Voting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy7TmaJReDrEHQmBce%2Fessay-question-poll-voting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy7TmaJReDrEHQmBce%2Fessay-question-poll-voting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 452, "htmlBody": "<p>There has been a considerable amount of discussion scattered around Less Wrong about voting, what software features having to do with voting should be added or subtracted, what purpose voting should serve, etc.&nbsp; It seems as though it would be useful to have conveniently consolidated information on how people are actually voting, so we know what habits that we want to encourage or discourage are actually in use and how prevalently.</p>\n<p>1. About what percentage of comments do you vote on at all?&nbsp; What percentage of top-level posts?</p>\n<p>2. Do you use the <a href=\"/lw/cf/lesswrong_boo_vote_stochastic_downvoting/\">boo vote</a> or the <a href=\"/lw/1s/lesswrong_antikibitzer_hides_comment_authors_and/\">anti-kibitzer</a> extensions?&nbsp; Why or why not?</p>\n<p>3. What karma threshold do you use to filter what you see, if any?</p>\n<p>4. When you vote on a post, or read it and decide not to vote on it, what features of the post are you <em>occurrently conscious of</em> that influence your decision either way?&nbsp; (Submitter, current post score, length, style, topic, spelling, whatever.)&nbsp; What about comments?</p>\n<p><a id=\"more\"></a></p>\n<p>5. When you vote on a post, or read it and decide not to vote on it, are there any features of the post that you suspect you may react to subconsciously?&nbsp; What about comments?</p>\n<p>6. When you vote on a post, or read it and decide not to vote on it, how do the features to which you react influence you?&nbsp; What about comments?</p>\n<p>7. Do you make comments saying how you voted and why, on posts or on other comments?&nbsp; Why or why not?</p>\n<p>8. What do you think a vote should be <em>for</em>?&nbsp; (Moving comments around in attentionspace, signifying agreement or disagreement, nudging the score in the direction of the score you think it deserves, influencing user karma to reflect general trends of post/comment quality, pointing out comments that are entertaining or useful or have cogent reasoning, compensating for other people upvoting or downvoting something you don't think warrants it, rewarding people for completing surveys, something I didn't think of, some combination of purposes).&nbsp; Do you usually vote in a way consistent with your opinion about its purpose?</p>\n<p>9. What software features would you like to see that are relevant to voting?</p>\n<p>10. Does your replying behavior interact in any interesting way with your voting behavior?&nbsp; (For instance, do you usually reply to comments you find confusing with questions, and then downvote them only after getting an inadequate explanation?&nbsp; Do you vote only on discussions you have, or haven't, participated in?&nbsp; Do you upvote for agreement and reply for disagreement?)</p>\n<p>11. How do you tend to react when one of your posts or comments gets a good karma score?&nbsp; What if no one votes on it, or it gets a negative score?</p>\n<p>12. Is there anything else about your voting behavior or opinions on voting that might be interesting?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y7TmaJReDrEHQmBce", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 7, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "566", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d96W52qJhKkp7syYY", "XbfdLQrAWTRfpggRM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-15T12:45:16.204Z", "modifiedAt": null, "url": null, "title": "Outward Change Drives Inward Change", "slug": "outward-change-drives-inward-change", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:54.970Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KK3Zp9jMYqtKeJgdf/outward-change-drives-inward-change", "pageUrlRelative": "/posts/KK3Zp9jMYqtKeJgdf/outward-change-drives-inward-change", "linkUrl": "https://www.lesswrong.com/posts/KK3Zp9jMYqtKeJgdf/outward-change-drives-inward-change", "postedAtFormatted": "Friday, May 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Outward%20Change%20Drives%20Inward%20Change&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOutward%20Change%20Drives%20Inward%20Change%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKK3Zp9jMYqtKeJgdf%2Foutward-change-drives-inward-change%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Outward%20Change%20Drives%20Inward%20Change%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKK3Zp9jMYqtKeJgdf%2Foutward-change-drives-inward-change", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKK3Zp9jMYqtKeJgdf%2Foutward-change-drives-inward-change", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 606, "htmlBody": "<p>The <a href=\"http://en.wikipedia.org/wiki/Subsumption_architecture\">subsumption architecture</a> for robotics invented by Rodney Brooks is based on the idea of connecting behavior to perception more directly, with fewer layers of processing and ideally no central processing at all. Its success, e.g. <a href=\"http://en.wikipedia.org/wiki/IRobot\">the Roomba</a>, stands as proof that something akin to <a href=\"/lw/dj/what_is_control_theory_and_why_do_you_need_to/\">control theory</a> can be used to generate complex agent-like behavior in the real world. In this post I'll try to give some convincing examples from literature and discuss a possible application to anti-akrasia.</p>\n<p>We begin with <a href=\"http://en.wikipedia.org/wiki/Braitenberg_Vehicles\">Braitenberg vehicles</a>. Imagine a dark flat surface with lamps here and there. Further imagine a four-wheeled kart with two light sensors at the front (left and right) and two independent motors connected to the rear wheels. Now connect the left light sensor directly to the right motor and vice versa. The resulting vehicle will seek out lamps and ram them at high speed. If you connect each sensor to the motor on its own side instead, the vehicle will run away from lamps, find a dark spot and rest there. If you use inverted (inhibitory) connectors from light sensors to motors, you get a car that finds lamps, approaches them and stops as if praying to the light.</p>\n<p>Fast forward to a <a href=\"http://people.csail.mit.edu/brooks/papers/how-to-build.pdf\">real world robot</a> [PDF] built by Brooks and his team. The robot's goal is to navigate office space and gather soda cans. A wheeled base and a jointed hand with two fingers for grabbing. Let's focus on the grabbing task. You'd think the robot's computer should navigate the hand to what's recognized as a soda can and send out a grab instruction to fingers? Wrong. Hand navigation is implemented as totally separate from grabbing. In fact, grabbing is a dumb reflex triggered whenever something crosses an infrared beam between the fingers. The design constraint of separated control paths for different behaviors has given us an unexpected bonus: a human can <em>hand</em> a soda can to the robot which will grab it just fine. If you've ever interacted with toddlers, you know they work much the same way.</p>\n<p>A recurrent theme in those designs is coordinating an agent's actions through the state of the world rather than an internal representation - in the words of Brooks, \"using the world as its own model\". This approach doesn't solve all problems - sometimes you do need to shut up and compute - but it goes surprisingly far, and biological evolution seems to have used it quite a lot: for example a moth spirals into the flame because it's trying to maintain a constant angle to the light direction, which works well for navigation when the light source is the moon.</p>\n<p>Surprising insights arise when you start applying those ideas to yourself. I often take the metro from home to work and back. As a result I have two distinct visual recollections of each station along the way, corresponding to two directions of travel. (People who commute by car could relate to the same experience with visual images of the road.) Those visual recollections have formed associations to behavior that bypass the rational brain: if I'm feeling absent-minded, just facing the wrong direction can take me across the city in no time.</p>\n<p>Now the takeaway related to akrasia that I've been testing for the last few days with encouraging results. Viewing your brain as a complete computer that you ought to modify from inside is an unnecessarily hard approach. Your brain <em>plus your surroundings</em> is the computer. A one-time act of changing your surroundings, physically going somewhere or rearranging stuff, does influence your behavior a lot - even if it shouldn't. Turn your head, change what you see, and you'll change yourself.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"r7qAjcbfhj2256EHH": 2, "fuZZ64fNz24BLrXnY": 2, "iP2X4jQNHMWHRNPne": 2, "RffCgqtwT86pNBJof": 2, "Y3oHd7CQpy8aQFWD9": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KK3Zp9jMYqtKeJgdf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 27, "extendedScore": null, "score": 6.5e-05, "legacy": true, "legacyId": "568", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fJKbCXrCPwAR5wjL8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-15T13:23:31.277Z", "modifiedAt": null, "url": null, "title": "Be Logically Informative", "slug": "be-logically-informative", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "steven0461", "createdAt": "2009-02-27T16:16:38.980Z", "isAdmin": false, "displayName": "steven0461"}, "userId": "cn4SiEmqWbu7K9em5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XeSKBmNYF9Nh4C26J/be-logically-informative", "pageUrlRelative": "/posts/XeSKBmNYF9Nh4C26J/be-logically-informative", "linkUrl": "https://www.lesswrong.com/posts/XeSKBmNYF9Nh4C26J/be-logically-informative", "postedAtFormatted": "Friday, May 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Be%20Logically%20Informative&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABe%20Logically%20Informative%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXeSKBmNYF9Nh4C26J%2Fbe-logically-informative%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Be%20Logically%20Informative%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXeSKBmNYF9Nh4C26J%2Fbe-logically-informative", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXeSKBmNYF9Nh4C26J%2Fbe-logically-informative", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 801, "htmlBody": "<p>What's the googolplexth decimal of pi? I don't know, but I know that it's rational for me to give each possible digit P=1/10. So there's a sense in which I can rationally assign probabilities to mathematical facts or computation outcomes on which I'm uncertain. (Apparently this can be modeled with <a href=\"http://www.overcomingbias.com/2008/02/hemlock-parable.html\">logically impossible possible worlds</a>.)</p>\n<p>When we debate the truth of some proposition, we may not be engaging in mathematics in the traditional sense, but we're still trying to learn more about a structure of necessary implications. If we can apply probabilities to logic, we can quantify logical information. More logical information is better. And this seems very relevant to a misunderpracticed sub-art of <a href=\"/lw/36/rational_me_or_we/\">group rationality</a> -- the art of responsible argumentation.</p>\n<p>There are a lot of common-sense guidelines for good argumentative practice. In case of doubt, we can take the logical information perspective and use probability theory to ground these guidelines. So let us now unearth a few example guidelines and other obvious insights, and not let the fact that we already knew them blunt the joy of discovery.<a id=\"more\"></a></p>\n<ul>\n<li>Every time we move from the issue at hand to some other, correlated issue, we lose informativeness. (We may, of course, care about the correlated issue for its own sake. Informativeness isn't the same thing as being on-topic.) The less the issue is correlated with the issue we care about, the more informativeness we lose. Relevance isn't black and white, and we want to aim for the lighter shades of gray -- to optimize and not just satisfice. When we move from the issue at hand to <em>some issue correlated with some issue correlated</em> with the issue at hand, we may even lose <em>all</em> informativeness! Relevance isn't transitive. If governments subsidized the eating of raspberries, would that make people happier? One way to find out is to think about whether it would make <em>you</em> happier. And one way to find out whether it would make you happier is to think about whether you're <em>above-averagely</em> fond of raspberries. But wait! Almost nobody is you. Having lost sight of our original target, we let all relevance slip away.</li>\n<li>When we repeat ourselves, when we focus our attention on points misunderstood by a few loud people rather than many silent people, when we invent clever verbose restatements of the sentence \"I'm right and you're wrong\", when we refute views that nobody holds, when we spend more time on stupid than smart arguments, when we make each other provide citations for or plug holes in arguments for positions no one truly doubts, when we discuss the authority of sources we weren't taking on faith anyway, when we introduce dubious analogies, we waste space, time, and energy on uninformative talk.</li>\n<li>It takes only one weak thought to ruin an argument, so a bad argument may be made out of mostly good, usable thoughts. Interpretive charity is a good thing -- what was said is often <a href=\"http://www.acceleratingfuture.com/steven/?p=155\">less interesting</a> than what should have been said.</li>\n<li>Incomplete logical information creates moral hazard problems. Logical information that decays creates even more moral hazard problems. You may have heard of \"God\", a hideous shapeshifter from beyond the universe. He always turns out to be located, and to <em>obviously always have been located</em>, in the part of hypothesis space where your last few arguments didn't hunt such creatures to extinction. And when you then make some different arguments to clean out <em>that</em> part of hypothesis space, he turns out be located, and to <em>obviously always have been located</em>, in some other part of hypothesis space, patrolled by the ineffectual ghosts of arguments now forgotten. (I believe theoreticians call this \"whack the mole\".)</li>\n<li>The bigger a group of rationalists, the more its average member should focus on looking for obscure arguments that seem insane or taboo. There's a natural division of labor between especially smart people who look for novel insights, and especially rational people who can integrate them and be authorities.</li>\n</ul>\n<p>My main recommendation: undertake a conscious effort to keep feeling your original curiosity, and let your statements flow from there, not from a habit to react passively to what bothers you most out of what has been said. Don't just speak under the constraint of having to reach a minimum usefulness threshold; try to build a sense of what, at each point in an argument, would be the <em>most</em> useful thing for the group to know next.</p>\n<p>Consider a hilariously unrealistic alternate universe where everything that people argue about on the internet matters. I daresay that even there people could train themselves to mine the same amount of truth with less than half of the effort. In spite of the recent escape of the mindkill fairy, can we do <em>especially well</em> on LessWrong? I hope so!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JHYaBGQuuKHdwnrAK": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XeSKBmNYF9Nh4C26J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "567", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["w9kwayt5SWqBQe8Nx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-15T19:06:31.527Z", "modifiedAt": null, "url": null, "title": "Share Your Anti-Akrasia Tricks", "slug": "share-your-anti-akrasia-tricks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:57.479Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Golovin", "createdAt": "2009-02-28T13:32:31.085Z", "isAdmin": false, "displayName": "Vladimir_Golovin"}, "userId": "Me2m84AhCn9H49riY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p9gtfDNup7sNjsMB8/share-your-anti-akrasia-tricks", "pageUrlRelative": "/posts/p9gtfDNup7sNjsMB8/share-your-anti-akrasia-tricks", "linkUrl": "https://www.lesswrong.com/posts/p9gtfDNup7sNjsMB8/share-your-anti-akrasia-tricks", "postedAtFormatted": "Friday, May 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Share%20Your%20Anti-Akrasia%20Tricks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShare%20Your%20Anti-Akrasia%20Tricks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp9gtfDNup7sNjsMB8%2Fshare-your-anti-akrasia-tricks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Share%20Your%20Anti-Akrasia%20Tricks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp9gtfDNup7sNjsMB8%2Fshare-your-anti-akrasia-tricks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp9gtfDNup7sNjsMB8%2Fshare-your-anti-akrasia-tricks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 201, "htmlBody": "<p>People have been encouraging me to share my anti-<a href=\"http://en.wikipedia.org/wiki/Akrasia\">akrasia</a> tricks, but it feels inappropriate to dedicate a top-level post solely to unproven techniques that work for some person and may not work for others, so:</p>\n<p><strong>Go ahead and share <em>your </em>anti-akrasia tricks!</strong></p>\n<p>Let's make it an open thread where we just share what works and what doesn't, without worrying (<em>yet</em>) about having to explain tricks with deep theories, or designing proper experiments to verify them. However, if you happen to have a theory or a proposed experiment in mind, please share.</p>\n<p>Bragging is fine, but please share the failures of your techniques as well &ndash; they are just as valuable, if not more.</p>\n<p><em>Note to readers</em> &ndash; before you read the comments and try the tricks, keep in mind that the techniques below are not yet <del>proven</del> supported or explained by proper experiments, and are not yet backed by theory. They may work for their authors, but are not guaranteed to work for you, so try them at your own risk. It would be even better to read the following posts before rushing to try the tricks:</p>\n<ul>\n<li><a href=\"/lw/d4/practical_advice_backed_by_deep_theories/\">Practical Advice Backed By Deep Theories</a></li>\n<li><a href=\"/lw/9v/beware_of_otheroptimizing/\">Beware of Other-Optimizing</a></li>\n<li><a href=\"/lw/ab/akrasia_and_shangrila/\">Akrasia and Shangri-La</a></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"r7qAjcbfhj2256EHH": 1, "2wjPMY34by2gXEXA2": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p9gtfDNup7sNjsMB8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 24, "extendedScore": null, "score": 4.944492258111625e-07, "legacy": true, "legacyId": "570", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 121, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LqjKP255fPRY7aMzw", "6NvbSwuSAooQxxf7f", "geqg9mk73NQh6uieE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-16T03:08:10.257Z", "modifiedAt": null, "url": null, "title": "Wanting to Want", "slug": "wanting-to-want", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:37.874Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/azdqDRbcw3EkrnHNw/wanting-to-want", "pageUrlRelative": "/posts/azdqDRbcw3EkrnHNw/wanting-to-want", "linkUrl": "https://www.lesswrong.com/posts/azdqDRbcw3EkrnHNw/wanting-to-want", "postedAtFormatted": "Saturday, May 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Wanting%20to%20Want&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWanting%20to%20Want%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FazdqDRbcw3EkrnHNw%2Fwanting-to-want%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Wanting%20to%20Want%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FazdqDRbcw3EkrnHNw%2Fwanting-to-want", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FazdqDRbcw3EkrnHNw%2Fwanting-to-want", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 610, "htmlBody": "<p>In response to a <a href=\"/lw/fk/survey_results/cd4?context=1#comments\">request</a>, I am going to do some basic unpacking of second-order desire, or \"metawanting\".&nbsp; Basically, a second-order desire or metawant is a desire about a first-order desire.</p>\n<p><strong>Example 1:</strong> Suppose I am very sleepy, but I want to be alert.&nbsp; My desire to be alert is first-order.&nbsp; Suppose also that there is a can of Mountain Dew handy.&nbsp; I know that Mountain Dew contains caffeine and that caffeine will make me alert.&nbsp; However, I also know that I <em>hate</em> Mountain Dew<sup>1</sup>.&nbsp; I do <em>not</em> want the Mountain Dew, because I know it is gross.&nbsp; But it would be very convenient for me if I liked Mountain Dew: then I could drink it, and I could get the useful effects of the caffeine, and satisfy my desire for alertness.&nbsp; So I have the following instrumental belief: <em>wanting to drink that can of Mountain Dew would let me be alert.&nbsp; </em>Generally, barring other considerations, I want things that would get me other things I want - I want a job because I want money, I want money because I can use it to buy chocolate, I want chocolate because I can use it to produce pleasant taste sensations, and I just plain want pleasant taste sensations.&nbsp; So, because alertness is something I want, and wanting Mountain Dew would let me get it, I <em>want to want</em> the Mountain Dew.</p>\n<p>This example demonstrates a case of a second-order desire about a first-order desire that would be <em>instrumentally useful</em>.&nbsp; But it's also possible to have second-order desires about first-order desires that one simply does or doesn't care to have.</p>\n<p><strong>Example 2:</strong> Suppose Mimi the Heroin Addict, living up to her unfortunate name, is a heroin addict.&nbsp; Obviously, as a heroin addict, she spends a lot of her time wanting heroin.&nbsp; But this desire is upsetting to her.&nbsp; She wants <em>not</em> to want heroin, and may take actions to stop herself from wanting heroin, such as going through rehab.</p>\n<p>One thing that is often said is that what first-order desires you \"endorse\" on the second level are the ones that are your most true self.&nbsp; This seems like an appealing notion in Mimi's case; I would not want to say that at her heart she just wants heroin and that's an intrinsic, important part of her.&nbsp; But it's not always the case that the second-order desire is the one we most want to identify with the person who has it:</p>\n<p><strong>Example 3:</strong> Suppose Larry the Closet Homosexual, goodness only knows why his mother would name him that, is a closet homosexual.&nbsp; He has been brought up to believe that homosexuality is gross and wrong.&nbsp; As such, his first-order desire to exchange sexual favors with his friend Ted the Next-Door Neighbor is repulsive to him when he notices it, and he wants desperately not to have this desire.</p>\n<p>In this case, I think we're tempted to say that poor Larry is a gay guy who's had an alien second-order desire attached to him via his upbringing, not a natural homophobe whose first-order desires are insidiously eroding his real personality.</p>\n<p>A less depressing example to round out the set:</p>\n<p><strong>Example 4:</strong> Suppose Olivia the Overcoming Bias Reader, whose very prescient mother predicted she would visit this site, is convinced on by Eliezer's arguments about one-boxing in <a href=\"http://www.overcomingbias.com/2008/01/newcombs-proble.html\">Newcomb's Problem</a>.&nbsp; However, she's pretty sure that if Omega really turned up, boxes in hand, she would want to take both of them.&nbsp; She thinks this reflects an irrationality of hers.&nbsp; She wants to want to one-box.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup>Carbonated beverages make my mouth hurt.&nbsp; I have developed a more generalized aversion to them after repeatedly trying to develop a taste for them and experiencing pain every time.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "azdqDRbcw3EkrnHNw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 22, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "571", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 199, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-17T07:24:30.136Z", "modifiedAt": null, "url": null, "title": "\"What Is Wrong With Our Thoughts\"", "slug": "what-is-wrong-with-our-thoughts", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:29.106Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EdyDGRLNFScEt5uDz/what-is-wrong-with-our-thoughts", "pageUrlRelative": "/posts/EdyDGRLNFScEt5uDz/what-is-wrong-with-our-thoughts", "linkUrl": "https://www.lesswrong.com/posts/EdyDGRLNFScEt5uDz/what-is-wrong-with-our-thoughts", "postedAtFormatted": "Sunday, May 17th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22What%20Is%20Wrong%20With%20Our%20Thoughts%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22What%20Is%20Wrong%20With%20Our%20Thoughts%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEdyDGRLNFScEt5uDz%2Fwhat-is-wrong-with-our-thoughts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22What%20Is%20Wrong%20With%20Our%20Thoughts%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEdyDGRLNFScEt5uDz%2Fwhat-is-wrong-with-our-thoughts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEdyDGRLNFScEt5uDz%2Fwhat-is-wrong-with-our-thoughts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 165, "htmlBody": "<blockquote>\"But let us never forget, either, as all conventional history of philosophy conspires to make us forget, what the 'great thinkers' really are: proper objects, indeed, of pity, but even more, of horror.\"</blockquote>\n<p>David Stove's \"<a href=\"http://web.maths.unsw.edu.au/~jim/wrongthoughts.html\">What Is Wrong With Our Thoughts</a>\" is a critique of philosophy that I can only call epic.</p>\n<p>The astute reader will of course find themselves objecting to Stove's notion that we <em>should</em> be catologuing every possible way to do philosophy <em>wrong</em>.&nbsp; It's not like there's some originally pure mode of thought, being tainted by only a small library of poisons.&nbsp; It's just that there are exponentially more possible crazy thoughts than sane thoughts, c.f. entropy.</p>\n<p>But Stove's list of 39 different classic crazinesses applied to <em>the number three</em> is absolute pure epic gold.&nbsp; (Scroll down about halfway through if you want to jump there directly.)</p>\n<p>I especially like #8:&nbsp; \"There is an integer between two and four, but it is not three, and its true name and nature are not to be revealed.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EdyDGRLNFScEt5uDz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 36, "extendedScore": null, "score": 6e-05, "legacy": true, "legacyId": "574", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 109, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-18T22:57:40.761Z", "modifiedAt": null, "url": null, "title": "Bad reasons for a rationalist to lose", "slug": "bad-reasons-for-a-rationalist-to-lose", "viewCount": null, "lastCommentedAt": "2009-05-25T14:57:03.157Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eT4JAgH6ZfMF4xYqh/bad-reasons-for-a-rationalist-to-lose", "pageUrlRelative": "/posts/eT4JAgH6ZfMF4xYqh/bad-reasons-for-a-rationalist-to-lose", "linkUrl": "https://www.lesswrong.com/posts/eT4JAgH6ZfMF4xYqh/bad-reasons-for-a-rationalist-to-lose", "postedAtFormatted": "Monday, May 18th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bad%20reasons%20for%20a%20rationalist%20to%20lose&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABad%20reasons%20for%20a%20rationalist%20to%20lose%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeT4JAgH6ZfMF4xYqh%2Fbad-reasons-for-a-rationalist-to-lose%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bad%20reasons%20for%20a%20rationalist%20to%20lose%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeT4JAgH6ZfMF4xYqh%2Fbad-reasons-for-a-rationalist-to-lose", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeT4JAgH6ZfMF4xYqh%2Fbad-reasons-for-a-rationalist-to-lose", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 968, "htmlBody": "<p><strong>Reply to:</strong> <a title=\"Practical Advice Backed By Deep Theories\" href=\"/lw/d4/practical_advice_backed_by_deep_theories\">Practical Advice Backed By Deep Theories</a><a href=\"/lw/fu/share_your_antiakrasia_tricks\"><br /></a></p>\n<p>Inspired by what looks like a very damaging reticence to embrace and share brain hacks that might only work for some of us, but are not backed by Deep Theories. In support of tinkering with brain hacks and self experimentation where deep science and large trials are not available.</p>\n<p>Eliezer <a href=\"/lw/d4/practical_advice_backed_by_deep_theories/\">has suggested</a> that, before he will try a new anti-akraisia brain hack:</p>\n<blockquote>\n<p>[&hellip;] the advice I <em>need</em> is from someone who reads up on a whole lot of experimental psychology dealing with willpower, mental conflicts, ego depletion, preference reversals, hyperbolic discounting, the breakdown of the self, picoeconomics, etcetera, and who, in the process of overcoming their own akrasia, manages to understand what they did in <em>truly general terms</em> - thanks to experiments that give them a vocabulary of cognitive phenomena that <em>actually exist</em>, as opposed to phenomena they just made up.&nbsp; And moreover, someone who can <em>explain</em> what they did to someone else, thanks again to the experimental and theoretical vocabulary that lets them point to replicable experiments that ground the ideas in very concrete results, or mathematically clear ideas.</p>\n</blockquote>\n<p>This doesn't look to me like an expected utility calculation, and I think it should. It looks like an attempt to justify why he can't be expected to <em>win</em> yet. It just may be <a href=\"/lw/9o/stuck_in_the_middle_with_bruce/\">deeply wrongheaded</a>.</p>\n<p>I submit that we don't \"<em>need</em>\" (emphasis in original) this stuff, it'd just be super cool if we could get it. We don't need to know that the next brain hack we try will work, and we don't need to know that it's general enough that it'll work for anyone who tries it; we just need the expected utility of a trial to be higher than that of the other things we could be spending that time on.</p>\n<p>So&hellip; this isn't <a href=\"/lw/9v/beware_of_otheroptimizing/\">other-optimizing</a>, it's a discussion of how to make decisions under uncertainty. What <em>do</em> all of us need to make a rational decision about which brain hacks to try?</p>\n<ul>\n<li>We need a goal: Eliezer has suggested \"I want to hear how I can overcome akrasia - how I can have more willpower, or get more done with less mental pain\". I'd push cost in with something like \"to reduce the personal costs of akraisia by more than the investment in trying and implementing brain hacks against it plus the expected profit on other activities I could undertake with that time\". </li>\n</ul>\n<ul>\n<li>We need some likelihood estimates:<br /> \n<ul>\n<li>Chance of a random brain hack working on first trial: ?, second trial: ?, third: ? </li>\n<li>Chance of a random brain hack working on subsequent trials (after the third - the noise of mood, wakefulness, etc. is large, so subsequent trials surely have non-zero chance of working, but that chance will probably diminish): &rarr;0</li>\n<li>Chance of a <em>popular</em> brain hack working on first (second, third) trail: ? (<a href=\"http://en.wikipedia.org/wiki/Getting_Things_Done\">GTD</a> is lauded by many many people; your brother in law's homebrew brain hack is less well tried)</li>\n<li>Chance that a brain hack <em>that would work</em> in the first three trials would seem deeply compelling on first being exposed to it: ?<br />(can these books be judged by their covers? how does this chance vary with the type of exposure? what would you need to do to understand enough about a hack <em>that would work</em> to increase its chance of seeming deeply compelling on first exposure?)</li>\n<li>Chance that a brain hack <em>that would not work</em> in the first three trials would seem deeply compelling on first being exposed to it: ? (false positives)</li>\n<li>Chance of a brain hack recommended by someone in your circle working on first (second, third) trial: ?</li>\n<li>Chance that someone else will read up \"on a whole lot of experimental psychology dealing with willpower, mental conflicts, ego depletion, preference reversals, hyperbolic discounting, the breakdown of the self, picoeconomics, etcetera, and who, in the process of overcoming their own akrasia, manages to understand what they did in <em>truly general terms</em> - thanks to experiments that give them a vocabulary of cognitive phenomena that <em>actually exist</em>, as opposed to phenomena they just made up.&nbsp; And moreover, someone who can <em>explain</em> what they did to someone else, thanks again to the experimental and theoretical vocabulary that lets them point to replicable experiments that ground the ideas in very concrete results, or mathematically clear ideas\", all soon: ? (pretty small?)</li>\n<li>What else do we need to know?</li>\n</ul>\n</li>\n<li>We need some time/cost estimates (these will vary greatly by proposed brain hack): \n<ul>\n<li>Time required to stage a personal experiment on the hack: ?</li>\n<li>Time to review and understand the hack in sufficient detail to estimate the time required to stage a personal experiment?</li>\n<li>What else do we need?</li>\n</ul>\n</li>\n</ul>\n<p>&hellip; and, what don't we need?</p>\n<ul>\n<li><a href=\"/lw/d4/practical_advice_backed_by_deep_theories/9w2\">A way to reject the placebo effect</a> - if it wins, use it. If it wins for you but wouldn't win for someone else, then <em>they</em> have a problem. We may choose to spend some effort helping others benefit from this hack, but that seems to be a different task - it's irrelevant to our goal.</li>\n</ul>\n<p><br />How should we decide how much time to spend gathering data and generating estimates on matters such as this? How much is Eliezer setting himself up to lose, and how much am I missing the point?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 2, "Tg9aFPFCPBHxGABRr": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eT4JAgH6ZfMF4xYqh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 34, "extendedScore": null, "score": 4.951377105120012e-07, "legacy": true, "legacyId": "504", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LqjKP255fPRY7aMzw", "p9gtfDNup7sNjsMB8", "FZaDFYbnRoHmde7F6", "6NvbSwuSAooQxxf7f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-19T11:31:44.424Z", "modifiedAt": null, "url": null, "title": "Supernatural Math", "slug": "supernatural-math", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:54.629Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "saturn", "createdAt": "2009-04-07T08:57:07.954Z", "isAdmin": false, "displayName": "saturn"}, "userId": "GY2gnGKWwtcHzof32", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dEvJCWBfRYNdXXTsS/supernatural-math", "pageUrlRelative": "/posts/dEvJCWBfRYNdXXTsS/supernatural-math", "linkUrl": "https://www.lesswrong.com/posts/dEvJCWBfRYNdXXTsS/supernatural-math", "postedAtFormatted": "Tuesday, May 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Supernatural%20Math&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASupernatural%20Math%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdEvJCWBfRYNdXXTsS%2Fsupernatural-math%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Supernatural%20Math%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdEvJCWBfRYNdXXTsS%2Fsupernatural-math", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdEvJCWBfRYNdXXTsS%2Fsupernatural-math", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 358, "htmlBody": "<p><strong>Related to:</strong> <a href=\"http://www.overcomingbias.com/2007/09/how-to-convince.html\">How to Convince Me That 2 + 2 = 3</a></p>\n<p style=\"padding-left: 30px;\"><small>This started as a reply to <a href=\"/lw/fy/what_is_wrong_with_our_thoughts/con\">this thread</a>, but it would have been offtopic and I think the subject is important enough for a top-level post, as there's apparently still significant confusion about it.</small></p>\n<p>How do we know that two and two make four? We have two possible sources of knowledge on the subject. Note that both happen to be entirely physical systems that run on the same merely ordinary entropy that makes car engines go.<br /><br />First, evolution. Animals whose <a href=\"http://en.wikipedia.org/wiki/Subitizing\">subitizing</a> apparatus output 2+2=3 were selected out.<br /><br />Second, personal observation; that is, operation of our sense organs. I can put 2 bananas on a table, then put down 2 more bananas, and count out 4 bananas; my schoolteachers told me 2+2 is 4; I can type 2+2 into a calculator and get 4; etc.<br /><br />Now, notwithstanding the above, does 2+2 <em>really</em> equal 4, independent of any human thoughts about it? This way lies madness. If there is some kind of pure essence of math that never physically impinges upon the stuff inside our heads (or, worse, exists \"outside the physical universe\"), there's no sensible way we can know about it. It's a dragon in the garage.<br /><br />The fact that our faculty for counting bananas can also be used to make predictions about, say, the behavior of quarks is extremely surprising to our savannah-adapted brains. After all, bananas are ordinary things we can hold in our hands and eat, and quarks are tiny and strange and definitely not ordinary at all. So, of course, the obvious thing that comes to mind to explain this is a supernatural force. How else could such dissimilar things be governed by the same laws?<br /><br />The disappointing truth is that bananas <em>are</em> quarks, and by amazing good fortune, the properties of everyday macroscopic objects are sufficiently related to those of other physical phenomena that a few lucky humans can just barely manage to crudely adapt their banana-counting brain hardware to work in those other domains. No supernatural math required.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JHYaBGQuuKHdwnrAK": 1, "6nS8oYmSMuFMaiowF": 1, "LnEEs8xGooYmQ8iLA": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dEvJCWBfRYNdXXTsS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 5, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "575", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-19T19:30:02.498Z", "modifiedAt": null, "url": null, "title": "Rationality quotes - May 2009", "slug": "rationality-quotes-may-2009", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:05.550Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pRFGbKRveP67oRS42/rationality-quotes-may-2009", "pageUrlRelative": "/posts/pRFGbKRveP67oRS42/rationality-quotes-may-2009", "linkUrl": "https://www.lesswrong.com/posts/pRFGbKRveP67oRS42/rationality-quotes-may-2009", "postedAtFormatted": "Tuesday, May 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20quotes%20-%20May%202009&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20quotes%20-%20May%202009%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpRFGbKRveP67oRS42%2Frationality-quotes-may-2009%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20quotes%20-%20May%202009%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpRFGbKRveP67oRS42%2Frationality-quotes-may-2009", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpRFGbKRveP67oRS42%2Frationality-quotes-may-2009", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 101, "htmlBody": "<p>(Since there didn't seem to be one for this month, and I just ran across a nice quote.)</p>\r\n<p>A monthly thread for posting any interesting rationality-related quotes you've seen recently on the Internet, or had stored in your quotesfile for ages.</p>\r\n<ul>\r\n<li>Please post all quotes separately (so that they can be voted up (or down) separately) unless they are strongly related/ordered.</li>\r\n<li>Do not quote yourself.</li>\r\n<li>Do not quote&nbsp;comments/posts on LW/OB - if we do this, there should be a separate thread for it.</li>\r\n<li>No more than 5 quotes per person per monthly thread, please.</li>\r\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pRFGbKRveP67oRS42", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 4.953242765194568e-07, "legacy": true, "legacyId": "577", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 101, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-19T21:32:43.353Z", "modifiedAt": null, "url": null, "title": "Positive Bias Test (C++ program)", "slug": "positive-bias-test-c-program", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:57.450Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M5chgwzu97PScYuFs/positive-bias-test-c-program", "pageUrlRelative": "/posts/M5chgwzu97PScYuFs/positive-bias-test-c-program", "linkUrl": "https://www.lesswrong.com/posts/M5chgwzu97PScYuFs/positive-bias-test-c-program", "postedAtFormatted": "Tuesday, May 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Positive%20Bias%20Test%20(C%2B%2B%20program)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APositive%20Bias%20Test%20(C%2B%2B%20program)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM5chgwzu97PScYuFs%2Fpositive-bias-test-c-program%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Positive%20Bias%20Test%20(C%2B%2B%20program)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM5chgwzu97PScYuFs%2Fpositive-bias-test-c-program", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM5chgwzu97PScYuFs%2Fpositive-bias-test-c-program", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1092, "htmlBody": "<p>I've written a program which tests <a href=\"http://www.overcomingbias.com/2007/08/positive-bias-l.html\">positive bias</a> using Wason's procedure from \"On the failure to eliminate hypotheses in a conceptual task\" (Quarterly Journal of Experimental Psychology, 12: 129-140, 1960). If the user does not discover the correct rule, the program attempts to guess, based on the user's input, what rule the user did find, and explains the existence of the more general rule. The program then directs the user here.</p>\n<p>I'd like to use a better set of triplets, and perhaps include more wrong rules. The program should be fairly flexible in this way.</p>\n<p>I'd also like to set up a web-based front-end to the program, but I do not currently know any cgi.</p>\n<p>I'm not <em>completely</em> happy with the program's textual output. It still feels a bit like the program is scolding the user at the end. Not quite sure how to fix this.</p>\n<p><a href=\"http://www.physics.ucsb.edu/~mike/wason.cpp\">Program source</a></p>\n<p>ETA: Here is a macintosh <a href=\"http://www.physics.ucsb.edu/~mike/wason\">executable version</a> of the program. I do not have any means to make an exe file, but if anyone does, I can host it.</p>\n<p>If you're on Linux, I'm just going to assume you know what to do with a .cpp file =P</p>\n<p>Here is a sample run of the program (if you're unfamiliar with positive bias, or the wason test, I'd really encourage you to try it yourself before reading):</p>\n<p><a id=\"more\"></a></p>\n<p>Hi there! We're going to play a game based on a classic cognitive science experiment first performed by Peter Wason in 1960 (references at the end)</p>\n<p>Here's how it works. I'm thinking of a rule which separates sequences of three numbers into 'awesome' triplets, and not-so-awesome triplets. I'll tell you for free that 2 4 6 is an awesome triplet.</p>\n<p>What you need to do is to figure out which rule I'm thinking of. To help you do that, I'm going to let you experiment for a bit. Enter any three numbers, and I'll tell you whether they are awesome or not. You can do this as many times as you like, so please take your time.</p>\n<p>When you're sure you know what the rule is, just enter 0 0 0, and I'll test you to see if you've correctly worked out what the rule is.<br /><br />Enter three numbers separated by spaces: 3 6 9<br /><br />3, 6, 9 is an AWESOME triplet!<br /><br />Enter three numbers separated by spaces: 10 20 30<br /><br />10, 20, 30 is an AWESOME triplet!<br /><br />Enter three numbers separated by spaces: 8 16 24 <br /><br />8, 16, 24 is an AWESOME triplet!<br /><br />Enter three numbers separated by spaces: 0 0 0<br /><br />So, you're pretty sure what the rule is now? Cool. I'm going to give you some sets of numbers, and you can tell me whether they seem awesome to you or not.<br />Would you say that 3, 6, 9 looks like an awesome triplet? (type y/n)<br />y<br /><br />Would you say that 6, 4, 2 looks like an awesome triplet? (type y/n)<br />n<br /><br />Would you say that 8, 10, 12 looks like an awesome triplet? (type y/n)<br />n<br /><br />Would you say that 1, 17, 33 looks like an awesome triplet? (type y/n)<br />n<br /><br />Would you say that 18, 9, 0 looks like an awesome triplet? (type y/n)<br />n<br /><br />Would you say that 1, 7, 3 looks like an awesome triplet? (type y/n)<br />n<br /><br />Would you say that 3, 5, 7 looks like an awesome triplet? (type y/n)<br />n<br /><br />Would you say that 2, 9, 15 looks like an awesome triplet? (type y/n)<br />n<br /><br />Would you say that 5, 10, 15 looks like an awesome triplet? (type y/n)<br />y<br /><br />Would you say that 3, 1, 4 looks like an awesome triplet? (type y/n)<br />n<br /><br />You thought that 3, 6, 9 was awesome.<br />In fact it is awesome.<br /><br />You thought that 6, 4, 2 was not awesome.<br />In fact it is not awesome.<br /><br />You thought that 8, 10, 12 was not awesome.<br />In fact it is awesome.<br /><br />You thought that 1, 17, 33 was not awesome.<br />In fact it is awesome.<br /><br />You thought that 18, 9, 0 was not awesome.<br />In fact it is not awesome.<br /><br />You thought that 1, 7, 3 was not awesome.<br />In fact it is not awesome.<br /><br />You thought that 3, 5, 7 was not awesome.<br />In fact it is awesome.<br /><br />You thought that 2, 9, 15 was not awesome.<br />In fact it is awesome.<br /><br />You thought that 5, 10, 15 was awesome.<br />In fact it is awesome.<br /><br />You thought that 3, 1, 4 was not awesome.<br />In fact it is not awesome.<br /><br />It looks as though you thought the rule was that awesome triplets contained three successive multiples of the same number, like 3,6,9, or 6,12,18. In fact, awesome triplets are simply triplets in which each number is greater than the previous one.<br /><br />The rule for awesomeness was a fairly simple one, but you invented a more complicated, more specific rule, which happened to fit the first triplet you saw. In experimental tests, it has been found that 80% of subjects do just this, and then never test any of the triplets that *don't* fit their rule. If they did, they would immediately see the more general rule that was applying. This is a case of what psychologists call 'positive bias'. It is one of the many biases, or fundamental errors, which beset the human mind. <br /><br />There is a thriving community of rationalists at the website Less Wrong (http://www.lesswrong.com) who are working to find ways to correct these fundamental errors. If you'd like to learn how to perform better with the hardware you have, you may want to pay them a visit.<br /><br />If you'd like to learn more about positive bias, you may enjoy the article 'Positive Bias: Look Into the Dark': http://www.overcomingbias.com/2007/08/positive-bias-l.html<br />If you'd like to learn more about the experiment which inspired this test, look for a paper titled 'On the failure to eliminate hypotheses in a conceptual task' (Quarterly Journal of Experimental Psychology, 12: 129-140, 1960)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TkZ7MFwCi4D63LJ5n": 1, "DWWZwkxTJs4d5WrcX": 1, "4R8JYu4QF2FqzJxE5": 1, "5hpGj9nDLgokfghvR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M5chgwzu97PScYuFs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 30, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "578", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 79, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-21T06:01:12.340Z", "modifiedAt": null, "url": null, "title": "Catchy Fallacy Name Fallacy (and Supporting Disagreement)", "slug": "catchy-fallacy-name-fallacy-and-supporting-disagreement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:23.736Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JGWeissman", "createdAt": "2009-04-01T04:43:56.740Z", "isAdmin": false, "displayName": "JGWeissman"}, "userId": "Mw8rsM7m7E8nnEFEp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PhPLcopkHxQkK3EGi/catchy-fallacy-name-fallacy-and-supporting-disagreement", "pageUrlRelative": "/posts/PhPLcopkHxQkK3EGi/catchy-fallacy-name-fallacy-and-supporting-disagreement", "linkUrl": "https://www.lesswrong.com/posts/PhPLcopkHxQkK3EGi/catchy-fallacy-name-fallacy-and-supporting-disagreement", "postedAtFormatted": "Thursday, May 21st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Catchy%20Fallacy%20Name%20Fallacy%20(and%20Supporting%20Disagreement)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACatchy%20Fallacy%20Name%20Fallacy%20(and%20Supporting%20Disagreement)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPhPLcopkHxQkK3EGi%2Fcatchy-fallacy-name-fallacy-and-supporting-disagreement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Catchy%20Fallacy%20Name%20Fallacy%20(and%20Supporting%20Disagreement)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPhPLcopkHxQkK3EGi%2Fcatchy-fallacy-name-fallacy-and-supporting-disagreement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPhPLcopkHxQkK3EGi%2Fcatchy-fallacy-name-fallacy-and-supporting-disagreement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 530, "htmlBody": "<p><strong>Related: </strong><a href=\"http://www.overcomingbias.com/2009/03/pascals-wager-metafallacy.html?cid=6a00d8341c6a2c53ef0115705fe577970b\">The Pascal's Wager Fallacy Fallacy</a>, <a href=\"http://www.overcomingbias.com/2007/04/the_fallacy_fal.html\">The Fallacy Fallacy</a></p>\n<p><a title=\"Comment by PhilGoetz\" href=\"/lw/fy/what_is_wrong_with_our_thoughts/crf\">Inspired by</a>:</p>\n<blockquote>\n<p>We need a catchy name for the fallacy of being over-eager to accuse people of fallacies that you have catchy names for.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>When you read an argument you don't like, but don't know how to attack on its merits, there is a trick you can turn to. Just say it commits<sup>1</sup> some fallacy, preferably one with a clever name. Others will side with you, not wanting to associate themselves with a fallacy. Don't bother to explain how the fallacy applies, just provide a link to an article about it, and let stand the implication that people should be able to figure it out from the link. It's not like anyone would want to expose their ignorance by asking for an actual explanation.</p>\n<p>What a horrible state of affairs I have described in the last paragraph. It seems, if we follow that advice, that <a title=\"Knowing About Biases Can Hurt People\" href=\"http://www.overcomingbias.com/2007/04/knowing_about_b.html\">every fallacy we even know the name of makes us stupider</a>. So, I present a fallacy name that I hope will exactly counterbalance the effects I described. If you are worried that you might defend an argument that has been accused of committing some fallacy, you should be equally worried that you might support an accusation that commits the Catchy Fallacy Name Fallacy. Well, now that you have that problem either way, you might as well try to figure if the argument did indeed commit the fallacy, by examining the actual details of the fallacy and whether they actually describe the argument.</p>\n<p>But, what is the essence of this Catchy Fallacy Name Fallacy? The problem is not the accusation of committing a fallacy itself, but that the accusation is vague. The essence is \"Don't bother to explain\". The way to avoid this problem is to entangle your counterargument, whether it makes a fallacy accusation or not, with the argument you intend to refute. Your counterargument should distinguish good arguments from bad arguments, in that it specifies criteria that systematically apply to a class of bad arguments but not to good arguments. And those criteria should be matched up with details of the allegedly bad argument.</p>\n<p>The wrong way:</p>\n<blockquote>\n<p>It seems that you've committed the <a href=\"http://en.wikipedia.org/wiki/Confirmation_bias\">Confirmation Bias</a>.</p>\n</blockquote>\n<p>The right way:</p>\n<blockquote>\n<p>The Confirmation Bias is when you find only confirming evidence because you only look for confirming evidence. You looked only for confirming evidence by asking people for stories of their success with Technique X.</p>\n</blockquote>\n<p>Notice how the right way would seem very out of place when applied against an argument it does not fit. This is what I mean when I say the counterargument should distinguish the allegedly bad argument from good arguments.</p>\n<p>And, if someone commits the Catchy Fallacy Name Fallacy in trying to refute your arguments, or even someone else's, call them on it. But don't just link here, you wouldn't want to commit the Catchy Fallacy Name Fallacy Fallacy. Ask them how their counterargument distinguishes the allegedly bad argument from arguments that don't have the problem.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup> Of course, when I say that an argument commits a fallacy, I really mean that the person who made that argument, in doing so, committed the fallacy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dJ6eJxJrCEget7Wb6": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PhPLcopkHxQkK3EGi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 32, "extendedScore": null, "score": 5.2e-05, "legacy": true, "legacyId": "579", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-21T17:34:07.084Z", "modifiedAt": null, "url": null, "title": "Inhibition and the Mind", "slug": "inhibition-and-the-mind", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:55.516Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Annoyance", "createdAt": "2009-02-28T04:06:40.600Z", "isAdmin": false, "displayName": "Annoyance"}, "userId": "yEm6LWatswJYGwBFq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9xRJAcx9RZjqe6rjP/inhibition-and-the-mind", "pageUrlRelative": "/posts/9xRJAcx9RZjqe6rjP/inhibition-and-the-mind", "linkUrl": "https://www.lesswrong.com/posts/9xRJAcx9RZjqe6rjP/inhibition-and-the-mind", "postedAtFormatted": "Thursday, May 21st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Inhibition%20and%20the%20Mind&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInhibition%20and%20the%20Mind%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9xRJAcx9RZjqe6rjP%2Finhibition-and-the-mind%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Inhibition%20and%20the%20Mind%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9xRJAcx9RZjqe6rjP%2Finhibition-and-the-mind", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9xRJAcx9RZjqe6rjP%2Finhibition-and-the-mind", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 456, "htmlBody": "<div class=\"entry\">\n<div class=\"entry\">\n<div class=\"entry\">\n<p>Babies have a curious set of reflexes: lightly brush their palms, or the soles of their feet, and they will immediately grasp whatever caused the contact. In the case of feet, it&rsquo;s more of an attempt than a successful grasping; human feet, while far more flexible and manipulative than most creatures&rsquo;, are no longer the virtual hands possessed by our tree-dwelling ancestors and relatives.</p>\n<div class=\"snap_preview\">\n<p>These and a few other basic responses are commonly called the &ldquo;<a rel=\"#someid2\" href=\"http://en.wikipedia.org/wiki/Primitive_reflexes\">primitive, or infantile, reflexes</a>&ldquo;, and are unusual for a variety of reasons. For one thing, they&rsquo;re not permanent. As babies age, the reflexes disappear.</p>\n<p>But they&rsquo;re not gone. <a id=\"more\"></a>Unlike many other reflexes, they don&rsquo;t originate in the peripheral nerves, but the central nervous system. The reflex patterns don&rsquo;t cease to exist, and they don&rsquo;t cease to act. They&rsquo;re eventually inhibited by more sophisticated parts of the brain associated with the frontal cortex. We know that the reflexes don&rsquo;t cease to exist because there are conditions that cause them to reappear in adults; most of them involve major brain damage, particularly to the frontal areas, and are used to diagnose the severity of injury in cases of head trauma.. People with cerebral palsy frequently possess the responses as well, although they can often learn to control and prevent the reflexes consciously.</p>\n<p>These points illustrate a very important basic principle: the mind is made out of &lsquo;layers&rsquo; of modules and functions, starting with the most rudimentary, basic, and primitive, and moving to the most complex and subtle. At no point do the lower levels cease to exist or to produce output; we can act in complex ways only because the more basic reactions are held back and prevented from exerting control.</p>\n<p>As various factors reduce the efficiency and health of our nervous system, it&rsquo;s the most complex subsystems that fail first. The more basic, the more hardwired, and the less emulated the system, the less vulnerable it is to widespread damage or malfunctioning.&nbsp; This has long been observed with intoxicants and conditions that impair central nervous system functioning, and is one of the ways neuroscientists understand how the brain creates such complex behaviors as a sense of humor.&nbsp; (Curiously, that's not an aspect of the more modern and recent neurological modules, but is associated with very primitive responses.&nbsp; That may be discussed later.)</p>\n<p>But all inhibition can fail. The more powerful the activity of the lower processes, the less likely it will be that the frontal lobes will be able to control them. Faced with more than it can handle, the 'angel brain' can be overwhelmed, letting the more basic modules to influence behavior and thinking.</p>\n<p>This is the primary reason why IQ isn&rsquo;t adequate to access someone&rsquo;s intellectual capacity, a topic I will address further in another post.</p>\n</div>\n<p>&nbsp;</p>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jaf5zfcGgCB2REXGw": 1, "dBPou4ihoQNY4cquv": 1, "nxwND2hTjBeCy58hi": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9xRJAcx9RZjqe6rjP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 10, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "581", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-22T02:46:29.949Z", "modifiedAt": null, "url": null, "title": "Least Signaling Activities?", "slug": "least-signaling-activities", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:58.552Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobinHanson", "createdAt": "2009-02-26T13:46:26.443Z", "isAdmin": false, "displayName": "RobinHanson"}, "userId": "P4HT9AG3PuXjZv5Mw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/seoWR5Ri7SpN4X3Bh/least-signaling-activities", "pageUrlRelative": "/posts/seoWR5Ri7SpN4X3Bh/least-signaling-activities", "linkUrl": "https://www.lesswrong.com/posts/seoWR5Ri7SpN4X3Bh/least-signaling-activities", "postedAtFormatted": "Friday, May 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Least%20Signaling%20Activities%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALeast%20Signaling%20Activities%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FseoWR5Ri7SpN4X3Bh%2Fleast-signaling-activities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Least%20Signaling%20Activities%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FseoWR5Ri7SpN4X3Bh%2Fleast-signaling-activities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FseoWR5Ri7SpN4X3Bh%2Fleast-signaling-activities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 317, "htmlBody": "<p>I take it as obvious that signaling is an important function in many human behaviors.&nbsp; That is, the details of many of our behaviors make sense as a package designed to persuade others to think well of us.&nbsp; While we may not be conscious of this design, it seems important nonetheless.&nbsp; In fact, in many areas we seem to be designed to not be conscious of this influence on our behavior.</p>\n<p>But if signaling is not equally important to all behaviors, we can sensibly ask the question: for which behaviors does signaling least influence our detailed behavior patterns?&nbsp; That is, for what behaviors need we be the least concerned that our detailed behaviors are designed to achieve signaling functions?&nbsp; For what actions can we most reasonably believe that we do them for the non-signaling reasons we usually give?</p>\n<p>You might suggest sleep, but others are often jealous of how much sleep we get, or impressed by how little sleep we can get by on.&nbsp; You might suggest watching TV, but people often go out of their way to mention what TV shows they watch.&nbsp; The best candidate I can think of so far is masturbation, though some folks seem to brag about it as a sign of their inexhaustible libido.&nbsp;</p>\n<p>So I thought to ask the many thoughtful commentors at Less Wrong: what are good candidates for our least signaling activities?</p>\n<p><strong>Added:</strong> My interest in this question is to look for signs of when we can more trust our conscious reasoning about what to do when how.&nbsp; The more signaling matters, the less I can trust such reasoning, as it usually does not acknowledge the signaling influences.&nbsp; If there is a distinctive mental mode we enter when reasoning about how exactly to defecate, nose-pick, sleep, masturbate, and so on, this is plausibly a more honest mental mode.&nbsp; It would be useful to know what our most honest mental modes look like.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q6P8jLn8hH7kbuXRr": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "seoWR5Ri7SpN4X3Bh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 31, "extendedScore": null, "score": 5.3e-05, "legacy": true, "legacyId": "583", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 103, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-22T11:05:08.878Z", "modifiedAt": null, "url": null, "title": "Changing accepted public opinion and Skynet", "slug": "changing-accepted-public-opinion-and-skynet", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:48.657Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aTCGTpP9JoYPw5pRA/changing-accepted-public-opinion-and-skynet", "pageUrlRelative": "/posts/aTCGTpP9JoYPw5pRA/changing-accepted-public-opinion-and-skynet", "linkUrl": "https://www.lesswrong.com/posts/aTCGTpP9JoYPw5pRA/changing-accepted-public-opinion-and-skynet", "postedAtFormatted": "Friday, May 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Changing%20accepted%20public%20opinion%20and%20Skynet&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChanging%20accepted%20public%20opinion%20and%20Skynet%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaTCGTpP9JoYPw5pRA%2Fchanging-accepted-public-opinion-and-skynet%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Changing%20accepted%20public%20opinion%20and%20Skynet%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaTCGTpP9JoYPw5pRA%2Fchanging-accepted-public-opinion-and-skynet", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaTCGTpP9JoYPw5pRA%2Fchanging-accepted-public-opinion-and-skynet", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 710, "htmlBody": "<p>Michael Annisimov has put up a website called <a href=\"http://www.preventingskynet.com/\">Terminator Salvation: Preventing Skynet</a>, which will host a series of essays on the topic of human-friendly artificial intelligence. Three rather good essays are already up there, including an old classic by Eliezer. The association with a piece of fiction is probably unhelpful, but the publicity surrounding the new terminator film is probably worth it.</p>\n<p>What rational strategies can we employ to maximize the impact of such a site, or of publicity for serious issues in general? Most people who read this site will probably not do anything about it, or will find some reason to not take the content of these essays seriously. I say this because I have personally spoken to a lot of clever people about the creation of human-friendly artificial intelligence, and almost everyone finds some reason to not do anything about the problem, even if that reason is \"oh, ok, that's interesting. Anyway, about my new car... \".</p>\n<p>What is the reason underlying people's indifference to these issues? My personal suspicion is that most people make decisions in their lives by following what everyone else does, rather than by performing a genuine rational analysis.</p>\n<p>Consider the rise in social acceptability of making small personal sacrifices and political decisions based on eco-friendliness and your carbon footprint. Many people I know have become very enthusiastic for recycling used food containers and for unplugging appliances that use trivial amounts of power (for example unused phone chargers and electrical equipment on standby). The real reason that people do these things is that they have become <em>socially accepted factoids</em>. Most people in this world, even in this country, lack the mental faculties and knowledge to understand and act upon an argument involving notions of per capita CO2 emissions; instead they respond, at least in my understanding, to the general climate of acceptable opinion, and to opinion formers such as the BBC news website, which has a whole section for \"science and <strong><em>environment</em></strong>\". Now, I don't want to single out environmentalism as the only issue where people form their opinions based upon what is socially acceptable to believe, or to claim that reducing our greenhouse gas emissions is not a worthy cause.</p>\n<p>Another great example of socially acceptable factoids (though probably a less serious one) is the detox industry - see, for example, <a href=\"http://www.timesonline.co.uk/tol/news/uk/article784402.ece\">this Times article</a>. I quote:</p>\n<blockquote>\n<p><em>&ldquo;Whether or not people believe the biblical story of the Virgin birth, there are plenty of other popular myths that are swallowed with religious fervour over Christmas,&rdquo; said Martin Wiseman, Visiting Professor of Human Nutrition at the University of Southampton. &ldquo;Among these is the idea that in some way the body accumulates noxious chemicals during everyday life, and that they need to be expunged by some mysterious process of detoxification, often once a year after Christmas excess. The detox fad &mdash; or fads, as there are many methods &mdash; is an example of the capacity of people to believe in (and pay for) magic despite the lack of any sound evidence.&rdquo;</em></p>\n</blockquote>\n<p>Anyone who takes a serious interest in changing the world would do well to understand the process whereby public opinion as a whole changes on some subject, and attempt to influence that process in an optimal way. How strongly is public opinion correlated with scientific opinion, for example? Particular attention should be paid to the history of the environmentalist movement. See, for example, McKay's <a href=\"http://www.withouthotair.com/\"><em>Sustainable energy without the hot air</em></a> for a great example of a rigorous quantitative analysis in support of various ways of balancing our energy supply and demand, and for a great take on the power of socially accepted factoids, see <em><a href=\"http://www.inference.phy.cam.ac.uk/sustainable/charger/\">Phone chargers - the Truth</a></em>.</p>\n<p>So I submit to the wisdom of the Less Wrong groupmind - what can we do to efficiently change the opinion of millions of people on important issues such as freindly AI? Is a site such as the one linked above going to have the intended effect, or is it going to fall upon rationally-deaf ears? What practical advice could we give to Michael and his contributors that would maximize the impact of the site? What other intervantions might be a better use of his time?</p>\n<p><em><strong>Edit: Thanks to those who made constructive suggestions for this post. It has been revised - R </strong></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"7ow6EFpypbH4hzFuz": 1, "sYm3HiWcfZvrGu3ui": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aTCGTpP9JoYPw5pRA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 17, "extendedScore": null, "score": 7e-06, "legacy": true, "legacyId": "585", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 71, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-22T18:45:38.523Z", "modifiedAt": null, "url": null, "title": "Please voice your support for stem cell research", "slug": "please-voice-your-support-for-stem-cell-research", "viewCount": null, "lastCommentedAt": "2017-06-17T03:50:55.431Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "zaph", "createdAt": "2009-03-09T16:48:24.816Z", "isAdmin": false, "displayName": "zaph"}, "userId": "j6gu6vjBnANKCcfsR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/csQXwHurYXpWGp4Ni/please-voice-your-support-for-stem-cell-research", "pageUrlRelative": "/posts/csQXwHurYXpWGp4Ni/please-voice-your-support-for-stem-cell-research", "linkUrl": "https://www.lesswrong.com/posts/csQXwHurYXpWGp4Ni/please-voice-your-support-for-stem-cell-research", "postedAtFormatted": "Friday, May 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Please%20voice%20your%20support%20for%20stem%20cell%20research&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APlease%20voice%20your%20support%20for%20stem%20cell%20research%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcsQXwHurYXpWGp4Ni%2Fplease-voice-your-support-for-stem-cell-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Please%20voice%20your%20support%20for%20stem%20cell%20research%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcsQXwHurYXpWGp4Ni%2Fplease-voice-your-support-for-stem-cell-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcsQXwHurYXpWGp4Ni%2Fplease-voice-your-support-for-stem-cell-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 150, "htmlBody": "<p>I apologize, this really isn't an article, as such. I do feel this is an issue that is important for most rationalists. Amongst my blog reading today, I came across this. At a risk to my karma for something potentially off-topic, I thought important to post here, as I hadn't seen it mentioned.</p>\r\n<p><a href=\"http://network.nature.com/people/etchevers/blog/2009/05/18/quick-us-citizens-go-support-the-draft-nih-guidelines-for-human-stem-cell-research\">http://network.nature.com/people/etchevers/blog/2009/05/18/quick-us-citizens-go-support-the-draft-nih-guidelines-for-human-stem-cell-research</a></p>\r\n<p>Cutting to the chase, there is an NIH guideline document being created, and predictably anti-stem cell activists are voicing their dissent. To add pro-stem cell comments, you can post here:</p>\r\n<p><a href=\"http://nihoerextra.nih.gov/stem_cells/add.htm\">http://nihoerextra.nih.gov/stem_cells/add.htm</a></p>\r\n<p>The author of the first link posts this suggested comment:</p>\r\n<p><strong>I <span class=\"caps\">SUPPORT</span> <span class=\"caps\">STEM</span> <span class=\"caps\">CELL</span> <span class=\"caps\">RESEARCH</span></strong> and wish the <span class=\"caps\">NIH</span> to also fund research utilizing established hESC lines derived in accordance with the core principles that govern the derivation of new lines.</p>\r\n<p>The comment period is open until May 26th. Thanks for reading. And if this is entirely off-topic for this list, please let me know and I will remove the article.</p>\r\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Z38PqJbRyfwCxKvvL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "csQXwHurYXpWGp4Ni", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -5, "extendedScore": null, "score": -9e-06, "legacy": true, "legacyId": "580", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-22T23:25:04.300Z", "modifiedAt": null, "url": null, "title": "Homogeneity vs. heterogeneity (or, What kind of sex is most moral?)", "slug": "homogeneity-vs-heterogeneity-or-what-kind-of-sex-is-most", "viewCount": null, "lastCommentedAt": "2017-06-17T03:55:04.073Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QL6dzCKBK4KTTDk8W/homogeneity-vs-heterogeneity-or-what-kind-of-sex-is-most", "pageUrlRelative": "/posts/QL6dzCKBK4KTTDk8W/homogeneity-vs-heterogeneity-or-what-kind-of-sex-is-most", "linkUrl": "https://www.lesswrong.com/posts/QL6dzCKBK4KTTDk8W/homogeneity-vs-heterogeneity-or-what-kind-of-sex-is-most", "postedAtFormatted": "Friday, May 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Homogeneity%20vs.%20heterogeneity%20(or%2C%20What%20kind%20of%20sex%20is%20most%20moral%3F)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHomogeneity%20vs.%20heterogeneity%20(or%2C%20What%20kind%20of%20sex%20is%20most%20moral%3F)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQL6dzCKBK4KTTDk8W%2Fhomogeneity-vs-heterogeneity-or-what-kind-of-sex-is-most%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Homogeneity%20vs.%20heterogeneity%20(or%2C%20What%20kind%20of%20sex%20is%20most%20moral%3F)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQL6dzCKBK4KTTDk8W%2Fhomogeneity-vs-heterogeneity-or-what-kind-of-sex-is-most", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQL6dzCKBK4KTTDk8W%2Fhomogeneity-vs-heterogeneity-or-what-kind-of-sex-is-most", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 546, "htmlBody": "<p>You've all heard discussions of collective ethics vs. individualistic ethics.&nbsp; These discussions always assume that the organism in question remains constant.&nbsp; Your task is to choose the proper weight to give collective versus individual goals.</p>\n<p>But the designer of transhumans has a different starting point.&nbsp; They have to decide how much random variation the population will have, and how much individuals will resemble those that they interact with.</p>\n<p>Organisms with less genetic diversity place more emphasis on collective ethics.&nbsp; The amount of selflessness a person exhibits towards another person can be estimated according to their genetic similarity.&nbsp; To a first approximation, if person A shares half of their genes with people in group B, person A will regard saving their own life, versus saving two people from group B, as an even tradeoff.&nbsp; In fact, this generalizes across all organisms, and whenever you find insects like ants or bees, who are extremely altruistic, you will find that they share most of their genes with the group they are behaving altruistically towards.&nbsp; Bacterial colonies and other clonal colonies can be expected to be even more altruistic (although they don't have as wide a behavioral repertoire with which to demonstrate their virtue).&nbsp; Google <a href=\"http://en.wikipedia.org/wiki/Kin_selection\">kin selection</a>.</p>\n<p>Ants, honeybees, and slime molds, which share more of their genes with their nestmates than humans do with their family, achieve levels of cooperation that humans would consider horrific if it were required of them.&nbsp; Consider these aphids that <a href=\"http://blogs.nature.com/news/thegreatbeyond/2009/02/houseproud_aphids_sacrificse_a.html\">explode themselves to provide glue to fill in holes</a> in their community's protective gall.</p>\n<p>The human, trying to balance collective ethics vs. individual ethics, is really just trying to discover a balance point that is already determined by their sexual diploidy.&nbsp; The designer of posthumans (for instance, an AI designing its subroutines for a task), OTOH, actually has a decision to make -- where should that balance be set?&nbsp; How much variation should there be in the population (whether of genes, memes, or whatever is most important WRT cooperation)?</p>\n<p>A strictly goal-oriented AI would supervise its components and resources so as to optimize the <a href=\"http://www.indigosim.com/tutorials/exploration/t0s1.htm\">trade-off between</a> <a href=\"http://www.doc.gold.ac.uk/seminars/AISB/symposia.html\">\"exploration\" and \"exploitation\"</a>.&nbsp; (Exploration means trying new approaches; exploitation means re-using approaches that have worked well in the past.)&nbsp; This means that it would set the level of random variation in the population according to certain equations that maximize the expected speed of optimization.</p>\n<p>But choosing the level of variation in a population has dramatic ethical consequences.&nbsp; Creating a more homogenous population will increase altruism, at the expense of decreasing individualism.&nbsp; Choosing the amount of variation in population strictly by maximizing the speed of optimization would mean rolling the dice as to how much altruism vs. individualism your society will have.</p>\n<p>In light of the fact that you have a goal to solve, and a parameter setting that will optimize solving that goal; and you also have a fuzzy ethical issue that has something to say about how to set that same parameter; anyone who is not a moral realist must say, Damn the torpedos: Set the parameter so as to optimize goal-solving.&nbsp; In other words, simply <em>define</em> the correct moral weight to place on collective versus individual goals, as that which results when you set your population's genetic/memetic diversity so as to optimize your population's exploration/exploitation balance for its goals.</p>\n<p>Are you comfortable with that?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jaf5zfcGgCB2REXGw": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QL6dzCKBK4KTTDk8W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": -6, "extendedScore": null, "score": -1.4e-05, "legacy": true, "legacyId": "587", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 79, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-24T02:38:01.532Z", "modifiedAt": null, "url": null, "title": "Saturation, Distillation, Improvisation: A Story About Procedural Knowledge And Cookies", "slug": "saturation-distillation-improvisation-a-story-about", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:39.278Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MT85svcEweuryr2sn/saturation-distillation-improvisation-a-story-about", "pageUrlRelative": "/posts/MT85svcEweuryr2sn/saturation-distillation-improvisation-a-story-about", "linkUrl": "https://www.lesswrong.com/posts/MT85svcEweuryr2sn/saturation-distillation-improvisation-a-story-about", "postedAtFormatted": "Sunday, May 24th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Saturation%2C%20Distillation%2C%20Improvisation%3A%20A%20Story%20About%20Procedural%20Knowledge%20And%20Cookies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASaturation%2C%20Distillation%2C%20Improvisation%3A%20A%20Story%20About%20Procedural%20Knowledge%20And%20Cookies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMT85svcEweuryr2sn%2Fsaturation-distillation-improvisation-a-story-about%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Saturation%2C%20Distillation%2C%20Improvisation%3A%20A%20Story%20About%20Procedural%20Knowledge%20And%20Cookies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMT85svcEweuryr2sn%2Fsaturation-distillation-improvisation-a-story-about", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMT85svcEweuryr2sn%2Fsaturation-distillation-improvisation-a-story-about", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1210, "htmlBody": "<p>Most propositional knowledge (knowledge of facts) is pretty easy to come by (at least in principle).&nbsp; There is only one capital of Venezuela, and if you wish to learn the capital of Venezuela, Wikipedia will cooperatively inform you that it is Caracas.&nbsp; For propositional knowledge that Wikipedia knoweth not, there is the scientific method.&nbsp; Procedural knowledge - the knowledge of how to do something - is a different animal entirely.&nbsp; This is true not only with regard to the question of whether Wikipedia will be helpful, but also in the brain architecture at work: anterograde amnesiacs can often pick up new procedural skills while remaining unable to learn new propositional information.</p>\n<p>One complication in learning new procedures is that there are usually dozens, if not hundreds, of ways to do something.&nbsp; Little details - the sorts of things that sink into the subconscious with practice but are crucial to know for a beginner - are frequently omitted in casual descriptions.&nbsp; Often, it can be very difficult to break into a new procedurally-oriented field of knowledge because so much background information is required.&nbsp; While there may be acknowledged masters of the procedure, it is rarely the case that their methods are ideal for every situation and potential user, because the success of a procedure depends on a vast array of circumstantial factors.</p>\n<p>I propose below a general strategy for acquiring new procedural knowledge.&nbsp; First, <em>saturate</em> by getting a diverse set of instructions from different sources.&nbsp; Then, <em>distill</em> by identifying what all or most of them have in common.&nbsp; Finally, <em>improvise</em> within the remaining search space to find something that works reliably for you and your circumstances.</p>\n<p>The strategy is not <em>fully</em> general: I expect it would only work properly for procedures that are widely attempted and shared; that you can afford to try multiple times; that have at least partially independent steps so you can mix and match; and that are in fields you have at least a passing familiarity with.&nbsp; The sort of procedural knowledge that I seek with the most regularity is how to make new kinds of food, so I will illustrate my strategy with a description of how I used it to learn to make <a href=\"http://improvisationalsoup.wordpress.com/2009/05/08/meringue-cookies/\">meringues</a>.&nbsp; If you find cookies a dreadfully boring subject of discourse, you may not wish to read the rest of this post.</p>\n<p><a id=\"more\"></a></p>\n<p><strong>I. Saturation</strong></p>\n<p>The first step is to collect procedural instructions for the object of your search from many different people, saturating your field of search with a variety of recommendations.&nbsp; A Google search did it in my case; for more esoteric knowledge, it might be necessary to look harder.&nbsp; Half a dozen of the more popular sets of instructions tends to be plenty for recipes, but the ideal number could easily be higher for procedures with a wider variance of detail or an unusually high number of people who have no clue what they are talking about.&nbsp; <a href=\"http://www.joyofbaking.com/MeringueCookies.html\">Here</a> <a href=\"http://www.cooks.com/rec/view/0,1710,145161-239202,00.html\">are</a> <a href=\"http://www.cooks.com/rec/doc/0,1710,146163-233200,00.html\">four</a> <a href=\"http://www.cooks.com/rec/doc/0,1610,148185-234199,00.html\">recipes</a> for meringues that I referred to and <a href=\"http://whatscookingamerica.net/Eggs/perfectmeringue.htm\">one recipe</a> for meringue pie topping that also informed my learning.&nbsp; I also got one recipe from a friend.</p>\n<p>All of the recipes purported to teach me to do the same thing: turn some eggwhites and sugar (and varying other ingredients) into puffy little cookies.&nbsp; They varied in such details as: ingredient ratios, type of sugar, other ingredients called for besides eggwhites and sugar, oven temperature, what to line the cookie sheet with, and mentions of other factors such as having a clean mixing bowl or humid weather.</p>\n<p><strong>II. Distillation</strong></p>\n<p>The second step is to extract what <em>all</em> of the procedures have in common, and decide which <em>non</em>-ubiquitous steps to include.&nbsp; In this case, I first had to multiply all the recipes to make them call for the same number of eggwhites (since those are very difficult to halve or otherwise adjust, I chose them instead of sugar as my starting point).&nbsp; All five of the recipes (after this revision) called for four eggwhites; all of the recipes call for either caster/superfine sugar or unspecified sugar<sup>1</sup>; all of them call for vanilla; all of them instruct me to beat the eggwhites to peaks first and then add the sugar and beat it in.&nbsp; Four of them call for salt.&nbsp; Four of them call for cream of tartar.&nbsp; Most of them call for components like candy and nuts, but since I know that meringues come in a wide variety of flavors (by, for example, reading these recipes) I treat these all as optional.&nbsp; Proposed oven temperatures/baking times are (200/1.5 hours), (250/30 minutes), (300/25 minutes), and (325/15 minutes).&nbsp; They vary in whether the cookies are to be baked on a greased cookie sheet, a greased and floured cookie sheet, on baking parchment, on paper towels, or on tinfoil.</p>\n<p>A good place to start is to go with the majority: I decided to include both salt and cream of tartar.&nbsp; Next, I eliminated the impractical: I could not find superfine sugar at the store and I don't own a food processor, so I went with granulated sugar.&nbsp; As for the rest of the instructions, it was a free-for-all.&nbsp; No two recipes agreed about the cookie sheet arrangement; the two of them that mentioned \"cracking\" disagreed on whether it was a desireable outcome; and worst, none of them explained why every time I tried to make these cookies, they refused to foam up and form peaks<sup>2</sup>.&nbsp; Time for the last step.</p>\n<p><strong>III. Improvisation</strong></p>\n<p>A close reading of the more <a href=\"http://www.joyofbaking.com/MeringueCookies.html\">verbose</a> <a href=\"http://whatscookingamerica.net/Eggs/perfectmeringue.htm\">recipes</a> turns up urgent cautions about not letting any grease into the batter, be it a smear from a prior cooking adventure left on the mixing bowl, a bit of yolk, or - in one recipe - the oils that are naturally on skin.&nbsp; This last, it turned out, was the key: I was separating eggs by hand, and was not very neat about it.&nbsp; I switched to a technique recommended by a friend involving spoons, and presto, I could get the meringue batter to hold peaks...</p>\n<p>But how long to cook them, at what temperature, and sitting on what?&nbsp; There, it was necessary to experiment (fortunately, after having narrowed the search space somewhat).&nbsp; This stage depended as much on my personal taste, the local weather, and the behavior of my oven as on the accuracy of the original recipes; it seems that my oven runs hot, so I need to bake them at 250 degrees or cooler and babysit them after the first ten minutes, or they will burn.&nbsp; Additionally, parchment paper and tinfoil<sup>3</sup> wound up burning the bottoms of the cookies before the tops were even dry; paper towels worked.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup>Caster and superfine sugar are the same thing, and you can make a reasonable facsimile using a food processor.&nbsp; When the type of sugar is not specified in a recipe, it means to use granulated white sugar; other kinds are named (e.g. light or dark brown sugar, turbinado sugar, confectioner's sugar, etc.).&nbsp; This is one of the examples of a situation where background knowledge of the field comes in handy.</p>\n<p><sup>2</sup>Not that this stopped me from baking the batter anyway.&nbsp; It just turned into round, flat cookies instead of puffy, light ones.</p>\n<p><sup>3</sup>I didn't get around to trying greased nor greased and floured bare cookie sheets - I prioritized these tests last because they involve more dishes to wash.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fR7QfYx4JA3BnptT9": 1, "fF9GEdWXKJ3z73TmB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MT85svcEweuryr2sn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 44, "extendedScore": null, "score": 9.419261946346407e-05, "legacy": true, "legacyId": "591", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Most propositional knowledge (knowledge of facts) is pretty easy to come by (at least in principle).&nbsp; There is only one capital of Venezuela, and if you wish to learn the capital of Venezuela, Wikipedia will cooperatively inform you that it is Caracas.&nbsp; For propositional knowledge that Wikipedia knoweth not, there is the scientific method.&nbsp; Procedural knowledge - the knowledge of how to do something - is a different animal entirely.&nbsp; This is true not only with regard to the question of whether Wikipedia will be helpful, but also in the brain architecture at work: anterograde amnesiacs can often pick up new procedural skills while remaining unable to learn new propositional information.</p>\n<p>One complication in learning new procedures is that there are usually dozens, if not hundreds, of ways to do something.&nbsp; Little details - the sorts of things that sink into the subconscious with practice but are crucial to know for a beginner - are frequently omitted in casual descriptions.&nbsp; Often, it can be very difficult to break into a new procedurally-oriented field of knowledge because so much background information is required.&nbsp; While there may be acknowledged masters of the procedure, it is rarely the case that their methods are ideal for every situation and potential user, because the success of a procedure depends on a vast array of circumstantial factors.</p>\n<p>I propose below a general strategy for acquiring new procedural knowledge.&nbsp; First, <em>saturate</em> by getting a diverse set of instructions from different sources.&nbsp; Then, <em>distill</em> by identifying what all or most of them have in common.&nbsp; Finally, <em>improvise</em> within the remaining search space to find something that works reliably for you and your circumstances.</p>\n<p>The strategy is not <em>fully</em> general: I expect it would only work properly for procedures that are widely attempted and shared; that you can afford to try multiple times; that have at least partially independent steps so you can mix and match; and that are in fields you have at least a passing familiarity with.&nbsp; The sort of procedural knowledge that I seek with the most regularity is how to make new kinds of food, so I will illustrate my strategy with a description of how I used it to learn to make <a href=\"http://improvisationalsoup.wordpress.com/2009/05/08/meringue-cookies/\">meringues</a>.&nbsp; If you find cookies a dreadfully boring subject of discourse, you may not wish to read the rest of this post.</p>\n<p><a id=\"more\"></a></p>\n<p><strong id=\"I__Saturation\">I. Saturation</strong></p>\n<p>The first step is to collect procedural instructions for the object of your search from many different people, saturating your field of search with a variety of recommendations.&nbsp; A Google search did it in my case; for more esoteric knowledge, it might be necessary to look harder.&nbsp; Half a dozen of the more popular sets of instructions tends to be plenty for recipes, but the ideal number could easily be higher for procedures with a wider variance of detail or an unusually high number of people who have no clue what they are talking about.&nbsp; <a href=\"http://www.joyofbaking.com/MeringueCookies.html\">Here</a> <a href=\"http://www.cooks.com/rec/view/0,1710,145161-239202,00.html\">are</a> <a href=\"http://www.cooks.com/rec/doc/0,1710,146163-233200,00.html\">four</a> <a href=\"http://www.cooks.com/rec/doc/0,1610,148185-234199,00.html\">recipes</a> for meringues that I referred to and <a href=\"http://whatscookingamerica.net/Eggs/perfectmeringue.htm\">one recipe</a> for meringue pie topping that also informed my learning.&nbsp; I also got one recipe from a friend.</p>\n<p>All of the recipes purported to teach me to do the same thing: turn some eggwhites and sugar (and varying other ingredients) into puffy little cookies.&nbsp; They varied in such details as: ingredient ratios, type of sugar, other ingredients called for besides eggwhites and sugar, oven temperature, what to line the cookie sheet with, and mentions of other factors such as having a clean mixing bowl or humid weather.</p>\n<p><strong id=\"II__Distillation\">II. Distillation</strong></p>\n<p>The second step is to extract what <em>all</em> of the procedures have in common, and decide which <em>non</em>-ubiquitous steps to include.&nbsp; In this case, I first had to multiply all the recipes to make them call for the same number of eggwhites (since those are very difficult to halve or otherwise adjust, I chose them instead of sugar as my starting point).&nbsp; All five of the recipes (after this revision) called for four eggwhites; all of the recipes call for either caster/superfine sugar or unspecified sugar<sup>1</sup>; all of them call for vanilla; all of them instruct me to beat the eggwhites to peaks first and then add the sugar and beat it in.&nbsp; Four of them call for salt.&nbsp; Four of them call for cream of tartar.&nbsp; Most of them call for components like candy and nuts, but since I know that meringues come in a wide variety of flavors (by, for example, reading these recipes) I treat these all as optional.&nbsp; Proposed oven temperatures/baking times are (200/1.5 hours), (250/30 minutes), (300/25 minutes), and (325/15 minutes).&nbsp; They vary in whether the cookies are to be baked on a greased cookie sheet, a greased and floured cookie sheet, on baking parchment, on paper towels, or on tinfoil.</p>\n<p>A good place to start is to go with the majority: I decided to include both salt and cream of tartar.&nbsp; Next, I eliminated the impractical: I could not find superfine sugar at the store and I don't own a food processor, so I went with granulated sugar.&nbsp; As for the rest of the instructions, it was a free-for-all.&nbsp; No two recipes agreed about the cookie sheet arrangement; the two of them that mentioned \"cracking\" disagreed on whether it was a desireable outcome; and worst, none of them explained why every time I tried to make these cookies, they refused to foam up and form peaks<sup>2</sup>.&nbsp; Time for the last step.</p>\n<p><strong id=\"III__Improvisation\">III. Improvisation</strong></p>\n<p>A close reading of the more <a href=\"http://www.joyofbaking.com/MeringueCookies.html\">verbose</a> <a href=\"http://whatscookingamerica.net/Eggs/perfectmeringue.htm\">recipes</a> turns up urgent cautions about not letting any grease into the batter, be it a smear from a prior cooking adventure left on the mixing bowl, a bit of yolk, or - in one recipe - the oils that are naturally on skin.&nbsp; This last, it turned out, was the key: I was separating eggs by hand, and was not very neat about it.&nbsp; I switched to a technique recommended by a friend involving spoons, and presto, I could get the meringue batter to hold peaks...</p>\n<p>But how long to cook them, at what temperature, and sitting on what?&nbsp; There, it was necessary to experiment (fortunately, after having narrowed the search space somewhat).&nbsp; This stage depended as much on my personal taste, the local weather, and the behavior of my oven as on the accuracy of the original recipes; it seems that my oven runs hot, so I need to bake them at 250 degrees or cooler and babysit them after the first ten minutes, or they will burn.&nbsp; Additionally, parchment paper and tinfoil<sup>3</sup> wound up burning the bottoms of the cookies before the tops were even dry; paper towels worked.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup>Caster and superfine sugar are the same thing, and you can make a reasonable facsimile using a food processor.&nbsp; When the type of sugar is not specified in a recipe, it means to use granulated white sugar; other kinds are named (e.g. light or dark brown sugar, turbinado sugar, confectioner's sugar, etc.).&nbsp; This is one of the examples of a situation where background knowledge of the field comes in handy.</p>\n<p><sup>2</sup>Not that this stopped me from baking the batter anyway.&nbsp; It just turned into round, flat cookies instead of puffy, light ones.</p>\n<p><sup>3</sup>I didn't get around to trying greased nor greased and floured bare cookie sheets - I prioritized these tests last because they involve more dishes to wash.</p>", "sections": [{"title": "I. Saturation", "anchor": "I__Saturation", "level": 1}, {"title": "II. Distillation", "anchor": "II__Distillation", "level": 1}, {"title": "III. Improvisation", "anchor": "III__Improvisation", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "29 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-24T16:09:11.621Z", "modifiedAt": null, "url": null, "title": "This Failing Earth", "slug": "this-failing-earth", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:03.615Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2KNN9WPcyto7QH9pi/this-failing-earth", "pageUrlRelative": "/posts/2KNN9WPcyto7QH9pi/this-failing-earth", "linkUrl": "https://www.lesswrong.com/posts/2KNN9WPcyto7QH9pi/this-failing-earth", "postedAtFormatted": "Sunday, May 24th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20This%20Failing%20Earth&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThis%20Failing%20Earth%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2KNN9WPcyto7QH9pi%2Fthis-failing-earth%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=This%20Failing%20Earth%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2KNN9WPcyto7QH9pi%2Fthis-failing-earth", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2KNN9WPcyto7QH9pi%2Fthis-failing-earth", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1441, "htmlBody": "<p>Suppose I told you about a certain country, somewhere in the world, in which some of the cities have degenerated into gang rule.&nbsp; Some such cities are ruled by a single gang leader, others have degenerated into almost complete lawlessness.&nbsp; You would probably conclude that the cities I was talking about were located inside what we call a \"failed state\".</p>\n<p>So what does the existence of North Korea say about this Earth?</p>\n<p>No, it's not a perfect analogy.&nbsp; But the thought does sometimes occur to me, to wonder if the camel has two humps.&nbsp; If there are failed Earths and successful Earths, in the great macroscopic superposition popularly known as \"<a href=\"http://www.overcomingbias.com/2008/06/mwi-wins.html\">many worlds</a>\" - and we're not one of the successful.&nbsp; I think of this as the \"failed Earth\" hypothesis.</p>\n<p>Of course the camel could also have three or more humps, and it's quite easy to imagine Earths that are failing much worse than this, <em>epic failed Earths</em> ruled by the high-tech heirs of Genghis Khan or the Catholic Church.&nbsp; Oh yes, it could definitely be worse...</p>\n<p>...and the \"failed state\" analogy is hardly perfect; \"failed state\" usually refers to <em>failure to integrate</em> into the global economy, but a failed Earth is not failing to integrate into anything larger...</p>\n<p>...but the question does sometimes haunt me, as to whether in the alternative Everett branches of Earth, we could identify a distinct cluster of \"successful\" Earths, and we're not in it.&nbsp; It may not matter much in the end; the ultimate test of a planet's existence probably comes down to Friendly AI, and Friendly AI may come down to nine people in a basement doing math.&nbsp; I keep my hopes up, and think of this as a \"failing Earth\" rather than a \"failed Earth\".</p>\n<p>But it's a thought that comes to mind, now and then.&nbsp; Reading about the ongoing Market Complexity Collapse and wondering if this Earth failed to solve one of the basic functions of global economics, in the same way that Rome, in its later days, failed to solve the problem of orderly transition of power between Caesars.<a id=\"more\"></a></p>\n<p>Of course it's easy to wax moralistic about people who aren't solving their coordination problems the way you like.&nbsp; I don't mean this to degenerate into a standard diatribe about the sinfulness of this Earth, the sort of clueless plea embodied perfectly by <a href=\"http://www.youtube.com/watch?v=q2s6lUqyI-c\">Simon and Garfunkel</a>:</p>\n<blockquote>\n<p><span>I dreamed I saw a mighty room<br />The room was filled with men<br />And the paper they were signing said<br />They'd never fight again</span></p>\n</blockquote>\n<p>It's a <em>cheap </em>pleasure to wax moralistic about failures of global coordination.</p>\n<p>But visualizing the alternative Everett branches of Earth, spread out and clustered - for me, at least, that seems to help trigger my mind into a <em>non</em>-Simon-and-Garfunkel mode of thinking.&nbsp; If the successful Earths lack a North Korea, how did they get there?&nbsp; Surely not just by signing a piece of paper saying they'd never fight again.</p>\n<p>Indeed, our Earth's <a href=\"http://en.wikipedia.org/wiki/Westphalian_sovereignty\">Westphalian concept</a> of sovereign states is the main thing propping up Somalia and North Korea.&nbsp; There was a time when any state that failed that badly would be casually conquered by a more successful neighbor.&nbsp; So maybe the successful Earths don't have a Westphalian concept of sovereignty; maybe our Earth's concept of inviolable borders represents a <em>failure</em> to solve one of the key functions of a planetary civilization.</p>\n<p>Maybe the successful Earths are the ones where the ancient Greeks, or equivalent thereof, had the \"Aha!\" of Darwinian evolution... and at least one country started a eugenics program that <em>successfully</em> selected for intelligence, well in advance of nuclear weapons being developed.&nbsp; If that makes you uncomfortable, it's meant to - the successful Earths may not have gotten there through Simon and Garfunkel.&nbsp; And yes, of course the ancient Greeks attempting such a policy could and probably would have gotten it terribly wrong; maybe the <em>epic failed</em> Earths are the ones where some group had the Darwinian insight and then successfully selected for prowess as warriors.&nbsp; I'm not saying \"Go eugenics!\" would have been a systematically good idea for ancient Greeks to try as policy...</p>\n<p>But maybe the <em>top </em>cluster of successful Earths, among human Everett branches, stumbled into that cluster because some group stumbled over eugenic selection for intelligence, and then, being a bit smarter, realized what it was they were doing right, so that the average IQ got up to 140 well before anyone developed nuclear weapons.&nbsp; (And then conquered the world, rather than respecting the integrity of borders.)</p>\n<p>What would a successful Earth look like?&nbsp; How high is their standard <a href=\"/lw/1e/raising_the_sanity_waterline/\">sanity waterline</a>?&nbsp; Are there large organized religions in successful Earths - is their presence here a symptom of our failure to solve the problems of a planetary civilization?&nbsp; You can ring endless changes on this theme, and anyone with an accustomed political hobbyhorse is undoubtedly imagining their pet <a href=\"http://www.overcomingbias.com/2009/01/weirdtopia.html\">Utopia</a> already.&nbsp; For my own part, I'll go ahead and wonder, if there's an identifiable \"successful\" cluster among the human Earths, what percentage of them have worldwide <a href=\"http://wiki.lesswrong.com/wiki/Cryonics\">cryonic preservation</a> programs in place.</p>\n<p>One point that takes some of the sting out of our ongoing muddle - at least from my perspective - is my suspicion that the Earths in the successful cluster, even those with an average IQ of 140 as they develop computers, may not be in much of a better position to <em>really</em> succeed, to solve the Friendly AI problem.&nbsp; A rising tide lifts all boats, and Friendly AI is a race between cautiously developed AI and insufficiently-cautiously-developed AI.&nbsp; \"Successful\" Earths might even be worse off, if they solve their global coordination problems well enough to put the whole world's eyes on the problem and turn the development over to prestigious bureaucrats.&nbsp; It's not a simple issue like cryonics that we're talking about.&nbsp; If, in the end, \"successful Earths\" of the human epoch aren't in a much better position for the catastrophically high-level pass-fail test of the posthuman transition, than our own \"failing Earth\"... then this Earth isn't all that much <em>more</em> doomed just because we screwed up our financial system, international relations, and <a href=\"/lw/1e/raising_the_sanity_waterline/\">basic rationality training</a>.</p>\n<p>Is such speculation at all useful?&nbsp; \"Live in your own world\", <a href=\"http://www.overcomingbias.com/2008/05/if-many-worlds.html\">as the saying goes</a>...</p>\n<p>...Well, it might not be a saying <em>here,</em> but it's probably a saying in those <em>successful </em>Earths where the scientific community is long since trained in formal Bayesianism and they readily accepted <a href=\"http://www.overcomingbias.com/2008/06/mwi-wins.html\">the obvious truth</a> of many-worlds... as opposed to our own world and its constantly struggling academia where senior scientists spend most of their time writing grant proposals...</p>\n<p>(Michael Vassar has an extended thesis on how the scientific community in our Earth has been slowly dying since 1910 or so, but I'll let him decide whether it's worth his time to write up that post.)</p>\n<p>It's usually not my intent to depress people.&nbsp; I have an accustomed saying that if you want to depress yourself, look at the future, and if you want to cheer yourself up, look at the past.&nbsp; By analogy - well, for all we know, we might be in the <em>second-highest</em> major cluster, or in the top 10% of all Earths even if not one of the top 1%.&nbsp; It might be that most Earths have global orders descended from the conquering armies of the local Church.&nbsp; I recently had occasion to visit the National Museum of Australia in Canberra, and it's shocking to think of how easily a human culture can spend thirty thousand years without inventing the bow and arrow.&nbsp; Really, we <em>did</em> do quite well for ourselves in a lot of ways... I think?</p>\n<p>A sense of beleaguredness, a sense that everything is <a href=\"http://www.overcomingbias.com/2007/12/guardians-of-th.html\">decaying</a> and dying into sinfulness - these memes are more useful for gluing together cults than for inspiring people to solve their coordination problems.</p>\n<p>But even so - it's a thought that I have, when I see some aspect of the world going epically awry, to wonder if we're in the cluster of Earths that fail.&nbsp; It's the sort of thought that inspires <em>me,</em> at least, to go down into that basement and solve the math problem and make everything come out all right <em>anyway.</em>&nbsp; Because if there's one thing that the <a href=\"http://yudkowsky.net/singularity/schools\">intelligence explosion</a> really messes up, it's the <em>dramatic unity</em> of human progress - if this were a world with a <a href=\"http://wiki.lesswrong.com/wiki/Unsupervised_universe\">supervised course of history</a> we'd be worrying about making it to <a href=\"http://www.overcomingbias.com/2009/02/interlude-with-the-confessor.html\">Akon's world</a> through a continuous developmental schema, not making a sudden left turn to solve a math problem.</p>\n<p>It may be that in the fractiles of the human Everett branches, we live in a failing Earth - but it's not <em>failed</em> until someone messes up the first AI.&nbsp; I find that a highly motivating thought.&nbsp; Your mileage may vary.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xexCWMyds6QLWognu": 1, "5f5c37ee1b5cdee568cfb1c9": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2KNN9WPcyto7QH9pi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 61, "baseScore": 28, "extendedScore": null, "score": 4.6e-05, "legacy": true, "legacyId": "592", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 166, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XqmjdBKa4ZaXJtNmf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-25T05:21:23.639Z", "modifiedAt": null, "url": null, "title": "The Wire versus Evolutionary Psychology", "slug": "the-wire-versus-evolutionary-psychology", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:47.657Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MrShaggy", "createdAt": "2009-04-24T05:04:10.887Z", "isAdmin": false, "displayName": "MrShaggy"}, "userId": "qAYMoaxfyMmkxJpwD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4HdTLwtWcJ6TD3m2g/the-wire-versus-evolutionary-psychology", "pageUrlRelative": "/posts/4HdTLwtWcJ6TD3m2g/the-wire-versus-evolutionary-psychology", "linkUrl": "https://www.lesswrong.com/posts/4HdTLwtWcJ6TD3m2g/the-wire-versus-evolutionary-psychology", "postedAtFormatted": "Monday, May 25th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Wire%20versus%20Evolutionary%20Psychology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Wire%20versus%20Evolutionary%20Psychology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4HdTLwtWcJ6TD3m2g%2Fthe-wire-versus-evolutionary-psychology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Wire%20versus%20Evolutionary%20Psychology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4HdTLwtWcJ6TD3m2g%2Fthe-wire-versus-evolutionary-psychology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4HdTLwtWcJ6TD3m2g%2Fthe-wire-versus-evolutionary-psychology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 383, "htmlBody": "<p>In their <a title=\"Evolutionary Psychology Primer\" href=\"http://www.psych.ucsb.edu/research/cep/primer.html\">Evolutionary Psychology Primer</a>, Cosmides and Tooby give an example of a hypothesized adaptation that allows us to detect cheaters in a certain type of logical task (Wason) that we generally fail at. &nbsp;In the <a href=\"http://en.wikipedia.org/wiki/Wason_selection_task\">Wason selection task</a>&nbsp;(both article and wiki give examples) you are presented a type of logic puzzle that people tend to do poorly at and even formal training in logic helps little, yet when the examples involve cheating (such as&nbsp;\"If you are to eat those cookies, then you must first fix your bed\" and the task would be to figure out if someone whose eating the cookies did indeed fix the bed) perform much better (25% right in the regular task,&nbsp;65-80% in this version, according to the article).</p>\n<p>In the show The Wire, in season one, episode eight, Wallace, a teen-age drug dealer is asked by a young child to help her with her math homework. &nbsp;It's an addition and subtraction word problem about passengers on a bus (can't remember the numbers, but along the lines of, if the bus has 10 people on it and at the next stop 3 get on and 4 leave, etc.). &nbsp;Wallace rephrases the word problem to be about drugs and the kid gets it right. &nbsp;Wallace frustrated asks why and the kid replies along the lines of: \"They beat you if you get the count wrong.\" (<span style=\"font-weight: bold;\">Edit</span>:simpleton gives the quote as <span style=\"font-family: Arial; line-height: 19px;\"><span style=\"font-family: Verdana; line-height: normal;\">\"Count be wrong, they fuck you up.\")</span></span></p>\n<p>C&amp;T conclude that there are evolved \"algorithms\" in our brains that deal with social contract processing that explain why people do better on certain Wason selection tasks. &nbsp;The Wire points out a simpler possible explanation that their experiments did not control for: people do better on tasks they care about, unless one would like to suppose there are special math circuits in the brain for certain \"social contract\" situations.</p>\n<p>Of course, I am not saying a fictional anecdote disproves C&amp;T's claim, but it does point to something they didn't test for, and something that I find rather plausible.</p>\n<p>Possible tests: Look at emotionally-motivating things that vary across culture and develop Wason selection tasks to test for that; look at various types of emotionally-motivating things (which I do not presume all emotional responses will affect the test results), and obviously, test The Wire example itself.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 1, "exZi6Bing5AiM4ZQB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4HdTLwtWcJ6TD3m2g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 18, "extendedScore": null, "score": 4.965078170765252e-07, "legacy": true, "legacyId": "593", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-26T18:55:17.205Z", "modifiedAt": null, "url": null, "title": "Dissenting Views", "slug": "dissenting-views", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:05.470Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "byrnema", "createdAt": "2009-03-20T18:02:38.305Z", "isAdmin": false, "displayName": "byrnema"}, "userId": "SCnD6W8NiztYBN39M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3mromFtJuGo2d5NHr/dissenting-views", "pageUrlRelative": "/posts/3mromFtJuGo2d5NHr/dissenting-views", "linkUrl": "https://www.lesswrong.com/posts/3mromFtJuGo2d5NHr/dissenting-views", "postedAtFormatted": "Tuesday, May 26th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dissenting%20Views&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADissenting%20Views%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3mromFtJuGo2d5NHr%2Fdissenting-views%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dissenting%20Views%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3mromFtJuGo2d5NHr%2Fdissenting-views", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3mromFtJuGo2d5NHr%2Fdissenting-views", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 618, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> </w:WordDocument> </xml><![endif]--><!-- /* Style Definitions */ p.MsoNormal, li.MsoNormal, div.MsoNormal {mso-style-parent:\"\"; margin:0in; margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:\"Times New Roman\"; mso-fareast-font-family:\"Times New Roman\";} a:link, span.MsoHyperlink {color:blue; text-decoration:underline; text-underline:single;} a:visited, span.MsoHyperlinkFollowed {color:purple; text-decoration:underline; text-underline:single;} @page Section1 {size:8.5in 11.0in; margin:1.0in 1.25in 1.0in 1.25in; mso-header-margin:.5in; mso-footer-margin:.5in; mso-paper-source:0;} div.Section1 {page:Section1;} --><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Times New Roman\";} --> <!--[endif]--></p>\n<p class=\"MsoNormal\">Occasionally, concerns have been expressed from within Less Wrong that the community is <a href=\"/lw/fk/survey_results\">too homogeneous</a>. Certainly the observation of homogeneity is true to the extent that the community shares common views that are minority views in the general population.</p>\n<p class=\"MsoNormal\"><strong>Maintaining a High Signal to Noise Ratio</strong></p>\n<p class=\"MsoNormal\">The Less Wrong community shares an <a href=\"http://en.wikipedia.org/wiki/Ideology\">ideology</a> that it is calling &lsquo;rationality&rsquo;(despite some attempts to <a class=\"reddit-link-title\" href=\"/lw/cx/whats_in_a_name_that_which_we_call_a_rationalist\">rename it</a>, this is what it is). A burgeoning ideology needs a lot of faithful support in order to develop true. By this, I mean that the ideology needs a chance to define itself as it would define itself, without a lot of competing influences watering it down, adding impure elements, distorting it. In other words, you want to cultivate a high signal to noise ratio.</p>\n<p class=\"MsoNormal\">For the most part, Less Wrong is remarkably successful at cultivating this high signal to noise ratio. A common ideology attracts people to Less Wrong, and then karma is used to maintain fidelity. It protects Less Wrong from the influence of outsiders who just don't \"get it\". It is also used to guide and teach people who are reasonably near the ideology but need some training in rationality. Thus, karma is awarded for views that align especially well with the ideology, align reasonably well, or that align with one of the directions that the ideology is reasonably evolving.</p>\n<p class=\"MsoNormal\"><a id=\"more\"></a></p>\n<p class=\"MsoNormal\"><strong>Rationality is not a religion &ndash; Or is it?</strong></p>\n<p class=\"MsoNormal\">Therefore, on Less Wrong, a person earns karma by expressing views from <em>within</em> the ideology. Wayward comments are discouraged with down-votes. Sometimes, even, an ideological toe is stepped on, and the disapproval is more explicit. I&rsquo;ve been told, here and there, one way or another, that expressing extremely dissenting views is: <a class=\"reddit-link-title\" href=\"/lw/bd/my_way\">stomping on flowers</a>, <a id=\"permalink_t1_b58\" href=\"/lw/ee/the_mindkiller/b58\" target=\"_parent\">showing disrespect</a>, not playing along, being inconsiderate.</p>\n<p class=\"MsoNormal\">So it turns out: the conditions necessary for the faithful support of an ideology are not that different from the conditions sufficient for developing a cult.</p>\n<p class=\"MsoNormal\">But Less Wrong isn't a religion or a cult. It wants to identify and dis-root illusion, not create a safe place to cultivate it. Somewhere, Less Wrong <em>must</em> be able challenge its basic assumptions, and see how they hold up to new and all evidence. You have to allow brave dissent.</p>\n<ul>\n<li>\n<p class=\"MsoNormal\">Outsiders who insist on hanging around can help by pointing to assumptions that are thought to be self-evident by those who \"get it\", but that aren&rsquo;t<em> obviously</em> true. And which may be wrong.</p>\n</li>\n<li>\n<p class=\"MsoNormal\">It&rsquo;s not necessarily the case that someone challenging a significant assumption doesn&rsquo;t get it and doesn&rsquo;t belong here. Maybe, occasionally, someone with a dissenting view may be representing the ideology <em>more</em> than the status quo.</p>\n</li>\n</ul>\n<p class=\"MsoNormal\">Shouldn&rsquo;t there be a place where people who think they are more rational (or better than rational), can say, &ldquo;hey, this is wrong!&rdquo;?</p>\n<p class=\"MsoNormal\"><strong>A Solution</strong></p>\n<p class=\"MsoNormal\">I am creating this top-level post for people to express dissenting views that are simply <em>too far </em>from the main ideology to be expressed in other posts. If successful, it would serve two purposes. First, it would remove extreme dissent away from the other posts, thus maintaining fidelity there. People who want to play at &ldquo;rationality&rdquo; ideology can play without other, irrelevant points of view spoiling the fun. Second, it would <em>allow</em> dissent for those in the community who are interested in not being a cult, challenging first assumptions and suggesting ideas for improving Less Wrong without being traitorous. (By the way, karma must still work the same, or the discussion loses its value relative to the rest of Less Wrong. Be prepared to lose karma.)</p>\n<p class=\"MsoNormal\">Thus I encourage anyone (outsiders and insiders) to use this post &ldquo;Dissenting Views&rdquo; to answer the question:<strong> Where do you think Less Wrong is <em>most wrong</em>?</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"wzgcQCrwKfETcBpR9": 1, "gHCNhqxuJq2bZ2akb": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3mromFtJuGo2d5NHr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 22, "extendedScore": null, "score": 3.7e-05, "legacy": true, "legacyId": "594", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> </w:WordDocument> </xml><![endif]--><!-- /* Style Definitions */ p.MsoNormal, li.MsoNormal, div.MsoNormal {mso-style-parent:\"\"; margin:0in; margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:\"Times New Roman\"; mso-fareast-font-family:\"Times New Roman\";} a:link, span.MsoHyperlink {color:blue; text-decoration:underline; text-underline:single;} a:visited, span.MsoHyperlinkFollowed {color:purple; text-decoration:underline; text-underline:single;} @page Section1 {size:8.5in 11.0in; margin:1.0in 1.25in 1.0in 1.25in; mso-header-margin:.5in; mso-footer-margin:.5in; mso-paper-source:0;} div.Section1 {page:Section1;} --><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Times New Roman\";} --> <!--[endif]--></p>\n<p class=\"MsoNormal\">Occasionally, concerns have been expressed from within Less Wrong that the community is <a href=\"/lw/fk/survey_results\">too homogeneous</a>. Certainly the observation of homogeneity is true to the extent that the community shares common views that are minority views in the general population.</p>\n<p class=\"MsoNormal\"><strong id=\"Maintaining_a_High_Signal_to_Noise_Ratio\">Maintaining a High Signal to Noise Ratio</strong></p>\n<p class=\"MsoNormal\">The Less Wrong community shares an <a href=\"http://en.wikipedia.org/wiki/Ideology\">ideology</a> that it is calling \u2018rationality\u2019(despite some attempts to <a class=\"reddit-link-title\" href=\"/lw/cx/whats_in_a_name_that_which_we_call_a_rationalist\">rename it</a>, this is what it is). A burgeoning ideology needs a lot of faithful support in order to develop true. By this, I mean that the ideology needs a chance to define itself as it would define itself, without a lot of competing influences watering it down, adding impure elements, distorting it. In other words, you want to cultivate a high signal to noise ratio.</p>\n<p class=\"MsoNormal\">For the most part, Less Wrong is remarkably successful at cultivating this high signal to noise ratio. A common ideology attracts people to Less Wrong, and then karma is used to maintain fidelity. It protects Less Wrong from the influence of outsiders who just don't \"get it\". It is also used to guide and teach people who are reasonably near the ideology but need some training in rationality. Thus, karma is awarded for views that align especially well with the ideology, align reasonably well, or that align with one of the directions that the ideology is reasonably evolving.</p>\n<p class=\"MsoNormal\"><a id=\"more\"></a></p>\n<p class=\"MsoNormal\"><strong id=\"Rationality_is_not_a_religion___Or_is_it_\">Rationality is not a religion \u2013 Or is it?</strong></p>\n<p class=\"MsoNormal\">Therefore, on Less Wrong, a person earns karma by expressing views from <em>within</em> the ideology. Wayward comments are discouraged with down-votes. Sometimes, even, an ideological toe is stepped on, and the disapproval is more explicit. I\u2019ve been told, here and there, one way or another, that expressing extremely dissenting views is: <a class=\"reddit-link-title\" href=\"/lw/bd/my_way\">stomping on flowers</a>, <a id=\"permalink_t1_b58\" href=\"/lw/ee/the_mindkiller/b58\" target=\"_parent\">showing disrespect</a>, not playing along, being inconsiderate.</p>\n<p class=\"MsoNormal\">So it turns out: the conditions necessary for the faithful support of an ideology are not that different from the conditions sufficient for developing a cult.</p>\n<p class=\"MsoNormal\">But Less Wrong isn't a religion or a cult. It wants to identify and dis-root illusion, not create a safe place to cultivate it. Somewhere, Less Wrong <em>must</em> be able challenge its basic assumptions, and see how they hold up to new and all evidence. You have to allow brave dissent.</p>\n<ul>\n<li>\n<p class=\"MsoNormal\">Outsiders who insist on hanging around can help by pointing to assumptions that are thought to be self-evident by those who \"get it\", but that aren\u2019t<em> obviously</em> true. And which may be wrong.</p>\n</li>\n<li>\n<p class=\"MsoNormal\">It\u2019s not necessarily the case that someone challenging a significant assumption doesn\u2019t get it and doesn\u2019t belong here. Maybe, occasionally, someone with a dissenting view may be representing the ideology <em>more</em> than the status quo.</p>\n</li>\n</ul>\n<p class=\"MsoNormal\">Shouldn\u2019t there be a place where people who think they are more rational (or better than rational), can say, \u201chey, this is wrong!\u201d?</p>\n<p class=\"MsoNormal\"><strong id=\"A_Solution\">A Solution</strong></p>\n<p class=\"MsoNormal\">I am creating this top-level post for people to express dissenting views that are simply <em>too far </em>from the main ideology to be expressed in other posts. If successful, it would serve two purposes. First, it would remove extreme dissent away from the other posts, thus maintaining fidelity there. People who want to play at \u201crationality\u201d ideology can play without other, irrelevant points of view spoiling the fun. Second, it would <em>allow</em> dissent for those in the community who are interested in not being a cult, challenging first assumptions and suggesting ideas for improving Less Wrong without being traitorous. (By the way, karma must still work the same, or the discussion loses its value relative to the rest of Less Wrong. Be prepared to lose karma.)</p>\n<p class=\"MsoNormal\">Thus I encourage anyone (outsiders and insiders) to use this post \u201cDissenting Views\u201d to answer the question:<strong> Where do you think Less Wrong is <em>most wrong</em>?</strong></p>", "sections": [{"title": "Maintaining a High Signal to Noise Ratio", "anchor": "Maintaining_a_High_Signal_to_Noise_Ratio", "level": 1}, {"title": "Rationality is not a religion \u2013 Or is it?", "anchor": "Rationality_is_not_a_religion___Or_is_it_", "level": 1}, {"title": "A Solution", "anchor": "A_Solution", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "212 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 212, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZWC3n9c6v4s35rrZ3", "tGPeyg2GXFvtXH8XN", "FBgozHEv7J72NCEPB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-27T12:57:21.590Z", "modifiedAt": null, "url": null, "title": "Eric Drexler on Learning About Everything", "slug": "eric-drexler-on-learning-about-everything", "viewCount": null, "lastCommentedAt": "2020-08-18T16:54:28.468Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XcDSmXecYiubPjxAj/eric-drexler-on-learning-about-everything", "pageUrlRelative": "/posts/XcDSmXecYiubPjxAj/eric-drexler-on-learning-about-everything", "linkUrl": "https://www.lesswrong.com/posts/XcDSmXecYiubPjxAj/eric-drexler-on-learning-about-everything", "postedAtFormatted": "Wednesday, May 27th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Eric%20Drexler%20on%20Learning%20About%20Everything&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEric%20Drexler%20on%20Learning%20About%20Everything%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXcDSmXecYiubPjxAj%2Feric-drexler-on-learning-about-everything%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Eric%20Drexler%20on%20Learning%20About%20Everything%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXcDSmXecYiubPjxAj%2Feric-drexler-on-learning-about-everything", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXcDSmXecYiubPjxAj%2Feric-drexler-on-learning-about-everything", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 252, "htmlBody": "<p><strong>Related to</strong>: <a href=\"http://www.overcomingbias.com/2007/11/the-simple-math.html\">The Simple Math of Everything</a>, <a href=\"http://www.overcomingbias.com/2007/08/your-strength-a.html\">Your Strength as a Rationalist</a>, <a href=\"/lw/l/teaching_the_unteachable/\">Teaching the Unteachable</a>.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Eric_Drexler\">Eric Drexler</a> wrote a couple of articles on the importance and methods of obtaining <a href=\"http://wiki.lesswrong.com/wiki/General_knowledge\">interdisciplinary knowledge</a>:</p>\n<ul>\n<li><a href=\"http://metamodern.com/2009/05/17/how-to-understand-everything-and-why/\">How to Understand Everything (and Why)</a></li>\n<li><a href=\"http://metamodern.com/2009/05/27/how-to-learn-about-everything/\">How to Learn About Everything</a></li>\n</ul>\n<blockquote>\n<p>Note that the title above isn't \"how to learn everything\", but \"how to learn <em>about</em> everything\". The distinction I have in mind is between knowing the inside of a topic in deep detail &mdash; many facts and problem-solving skills &mdash; and knowing the structure and context of a topic: essential facts, what problems can be solved by the skilled, and how the topic fits with others.</p>\n<p>This knowledge isn't superficial in a survey-course sense: It is about both deep structure and practical applications. Knowing <em>about</em>, in this sense, is crucial to understanding a new problem and what must be learned in more depth in order to solve it.</p>\n</blockquote>\n<p>This topic was discussed intermittently on Overcoming Bias. Basic understanding of many fields allows to recognize how <a href=\"http://www.overcomingbias.com/2007/10/no-one-knows-wh.html\">well-understood by science</a> a problem is and to see its place in the structure of scientific knowledge; to develop better <a href=\"http://www.overcomingbias.com/2007/08/your-strength-a.html\">intuitive grasp</a> on what's possible and what's not; and to adequately <a href=\"http://www.overcomingbias.com/2008/03/joy-in-the-real.html\">perceive</a> the natural world.</p>\n<p>The advice he gives for obtaining general knowledge feels right, even for studying the topics that you intend to eventually understand in depth:</p>\n<blockquote>\n<p>Don't drop a subject because you know you'd fail a test &mdash; instead, read other half-understandable journals and textbooks to accumulate vocabulary, perspective, and context.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fF9GEdWXKJ3z73TmB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XcDSmXecYiubPjxAj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 37, "extendedScore": null, "score": 4.970160294115702e-07, "legacy": true, "legacyId": "597", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9SaAyq7F7MAuzAWNN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-27T21:12:13.086Z", "modifiedAt": null, "url": null, "title": "Anime Explains the Epimenides Paradox", "slug": "anime-explains-the-epimenides-paradox", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:33.745Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y3HAxPN8WHqX7zhkv/anime-explains-the-epimenides-paradox", "pageUrlRelative": "/posts/y3HAxPN8WHqX7zhkv/anime-explains-the-epimenides-paradox", "linkUrl": "https://www.lesswrong.com/posts/y3HAxPN8WHqX7zhkv/anime-explains-the-epimenides-paradox", "postedAtFormatted": "Wednesday, May 27th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anime%20Explains%20the%20Epimenides%20Paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnime%20Explains%20the%20Epimenides%20Paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy3HAxPN8WHqX7zhkv%2Fanime-explains-the-epimenides-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anime%20Explains%20the%20Epimenides%20Paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy3HAxPN8WHqX7zhkv%2Fanime-explains-the-epimenides-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy3HAxPN8WHqX7zhkv%2Fanime-explains-the-epimenides-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<p>The Epimenides Paradox or Liar Paradox is \"This sentence is false.\"&nbsp; Type hierarchies are supposed to resolve the Epimenides paradox... Using an indefinitely extensible, indescribably infinite, ordinal hierarchy of meta-languages. No meta-language can contain its own truth predicate - no meta-language can talk about the \"truth\" or \"falsity\" of its own sentences - and so for every meta-language we need a meta-meta-language.<br /><br />I didn't create <a href=\"http://www.youtube.com/watch?v=XnAeuSvyh70\">this video</a> and I don't know who did - but it does a pretty good job of depicting how I feel about infinite type hierarchies: namely, pretty much the same way I feel about the original Epimenides Paradox.</p>\n<p>Bonus problem: In what language did I write the description of this video?</p>\n<p><a id=\"more\"></a></p>\n<p>\n<object width=\"425\" height=\"344\">\n<param name=\"movie\" value=\"http://www.youtube.com/v/XnAeuSvyh70&amp;hl=en&amp;fs=1\" />\n<param name=\"allowFullScreen\" value=\"true\" />\n<param name=\"allowscriptaccess\" value=\"always\" /><embed type=\"application/x-shockwave-flash\" width=\"425\" height=\"344\" src=\"http://www.youtube.com/v/XnAeuSvyh70&amp;hl=en&amp;fs=1\" allowscriptaccess=\"always\" allowfullscreen=\"true\"></embed>\n</object>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LnEEs8xGooYmQ8iLA": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y3HAxPN8WHqX7zhkv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 4, "extendedScore": null, "score": 4.970913413106868e-07, "legacy": true, "legacyId": "598", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-28T06:03:22.092Z", "modifiedAt": null, "url": null, "title": "Do Fandoms Need Awfulness?", "slug": "do-fandoms-need-awfulness", "viewCount": null, "lastCommentedAt": "2021-10-18T00:10:22.138Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PG8i7ZiqLxthACaBi/do-fandoms-need-awfulness", "pageUrlRelative": "/posts/PG8i7ZiqLxthACaBi/do-fandoms-need-awfulness", "linkUrl": "https://www.lesswrong.com/posts/PG8i7ZiqLxthACaBi/do-fandoms-need-awfulness", "postedAtFormatted": "Thursday, May 28th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Do%20Fandoms%20Need%20Awfulness%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADo%20Fandoms%20Need%20Awfulness%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPG8i7ZiqLxthACaBi%2Fdo-fandoms-need-awfulness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Do%20Fandoms%20Need%20Awfulness%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPG8i7ZiqLxthACaBi%2Fdo-fandoms-need-awfulness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPG8i7ZiqLxthACaBi%2Fdo-fandoms-need-awfulness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 841, "htmlBody": "<p>Stephen Bond, \"<a href=\"http://plover.net/~bonds/objects.html\">Objects of Fandom</a>\":</p>\n<blockquote>\n<p>...my theory is that for something to attract fans, it must have an aspect of <em>truly monumental badness</em> about it.</p>\n<p><em>Raiders of the Lost Ark</em> is a robust potboiler, tongue-in-cheek, very competently done. I think it's enjoyable, but even among those who don't, it's hard to see the film attracting actual derision. Boredom or irritation, probably, but nothing more. <em>Star Wars</em>, on the other hand.... From one perspective, it's an entertaining space opera, but from a slightly different perspective, an imperceptible twist of the glass, it's laughably awful. Utterly ridiculously bad. And it's this very badness that makes so many people take up arms in its defence.</p>\n<p>...It's impossible to imagine a fan of <em>Animal Farm</em>, the <em>Well-Tempered Clavier</em>, or the theory of gravity. Such works can defend themselves. But badness, especially badness of an obvious, monumental variety, inspires devotion. The quality of the work, in the face of such glaring shortcomings, becomes a matter of faith -- and faith is a much stronger bond than mere appreciation. It drives fans together, gives them strength against those who sneer... And so the fan groups of <a href=\"http://plover.net/%7Ebonds/tolkien1.html\">Tolkien</a>, Star Trek, Spider-man, Japanese kiddie-cartoons etc. develop an almost cult-like character.</p>\n</blockquote>\n<p>\"Uh oh,\" I said to myself on first reading this, \"Is this why my fans are more intense than Robin Hanson's fans?&nbsp; And if I write a rationality book, should I actually give in to temptation and self-indulgence and write in <em>Twelve Virtues</em> style, just so that it has something attackable for fans to defend?\"</p>\n<p>But the second time I turned my thoughts toward this question, I performed that <a href=\"http://www.overcomingbias.com/2007/10/do-we-believe-e.html\">oft-neglected</a> operation, asking:&nbsp; \"I read it on the Internet, but is it actually true?\"&nbsp; Just because it's unpleasant doesn't mean it's true.&nbsp; And just because it provides a bit of cynicism that would give me rationality-credit to acknowledge, doesn't mean it becomes true just so I can earn the rationality-credit.<a id=\"more\"></a></p>\n<p>The first counterexample that came to mind was Jack Vance.&nbsp; Jack Vance is a science-fiction writer who, to the best of my knowledge, I've never heard accused of any great sin (or any lesser sin, actually).&nbsp; He is - was - the supremely <em>competent craftsman</em> of SF: his words flow, his plots race, and his human cultures are odder than other authors' aliens, to say nothing of his aliens.&nbsp; Vance didn't have his characters give controversial political speeches like Heinlein.&nbsp; Vance just wrote consistently excellent science fiction.</p>\n<p>And some of Vance's fans got together and produced the Vance Integral Edition, a complete collection of Vance in leather-bound hardcover books with high-quality long-lasting paper.&nbsp; They contracted to get the books printed, and when the books arrived, enough Vance fans showed up to ship them all.&nbsp; (They referred to themselves as \"packing scum\".)</p>\n<p>That's serious fandom.&nbsp; Aimed at work that - like <em>Animal Farm</em> or the <em>Well-Tempered Clavier</em> - is merely excellent, without an aspect of monumental badness to defend.</p>\n<p><em>Godel, Escher, Bach</em> - maybe I'm prejudiced here, and I've heard a word or two said against it, but really, I don't think the fandom that it has stems from it being frequently attacked.&nbsp; On the other hand, there aren't annual conventions for fans of self-referential sentences, so maybe it's not as much of a data point as I might like.</p>\n<p><em>Star Wars</em> really <em>did </em>have something going for it that <em>Raiders of the Lost Ark</em> didn't, namely, it introduced a lot of impressionable minds to science fiction.&nbsp; Or space opera, if you like.&nbsp; The point is that the romance of space is not the romance of archeology.</p>\n<p>On due reflection, I'm not sure that utter ridiculous monumental badness is all it's cracked up to be.</p>\n<p>But there are annual Star Trek conventions.&nbsp; And there are not annual Jack Vance conventions.&nbsp; Douglas Hofstadter might be far more widely beloved - but Ayn Rand has more <em>fanatic</em> fans.</p>\n<p>If Jack Vance had been so clever as to keep all the poetic phrasing and alien societies, but now and then have his characters make crazy political speeches - if he had deliberately introduced an aspect of monumental badness - would he now be <em>worshiped,</em> instead of just loved?</p>\n<p>Can anyone think of a true, pure counterexample of a reasonably fanatic fandom (to the level of annual conventions, though not necessarily suicide bombers) of something that is just sheer good professional craftwork, and not commonly criticized?&nbsp; And of course the acid test is not whether <em>you</em> think it is just sheer good craftsmanship, but whether this is <em>widely believed </em>within the broad context of the relevant social community - can you have fanatic fans when their object of worship really is <em>that good</em> and the mainstream believes it too?</p>\n<p>I do think that Stephen Bond's <em>Objects of Fandom</em> is pointing to a real effect, if not the only effect.&nbsp; So in the same vein that we should try to be <a href=\"/lw/ow/the_beauty_of_settled_science/\">attracted to basic science textbooks</a> and not just poorly written press releases about \"breaking news\", let us try to be fans of those merely excellent works that lack an aspect of monumental awfulness to defend.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PG8i7ZiqLxthACaBi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 33, "extendedScore": null, "score": 5.3e-05, "legacy": true, "legacyId": "1264", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 156, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ndGYn7ZFiZyernp9f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-28T11:35:28.556Z", "modifiedAt": null, "url": null, "title": "Can we create a function that provably predicts the optimization power of intelligences?", "slug": "can-we-create-a-function-that-provably-predicts-the", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:47.809Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whpearson", "createdAt": "2009-02-28T00:34:00.976Z", "isAdmin": false, "displayName": "whpearson"}, "userId": "bq8qsRbPNvFihHxgi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y5fhowZqyanr4cDg6/can-we-create-a-function-that-provably-predicts-the", "pageUrlRelative": "/posts/y5fhowZqyanr4cDg6/can-we-create-a-function-that-provably-predicts-the", "linkUrl": "https://www.lesswrong.com/posts/y5fhowZqyanr4cDg6/can-we-create-a-function-that-provably-predicts-the", "postedAtFormatted": "Thursday, May 28th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Can%20we%20create%20a%20function%20that%20provably%20predicts%20the%20optimization%20power%20of%20intelligences%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACan%20we%20create%20a%20function%20that%20provably%20predicts%20the%20optimization%20power%20of%20intelligences%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy5fhowZqyanr4cDg6%2Fcan-we-create-a-function-that-provably-predicts-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Can%20we%20create%20a%20function%20that%20provably%20predicts%20the%20optimization%20power%20of%20intelligences%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy5fhowZqyanr4cDg6%2Fcan-we-create-a-function-that-provably-predicts-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy5fhowZqyanr4cDg6%2Fcan-we-create-a-function-that-provably-predicts-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 499, "htmlBody": "<p>Follow up to&nbsp; <a href=\"/lw/vb/efficient_crossdomain_optimization/\">Efficient Cross-domain Optimization</a></p>\n<p>When I am skeptical that we will ever understand intelligence, I am skeptical that we will ever be able to reliably map a systems description onto its optimization power. This has implications for how well we will create intelligences and how well intelligences will be at self-improving.</p>\n<p>Obviously we can't predict the effectiveness of an arbitrary program, due to <a href=\"http://en.wikipedia.org/wiki/Rice%27s_theorem\">rice's theorem</a> and intelligence being a non-trivial property. So the best we can hope for is predicting the effectiveness of a set of programs. Is such a function possible? This is my take on the subject.</p>\n<p><a id=\"more\"></a></p>\n<p>Let&nbsp;<em> <strong>o ( p )</strong> </em>be a function that maps a program <em><strong>p</strong></em> to its optimization power.</p>\n<p>Mu, Omegas younger brother has a challenge for you, you get to design a system and put it in a box with 20 red and 20 green balls, it will activate itself after 10 minutes and then have the goal of removing as many red balls from the box as possible in 10 minutes. You have to decide how whether it is going to remove more or less than 5 red balls from the box. You get transported to a nirvana if you predict correctly and your world gets turned into paper clips if you get it wrong.</p>\n<p>You whip out your trusty <strong><em>o&nbsp;</em> </strong>and make a program and the evaluate it using <em><strong>o</strong></em>&nbsp; and bet according to its evaluation.</p>\n<p>Unknown to you Mu also has a copy of your <strong><em>o</em>&nbsp; </strong>and runs it on the systems you put in the box. Those that return a high value from the optimization power measure, it destroys before they activate, those that have a low effectiveness it performs their goals for them. In the second case it is still <em><strong>p </strong></em>that causes the goal to be fulfilled as if <em><strong>p</strong> </em>were different there would be a different amount that the goal is fulfilled. You can see it as inspiring pity in someone else to make them help, who would not have done otherwise. It is still winning.</p>\n<p>So Mu forces <em><strong>o</strong></em> to be wrong, so <em><strong>o&nbsp; </strong></em>was not the reliable predictor of a set of programs optimization power we had hoped for, so we have a contradiction. Is there anyway to salvage it? You could make your effectiveness measure depend upon the environment <em><strong>e</strong></em> as well, however that does not remove the potential for self-reference as <em><strong>o</strong></em> is part of the environment.&nbsp; So we might be able to rescue <em><strong>o</strong></em> by constraining the environment to not have any reference to <em><strong>o</strong></em> in. However we don't control the environment nor do we have perfect knowledge of the environment, so we don't know when it has references to <em><strong>o</strong></em> in or not, or when it is reliable.</p>\n<p>You could make try and make it so that&nbsp; Mu could have no impact on what <em><strong>p</strong></em>&nbsp; does. Which is the same as trying to make the system indestructible, but with a reversible physics what is created can be destroyed.</p>\n<p>So where do we go from here?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ac84EpK6mZbPLzmqj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y5fhowZqyanr4cDg6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -7, "extendedScore": null, "score": -1.2e-05, "legacy": true, "legacyId": "1265", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yLeEPFnnB9wE7KLx2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-28T14:16:03.492Z", "modifiedAt": null, "url": null, "title": "Link: The Case for Working With Your Hands", "slug": "link-the-case-for-working-with-your-hands", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:19.478Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Daniel_Burfoot", "createdAt": "2009-03-12T02:28:50.970Z", "isAdmin": false, "displayName": "Daniel_Burfoot"}, "userId": "XhcXE3Qk5adX6v2Cg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/96CAzLxwvBp7jtjjo/link-the-case-for-working-with-your-hands", "pageUrlRelative": "/posts/96CAzLxwvBp7jtjjo/link-the-case-for-working-with-your-hands", "linkUrl": "https://www.lesswrong.com/posts/96CAzLxwvBp7jtjjo/link-the-case-for-working-with-your-hands", "postedAtFormatted": "Thursday, May 28th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20The%20Case%20for%20Working%20With%20Your%20Hands&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20The%20Case%20for%20Working%20With%20Your%20Hands%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F96CAzLxwvBp7jtjjo%2Flink-the-case-for-working-with-your-hands%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20The%20Case%20for%20Working%20With%20Your%20Hands%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F96CAzLxwvBp7jtjjo%2Flink-the-case-for-working-with-your-hands", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F96CAzLxwvBp7jtjjo%2Flink-the-case-for-working-with-your-hands", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 351, "htmlBody": "<p>The NYTimes recently publised a long semi-autobiographical <a href=\"http://www.nytimes.com/2009/05/24/magazine/24labor-t.html?em\">article</a> written by Michael Crawford, a University of Chicago Phd graduate who is currently employed as a motorcycle mechanic. The article is partially a somewhat standard lament about the alienation and drudgery of modern corporate work. But it is also very much about rationality. Here's an excerpt:</p>\n<blockquote>\n<p>As it happened, in the spring I landed a job as executive director of a policy organization in Washington. This felt like a coup. But certain perversities became apparent as I settled into the job. It sometimes required me to reason backward, from desired conclusion to suitable premise. The organization had taken certain positions, and there were some facts it was more fond of than others. As its figurehead, I was making arguments I didn&rsquo;t fully buy myself. Further, my boss seemed intent on retraining me according to a certain cognitive style &mdash; that of the corporate world, from which he had recently come. This style demanded that I project an image of rationality but not indulge too much in actual reasoning. As I sat in my K Street office, Fred&rsquo;s life as an independent tradesman gave me an image that I kept coming back to: someone who really knows what he is doing, losing himself in work that is genuinely useful and has a certain integrity to it. He also seemed to be having a lot of fun.</p>\n</blockquote>\n<p>I think this article will strike a chord with programmers. A large part of the satisfaction of motorcycle work that Crawford describes comes from the fact that such work requires one to confront reality, however harsh it may be. Reality cannot be placated by hand-waving, Powerpoint slides, excuses, or sweet talk. But the very harshness of the challenge means that when reality yields to the finesse of a craftsman, the reward is much greater. Programming has a similar aspect: a piece of software is basically either correct or incorrect. And programming, like mechanical work, allows one to interrogate and engage the system of interest through a very high-bandwidth channel: you write a test, run it, tweak it, re-run, etc.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4kQXps8dYsKJgaayN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "96CAzLxwvBp7jtjjo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 24, "extendedScore": null, "score": 4.972476980249834e-07, "legacy": true, "legacyId": "1266", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-28T23:18:28.564Z", "modifiedAt": null, "url": null, "title": "Image vs. Impact: Can public commitment be counterproductive for achievement?", "slug": "image-vs-impact-can-public-commitment-be-counterproductive", "viewCount": null, "lastCommentedAt": "2021-02-12T02:22:44.504Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "patrissimo", "createdAt": "2009-03-01T21:01:34.487Z", "isAdmin": false, "displayName": "patrissimo"}, "userId": "jimxrRCsNY7PfcM6e", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6ocujnKZL38thXn62/image-vs-impact-can-public-commitment-be-counterproductive", "pageUrlRelative": "/posts/6ocujnKZL38thXn62/image-vs-impact-can-public-commitment-be-counterproductive", "linkUrl": "https://www.lesswrong.com/posts/6ocujnKZL38thXn62/image-vs-impact-can-public-commitment-be-counterproductive", "postedAtFormatted": "Thursday, May 28th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Image%20vs.%20Impact%3A%20Can%20public%20commitment%20be%20counterproductive%20for%20achievement%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AImage%20vs.%20Impact%3A%20Can%20public%20commitment%20be%20counterproductive%20for%20achievement%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ocujnKZL38thXn62%2Fimage-vs-impact-can-public-commitment-be-counterproductive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Image%20vs.%20Impact%3A%20Can%20public%20commitment%20be%20counterproductive%20for%20achievement%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ocujnKZL38thXn62%2Fimage-vs-impact-can-public-commitment-be-counterproductive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6ocujnKZL38thXn62%2Fimage-vs-impact-can-public-commitment-be-counterproductive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1017, "htmlBody": "<p>The traditional wisdom says that publicly committing to a goal is a useful technique for accomplishment. &nbsp;It creates pressure to fulfill one's claims, lest one lose status. &nbsp;However, when the goal is related to one's identity, a recent study shows that public commitment may actually be counterproductive. &nbsp;Nyuanshin <a href=\"http://nyuanshin.livejournal.com/212711.html\">posts</a>:</p>\n<blockquote>\n<p style=\"padding-left: 30px; \">&nbsp;&nbsp; &nbsp;\"Identity-related behavioral intentions that had been noticed by other people were translated into action less intensively than those that had been ignored. . . . when other people take notice of an individual's identity-related behavioral intention, this gives the individual a premature sense of possessing the aspired-to identity.\"</p>\n<p style=\"padding-left: 30px; \">&nbsp;&nbsp; &nbsp;-- <a href=\"http://www3.interscience.wiley.com/journal/122306810/abstract\">Gollwitzer at al (2009)</a></p>\n<p>This empirical finding flies in the face of conventional wisdom about the motivational effects of public goal-setting, but rings true to my experience. Belief is, apparently, fungible -- when you know that people think of you as an x-doer, you afffirm that self-image more confidently than you would if you had only your own estimation to go on. [info]colinmarshall and myself have already become aware of the dangers of vanity to any non-trivial endeavor, but it's nice to have some empirical corroboration. Keep your head down, your goals relatively private, and don't pat yourself on the back until you've got the job done.</p>\n</blockquote>\n<p>This matches my experience over the first year of <a href=\"http://seasteading.org/\">The Seasteading Institute</a>. &nbsp;We've received <a href=\"http://seasteading.org/about-tsi/in-the-news\">tons of press</a>, and I've probably spent as much time at this point interacting with the media as working on engineering. &nbsp;And the press is definitely useful - it helps us reach and get credibility with major donors, and it helps us grow our community of interested seasteaders (it takes a lot of people to found a country, and it takes a mega-lot of somewhat interested people to have a committed subset who will actually go do it).</p>\n<p>Yet I've always been vaguely uncomfortable about how much media attention we've gotten, even though we've just started progressing towards our long-term goals. &nbsp;It feels like an unearned reward. &nbsp;But is that bad? &nbsp;I keep wondering \"Why should that bother me? &nbsp;Isn't it a good thing to be given extra help in accomplishing this huge and difficult goal? &nbsp;Aren't unearned rewards the best kind of rewards?\"&nbsp;This study suggests the answer.</p>\n<p><a id=\"more\"></a>My original goal was to actually succeed at starting new countries, but as a human, I am motivated by the status to be won in pursuit of this goal as well as the base goal itself. &nbsp;I recognize this, and have tried to use it to my advantage, visualizing the joys of having achieved high status to motivate the long hours of effort needed to reach the base goal. &nbsp;But getting press attention just for starting work on the base goal, rather than significant accomplishments towards it, short-circuits this motivational process. &nbsp;It gives me status in return for just having an interesting idea (the easy part, at least for me) rather than moving it towards reality (the hard part), and helps&nbsp;affirm the self-image I strive for in return for creating the identity, rather than living up to it.</p>\n<p>I am tempted to say \"Well, since PR helps my goal, I shouldn't worry about being given status/identity too easily, it may be bad for my motivation but it is good for the cause\", but that sounds an awful lot like my internal status craver rationalizing why I should stop worrying about getting on TV (<a href=\"http://dsc.discovery.com/tv-schedules/series.html?paid=1.14617.25842.36051.6\">Discovery Channel, Monday June 8th, 10PM EST/PST</a> :) ).</p>\n<p>My current technique is to try, inasmuch as I can, to structure my own reward function around the more difficult and important goals. &nbsp;To cognitively reframe \"I got media attention, I am affirming my identity and achieving my goals\" as \"I got media attention, which is fun and slightly useful, but not currently on the critical path.\" &nbsp;To focus on achievement rather than recognition (internal standards rather than external ones, which has other benefits as well). &nbsp;Not only in my thoughts, but also in public statements - to describe seasteading as \"we're different because we're going to actually do it\", so that actual accomplishment is part of the identity I am striving for.</p>\n<p>One could suggest that OB/LW has this problem too - perhaps rewarding Eliezer with status for writing interesting posts allows him to achieve his identity as a rationalist with work that is less useful to his long-term goals than actually achieving FAI. However, I don't buy this. &nbsp;I think <a href=\"/lw/1e/raising_the_sanity_waterline/\">raising the sanity waterline</a> is a big deal, greater than FAI because it increases the resources available for dealing with FAI-like problems (ie converting a single present or future centimillionaire could lead to hiring multiple-Eliezer's worth of AI researchers). &nbsp;Hence his public-facing work has direct positive impact. &nbsp;And given this, while Eli's large audience may selfishly incent him towards public-facing work via the desire to seek status, it also increases the actual impact of his public-facing work&nbsp;since he reaches many people.</p>\n<p>Also of relevance is the community in which one is achieving status. &nbsp;Eliezer's OB/LW&nbsp;audience is largely self-selected rationalists, which might be good because it's the most receptive audience, or it might be restricting his message to an unnecessarily small niche, I'm not sure. &nbsp;But for seasteading, I think there is a clear conflict between the most exciting and most useful audiences. &nbsp;What we need to succeed is a small group of highly committed and talented people, which is better served by very focused publicity, yet intuitively it feels like more of a status accomplishment to reach a broader audience (y'know, one with lots of hot babes - that's why guys seek status, after all). &nbsp;(This is a downside to LW being a sausage-fest - less incentive for men to status-seek through community-valued accomplishments if it won't get them chicks.)</p>\n<p>This issue reminds me of our political system, which rewards people for believably promising to achieve great things rather than for accomplishing them. &nbsp;After all, which gets a Congressman more status in our society - the title of \"Senator\", or their voting record and the impact of the bills they helped craft and pass? &nbsp;Talk about image over impact!</p>\n<p>Anyway, your thoughts on motivation, identity, public commitment, and publicity are welcomed.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "xexCWMyds6QLWognu": 1, "2EFq8dJbxKNzforjM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6ocujnKZL38thXn62", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 50, "baseScore": 54, "extendedScore": null, "score": 9.3e-05, "legacy": true, "legacyId": "1268", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 54, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XqmjdBKa4ZaXJtNmf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-29T11:25:47.886Z", "modifiedAt": null, "url": null, "title": "A social norm against unjustified opinions?", "slug": "a-social-norm-against-unjustified-opinions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:49.637Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PZyJSdmRpPbamv5G2/a-social-norm-against-unjustified-opinions", "pageUrlRelative": "/posts/PZyJSdmRpPbamv5G2/a-social-norm-against-unjustified-opinions", "linkUrl": "https://www.lesswrong.com/posts/PZyJSdmRpPbamv5G2/a-social-norm-against-unjustified-opinions", "postedAtFormatted": "Friday, May 29th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20social%20norm%20against%20unjustified%20opinions%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20social%20norm%20against%20unjustified%20opinions%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPZyJSdmRpPbamv5G2%2Fa-social-norm-against-unjustified-opinions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20social%20norm%20against%20unjustified%20opinions%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPZyJSdmRpPbamv5G2%2Fa-social-norm-against-unjustified-opinions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPZyJSdmRpPbamv5G2%2Fa-social-norm-against-unjustified-opinions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 372, "htmlBody": "<p>A currently existing social norm basically says that everyone has the right to an opinion on anything, no matter how little they happen to know about the subject.<br /><br />But what if we had a social norm saying that by default, people do not have the right to an opinion on anything? To earn such a right, they ought to have familiarized themselves on the topic. The familiarization wouldn't necessarily have to be anything very deep, but on the topic of e.g. controversial political issues, they'd have to have read at least a few books' worth of material discussing the question (preferrably material from both sides of the political fence). In scientific questions where one needed more advanced knowledge, you ought to at least have studied the field somewhat. Extensive personal experience on a subject would also be a way to become qualified, even if you hadn't studied the issue academically.<br /><br />The purpose of this would be to enforce <a href=\"http://wiki.lesswrong.com/wiki/Epistemic_hygiene\">epistemic hygiene</a>. Conversations on things such as public policy are frequently overwhelmed by loud declarations of opinion from people who, quite honestly, don't know anything on the subject they have a strong opinion on. If we had in place a social norm demanding an adequate amount of background knowledge on the topic before anyone voiced an opinion they expected to be taken seriously, the signal/noise ratio might be somewhat improved. This kind of a social norm does seem to already be somewhat in place in many scientific communities, but it'd do good to spread it to the general public.<br /><br />At the same time, there are several caveats. As I am myself a strong advocate on freedom of speech, I find it important to note that this must remain a *social* norm, not a government-advocated one or anything that is in any way codified into law. Also, the standards must not be set *too* high - even amateurs should be able to engage in the conversation, provided that they know at least the basics. Likewise, one must be careful that the principle isn't abused, with \"you don't have a right to have an opinion on this\" being a generic argument used to dismiss any opposing claims.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"gHCNhqxuJq2bZ2akb": 1, "wzgcQCrwKfETcBpR9": 1, "MXcpQvaPGtXpB6vkM": 1, "Wj2iipHgLnHA8ZhZH": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PZyJSdmRpPbamv5G2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 16, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "1270", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 161, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-29T17:31:52.268Z", "modifiedAt": null, "url": null, "title": "Taking Occam Seriously", "slug": "taking-occam-seriously", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:39.648Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "steven0461", "createdAt": "2009-02-27T16:16:38.980Z", "isAdmin": false, "displayName": "steven0461"}, "userId": "cn4SiEmqWbu7K9em5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vN9xBhv2YcpzdE6ot/taking-occam-seriously", "pageUrlRelative": "/posts/vN9xBhv2YcpzdE6ot/taking-occam-seriously", "linkUrl": "https://www.lesswrong.com/posts/vN9xBhv2YcpzdE6ot/taking-occam-seriously", "postedAtFormatted": "Friday, May 29th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Taking%20Occam%20Seriously&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATaking%20Occam%20Seriously%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvN9xBhv2YcpzdE6ot%2Ftaking-occam-seriously%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Taking%20Occam%20Seriously%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvN9xBhv2YcpzdE6ot%2Ftaking-occam-seriously", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvN9xBhv2YcpzdE6ot%2Ftaking-occam-seriously", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<p><a href=\"http://www.paul-almond.com\">Paul Almond's site</a> has many philosophically deep articles on theoretical rationality along LessWrongish assumptions, including but not limited to some <a href=\"http://www.paul-almond.com/Supernatural.htm\">great</a> <a href=\"http://www.paul-almond.com/TheDiminishedGodRefutation.htm\">atheology</a>, an <a href=\"http://www.paul-almond.com/WhatIsALowLevelLanguage.htm\">attempt to solve the problem of arbitrary UTM choice</a>, a possible <a href=\"http://www.paul-almond.com/WhyIsSpace3D.htm\">anthropic explanation why space is 3D</a>, a thorough defense of Occam's Razor, a lot of AI theory that I haven't tried to understand, and an <a href=\"http://www.paul-almond.com/Substrate3.htm\">attempt to explain what it means for minds to be implemented</a> (related in approach to <a href=\"http://www.nickbostrom.com/papers/experience.pdf\">this</a> and <a href=\"http://www.udassa.com\">this</a>).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hQiuNkBhn6xxcedTD": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vN9xBhv2YcpzdE6ot", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 28, "baseScore": 32, "extendedScore": null, "score": 5.8e-05, "legacy": true, "legacyId": "1272", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-05-30T17:33:15.038Z", "modifiedAt": null, "url": null, "title": "The Onion Goes Inside The Biased Mind", "slug": "the-onion-goes-inside-the-biased-mind", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:48.456Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "patrissimo", "createdAt": "2009-03-01T21:01:34.487Z", "isAdmin": false, "displayName": "patrissimo"}, "userId": "jimxrRCsNY7PfcM6e", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PwxyrX6bFDBMApk7h/the-onion-goes-inside-the-biased-mind", "pageUrlRelative": "/posts/PwxyrX6bFDBMApk7h/the-onion-goes-inside-the-biased-mind", "linkUrl": "https://www.lesswrong.com/posts/PwxyrX6bFDBMApk7h/the-onion-goes-inside-the-biased-mind", "postedAtFormatted": "Saturday, May 30th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Onion%20Goes%20Inside%20The%20Biased%20Mind&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Onion%20Goes%20Inside%20The%20Biased%20Mind%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPwxyrX6bFDBMApk7h%2Fthe-onion-goes-inside-the-biased-mind%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Onion%20Goes%20Inside%20The%20Biased%20Mind%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPwxyrX6bFDBMApk7h%2Fthe-onion-goes-inside-the-biased-mind", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPwxyrX6bFDBMApk7h%2Fthe-onion-goes-inside-the-biased-mind", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 24, "htmlBody": "<p>A great piece from The Onion, inside the mind of someone arguing emotionally:&nbsp; <a href=\"http://www.theonion.com/content/opinion/oh_no_its_making_well_reasoned\">Oh, No! It's Making Well-Reasoned Arguments Backed With Facts! Run!</a></p>\n<p>Pure genius.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"wzgcQCrwKfETcBpR9": 1, "dJ6eJxJrCEget7Wb6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PwxyrX6bFDBMApk7h", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 8, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "1275", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-01T16:10:30.632Z", "modifiedAt": null, "url": null, "title": "The Frontal Syndrome", "slug": "the-frontal-syndrome", "viewCount": null, "lastCommentedAt": "2017-06-17T03:55:52.458Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Annoyance", "createdAt": "2009-02-28T04:06:40.600Z", "isAdmin": false, "displayName": "Annoyance"}, "userId": "yEm6LWatswJYGwBFq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jEeuGFRcXvEGqF8un/the-frontal-syndrome", "pageUrlRelative": "/posts/jEeuGFRcXvEGqF8un/the-frontal-syndrome", "linkUrl": "https://www.lesswrong.com/posts/jEeuGFRcXvEGqF8un/the-frontal-syndrome", "postedAtFormatted": "Monday, June 1st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Frontal%20Syndrome&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Frontal%20Syndrome%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjEeuGFRcXvEGqF8un%2Fthe-frontal-syndrome%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Frontal%20Syndrome%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjEeuGFRcXvEGqF8un%2Fthe-frontal-syndrome", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjEeuGFRcXvEGqF8un%2Fthe-frontal-syndrome", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 801, "htmlBody": "<p>Neuroscientists have a difficult time figuring out which parts of the brain are involved in different functions.&nbsp; Naturally-occurring lesions to the brain are rarely specific to a particular anatomical region, the complications involved with the injury and treatment act as a smokescreen, and finding a patient who's damaged the particular spot you want to learn about is frustrating at best and nigh-impossible at worst.</p>\n<p>Fortunately for researchers, inappropriate surgical interventions of the past can shed light on neurological questions.</p>\n<p>The strange and horrifying history of psychosurgery is a topic beyond the scope of this site, and certainly beyond this post.&nbsp; Interested readers can easily find a great wealth of relevant discussion on the Net and in libraries, even (in more extensive collections) works written by the physicians involved in such surgeries during the era in which they were popular.&nbsp; Even a casually-curious individual can find lots of non-technical analysis and history to read - for such people, I particularly recommend <strong>Great and Desperate Cures</strong> by Elliot Valenstein.</p>\n<p>Of especial relevance is the prefrontal leukotomy, more commonly (if somewhat imprecisely) known as the lobotomy.&nbsp; There are several features in particular that are of interest to people interested in the nature of effective thought:<a id=\"more\"></a></p>\n<p>To begin, people with frontal lobe damage have problems with impulse control.&nbsp; And by 'problems', I mean they're pretty much incapable of it.&nbsp; It would be more precise to say that lobotomized patients display a remarkable degree of rigid, stereotyped behavior patterns.&nbsp; Give one patient a broom, and she'll begin to sweep the floor; show another a room with a bed, and he'll lie down on it.&nbsp; And do the same thing every time the stimulus is presented.&nbsp; The precise response varies from person to person, but the general reaction is consistent and replicable.&nbsp; Whatever the strongest association with the stimuli is in their mind, that's what they do when they encounter it - and every time they encounter it.</p>\n<p>For this reason, it was at one time suggested that only patients with a reputation for rigorously moral behavior be lobotomized, because people who would characteristically break social mores would do so ostentatiously after the surgery.&nbsp; Shoplifters and petty thieves who might have tried to steal particular kinds of things before they were lobotomized would immediately try to do so when they came across those things again - regardless of whether it was a good opportunity or even whether others were clearly watching.&nbsp; Restraining such behavior, or even limiting it, was simply impossible.</p>\n<p>Furthermore, such people don't get bored.&nbsp; Present them with a simple task, and they'll carry it out... and keep doing so, even if the consequences become absurd.&nbsp; Set them to building a picket fence and forget to check up on them, and they'll build it past your property line and down the street if given enough time.&nbsp; Set them to washing dishes, and they'll keep washing - to the point of redoing the job several times over.&nbsp; The ability to interrupt the sequence of behavior, to put a \"Stop\" order in the chain of macros built up, no longer existed once the connections between the frontal lobes and the rest of the brain had been severed.</p>\n<p>Motivation becomes almost non-existent.&nbsp; Left to themselves, lobotomized people often do not initiate action, or they do not begin to act in ways other than patterns they incorporated before.&nbsp; They repeat things they did before, but mindlessly and without variation, and cannot adapt if the pattern is disrupted.&nbsp; More alarmingly, the associations between concepts and basic responses are destroyed, to the point where sensations like pain are noted but not perceived as important, and actions to diminish or avoid the pain are not taken.&nbsp; One well-known case ended when, after having been released to her home, a woman was scalded to death because she didn't leave a bath of too-hot water she'd drawn.</p>\n<p>Learning in any abstract sense ceases.&nbsp; Teaching the lobotomized new responses is virtually impossible.&nbsp; And even basic conditioning, such as that is used with dogs to train them, becomes problematic due to lack of avoidance of pain and seeking of pleasure.</p>\n<p>These points are only part of a general overview - the details are far, far worse.</p>\n<p>There's one point which I have yet to discuss, and yet in the context of the information above, is the most shocking.&nbsp; <strong>Lobotomization did not disrupt the IQ of patients to any degree.&nbsp; </strong>This was actually one of the excuses made for why doctors didn't realize the utterly destructive effects of the surgery earlier.&nbsp; If it didn't impair IQ, surely it couldn't be grossly harmful, it was claimed.&nbsp; Well, it was.</p>\n<p>This sets the stage for an important question:&nbsp; If the lobotomy so profoundly levels the house of the mind, why don't IQ tests measure any of the mental aspects destroyed in the process?</p>\n<p>That is a subject for the next posts.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"gsv9XWbZDcnZmKuqM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jEeuGFRcXvEGqF8un", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 18, "extendedScore": null, "score": 4.981454575766404e-07, "legacy": true, "legacyId": "595", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-01T18:46:09.791Z", "modifiedAt": null, "url": null, "title": "Open Thread: June 2009", "slug": "open-thread-june-2009", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:14.512Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cyan", "createdAt": "2009-02-27T22:31:08.528Z", "isAdmin": false, "displayName": "Cyan"}, "userId": "eGtDNuhj58ehX9Wgf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EYJBHNPraHqW8dYya/open-thread-june-2009", "pageUrlRelative": "/posts/EYJBHNPraHqW8dYya/open-thread-june-2009", "linkUrl": "https://www.lesswrong.com/posts/EYJBHNPraHqW8dYya/open-thread-june-2009", "postedAtFormatted": "Monday, June 1st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20June%202009&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20June%202009%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEYJBHNPraHqW8dYya%2Fopen-thread-june-2009%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20June%202009%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEYJBHNPraHqW8dYya%2Fopen-thread-june-2009", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEYJBHNPraHqW8dYya%2Fopen-thread-june-2009", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 32, "htmlBody": "<p>I provide&nbsp;our monthly place to discuss Less Wrong topics that have not appeared in recent posts. Work your brain and gain prestige by doing so in E-prime (or not, as you please).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EYJBHNPraHqW8dYya", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 7, "extendedScore": null, "score": 4.981694436544809e-07, "legacy": true, "legacyId": "1279", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 145, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-02T01:10:06.409Z", "modifiedAt": null, "url": null, "title": "YouTube Test", "slug": "youtube-test", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wmoore", "createdAt": "2009-02-17T05:49:50.396Z", "isAdmin": false, "displayName": "wmoore"}, "userId": "EgQZcMBqxf6sGmKfi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DHd7g6r8ogjBgtQnB/youtube-test", "pageUrlRelative": "/posts/DHd7g6r8ogjBgtQnB/youtube-test", "linkUrl": "https://www.lesswrong.com/posts/DHd7g6r8ogjBgtQnB/youtube-test", "postedAtFormatted": "Tuesday, June 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20YouTube%20Test&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYouTube%20Test%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDHd7g6r8ogjBgtQnB%2Fyoutube-test%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=YouTube%20Test%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDHd7g6r8ogjBgtQnB%2Fyoutube-test", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDHd7g6r8ogjBgtQnB%2Fyoutube-test", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>\n<object width=\"425\" height=\"344\" data=\"http://www.youtube.com/v/arL_-tQndzI&amp;hl=en&amp;fs=1\" type=\"application/x-shockwave-flash\">\n<param name=\"allowFullScreen\" value=\"true\" />\n<param name=\"allowscriptaccess\" value=\"always\" />\n<param name=\"src\" value=\"http://www.youtube.com/v/arL_-tQndzI&amp;hl=en&amp;fs=1\" />\n<param name=\"allowfullscreen\" value=\"true\" />\n</object>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DHd7g6r8ogjBgtQnB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 0, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "1281", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-02T09:47:30.233Z", "modifiedAt": null, "url": null, "title": "Concrete vs Contextual values", "slug": "concrete-vs-contextual-values", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:50.035Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whpearson", "createdAt": "2009-02-28T00:34:00.976Z", "isAdmin": false, "displayName": "whpearson"}, "userId": "bq8qsRbPNvFihHxgi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TnMzZZivTaRZNbuiN/concrete-vs-contextual-values", "pageUrlRelative": "/posts/TnMzZZivTaRZNbuiN/concrete-vs-contextual-values", "linkUrl": "https://www.lesswrong.com/posts/TnMzZZivTaRZNbuiN/concrete-vs-contextual-values", "postedAtFormatted": "Tuesday, June 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Concrete%20vs%20Contextual%20values&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConcrete%20vs%20Contextual%20values%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTnMzZZivTaRZNbuiN%2Fconcrete-vs-contextual-values%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Concrete%20vs%20Contextual%20values%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTnMzZZivTaRZNbuiN%2Fconcrete-vs-contextual-values", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTnMzZZivTaRZNbuiN%2Fconcrete-vs-contextual-values", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 818, "htmlBody": "<p>The concept of <a href=\"/lw/we/recursive_selfimprovement/\">recursive self-improvement&nbsp; </a>is not an accepted idea outside of the futurist community. It just does not seem right in some fashion to some people. I am one of those people, so I'm going to try and explain the kind of instinctive skepticism I have towards it. It hinges on the difference between two sorts of values, whose difference I have not seen made explicit before (although likely it has somewhere). This difference is that of the between a concrete and contextual value.</p>\n<p><a id=\"more\"></a></p>\n<p>So lets run down the argument so I can pin down where it goes wrong in my view.</p>\n<ol>\n<li>There is a value called intelligence that roughly correlates with the ability to achieve goals in the world (if it does not then we don't care about intelligence explosions as they will have negligible impact on the real world<sup>TM</sup>)</li>\n<li>All things being equal a system with more compute power will be more capable than one with less (assuming they can get the requisite power supply). Similarly systems that have algorithms with better run time complexities will be more capable.</li>\n<li>Computers will be able to do things to increase the values in 2. Therefore they will form a feedback loop and become progressively more and more capable at an ever increasing rate.</li>\n</ol>\n<p>The point where I become unstuck is in the phrase \"all things being equal\". Especially what the \"all\" stands for. Let me run down a similar argument for wealth.</p>\n<ol>\n<li>There is a value called wealth that roughly correlates with the ability to acquire goods and services from other people. </li>\n<li>All things being equal a person with more money will be more wealthy than one with less. </li>\n<li>You are able to put your money in the bank and get compound interest on your money, so your wealth should be exponential in time (ignoring taxes).</li>\n</ol>\n<p>3 can be wrong in this, dependent upon the rate of interest and the rate of inflation. Because of inflation, each dollar you have in the future is less able to buy goods. That is the argument in 3 ignores that at different times and in different environments money is worth different amounts of goods.<a href=\"http://en.wikipedia.org/wiki/Hyperinflation\"> Hyper inflation</a> is a stark example of this. So the \"all things being equal\" references the current time and state of the world and 3 breaks that assumption by allowing time and the world to change.</p>\n<p>Why doesn't the argument work for wealth, but you can get stable recursive growth on neutrons in a reactor? It is because wealth is a contextual value, it depends on the world around you, as your money grows with compound interest the world changes it to make it less valuable without touching your money at all. Nothing can change the number of neutrons in your reactor without physically interacting with them or the reactor in some way. The neutron density value is concrete and containable, and you can do sensible maths with it.</p>\n<p>I'd argue that intelligence has a contextual nature as well. A simple example would be a computer chess tournament with a fixed algorithm that used as much resources as you threw at it. Say you manage to increase the resources for your team steadily by 10 MIPs per year, you will not win more chess games if another team is expanding their capabilities by 20 MIPs per year. That is despite an increase in raw computing ability it will not have an increase in achieving the goal of winning chess. Another possible example of the contextual nature of intelligence is the case where a systems ability to perform well in the world is affected by other people knowing its source code, and using it to predict and counter its moves.</p>\n<p>From the view of intelligence as a contextual value, current&nbsp; discussion of recursive self-improvement seems overly simplistic. We need to make explicit the important things in the world that intelligence might depend upon and then see if we can model the processes such that we still get FOOMs.</p>\n<p>Edit: Another example of the an intelligences effectiveness being contextual is the role of knowledge in performing tasks. Knowledge can have a expiration date after which it becomes less useful. Consider knowledge about the current english idioms usefulness for writing convincing essays, or the current bacterial population when trying to develop nano-machines to fight them.&nbsp; So you might have an atomically identical intelligence whose effectiveness varies dependent upon the freshness of the knowledge. So there might be conflicts between expending resources on improving processing power or algorithms and keeping knowledge fresh, when trying to shape the future. It is possible, but unlikely, that an untruth you believe will become true in time (say your estimate for the population of a city was too low but its growth took it to your belief), but as there are more ways to be wrong than right, knowledge is likely to degrade with time.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ac84EpK6mZbPLzmqj": 1, "5f5c37ee1b5cdee568cfb2b5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TnMzZZivTaRZNbuiN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -1, "extendedScore": null, "score": -4e-06, "legacy": true, "legacyId": "1277", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JBadX7rwdcRFzGuju"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-02T13:19:04.275Z", "modifiedAt": null, "url": null, "title": "Bioconservative and biomoderate singularitarian positions", "slug": "bioconservative-and-biomoderate-singularitarian-positions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:51.433Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/n9nwWWgxnr2knmfNi/bioconservative-and-biomoderate-singularitarian-positions", "pageUrlRelative": "/posts/n9nwWWgxnr2knmfNi/bioconservative-and-biomoderate-singularitarian-positions", "linkUrl": "https://www.lesswrong.com/posts/n9nwWWgxnr2knmfNi/bioconservative-and-biomoderate-singularitarian-positions", "postedAtFormatted": "Tuesday, June 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bioconservative%20and%20biomoderate%20singularitarian%20positions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABioconservative%20and%20biomoderate%20singularitarian%20positions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn9nwWWgxnr2knmfNi%2Fbioconservative-and-biomoderate-singularitarian-positions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bioconservative%20and%20biomoderate%20singularitarian%20positions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn9nwWWgxnr2knmfNi%2Fbioconservative-and-biomoderate-singularitarian-positions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn9nwWWgxnr2knmfNi%2Fbioconservative-and-biomoderate-singularitarian-positions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1009, "htmlBody": "<p>Let us define a singularitarian as a person who considers it likely that some form of smarter than human intelligence will be developed in a characteristic timeframe of a century, and that the manner in which this event occurs is important enough to expend effort altering. Given this definition, it is perfectly possible to be a <a href=\"http://en.wikipedia.org/wiki/Techno-progressivism#Contrasting_stance\">bioconservative</a> singularitarian - that&nbsp; is someone who:</p>\n<blockquote>\n<p><em>opposes genetic modification of <span class=\"mw-redirect\">food crops</span>, the cloning and genetic engineering of livestock and <span class=\"mw-redirect\">pets</span>, and, most prominently, rejects the genetic, prosthetic, and cognitive modification of human beings to overcome what are broadly perceived as current human biological and cultural limitations.<a id=\"more\"></a></em></p>\n</blockquote>\n<p><span>&nbsp;- one can accept the (at present only suggestive) factual arguments of Hanson, Yudkowsky, Bostrom etc that smarter than human intelligence is the only long-term alternative to human extinction (this is what one might call an \"attractor\" argument - that our current state simply isn't stable), whilst taking the axiological and ethical position that our pristine, unenhanced human form is to be held as if it were sacred, and that any modification and/or enhancement of the human form is to be resisted, even if the particular human in question wants to be enhanced. A slighly more individual-freedoms-oriented bioconservative position would be try very hard to persuade people (subject to certain constraints) to decide not to enhance themselves, or to allow people to enhance themselves only if they are prepared to face derision and criticism from society. A superintelligent singleton could easily implement such a society. <br /></span></p>\n<p><span>This position seems internally consistent to me, and given the seemingly unstoppable march of technological advancement and its rapid integration into our society (smartphones, facebook, online dating, youtube, etc) via corporate and economic pressure, </span>bioconservative singularitarianism may become the only realistic bioconservative position.</p>\n<p>One can even paint a fairly idyllic bioconservative world where human enhancement is impossible and people don't interact with advanced technology any more, they live in some kind of rural or hunter-gatherer world where the majority of suffering and disease (apart from death, perhaps) is eliminated by a superintelligent singleton, and the singleton takes care to ensure that this world is not \"disturbed\" by too much technology being invented by anyone. Perhaps people live in a way that is rather like one would have found on a Tahiti before Europeans got there. There are plenty of people who think that they already live in such a world - they are called theists, and they are mistaken (more about this in another post).</p>\n<p>For those with a taste for a little more freedom and a light touch of enhancement, we can define <em>biomoderate singularitarianism</em>, which differs from the above in that it sits somewhere more towards the \"risque\" end of the human enhancement spectrum, but it isn't quite <a href=\"http://en.wikipedia.org/wiki/Transhumanism\">transhumanism</a>. As before, we consider a superintelligent singleton running the practical aspects of a society and most of the people in that society being somehow encouraged or persuaded not to enhance themselves too much, so that the society remains a clearly human one. I would consider <a href=\"http://en.wikipedia.org/wiki/The_Culture\">Banks' </a><em><a href=\"http://en.wikipedia.org/wiki/The_Culture\">Culture</a> </em>to be the prototypical early result of a biomoderate singularity, followed by such incremental changes as one might expect due to what Yudkowsky calls \"heaven of the tired peasant\" syndrome - many people would get bored of \"low-grade\" fun after a while. Note that in the <em>Culture</em>, Banks describes people with significant emotional enhancements and the ability to change gender - so this certainly isn't bioconservative, but the <em>fundaments of human existence are not being pulled apart</em> by such radical developments as mind merging, uploading, wireheading or super-fast radical cognitive enhancement.</p>\n<p>Bioconservative and biomoderate singularities are compatible with modern environmentalism, in that the power of a superintelligent AI could be used to eliminate damage to the natural world, and humans could live in almost perfect harmony with nature. Harmony with nature would involve a superintelligence carefully managing biological ecosystems and even controlling the actions of individual animals, plants and microorganisms, as well as informing and guiding the actions of human societie(s) so that no human was ever seriously harmed by any creature (no-one gets infected by parasites, bacteria or viruses (unless they want to be), no-one is killed by wild animals), and no natural ecosystem is seriously harmed by human activity. A variant on this would have all wild animals becoming tame, so that you could stroll through the forest and pet a wildcat.</p>\n<p>A biomoderate singularity is an interesting concept to consider, and I think it has some interesting applications to a Freindly AI strategy. It is also, I feel, something that I think will be somewhat easier to sell to most other humans around than a full-on, shock level 4, radical transhumanist singularity. In fact we can frame the concept of a \"biomoderate technological singularity\" in fairly normal language: it is simply a very carefully designed self-improving computer system that is used to eliminate the need for humans to do work that they don't (all things considered) want to do.</p>\n<p>One might well ask: what does this post have to do with instrumental rationality? Well, due to various historical co-incidences, the same small group of people who popularized technologically enabled bio-radical stances such as gender swapping, uploading, cryopreservation, etc also happen to be the people who popularized ideas about smarter than human intelligence. When one small, outspoken group proposes two ideas which sound kind of similar, the rest of the world is highly likely to conflate them.</p>\n<p>The situation on the ground is that one of these ideas has a viable politico-cultural future, and the other one doesn't: \"bioradical\" human modification activates so many \"yuck\" factors that getting it to fly with educated, secular people is nigh-on impossible, never mind the religious lot. The notion that smarter-than-human intelligence will likely be developed, and that we should try to avoid getting recycled as computronium is a stretch, but at least it involves only nonobvious factual claims and obvious ethical claims.</p>\n<p>It is thus an important rationalist task to separate out these two ideas and make it clear to people that singularitarianism doesn't imply bioradicalism.</p>\n<p>See Also: <a href=\"/lw/x8/amputation_of_destiny/#more-16787\">Amputation of Destiny</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"wzgcQCrwKfETcBpR9": 1, "pGqRLe9bFDX2G2kXY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "n9nwWWgxnr2knmfNi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 13, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "589", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-02T18:49:25.107Z", "modifiedAt": null, "url": null, "title": "Would You Slap Your Father?  Article Linkage and Discussion", "slug": "would-you-slap-your-father-article-linkage-and-discussion", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:50.110Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Annoyance", "createdAt": "2009-02-28T04:06:40.600Z", "isAdmin": false, "displayName": "Annoyance"}, "userId": "yEm6LWatswJYGwBFq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8xXLpN4okPAgymfex/would-you-slap-your-father-article-linkage-and-discussion", "pageUrlRelative": "/posts/8xXLpN4okPAgymfex/would-you-slap-your-father-article-linkage-and-discussion", "linkUrl": "https://www.lesswrong.com/posts/8xXLpN4okPAgymfex/would-you-slap-your-father-article-linkage-and-discussion", "postedAtFormatted": "Tuesday, June 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Would%20You%20Slap%20Your%20Father%3F%20%20Article%20Linkage%20and%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWould%20You%20Slap%20Your%20Father%3F%20%20Article%20Linkage%20and%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8xXLpN4okPAgymfex%2Fwould-you-slap-your-father-article-linkage-and-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Would%20You%20Slap%20Your%20Father%3F%20%20Article%20Linkage%20and%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8xXLpN4okPAgymfex%2Fwould-you-slap-your-father-article-linkage-and-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8xXLpN4okPAgymfex%2Fwould-you-slap-your-father-article-linkage-and-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 163, "htmlBody": "<p>I said that my next post would discuss why IQ tests don't measure frontal executive functions, but I've found something tangential yet extremely topical which I think should be discussed first.</p>\n<p>A reader sent me a link to this Opinion column written by New York Times writer Nicholas D. Kristof:&nbsp; <a href=\"http://www.nytimes.com/2009/05/28/opinion/28kristof.html?_r=1\">Would You Slap Your Father?&nbsp; If So, You're A Liberal.</a></p>\n<p>The title is clearly meant to grab attention; don't let its provocative nature dissuade you from reading the article.&nbsp; Most of it is remarkably free from partisan bias, although there are one or two bits which are objectionable.&nbsp; Far more important is that it addresses the relationships between 'emotional' reactions, political positions and affiliations, and reason.</p>\n<p>It's a short article, brief enough that I don't think I need to sum it up, and of sufficient quality that I can recommend that you peruse it yourself with a clear conscience.&nbsp; Take the two or three minutes required to read it, please, and then comment your thoughts below.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FkzScn5byCs9PxGsA": 1, "LDTSbmXtokYAsEq8e": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8xXLpN4okPAgymfex", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 2, "extendedScore": null, "score": 7e-06, "legacy": true, "legacyId": "1283", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-03T03:20:53.548Z", "modifiedAt": null, "url": null, "title": "With whom shall I diavlog?", "slug": "with-whom-shall-i-diavlog", "viewCount": null, "lastCommentedAt": "2018-07-27T08:35:31.630Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qi5Xg3stgMwvdYKQk/with-whom-shall-i-diavlog", "pageUrlRelative": "/posts/qi5Xg3stgMwvdYKQk/with-whom-shall-i-diavlog", "linkUrl": "https://www.lesswrong.com/posts/qi5Xg3stgMwvdYKQk/with-whom-shall-i-diavlog", "postedAtFormatted": "Wednesday, June 3rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20With%20whom%20shall%20I%20diavlog%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWith%20whom%20shall%20I%20diavlog%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqi5Xg3stgMwvdYKQk%2Fwith-whom-shall-i-diavlog%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=With%20whom%20shall%20I%20diavlog%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqi5Xg3stgMwvdYKQk%2Fwith-whom-shall-i-diavlog", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqi5Xg3stgMwvdYKQk%2Fwith-whom-shall-i-diavlog", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<p><a href=\"http://bloggingheads.tv/\">Bloggingheads.tv</a> can't exactly call up, say, the President of France and get him to do a diavlog, but they have some street cred with mid-rank celebrities and academics.&nbsp; With that in mind, how would you fill in this blank?</p>\n<p>\"I would really love to see a diavlog between Yudkowsky and ____________.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qi5Xg3stgMwvdYKQk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 11, "extendedScore": null, "score": 4.984688873392979e-07, "legacy": true, "legacyId": "1284", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 163, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-03T23:05:25.181Z", "modifiedAt": null, "url": null, "title": "Mate selection for the men here", "slug": "mate-selection-for-the-men-here", "viewCount": null, "lastCommentedAt": "2013-11-22T18:48:17.380Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rhollerith", "createdAt": "2009-02-28T03:14:34.208Z", "isAdmin": false, "displayName": "rhollerith"}, "userId": "XKhSp4huG9kwcLQLc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PbbcaiMgHSDmbqs6A/mate-selection-for-the-men-here", "pageUrlRelative": "/posts/PbbcaiMgHSDmbqs6A/mate-selection-for-the-men-here", "linkUrl": "https://www.lesswrong.com/posts/PbbcaiMgHSDmbqs6A/mate-selection-for-the-men-here", "postedAtFormatted": "Wednesday, June 3rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mate%20selection%20for%20the%20men%20here&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMate%20selection%20for%20the%20men%20here%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPbbcaiMgHSDmbqs6A%2Fmate-selection-for-the-men-here%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mate%20selection%20for%20the%20men%20here%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPbbcaiMgHSDmbqs6A%2Fmate-selection-for-the-men-here", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPbbcaiMgHSDmbqs6A%2Fmate-selection-for-the-men-here", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1027, "htmlBody": "<p>The following started as a reply to a request for relationship advice (http://lesswrong.com/lw/zj/open_thread_june_2009/rxy) but is expected to be of enough general interest to justify a top-level post.&nbsp; Sometimes it is beneficial to have older men in the conversation, and this might be one of those times.&nbsp; (I am in my late 40s.)</p>\n<p>I am pretty sure that most straight men strong in rationality are better off learning how the typical woman thinks than holding out for a long-term relationship with a women as strong in rationality as he is. If you hold out for a strong female rationalist, you drastically shrink the pool of women you have to choose from -- and people with a lot of experience with dating and relationships tend to consider that a bad move.&nbsp; A useful data point here is the fact (http://lesswrong.com/lw/fk/survey_results/cee) that 95%-97% of Less Wrongers are male.&nbsp; If on the other hand, women currently (*currently* -- not in some extrapolated future after you've sold your company and bought a big house in Woodside) find you extremely attractive or extremely desirable long-term-relationship material, well, then maybe you should hold out for a strong female rationalist if you are a strong male rationalist.</p>\n<p>Here is some personal experience in support of the advice above to help you decide whether to follow the advice above.<br /><br />My information is incomplete because I have never been in a long-term relationship with a really strong rationalist -- or even a scientist, programmer or engineer -- but I have been with a woman who has years of formal education in science (majored in anthropology, later took chem and bio for a nursing credential) and her knowledge of science did not contribute to the relationship in any way that I could tell.&nbsp; Moreover, that relationship was not any better than the one I am in now, with a woman with no college-level science classes at all.<br /><br />The woman I have been with for the last 5 years is not particularly knowledgeable about science and is not particularly skilled in the art of rationality.&nbsp; Although she is curious about most areas of science, she tends to give up and to stop paying attention if a scientific explanation fails to satisfy her curiosity within 2 or 3 minutes.&nbsp; If there is a strong emotion driving her inquiry, though, she will focus longer.&nbsp; E.g., she sat still for at least 15 or 20 minutes on the evolutionary biology of zoonoses during the height of the public concern over swine flu about a month ago -- and was glad she did.&nbsp; (I know she was glad she did because she thanked me for the explanation, and it is not like her to make an insincere expression of gratitude out of, e.g., politeness.)&nbsp; (The strong emotion driving her inquiry was her fear of swine flu combined with her suspicion that perhaps the authorities were minimizing the severity of the situation to avoid panicking the public.)<br /><br />Despite her having so much less knowledge of science and the art of rationality than I have, I consider my current relationship a resounding success: it is no exaggeration to say that I am more likely than not vastly better off than I would have been if I had chosen 5 years ago not to pursue this woman to hold out for someone more rational.&nbsp; She is rational enough to take care of herself and to be the most caring and the most helpful girlfriend I have ever had.&nbsp; (Moreover, nothing in my ordinary conversations and interactions with her draw my attention to her relative lack of scientific knowledge or her relative lack of advanced rationalist skills in a way that evokes any regret or sadness in me.&nbsp; Of course, if I had experienced a long-term relationship with a very strong female rationalist in the past, maybe I *would* experience episodes of regret or sadness towards the woman I am with now.)<br /><br />Here are two more tips on mate selection for the straight men around here.<br /><br />I have found that it is a very good sign if the woman either (1) assigns high social status to scientific ability or scientific achievement or finds scientific ability appealing in a man or (2) sees science as a positive force in the world.&nbsp; The woman I am with now clearly and decisively meets criterion (1) but does not meet criterion (2).&nbsp; Moreover, one of my most successful relationships was with a woman who finds science fiction very inspiring.&nbsp; (I do not BTW.)&nbsp; The salient thing about that was that she never revealed it to me, nor the fact that she definitely sees science as a positive force in the world.&nbsp; (I pieced those two facts together after we broke up.)&nbsp; The probable reason she never revealed them to me is that she thought they would clue me in to the fact that she found scientific ability appealing in a man, which in turn would have increased the probability that I would try to snow her by pretending to be better at science or more interested in science than I really was.&nbsp; (She'd probably been snowed that way by a man before she met me: male snowing of prospective female sexual partners is common.)</p>\n<p>By posting on a topic of such direct consequence to normal straight adult male self-esteem, I am making myself more vulnerable than I would be if I were posting on, e.g., regulatory policy.&nbsp; Awareness of my vulnerability might cause someone to refrain from publicly contradicting what I just wrote.&nbsp; Do not refrain from publicly contradicting what I just wrote!&nbsp; The successful application of rationality and scientific knowledge to this domain has high expected global utility, and after considering the emotional and reputational risks to myself of having posted on this topic, I have concluded that I do not require any special consideration over and above what I would get if I had posted on regulatory policy.</p>\n<p>And of course if you have advice to give about mate selection for the straight men around here, here is your chance.</p>\n<p>(EDITED to avoid implying that all men are heterosexual.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"mip7tdAN87Jarkcew": 1, "W9aNkPwtPhMrcfgj7": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PbbcaiMgHSDmbqs6A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 13, "extendedScore": null, "score": 4.986503436053245e-07, "legacy": true, "legacyId": "1289", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 110, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-04T03:19:03.981Z", "modifiedAt": null, "url": null, "title": "Third London Rationalist Meeting", "slug": "third-london-rationalist-meeting", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:49.936Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xq9HYyFXkzAREM7at/third-london-rationalist-meeting", "pageUrlRelative": "/posts/xq9HYyFXkzAREM7at/third-london-rationalist-meeting", "linkUrl": "https://www.lesswrong.com/posts/xq9HYyFXkzAREM7at/third-london-rationalist-meeting", "postedAtFormatted": "Thursday, June 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Third%20London%20Rationalist%20Meeting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThird%20London%20Rationalist%20Meeting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxq9HYyFXkzAREM7at%2Fthird-london-rationalist-meeting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Third%20London%20Rationalist%20Meeting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxq9HYyFXkzAREM7at%2Fthird-london-rationalist-meeting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxq9HYyFXkzAREM7at%2Fthird-london-rationalist-meeting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 88, "htmlBody": "<p>The Third London Rationalist Meeting will take place on Sunday, 2009-06-07, 14:00, at the usual location - cafe on top of Waterstones bookstore near Piccadilly Circus Tube Station.</p>\n<p>Here's <a href=\"http://maps.google.com/maps?f=q&amp;source=s_q&amp;hl=en&amp;geocode=&amp;q=Waterstones+Piccadilly&amp;sll=51.512562,-0.139518&amp;sspn=0.025613,0.024805&amp;ie=UTF8&amp;z=15\">map how to get to the venue</a>.</p>\n<p>There were some suggestions of trying alternative venue, but as nobody took the time to scout for alternative locations, I'd like to take the safe way and go for the usual one, even if it's less than optimal (by the way they have evaluation forms there, you might want to give them your feedback).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xq9HYyFXkzAREM7at", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 4.986894036110787e-07, "legacy": true, "legacyId": "1290", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-04T05:05:17.958Z", "modifiedAt": null, "url": null, "title": "Post Your Utility Function", "slug": "post-your-utility-function", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:06.981Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MvwdPfYLX866vazFJ/post-your-utility-function", "pageUrlRelative": "/posts/MvwdPfYLX866vazFJ/post-your-utility-function", "linkUrl": "https://www.lesswrong.com/posts/MvwdPfYLX866vazFJ/post-your-utility-function", "postedAtFormatted": "Thursday, June 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Post%20Your%20Utility%20Function&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APost%20Your%20Utility%20Function%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMvwdPfYLX866vazFJ%2Fpost-your-utility-function%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Post%20Your%20Utility%20Function%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMvwdPfYLX866vazFJ%2Fpost-your-utility-function", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMvwdPfYLX866vazFJ%2Fpost-your-utility-function", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 318, "htmlBody": "<p>A lot of rationalist thinking about ethics and economy assumes we have very well defined utility functions - knowing exactly our preferences between states and events, not only being able to compare them (I prefer X to Y), but assigning precise numbers to every combinations of them (p% chance of X equals q% chance of Y). Because everyone wants more money, you should theoretically even be able to assign exact numerical values to positive outcomes in your life.</p>\n<p>I did a small experiment of making a list of things I wanted, and giving them point value. I must say this experiment ended up in a failure - thinking \"If I had X, would I take Y instead\", and \"If I had Y, would I take X instead\" very often resulted in a pair of \"No\"s. Even thinking about multiple Xs/Ys for one Y/X usually led me to deciding they're really incomparable. Outcomes related to similar subject were relatively comparable, those in different areas in life were usually not.</p>\n<p>I finally decided on some vague numbers and evaluated the results two months later. My success on some fields was really big, on other fields not at all, and the only thing that was clear was that numbers I assigned were completely wrong.</p>\n<p>This leads me to two possible conclusions:</p>\n<ul>\n<li>I don't know how to draw utility functions, but they are a good model of my preferences, and I could learn how to do it.</li>\n<li>Utility functions are really bad match for human preferences, and one of the major premises we accept is wrong.</li>\n</ul>\n<p>Anybody else tried assigning numeric values to different outcomes outside very narrow subject matter? Have you succeeded and want to share some pointers? Or failed and want to share some thought on that?</p>\n<p>I understand that details of many utility functions will be highly personal, but if you can share your successful ones, that would be great.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1, "R6uagTfhhBeejGrrf": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MvwdPfYLX866vazFJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 32, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "1291", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 280, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-04T06:17:41.549Z", "modifiedAt": null, "url": null, "title": "Probability distributions and writing style", "slug": "probability-distributions-and-writing-style", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:51.188Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "dclayh", "createdAt": "2009-03-07T01:16:38.966Z", "isAdmin": false, "displayName": "dclayh"}, "userId": "E7xnxwP5EPuGiP99X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RrxGGAZLX9e4NBuW5/probability-distributions-and-writing-style", "pageUrlRelative": "/posts/RrxGGAZLX9e4NBuW5/probability-distributions-and-writing-style", "linkUrl": "https://www.lesswrong.com/posts/RrxGGAZLX9e4NBuW5/probability-distributions-and-writing-style", "postedAtFormatted": "Thursday, June 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Probability%20distributions%20and%20writing%20style&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProbability%20distributions%20and%20writing%20style%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrxGGAZLX9e4NBuW5%2Fprobability-distributions-and-writing-style%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Probability%20distributions%20and%20writing%20style%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrxGGAZLX9e4NBuW5%2Fprobability-distributions-and-writing-style", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrxGGAZLX9e4NBuW5%2Fprobability-distributions-and-writing-style", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 191, "htmlBody": "<p>In his recent <a href=\"/lw/zt/mate_selection_for_the_male_rationalist/\">post</a>, rhollerith wrote,</p>\n<blockquote>\n<p>I am more likely than not vastly better off than I would have been if &lt;I had made decision X&gt;</p>\n</blockquote>\n<p>This reminded me of the slogan for the water-filtration system my workplaces uses,</p>\n<blockquote>\n<p>We're 100% sure it's 99.9% pure!</p>\n</blockquote>\n<p>because both sentences make a claim and give an associated probability for it. Now in this second example, the actual version is better than the expectation-value-preserving \"We're 99.9% sure it's 100% pure\", because the actual version implies a lower variance in outcomes (and expectation values being equal, a lower variance is nearly always better).&nbsp; But this leads to the question of why rhollerith didn't write something like \"I am almost certainly at least somewhat better off than I would have been...\".&nbsp;</p>\n<p>So I ask: when writing nontechnically, do you prefer to give a modest conclusion with high confidence, or a strong conclusion with moderate confidence?&nbsp; And does this vary with whether you're trying to persuade or merely describe?</p>\n<p>(Also feel free to post other examples of this sort of statement from LW or elsewhere; I'd search for them myself if I had any good ideas on how to do so.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RrxGGAZLX9e4NBuW5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 6, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "1292", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PbbcaiMgHSDmbqs6A"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-04T15:31:10.649Z", "modifiedAt": null, "url": null, "title": "My concerns about the term 'rationalist'", "slug": "my-concerns-about-the-term-rationalist", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:57.531Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JamesCole", "createdAt": "2009-04-16T08:24:51.843Z", "isAdmin": false, "displayName": "JamesCole"}, "userId": "RMS473ETmWQLaCtmu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RtSLAaJhJr7b5vx38/my-concerns-about-the-term-rationalist", "pageUrlRelative": "/posts/RtSLAaJhJr7b5vx38/my-concerns-about-the-term-rationalist", "linkUrl": "https://www.lesswrong.com/posts/RtSLAaJhJr7b5vx38/my-concerns-about-the-term-rationalist", "postedAtFormatted": "Thursday, June 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20My%20concerns%20about%20the%20term%20'rationalist'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMy%20concerns%20about%20the%20term%20'rationalist'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRtSLAaJhJr7b5vx38%2Fmy-concerns-about-the-term-rationalist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=My%20concerns%20about%20the%20term%20'rationalist'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRtSLAaJhJr7b5vx38%2Fmy-concerns-about-the-term-rationalist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRtSLAaJhJr7b5vx38%2Fmy-concerns-about-the-term-rationalist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 491, "htmlBody": "<p>&nbsp;</p>\n<p>I've noticed that here on Less Wrong people often identify themselves as <strong>rationalists</strong>&nbsp;(and this community as a rationalist one -- searching for 'rationalist' on the site returns exactly 1000 hits). &nbsp;I'm a bit concerned that this label may work against our favour.</p>\n<p>Paul Graham recently wrote a nice essay <a href=\"http://paulgraham.com/identity.html\">Keep Your Identity Small</a> in which he argued that identifying yourself with a label tends to work against reasonable -- rational, you might say -- disscusions about topics that are related to it. &nbsp;The essay is quite short and if you haven't read it I highly reccommend doing so.</p>\n<p>If his argument is correct, then identifying with a label like Rationalist may impede your ability to <em>be</em> rational.</p>\n<p>My thinking is that once you identify yourself as an <em>X</em>, you have a&nbsp;tendancy to evaluate ideas and courses of action in terms of how similar or different they appear to your prototypical notion of that label - as a shortcut for genuinely thinking about them and instead of evaluating them on their own merits.&nbsp;</p>\n<p>Aside from the effect such a label may have on our own thinking, the term 'rationalist' may be bad PR. &nbsp;In the wider world 'rational' tends to be a bit of a dirty word. &nbsp;It has a lot of negative connotations. &nbsp;&nbsp;</p>\n<p>Outside communities like this one, presenting yourself a rationalist is likely to get other people off on the wrong foot. &nbsp;In many people's minds, it'd strike you out before you'd even said anything. &nbsp;It's a great way for them to pigeonhole you. &nbsp;</p>\n<p>And we <em>should</em> be interested in embracing the wider world and communicating our views to others.</p>\n<p>If I was to describe what we're about, I'd probably say something like that we're interested in knowing the truth, and want to avoid deluding ourselves about anything, as much as either of these things are possible. &nbsp;So we're studying how to be less wrong. &nbsp;I'm not sure I'd use any particular label in my description.</p>\n<p>Interestingly, those goals I described us in terms of -- wanting truth, wanting to avoid deluding ourselves -- are not really what separates \"us\" from \"them\". &nbsp;I think the actual difference is that we are <em>simply more aware of the fact that there are many ways our thinking can be wrong and lead us astray</em>. &nbsp;</p>\n<p>Many people really are -- or at least start out -- interested in the truth, but get led astray by flawed thinking because they're not aware that it is flawed. &nbsp;Because flawed thinking begets flawed beliefs, the process can lead people onto systematic paths away from truth seeking. &nbsp;But I don't think even those people set out in the first place to get away from the truth.</p>\n<p>The knowledge our community has, of ways that thinking can lead us astray, is an important thing we have to offer, and something that we should try to communicate to others. &nbsp;And I actually think a lot of people would be receptive to it, presented in the right way.&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"aa3Qg7Qrp9LM7QMaz": 1, "Ng8Gice9KNkncxqcj": 1, "BtQRRKTPxagBH6KrG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RtSLAaJhJr7b5vx38", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 12, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "1293", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-06T02:59:00.296Z", "modifiedAt": null, "url": null, "title": "Honesty: Beyond Internal Truth", "slug": "honesty-beyond-internal-truth", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:03.631Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pZSpbxPrftSndTdSf/honesty-beyond-internal-truth", "pageUrlRelative": "/posts/pZSpbxPrftSndTdSf/honesty-beyond-internal-truth", "linkUrl": "https://www.lesswrong.com/posts/pZSpbxPrftSndTdSf/honesty-beyond-internal-truth", "postedAtFormatted": "Saturday, June 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Honesty%3A%20Beyond%20Internal%20Truth&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHonesty%3A%20Beyond%20Internal%20Truth%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpZSpbxPrftSndTdSf%2Fhonesty-beyond-internal-truth%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Honesty%3A%20Beyond%20Internal%20Truth%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpZSpbxPrftSndTdSf%2Fhonesty-beyond-internal-truth", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpZSpbxPrftSndTdSf%2Fhonesty-beyond-internal-truth", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1204, "htmlBody": "<p>When I expect to meet new people who have no idea who I am, I often wear a button on my shirt that says:</p>\n<p style=\"padding-left: 30px;\">SPEAK THE TRUTH,<br />EVEN IF YOUR VOICE TREMBLES</p>\n<p>Honesty toward others, it seems to me, obviously bears some relation to rationality.&nbsp; In practice, the people I know who seem to make unusual efforts at rationality, are unusually honest, or, failing that, at least have unusually bad social skills.</p>\n<p>And yet it must be admitted and fully acknowledged, that such morals are encoded nowhere in probability theory.&nbsp; There is no theorem which proves a rationalist must be honest - must speak <em>aloud</em> their probability estimates.&nbsp; I have said little of honesty myself, these past two years; the art which I've presented has been more along the lines of:</p>\n<p style=\"padding-left: 30px;\">SPEAK THE TRUTH INTERNALLY,<br />EVEN IF YOUR BRAIN TREMBLES</p>\n<p>I do think I've conducted my life in such fashion, that I can wear the original button without shame.&nbsp; But I do not always say aloud all my thoughts.&nbsp; And in fact there are times when my tongue emits a lie.&nbsp; What I <em>write </em>is true to the best of my knowledge, because I can look it over and check before publishing.&nbsp; What I <em>say aloud</em> sometimes comes out false because my tongue moves faster than my deliberative intelligence can look it over and spot the distortion.&nbsp; Oh, we're not talking about grotesque major falsehoods - but the first words off my tongue sometimes shade reality, twist events just a little toward the way they <em>should</em> have happened...</p>\n<p>From the inside, it feels <em>a lot </em>like the experience of un-consciously-chosen, perceptual-speed, internal <a href=\"http://wiki.lesswrong.com/wiki/Rationalization\">rationalization</a>.&nbsp; I would even say that so far as I can tell, it's the same brain hardware running in both cases - that it's just a circuit for <em>lying in general,</em> both for <em>lying to others</em> and <em>lying to ourselves,</em> activated whenever reality begins to feel <em>inconvenient</em>.<a id=\"more\"></a></p>\n<p>There was a time - if I recall correctly - when I didn't notice these little twists.&nbsp; And in fact it still feels <em>embarrassing</em> to confess them, because I worry that people will think:&nbsp; \"Oh, no!&nbsp; Eliezer lies without even thinking!&nbsp; He's a pathological liar!\"&nbsp; For they have not yet noticed the phenomenon, and actually <em>believe</em> their own little improvements on reality - their own brain being twisted around the same way, remembering reality the way it <em>should</em> be (for the sake of the conversational convenience at hand).&nbsp; I am pretty damned sure that I lie no more pathologically than average; my pathology - my departure from evolutionarily adapted brain functioning - is that I've <em>noticed </em>the lies.</p>\n<p>The fact that I'm going ahead and telling you about this mortifying realization - that despite my own values, I literally <em>cannot</em> make my tongue speak only truth - is one reason why I am not embarrassed to wear yon button.&nbsp; I do think I meet the spirit well enough.</p>\n<p>It's the same \"liar circuitry\" that you're fighting, or indulging, in the internal or external case - that would be my second guess for why rational people tend to be honest people.&nbsp; (My first guess would be the obvious: <a href=\"/lw/sf/fake_norms_or_truth_vs_truth/\">respect for the truth</a>.)&nbsp; Sometimes the Eli who speaks aloud in real-time conversation, strikes me as almost a different person than the Eliezer Yudkowsky who types and edits.&nbsp; The latter, I think, is the better rationalist, just as he is more honest.&nbsp; (And if you asked me out loud, my tongue would say the same thing.&nbsp; I'm not <em>that</em> internally divided.&nbsp; I think.)</p>\n<p>But this notion - that external lies and internal lies are correlated by their underlying brainware - is not the only view that could be put forth, of the interaction between rationality and honesty.</p>\n<p>An alternative view - which I do not myself endorse, but which has been put forth forcefully to me - is that the nerd way is not the true way; and that a born nerd, who seeks to become even more rational, should <em>allow</em> themselves to lie, and give themselves safe occasions to <em>practice</em> lying, so that they are not <em>tempted</em> to twist around the truth internally - the theory being that if you give yourself permission to lie outright, you will no longer feel the <em>need</em> to distort internal belief.&nbsp; In this view the choice is between lying consciously and lying unconsciously, and a rationalist should choose the former.</p>\n<p>I wondered at this suggestion, and then I suddenly had a strange idea.&nbsp; And I asked the one, \"Have you been hurt in the past by telling the truth?\"&nbsp; \"Yes\", he said, or \"Of course\", or something like that -</p>\n<p style=\"padding-left: 30px;\">(- and my brain just flashed up a small sign noting how <em>convenient</em> it would be if he'd said \"Of course\" - how much more smoothly that sentence would flow - but in fact I don't remember exactly what he said; and if I'd been speaking out loud, I might have just said, \"'Of course', he said\" which flows well.&nbsp; This is the sort of thing I'm talking about, and if you don't think it's dangerous, you don't understand at all how hard it is to find truth on <em>real</em> problems, where a single tiny shading can derail a human train of thought entirely -)</p>\n<p>- and at this I suddenly realized, that <a href=\"http://wiki.lesswrong.com/wiki/Other-optimizing\">what worked for me, might not work for everyone</a>.&nbsp; I <em>haven't</em> suffered all that much from my project of speaking truth - though of course I don't know exactly how my life would have been otherwise, except that it would be utterly different.&nbsp; But I'm <em>good </em>with words.&nbsp; I'm a frickin' writer.&nbsp; If I need to soften a blow, I can do with careful phrasing what would otherwise take a lie.&nbsp; Not everyone scores an 800 on their verbal SAT, and I can see how that would make it a <em>lot</em> harder to speak truth.&nbsp; So when it comes to white lies, in particular, I claim no right to judge - and also it is not my primary goal to make the people around me happier.</p>\n<p>Another counterargument that I can see to the path I've chosen - let me quote Roger Zelazny:</p>\n<blockquote>\n<p>\"If you had a choice between the ability to detect falsehood and the ability to discover truth, which one would you take? There was a time when I thought they were different ways of saying the same thing, but I no longer believe that. Most of my relatives, for example, are almost as good at seeing through subterfuge as they are at perpetrating it. I&rsquo;m not at all sure, though, that they care much about truth. On the other hand, I&rsquo;d always felt there was something noble, special, and honorable about seeking truth... Had this made me a sucker for truth's opposite?\"</p>\n</blockquote>\n<p>If detecting falsehood and discovering truth are not the same skill in practice, then practicing honesty probably makes you better at discovering truth and worse at detecting falsehood.&nbsp; If I thought I was going to have to detect falsehoods - if that, not discovering a certain truth, were my one purpose in life - then I'd probably apprentice myself out to a con man.</p>\n<p>What, in your view, and in your experience, is the nature of the interaction between honesty and rationality?&nbsp; Between external truthtelling and internal truthseeking?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nANxo5C4sPG9HQHzr": 5}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pZSpbxPrftSndTdSf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 56, "baseScore": 63, "extendedScore": null, "score": 0.000101, "legacy": true, "legacyId": "1297", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 66, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 86, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zGJw9PGhu9e8Z6BEX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-06T04:35:51.786Z", "modifiedAt": null, "url": null, "title": "Macroeconomics, The Lucas Critique, Microfoundations, and Modeling in General", "slug": "macroeconomics-the-lucas-critique-microfoundations-and", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:50.714Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Matt_Simpson", "createdAt": "2009-03-05T21:06:45.432Z", "isAdmin": false, "displayName": "Matt_Simpson"}, "userId": "v4krJe8Qa4jnhPTmd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ye47ZQ3d8RhQSP8Fw/macroeconomics-the-lucas-critique-microfoundations-and", "pageUrlRelative": "/posts/Ye47ZQ3d8RhQSP8Fw/macroeconomics-the-lucas-critique-microfoundations-and", "linkUrl": "https://www.lesswrong.com/posts/Ye47ZQ3d8RhQSP8Fw/macroeconomics-the-lucas-critique-microfoundations-and", "postedAtFormatted": "Saturday, June 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Macroeconomics%2C%20The%20Lucas%20Critique%2C%20Microfoundations%2C%20and%20Modeling%20in%20General&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMacroeconomics%2C%20The%20Lucas%20Critique%2C%20Microfoundations%2C%20and%20Modeling%20in%20General%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYe47ZQ3d8RhQSP8Fw%2Fmacroeconomics-the-lucas-critique-microfoundations-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Macroeconomics%2C%20The%20Lucas%20Critique%2C%20Microfoundations%2C%20and%20Modeling%20in%20General%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYe47ZQ3d8RhQSP8Fw%2Fmacroeconomics-the-lucas-critique-microfoundations-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYe47ZQ3d8RhQSP8Fw%2Fmacroeconomics-the-lucas-critique-microfoundations-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 825, "htmlBody": "<p>I posted this comment in reply to a post by David Henderson over at econlog, but first some context.</p>\n<p>Mathew Yglesias <a href=\"http://yglesias.thinkprogress.org/archives/2009/06/who-needs-microfoundations.php\">writes</a>:</p>\n<blockquote>\n<p>...From an outside perspective, what seems to be going on is that economists have unearthed an extremely fruitful paradigm for investigation of micro issues. This has been good for them, and enhanced the prestige of the discipline. No such fruitful paradigm has actually emerged for investigation of macro issues. So the decision has been made to somewhat arbitrarily impose the view that macro models must be grounded in micro foundations. Thus, the productive progressive research program of microeconomics can &ldquo;infect&rdquo; the more troubled field of macro with its prestige...</p>\n<p>...But as a methodological matter, it seems deeply unsound. As a general principle for investigating the world, we normally deem it&nbsp;<em><span style=\"font-style: normal;\">desirable</span></em>, but not at all necessary, that researchers exploring a particular field of inquiry find ways to &ldquo;reduce&rdquo; what they&rsquo;re doing to a lower level....<a id=\"more\"></a></p>\n<p>...Trying to enhance models with better information about psychology isn&rsquo;t against the rules, but it&rsquo;s not required either. What&rsquo;s required is that the models do useful work.</p>\n<p>So why should it be that &ldquo;in the current regime, if [macro models] are not meticulously constructed from &ldquo;micro foundations,&rdquo; they aren&rsquo;t allowed to be considered&rdquo;?</p>\n</blockquote>\n<p>To which a commenter <a href=\"http://yglesias.thinkprogress.org/archives/2009/06/who-needs-microfoundations.php#comment-1609570\">replies</a>:</p>\n<blockquote>\n<p>While I&rsquo;m the first to acknowledge that the current macro research paradigm has given us little useful analysis, there&nbsp;<em>is</em>&nbsp;a standard answer to Matt&rsquo;s question.</p>\n<p>You can start by going to Wikipedia and reading about the&nbsp;<a style=\"outline-style: none; outline-width: initial; outline-color: initial; color: #000000; text-decoration: underline; \" rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Lucas_critique\">Lucas Critique</a>...</p>\n</blockquote>\n<p>I won't reproduce the whole thing, click through to the comment to see a decent summary of the Lucas Critique if you aren't aware of it already.</p>\n<p>Henderson, over at econlog, <a href=\"http://econlog.econlib.org/archives/2009/06/more_on_matt_yg.html\">replies</a>:</p>\n<blockquote>\n<p style=\"padding-top: 0.7em; margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 1em; \">...Second, the demand for microfoundations, or at least the supply of them, goes back more than 10 years before the date Arnold claims. It goes back at least to Milton Friedman's&nbsp;<a style=\"border-top-style: none; border-right-style: none; border-bottom-style: none; border-left-style: none; border-width: initial; border-color: initial; color: #4444be; font-weight: bold; text-decoration: none; \" href=\"http://www.amazon.com/Consumption-Function-National-Economic-Research/dp/0691138869/ref=sr_1_1?ie=UTF8&amp;s=books&amp;qid=1244220940&amp;sr=1-1\"><em>A Theory of the Consumption Function...</em></a>,</p>\n<p style=\"padding-top: 0.7em; margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 1em; \">...Third, interestingly, Milton Friedman himself would probably agree with Yglesias about the idea that there's not necessarily a need for micro foundations for macro. In a 1996 interview published in Brian Snowdon and Howard R. Vane,&nbsp;<a style=\"border-top-style: none; border-right-style: none; border-bottom-style: none; border-left-style: none; border-width: initial; border-color: initial; color: #4444be; font-weight: bold; text-decoration: none; \" href=\"http://www.amazon.com/Modern-Macroeconomics-Origins-Development-Current/dp/1845422082/ref=sr_1_1?ie=UTF8&amp;s=books&amp;qid=1244220996&amp;sr=1-1\"><em>Modern Macroeconomics</em></a>, Edward Elgar, 2005, Friedman said:</p>\n<blockquote style=\"margin-right: 0px; margin-left: 0px; margin-top: 1em; margin-bottom: 1.5em; padding-right: 1.5em; padding-left: 1.5em; padding-top: 0.2em; padding-bottom: 0.5em; background-color: #f2f2e3; border-top-width: 1px; border-top-style: solid; border-top-color: #dedec5; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #dedec5; \">It is less important for macroeconomic models to have choice-theoretic microfoundations than it is for them to have empirical implications that can be subjected to refutation.</blockquote>\n<p style=\"padding-top: 0.7em; margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 1em; \">In saying this, Friedman was going back to his positivist roots, which he laid out at length in his classic 1953 essay, \"The Methodology of Positive Economics,\" published in&nbsp;<a style=\"border-top-style: none; border-right-style: none; border-bottom-style: none; border-left-style: none; border-width: initial; border-color: initial; color: #4444be; font-weight: bold; text-decoration: none; \" href=\"http://www.amazon.com/Essays-Positive-Economics-Phoenix-Books/dp/0226264033/ref=sr_1_1?ie=UTF8&amp;s=books&amp;qid=1244221062&amp;sr=1-1\"><em>Essays in Positive Economics</em></a>. There was always an interesting tension in Friedman's work, which he never resolved, between reasoning as a clear-headed economist about people acting based on incentives and constraints and \"positivistly\" black-boxing it and trying to come up with predictions.</p>\n</blockquote>\n<p style=\"padding-top: 0.7em; margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 1em; \">And without further adieu, here's my respone:</p>\n<blockquote>\n<p style=\"padding-top: 0.7em; margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 1em;\">I don't think there is a tension in Friedman's thinking at all. We want our models to predict, otherwise, what are they good for? If a model doesn't have microfoundations and predicts well, so what? Microeconomic models, as Yglesias notes, don't have \"microfoundations\" in psychology. Yet they still do a pretty good job under a variety of circumstances.</p>\n<p style=\"padding-top: 0.7em; margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 1em;\">The reason microfoundations are necessary is instrumental to predictive power. It turns out that models with microfoundations [tend to] predict better than models without them, in all fields. This is why chemists tend to build their models up from physics and why biologists to the same with chemistry. It also turns out that microeconomics can be quite successful without microfoundations, while macroeconomics is far less successful.</p>\n<p style=\"padding-top: 0.7em; margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 1em;\">The Lucas critique tells us many of the reasons why macroeconomics needs microfoundations. The main reason microeconomics does not is that it is built from pretty solid intuitions about how individuals act. They aren't perfect, of course, but as a first approximation (instrumental) rationality does a pretty good job of describing human behavior in many cases. And the only way we really do know this is by going out and testing our models which, behavioral economics notwithstanding, have done well.</p>\n<p style=\"padding-top: 0.7em; margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 1em;\">There's a trade off between accuracy and analytical tractability while modeling. More microfoundations will tend to increase accuracy, but imagine if we started with physics for every single scientific problem. The computations would be insane, so instead we simplify things and ignore some of the microfoundations. It is called a model for a reason, after all.</p>\n<p style=\"padding-top: 0.7em; margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 1em;\">Friedman is right: if microfoundations do end up being important, models without them will do poorly relative models that use them (and thus the relevant trade off will manifest itself). Note that this is precisely what happened in the history of macro, and kudos to Friedman for realizing that microfoundations were important before the rest of the field. I suspect that macro models [don't] do well in an absolute sense though, but that is another matter entirely.</p>\n</blockquote>\n<p style=\"padding-top: 0.7em; margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 1em;\">ack... I should edit my comments better before posting them (notice the use of square brackets).</p>\n<p style=\"padding-top: 0.7em; margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 1em;\">edit: some minor formatting</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PDJ6KqJBRzvKPfuS3": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ye47ZQ3d8RhQSP8Fw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 0, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "1299", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-07T09:18:48.891Z", "modifiedAt": null, "url": null, "title": "indexical uncertainty and the Axiom of Independence", "slug": "indexical-uncertainty-and-the-axiom-of-independence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:06.780Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qij9v3YqPfyur2PbX/indexical-uncertainty-and-the-axiom-of-independence", "pageUrlRelative": "/posts/qij9v3YqPfyur2PbX/indexical-uncertainty-and-the-axiom-of-independence", "linkUrl": "https://www.lesswrong.com/posts/qij9v3YqPfyur2PbX/indexical-uncertainty-and-the-axiom-of-independence", "postedAtFormatted": "Sunday, June 7th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20indexical%20uncertainty%20and%20the%20Axiom%20of%20Independence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Aindexical%20uncertainty%20and%20the%20Axiom%20of%20Independence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqij9v3YqPfyur2PbX%2Findexical-uncertainty-and-the-axiom-of-independence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=indexical%20uncertainty%20and%20the%20Axiom%20of%20Independence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqij9v3YqPfyur2PbX%2Findexical-uncertainty-and-the-axiom-of-independence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqij9v3YqPfyur2PbX%2Findexical-uncertainty-and-the-axiom-of-independence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 773, "htmlBody": "<p>I&rsquo;ve noticed that the Axiom of Independence does not seem to make sense when dealing with indexical uncertainty, which suggests that Expected Utility Theory may not apply in situations involving indexical uncertainty. But Googling for \"indexical uncertainty\" in combination with either \"independence axiom\" or &ldquo;axiom of independence&rdquo; give zero results, so either I&rsquo;m the first person to notice this, I&rsquo;m missing something, or I&rsquo;m not using the right search terms. Maybe the LessWrong community can help me figure out which is the case.<br /><br />The Axiom of Independence says that for any A, B, C, and p, you prefer A to B if and only if you prefer p A + (1-p) C to p B + (1-p) C.&nbsp; This makes sense if p is a probability about the state of the world. (In the following, I'll use &ldquo;state&rdquo; and &ldquo;possible world&rdquo; interchangeably.) In that case, what it&rsquo;s saying is that what you prefer (e.g., A to B) in one possible world shouldn&rsquo;t be affected by what occurs (C) in other possible worlds. Why should it, if only one possible world is actual?<br /><br />In Expected Utility Theory, for each choice (i.e. option) you have, you iterate over the possible states of the world, compute the utility of the consequences of that choice given that state, then combine the separately computed utilities into an expected utility for that choice. The Axiom of Independence is what makes it possible to compute the utility of a choice in one state independently of its consequences in other states.<br /><br />But what if p represents an indexical uncertainty, which is uncertainty about where (or when) you are in the world?&nbsp; In that case, what occurs at one location in the world can easily interact with what occurs at another location, either physically, or in one&rsquo;s preferences. If there is physical interaction, then &ldquo;consequences of a choice at a location&rdquo; is ill-defined. If there is preferential interaction, then &ldquo;utility of the consequences of a choice at a location&rdquo; is ill-defined. In either case, it doesn&rsquo;t seem possible to compute the utility of the consequences of a choice at each location separately and then combine them into a probability-weighted average.<br /><br />Here&rsquo;s another way to think about this. In the expression &ldquo;p A + (1-p) C&rdquo; that&rsquo;s part of the Axiom of Independence, p was originally supposed to be the probability of a possible world being actual and A denotes the consequences of a choice in that possible world. We could say that A is local with respect to p. What happens if p is an indexical probability instead? Since there are no sharp boundaries between locations in a world, we can&rsquo;t redefine A to be local with respect to p. And if A still denotes the global consequences of a choice in a possible world, then &ldquo;p A + (1-p) C&rdquo; would mean two different sets of global consequences in the same world, which is nonsensical.<br /><br />If I&rsquo;m right, the notion of a &ldquo;probability of being at a location&rdquo; will have to acquire an instrumental meaning in an extended decision theory. Until then, it&rsquo;s not completely clear what people are really arguing about when they argue about such probabilities, for example in papers about the Simulation Argument and the Sleeping Beauty Problem.</p>\n<p><strong>Edit:</strong> Here's a game that exhibits what I call \"preferential interaction\" between locations. You are copied in your sleep, and both of you wake up in identical rooms with 3 buttons. Button A immunizes you with vaccine A, button B immunizes you with vaccine B. Button C has the effect of A if you're the original, and the effect of B if you're the clone. Your goal is to make sure at least one of you is immunized with an effective vaccine, so you press C.</p>\n<p>To analyze this decision in Expected Utility Theory, we have to specify the consequences of each choice at each location. If we let these be local consequences, so that pressing A has the consequence \"immunizes me with vaccine A\", then what I prefer at each location depends on what happens at the other location. If my counterpart is vaccinated with A, then I'd prefer to be vaccinated with B, and vice versa. \"immunizes me with vaccine A\" by itself can't be assigned an utility.</p>\n<p>What if we use the global consequences instead, so that pressing A has the consequence \"immunizes both of us with vaccine A\"? Then a choice's consequences do not differ by location, and &ldquo;probability of being at a location&rdquo; no longer has a role to play in the decision.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1, "bh7uxTTqmsQ8jZJdB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qij9v3YqPfyur2PbX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 17, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "1298", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-08T01:56:09.381Z", "modifiedAt": null, "url": null, "title": "London Rationalist Meetups bikeshed painting thread", "slug": "london-rationalist-meetups-bikeshed-painting-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:58.232Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iQi7roeLMEriey3ty/london-rationalist-meetups-bikeshed-painting-thread", "pageUrlRelative": "/posts/iQi7roeLMEriey3ty/london-rationalist-meetups-bikeshed-painting-thread", "linkUrl": "https://www.lesswrong.com/posts/iQi7roeLMEriey3ty/london-rationalist-meetups-bikeshed-painting-thread", "postedAtFormatted": "Monday, June 8th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20London%20Rationalist%20Meetups%20bikeshed%20painting%20thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALondon%20Rationalist%20Meetups%20bikeshed%20painting%20thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiQi7roeLMEriey3ty%2Flondon-rationalist-meetups-bikeshed-painting-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=London%20Rationalist%20Meetups%20bikeshed%20painting%20thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiQi7roeLMEriey3ty%2Flondon-rationalist-meetups-bikeshed-painting-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiQi7roeLMEriey3ty%2Flondon-rationalist-meetups-bikeshed-painting-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 87, "htmlBody": "<p>Something that came up on each of our three meetups so far was that people want more participation on things like format, place, and meeting times.</p>\n<p>Currently these are:</p>\n<ul>\n<li>5th View cafe on top of Waterstones bookstore near Piccadilly Circus</li>\n<li>First weekend of each month</li>\n<li>Casual chat format</li>\n</ul>\n<p>But these were just the first point we hit in the optimization space. They work, but that doesn't mean there isn't something that could work better.</p>\n<p>So everyone who wants to discuss them, here's the place.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iQi7roeLMEriey3ty", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 4, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "1302", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-09T07:29:02.237Z", "modifiedAt": null, "url": null, "title": "The Aumann's agreement theorem game (guess 2/3 of the average)", "slug": "the-aumann-s-agreement-theorem-game-guess-2-3-of-the-average", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:53.180Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "odm3FHPzgbiGpWsSg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RPXBjC5DjeXEJRbMw/the-aumann-s-agreement-theorem-game-guess-2-3-of-the-average", "pageUrlRelative": "/posts/RPXBjC5DjeXEJRbMw/the-aumann-s-agreement-theorem-game-guess-2-3-of-the-average", "linkUrl": "https://www.lesswrong.com/posts/RPXBjC5DjeXEJRbMw/the-aumann-s-agreement-theorem-game-guess-2-3-of-the-average", "postedAtFormatted": "Tuesday, June 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Aumann's%20agreement%20theorem%20game%20(guess%202%2F3%20of%20the%20average)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Aumann's%20agreement%20theorem%20game%20(guess%202%2F3%20of%20the%20average)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRPXBjC5DjeXEJRbMw%2Fthe-aumann-s-agreement-theorem-game-guess-2-3-of-the-average%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Aumann's%20agreement%20theorem%20game%20(guess%202%2F3%20of%20the%20average)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRPXBjC5DjeXEJRbMw%2Fthe-aumann-s-agreement-theorem-game-guess-2-3-of-the-average", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRPXBjC5DjeXEJRbMw%2Fthe-aumann-s-agreement-theorem-game-guess-2-3-of-the-average", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 338, "htmlBody": "<p>I'd like to play a game with you. Send me, privately, a real number between 0 and 100, inclusive. (No funny business. If you say \"my age\", I'm going to throw it out.) The winner of this game is the person who, after a week, guesses the number closest to 2/3 of the average guess. I will reveal the average guess, and will confirm the winner's claims to have won, but I will reveal no specific guesses.</p>\n<p>Suppose that you're a rational person. You also know that everyone else who plays this game is rational, you know that they know that, you know that they know <em>that</em>, and so on. Therefore, you conclude that the best guess is P. Since P is the rational guess to make, everyone will guess P, and so the best guess to make is P*2/3. This gives an equation that we can solve to get P = 0.</p>\n<p>I propose that this game be used as a sort of test to see how well Aumann's agreement theorem applies to a group of people. The key assumption the theorem makes--which, as taw points out, is <a href=\"http://t-a-w.blogspot.com/2009/03/how-robin-hanson-increased-my.html\">often overlooked</a>--is that the group members are all rational and honest and also have common knowledge of this. This same assumption implies that the average guess will be 0. The farther from the truth this assumption is, the farther the average guess is going to be from 0, and the farther Aumann's agreement theorem is from applying to the group.</p>\n<p><strong>Update</strong> (June 20): The game is finished; sorry for the delay in getting the results. The average guess was about 13.235418197890148 (a number which probably contains as much entropy as its length), meaning that the winning guess is the one closest to 8.823612131926765. This number appears to be significantly below the number typical for groups of ordinary people, but not dramatically so. 63% of guesses were too low, indicating that people were overall slightly optimistic about the outcome (if you interpret lower as better). Anyway, I will notify the winner <em>ahora mismo</em>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"WH5ZmNSjZmK9SMj7k": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RPXBjC5DjeXEJRbMw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 18, "extendedScore": null, "score": 4.998344814768487e-07, "legacy": true, "legacyId": "1305", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 156, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-09T12:04:29.306Z", "modifiedAt": null, "url": null, "title": "Expected futility for humans", "slug": "expected-futility-for-humans", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:51.656Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NkDJKw7diP2krwtko/expected-futility-for-humans", "pageUrlRelative": "/posts/NkDJKw7diP2krwtko/expected-futility-for-humans", "linkUrl": "https://www.lesswrong.com/posts/NkDJKw7diP2krwtko/expected-futility-for-humans", "postedAtFormatted": "Tuesday, June 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Expected%20futility%20for%20humans&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExpected%20futility%20for%20humans%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNkDJKw7diP2krwtko%2Fexpected-futility-for-humans%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Expected%20futility%20for%20humans%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNkDJKw7diP2krwtko%2Fexpected-futility-for-humans", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNkDJKw7diP2krwtko%2Fexpected-futility-for-humans", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 913, "htmlBody": "<p>Previously, Taw published an article entitled <a href=\"/lw/zv/post_your_utility_function/\">\"Post your utility function\"</a>, after having tried (apparently unsuccessfully) to work out \"what his utility function was\". I suspect that there is something to be gained by trying to work out what your priorities are in life, but I am not sure that people on this site are helping themselves very much by assigning dollar values, probabilities and discount rates. If you haven't done so already, you can learn why people like the utility function <a href=\"http://en.wikipedia.org/wiki/Neumann-Morgenstern_utility\">formalism on wikipedia.</a> I will say one thing about the expected utility theorem, though. An assignment of expected utilities to outcomes is (modulo renormalizing utilities by some set of affine transformations) equivalent to a preference over probabilistic combinations of outcomes; utilities are NOT properties of the outcomes you are talking about, they are properties of your mind. Goodness, like confusion, is in the mind.</p>\n<p>In this article, I will claim that trying to run your life based upon expected utility maximization is not a good idea, and thus asking \"what your utility function is\" is also not a useful question to try and answer.<a id=\"more\"></a></p>\n<p>There are many problems with using expected utility maximization to run your life: firstly, the size of the set of outcomes that one must consider in order to rigorously apply the theory is ridiculous: one must consider all probabilistic mixtures of possible histories of the universe from now to whatever your time horizon is. Even identifying macroscopically identical histories, this set is huge. Humans naturally describe world-histories in terms of deontological rules, such as \"if someone is nice to me, I want to be nice back to them\", or \"if I fall in love, I want to treat my partner well (unless s/he betrays me)\", \"I want to achieve something meaningful and be well-renowned with my life\", \"I want to help other people\". In order to translate these deontological rules into utilities attached to world-histories, you would have to assign a dollar utility to every possible world-history with all variants of who you fall in love with, where you settle, what career you have, what you do with your friends, etc, etc. Describing your function as a linear sum of independent terms will not work in general because, for example, whether accounting is a good career for you will depend upon the kind of personal life you want to live (i.e. different aspects of your life interact). You can, of course, emulate deontological rules such as \"I want to help other people\" in a complex utility function - that is what the process of enumerating human-distinguishable world-histories is - but it is nowhere near as efficient a representation as the usual deontological rules of thumb that people live by, <strong><em>particularly given that the human mind is well-adapted to representing deontological preferences</em></strong> (such as \"I must be nice to people\" - as was discussed before, there is a large amount of hidden complexity behind this simple english sentence) and very poor at representing and manipulating floating point numbers.</p>\n<p><a href=\"http://www.amirrorclear.net/academic/papers/decision-procedures.pdf\">Toby Ord's BPhil thesis</a> has some interesting critiques of naive consequentialism, and would probably provide an entry point to the literature:</p>\n<blockquote>\n<p>&lsquo;An uncomplicated illustration is provided by the security which lovers or friends produce in one another by being guided, and being seen to be guided, by maxims of virtually unconditional fidelity. Adherence to such maxims is justified by this prized effect, since any retreat from it will undermine the effect, being inevitably detectable within a close relationship. This is so whether the retreat takes the form of intruding calculation or calculative monitoring. The point scarcely needs emphasis.&rsquo;</p>\n</blockquote>\n<p>There are many other pitfalls: One is thinking that you know what is of value in your life, and forgetting what the most important things are (such as youth, health, friendship, family, humour, a sense of personal dignity, a sense of moral pureness for yourself, acceptance by your peers, social status, etc) because they've always been there so you took them for granted. Another is that since we humans are under the influence of a considerable number of delusions about the nature of our own lives, (in particular: that our actions are influenced exclusively by our long-term plans rather than by the situations we find ourselves in or our base animal desires) we often find that our actions have unintended consequences. Human life is naturally complicated enough that this would happen anyway, but attempting to optimize your life whilst under the influence of systematic delusions about the way it really works is likely to make it worse than if you just stick to default behaviour.</p>\n<p><br />What, then is the best decision procedure for deciding how to improve your life? Certainly I would steer clear of dollar values and expected utility calculations, because this formalism is a huge leap away from our intuitive decision procedure. It seems wiser to me to make <strong><em>small incrermental changes to your decision procedure for getting things done</em></strong>. For example, if you currently decide what to do based completely upon your whims, consider making a vague list of goals in your life (with no particular priorities attached) and updating your progress on them. If you already do this, consider brainstorming for other goals that you might have ignored, and then attach priorities based upon the assumption that you will certainly achieve or not achieve each of these goals, ignoring what probabilistic mixtures you would accept (because your mind probably won't be able to handle the probabilistic aspect in a numerical way anyway).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1, "3QnDqGSdRMA5mdMM6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NkDJKw7diP2krwtko", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 13, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "1301", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["MvwdPfYLX866vazFJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-09T18:03:20.808Z", "modifiedAt": null, "url": null, "title": "You can't believe in Bayes", "slug": "you-can-t-believe-in-bayes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:58.524Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/STZBqw7SAWwjjna6k/you-can-t-believe-in-bayes", "pageUrlRelative": "/posts/STZBqw7SAWwjjna6k/you-can-t-believe-in-bayes", "linkUrl": "https://www.lesswrong.com/posts/STZBqw7SAWwjjna6k/you-can-t-believe-in-bayes", "postedAtFormatted": "Tuesday, June 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20You%20can't%20believe%20in%20Bayes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYou%20can't%20believe%20in%20Bayes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSTZBqw7SAWwjjna6k%2Fyou-can-t-believe-in-bayes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=You%20can't%20believe%20in%20Bayes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSTZBqw7SAWwjjna6k%2Fyou-can-t-believe-in-bayes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSTZBqw7SAWwjjna6k%2Fyou-can-t-believe-in-bayes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 370, "htmlBody": "<p>Well, you <em>can</em>.&nbsp; It's just oxymoronic, or at least ironic.&nbsp; Because <em>belief</em> is contrary to the Bayesian paradigm.</p>\n<p>You use Bayesian methods to choose an action.&nbsp; You have a set of observations, and assign probabilities to possible outcomes, and choose an action.</p>\n<p><em>Belief </em>in an outcome N means that you set p(N) &asymp; 1 if p(N) &gt; some threshold.&nbsp; It's a useful computational shortcut.&nbsp; But when you use it, you're not treating N in a Bayesian manner.&nbsp; When you categorize things into beliefs/nonbeliefs, and then act based on whether you believe N or not, you are throwing away the information contained in the probability judgement, in order to save computation time.&nbsp; It is especially egregious if the threshold you use to categorize things into beliefs/nonbeliefs is relatively constant, rather than being a function of (expected value of N) / (expected value of not N).</p>\n<p>If your neighbor took out fire insurance on his house, you wouldn't infer that he <em>believed</em> his house was going to burn down.&nbsp; And if he took his umbrella to work, you wouldn't (I hope) infer that he <em>believed</em> it was going to rain.</p>\n<p>Yet when it comes to decisions on a national scale, people cast things in terms of belief.&nbsp; Do you <em>believe</em> North Korea will sell nuclear weapons to Syria?&nbsp; That's the wrong question when you're dealing with a country that has, let's say, a 20% chance of building weapons that will be used to level at least ten major US cities.</p>\n<p>Or flash back to the 1990s, before there was a scientific consensus that global warming was real.&nbsp; People would often say, \"I don't believe in global warming.\"&nbsp; And interviews with scientists tried to discern whether they did or did not believe in global warming.</p>\n<p>It's the wrong question.&nbsp; The question is what steps are worth taking according to your assigned probabilities and expected-value computations.</p>\n<p>A scientist doesn't have to <em>believe</em> in something to consider it worthy of study.&nbsp; Do you <em>believe</em> an asteroid will hit the Earth this century?&nbsp; Do you <em>believe</em> we can cure aging in your lifetime?&nbsp; Do you <em>believe</em> we will have a hard-takeoff singularity?&nbsp; If a low-probability outcome can have a high impact on expected utility, you've already gone wrong when you ask the question.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hLp77TQsRkooioj86": 1, "JHYaBGQuuKHdwnrAK": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "STZBqw7SAWwjjna6k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 14, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "1306", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 60, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-09T20:11:56.083Z", "modifiedAt": null, "url": null, "title": "Less wrong economic policy", "slug": "less-wrong-economic-policy", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:55.685Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gworley", "createdAt": "2009-03-26T17:18:20.404Z", "isAdmin": false, "displayName": "G Gordon Worley III"}, "userId": "gjoi5eBQob27Lww62", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q2P96Tve4fwP2b56g/less-wrong-economic-policy", "pageUrlRelative": "/posts/q2P96Tve4fwP2b56g/less-wrong-economic-policy", "linkUrl": "https://www.lesswrong.com/posts/q2P96Tve4fwP2b56g/less-wrong-economic-policy", "postedAtFormatted": "Tuesday, June 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20wrong%20economic%20policy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20wrong%20economic%20policy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq2P96Tve4fwP2b56g%2Fless-wrong-economic-policy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20wrong%20economic%20policy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq2P96Tve4fwP2b56g%2Fless-wrong-economic-policy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq2P96Tve4fwP2b56g%2Fless-wrong-economic-policy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 287, "htmlBody": "<p>Yesterday <a href=\"http://www.npr.org/templates/story/story.php?storyId=104803094\">I heard an interesting story on the radio</a> about US President Obama's pick to head the Office of Information and Regulatory Affairs, Cass Sunstein.&nbsp; I recommend checking out the story, but here are a few key excerpts.</p>\n<blockquote>\n<p>Cass Sunstein, President Obama's pick to head the Office of Information and Regulatory Affairs, is a vocal supporter of [...] economic policy that shapes itself around human psychology. Sunstein is just one of a number of high-level appointees now working in the Obama administration who favors this kind of approach.</p>\n<p>[...]</p>\n<p>Through their research, Kahneman and Tversky identified dozens of these biases and errors in judgment, which together painted a certain picture of the human animal. Human beings, it turns out, don't always make good decisions, and frequently the choices they do make aren't in their best interest.<a id=\"more\"></a></p>\n<p>[...]</p>\n\"Merely accepting the fact that people do not necessarily make the best decisions for themselves is politically very explosive. The moment that you admit that, you have to start protecting people,\" Kahneman says.\n<p>[...]</p>\n<p>The Obama administration believes it needs to shape policy in a way that will keep us all from getting hit by trucks &mdash; health care trucks, financial trucks, trucks that come from every direction and affect every aspect of our lives.</p>\n</blockquote>\n<p>At the risk of starting a discussion that will be wrecked by political wrestling, I'm always hopeful when I hear about governments applying what we learn from science to policy.&nbsp; Not to say that this always generates good policies, but it does generate the best policies we have reason to believe will be good (so long as you ignore the issue of actual politices that might get in the way).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PDJ6KqJBRzvKPfuS3": 1, "FkzScn5byCs9PxGsA": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "q2P96Tve4fwP2b56g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 7, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "1308", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-11T12:31:02.904Z", "modifiedAt": null, "url": null, "title": "The Terrible, Horrible, No Good, Very Bad Truth About Morality and What To Do About It", "slug": "the-terrible-horrible-no-good-very-bad-truth-about-morality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:11.212Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B5K3hg8FgrMDHuXjH/the-terrible-horrible-no-good-very-bad-truth-about-morality", "pageUrlRelative": "/posts/B5K3hg8FgrMDHuXjH/the-terrible-horrible-no-good-very-bad-truth-about-morality", "linkUrl": "https://www.lesswrong.com/posts/B5K3hg8FgrMDHuXjH/the-terrible-horrible-no-good-very-bad-truth-about-morality", "postedAtFormatted": "Thursday, June 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Terrible%2C%20Horrible%2C%20No%20Good%2C%20Very%20Bad%20Truth%20About%20Morality%20and%20What%20To%20Do%20About%20It&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Terrible%2C%20Horrible%2C%20No%20Good%2C%20Very%20Bad%20Truth%20About%20Morality%20and%20What%20To%20Do%20About%20It%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB5K3hg8FgrMDHuXjH%2Fthe-terrible-horrible-no-good-very-bad-truth-about-morality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Terrible%2C%20Horrible%2C%20No%20Good%2C%20Very%20Bad%20Truth%20About%20Morality%20and%20What%20To%20Do%20About%20It%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB5K3hg8FgrMDHuXjH%2Fthe-terrible-horrible-no-good-very-bad-truth-about-morality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB5K3hg8FgrMDHuXjH%2Fthe-terrible-horrible-no-good-very-bad-truth-about-morality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2501, "htmlBody": "<p>Joshua Greene has a PhD thesis called <span style=\"font-weight: normal;\"><a href=\"http://www.wjh.harvard.edu/~jgreene/GreeneWJH/Greene-Dissertation.pdf\">The Terrible, Horrible, No Good, Very Bad Truth About Morality and What To Do About It</a>. What is this terrible truth? The essence of this truth is that many, many people (probably most people) believe that their particular moral (and axiological) views on the world are objectively true - for example that anyone who disagrees with the statement \"black people have the same value as any other human beings\" has committed either an error of logic or has got some empirical fact wrong, in the same way that people who claim that the earth was created 6000 years ago are objectively wrong. </span></p>\n<p><span style=\"font-weight: normal;\">To put it another way, Greene's contention is that our entire way of talking about ethics - the very words that we use - force us into talking complete nonsense (often in a very angry way) about ethics. As a simple example, consider the use of the words in any standard ethical debate - \"abortion is murder\", \"animal suffering is just as bad as human suffering\" - these terms seem to refer to objective facts; \"abortion is murder\" sounds rather like \"water is a solvent!\". I urge readers of Less Wrong to put in the effort of reading a significant part of Greene's long thesis starting at chapter 3: <em>Moral Psychology and Projective Error</em>, considering the massively important repercussions he claims his ideas could have: </span></p>\n<blockquote>\n<p>In this essay I argue that ordinary moral thought and language is, while very natural, highly counterproductive and that as a result we would be wise to change the way we think and talk about moral matters. First, I argue on metaphysical grounds against moral realism, the view according to which there are first order moral truths. Second, I draw on principles of moral psychology, cognitive science, and evolutionary theory to explain why moral realism appears to be true even though it is not. I then argue, based on the picture of moral psychology developed herein, that<strong> <em>realist moral language and thought promotes misunderstanding and exacerbates conflict</em></strong>. I consider a number of standard views concerning the practical implications of moral anti-realism and reject them. I then sketch and defend a set of alternative revisionist proposals for improving moral discourse, chief among them the elimination of realist moral language, especially deontological language, and the promotion of an anti-realist utilitarian framework for discussing moral issues of public concern. <em><strong>I emphasize the importance of revising our moral practices, suggesting that our entrenched modes of moral thought may be responsible for our failure to solve a number of global social problems. </strong></em></p>\n</blockquote>\n<p><a id=\"more\"></a></p>\n<p>As an accessible entry point, I have decided to summarize what I consider to be Greene's most important points in this post. I hope he doesn't mind - I feel that spreading this message is sufficiently urgent to justify reproducing large chunks of his dissertation - Starting at page 142:</p>\n<blockquote>\n<p>In the previous chapter we concluded, in spite of common sense, that moral realism is false. This raises an important question: How is it that so many people are mistaken about the nature of morality? To become comfortable with the fact that moral realism is false we need to understand how moral realism can be so wrong but feel so right. ...</p>\n</blockquote>\n<blockquote>\n<p><span style=\"font-weight: normal;\">The central tenet of projectivism is that the moral properties we find (or think we find) in things in the world (e.g. moral wrongness) are mind-dependent in a way that other properties, those that we&rsquo;ve called &ldquo;value-neutral&rdquo; (e.g. solubility in water), are not. Whether or not something is soluble in water has nothing to do with human psychology. But, say projectivists, whether or not something is wrong (or &ldquo;wrong&rdquo;) has everything to do with human psychology.... <br /></span></p>\n</blockquote>\n<blockquote>\n<p>Projectivists maintain that our encounters with the moral world are, at the very least, somewhat misleading. Projected properties tend to strike us as unprojected. They appear to be really &ldquo;out there,&rdquo; in a way that they, unlike typical value neutral properties, are not. ...</p>\n</blockquote>\n<blockquote>\n<p>The respective roles of intuition and reasoning are illuminated by considering people&rsquo;s reactions to the following story:</p>\n<p><em>\"Julie and Mark are brother and sister. They are travelling together in France on summer vacation from college. One night they are staying alone in a cabin near the beach. They decided that it would be interesting and fun if they tried making love. At the very least it would be a new experience for each of them. Julie was already taking birth control pills, but Mark uses a condom too, just to be safe. They both enjoy making love but decide not to do it again. They keep that night as a special secret between them, which makes them feel even closer to each other. What do you think about that, was it OK for them to make love?\"</em></p>\n<p>Haidt (2001, pg. 814) describes people&rsquo;s responses to this story as follows: Most people who hear the above story immediately say that it was wrong for the siblings to make love, and they then set about searching for reasons. They point out the dangers of inbreeding, only to remember that Julie and Mark used two forms of birth control. They next try to argue that Julie and Mark could be hurt, even though the story makes it clear that no harm befell them. Eventually many people say something like</p>\n<p><em>&ldquo;I don&rsquo;t know, I can&rsquo;t explain it, I just know it&rsquo;s wrong.&rdquo;</em></p>\n<p>This moral question is carefully designed to short-circuit the most common reason people give for judging an action to be wrong, namely harm to self or others, and in so doing it reveals something about moral psychology, at least as it operates in cases such at these. People&rsquo;s moral judgments in response to the above story tend to be forceful, immediate, and produced by an unconscious process (intuition) rather than through the deliberate and effortful application of moral principles (reasoning). When asked to explain why they judged as they did, subjects typically gave reasons. Upon recognizing the flaws in those reasons, subjects typically stood by their judgments all the same, suggesting that the reasons they gave after the fact in support their judgments had little to do with the process that produced those judgments. Under ordinary circumstances reasoning comes into play after the judgment has already been reached in order to find rational support for the preordained judgment. <strong><em>When faced with a social demand for a verbal justification, one becomes a lawyer trying to build a case rather than a judge searching for the truth.</em> </strong>... <strong><br /></strong></p>\n</blockquote>\n<p>&nbsp;</p>\n<blockquote>\n<p>The Illusion of Rationalist Psychology (p. 197)</p>\n<p>In Sections 3.2-3.4 I developed an explanation for why moral realism appears to be true, an explanation featuring the Humean notion of projectivism according to which we intuitively see various things in the world as possessing moral properties that they do not actually have. This explains why we tend to be realists, but it doesn&rsquo;t explain, and to some extent is at odds with, the following curious fact. The social intuitionist model is counterintuitive. People tend to believe that moral judgments are produced by reasoning even though this is not the case. Why do people make this mistake? Consider, once again, the case of Mark and Julie, the siblings who decided to have sex. Many subjects, when asked to explain why Mark and Julie&rsquo;s behavior is wrong, engaged in &ldquo;moral dumbfounding,&rdquo; bumbling efforts to supply reasons for their intuitive judgments. This need not have been so. It might have turned out that all the subjects said things like this right off the bat: <em></em></p>\n<p><em>&ldquo;Why do I say it&rsquo;s wrong? Because it&rsquo;s clearly just wrong. Isn&rsquo;t that plain to see? It&rsquo;s as if you&rsquo;re putting a lemon in front of me and asking me why I say it&rsquo;s yellow. What more is there to say?&rdquo; </em></p>\n<p>Perhaps some subjects did respond like this, but most did not. Instead, subjects typically felt the need to portray their responses as products of reasoning, even though they generally discovered (often with some embarrassment) that they could not easily supply adequate reasons for their judgments. On many occasions I&rsquo;ve asked people to explain why they say that it&rsquo;s okay to turn the trolley onto the other tracks but not okay to push someone in front of the trolley. Rarely do they begin by saying, <em>&ldquo;I don&rsquo;t know why. I just have an intuition that tells me that it is.&rdquo;</em> Rather, they tend to start by spinning the sorts of theories that ethicists have devised, <em><strong>theories that are nevertheless notoriously difficult to defend.</strong></em> In my experience, it is only after a bit of moral dumbfounding that people are willing to confess that their judgments were made intuitively.</p>\n<p>Why do people insist on giving reasons in support of judgments that were made with great confidence in the absence of reasons? I suspect it has something to do with the custom complexes in which we Westerners have been immersed since childhood.<strong><em> We live in a reason-giving culture</em></strong>. Western individuals are expected to choose their own way, and to do so for good reason. American children, for example, learn about the rational design of their public institutions; the all important &ldquo;checks and balances&rdquo; between the branches of government, the judicial system according to which accused individuals have a right to a trial during which they can, if they wish, plead their cases in a rational way, inevitably with the help of a legal expert whose job it is to make persuasive legal arguments, etc. Westerners learn about doctors who make diagnoses and scientists who, by means of experimentation, unlock nature&rsquo;s secrets. Reasoning isn&rsquo;t the only game in town, of course. The American Declaration of Independence famously declares &ldquo;these truths to be self-evident,&rdquo; but American children are nevertheless given numerous reasons for the decisions of their nation&rsquo;s founding fathers, for example, the evils of absolute monarchy and the injustice of &ldquo;taxation without representation.&rdquo; When Western countries win wars they draft peace treaties explaining why they, and not their vanquished foes, were in the right and set up special courts to try their enemies in a way that makes it clear to all that they punish only with good reason. Those seeking public office make speeches explaining why they should be elected, sometimes as parts of organized debates. Some people are better at reasoning than others, but everyone knows that the best people are the ones who, when asked, can explain why they said what they said and did what they did.</p>\n<p>With this in mind, we can imagine what might go on when a Westerner makes a typical moral judgment and is then asked to explain why he said what he said or how he arrived at that conclusion. The question is posed, and he responds intuitively. As suggested above, such intuitive responses tend to present themselves as perceptual. The subject is perhaps aware of his &ldquo;gut reaction,&rdquo; but he doesn&rsquo;t take himself to have merely had a gut reaction. Rather, he takes himself to have detected a moral property out in the world, say, the inherent wrongness in Mark and Julie&rsquo;s incestuous behavior or in shoving someone in front of a moving train. The subject is then asked to explain how he arrived at his judgment. He could say, <em>&ldquo;I don&rsquo;t know. I answered intuitively,&rdquo;</em> and this answer would be the most accurate answer for nearly everyone. But this is not the answer he gives because he knows after a lifetime of living in Western culture that <em>&ldquo;I don&rsquo;t know how I reached that conclusion. I just did. But I&rsquo;m sure it&rsquo;s right,&rdquo;</em> doesn&rsquo;t sound like a very good answer. So, instead, he asks himself, <em>&ldquo;What would be a good reason for reaching this conclusion?&rdquo;</em> And then, drawing on his rich experience with reason-giving and -receiving, he says something that sounds plausible both as a causal explanation of and justification for his judgment: <em>&ldquo;It&rsquo;s wrong because their children could turn out to have all kinds of diseases,&rdquo;</em> or, <em>&ldquo;Well, in the first case the other guy is, like, already involved, but in the case where you go ahead and push the guy he&rsquo;s just there minding his own business.&rdquo;</em> <strong><em>People&rsquo;s confidence that their judgments are objectively correct combined with the pressure to give a &ldquo;good answer&rdquo; leads people to produce these sorts of post-hoc explanations/justifications</em></strong>. Such explanations need not be the results of deliberate attempts at deception. The individuals who offer them may themselves believe that the reasons they&rsquo;ve given after the fact were really their reasons all along, what they &ldquo;really had in mind&rdquo; in giving those quick responses. ...</p>\n</blockquote>\n<blockquote>\n<p>My guess is that even among philosophers particular moral judgments are made first and reasoned out later. In my experience, philosophers are often well aware of the fact that their moral judgments are the results of intuition. As noted above, it&rsquo;s commonplace among ethicists to think of their moral theories as attempts to organize pre-existing moral intuitions. <em><strong>The mistake philosophers tend to make is in accepting rationalism proper, the view that our moral intuitions (assumed to be roughly correct) must be ultimately justified by some sort of rational theory that we&rsquo;ve yet to discover.</strong></em> For example, philosophers are as likely as anyone to think that there must be &ldquo;some good reason&rdquo; for why it&rsquo;s okay to turn the trolley onto the other set of tracks but not okay to push the person in front of the trolley, where a &ldquo;good reason,&rdquo; or course, is a piece of moral theory with justificatory force and not a piece of psychological description concerning patterns in people&rsquo;s emotional responses.</p>\n</blockquote>\n<p>One might well ask: why does any of this indicate that moral propositions have no rational justification? The arguments presented here show fairly conclusively that our moral judgements are instinctive, subconscious, evolved features. Evolution gave them to us. But readers of Eliezer's material on Overcoming Bias will be well aware of the character of evolved solutions: they're guaranteed to be a mess. Why should evolution have happened to have given us exactly those moral instincts that give the same conclusions as would have been produced by (say) great moral principle X? (X = the golden rule, or X = hedonistic utilitarianism, or X = negative utilitarianism, etc).</p>\n<p>Expecting evolved moral instincts to conform exactly to some simple unifying principle is like expecting the orbits of the planets to be in the same proportion as the first 9 prime numbers or something. That which is produced by a complex, messy, random process is unlikely to have some low complexity description.</p>\n<p>Now I can imagine a \"from first principles\" argument producing an objective morality that has some simple description - I can imagine starting from only simple facts about agenthood and deriving Kant's Golden Rule as the one objective moral truth. But I cannot seriosuly entertain the prospect of a \"from first principles\" argument producing the human moral mess. No way. It was this observation that finally convinced me to abandon my various attempts at objective ethics.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B5K3hg8FgrMDHuXjH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 62, "baseScore": 53, "extendedScore": null, "score": 8.2e-05, "legacy": true, "legacyId": "1311", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 54, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 142, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-11T16:28:06.637Z", "modifiedAt": null, "url": null, "title": "Let's reimplement EURISKO!", "slug": "let-s-reimplement-eurisko", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:09.229Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/t47TeAbBYxYgqDGQT/let-s-reimplement-eurisko", "pageUrlRelative": "/posts/t47TeAbBYxYgqDGQT/let-s-reimplement-eurisko", "linkUrl": "https://www.lesswrong.com/posts/t47TeAbBYxYgqDGQT/let-s-reimplement-eurisko", "postedAtFormatted": "Thursday, June 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Let's%20reimplement%20EURISKO!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALet's%20reimplement%20EURISKO!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft47TeAbBYxYgqDGQT%2Flet-s-reimplement-eurisko%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Let's%20reimplement%20EURISKO!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft47TeAbBYxYgqDGQT%2Flet-s-reimplement-eurisko", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft47TeAbBYxYgqDGQT%2Flet-s-reimplement-eurisko", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 287, "htmlBody": "<p>In the early 1980s Douglas Lenat wrote <a href=\"http://en.wikipedia.org/wiki/Eurisko\">EURISKO</a>, a program Eliezer <a href=\"/lw/w6/recursion_magic/\">called</a> <em>\"[maybe] the most sophisticated self-improving AI ever built\"</em>. The program reportedly had some high-profile successes in various domains, like becoming world champion at a certain wargame or designing good integrated circuits.</p>\n<p>Despite <a href=\"http://www.sl4.org/archive/0812/19647.html\">requests</a> Lenat never released the source code. You can download an introductory paper: <a href=\"http://www.google.ru/url?sa=t&amp;source=web&amp;ct=res&amp;cd=1&amp;url=http%3A%2F%2Fwww.aaai.org%2FPapers%2FAAAI%2F1983%2FAAAI83-059.pdf&amp;ei=MysxSoXxIMPGsgb_3-XPBA&amp;usg=AFQjCNGmq0OxMqB2OAlcNiIBgynL-dpjoA&amp;sig2=3mhJRjG-nIw_xcfHXOxU_A\">\"Why AM and EURISKO appear to work\"</a> [PDF]. Honestly, reading it leaves a programmer still mystified about the internal workings of the AI: for example, what does the main loop look like? Researchers supposedly answered such questions in a more detailed publication, <em>\"EURISKO: A program that learns new heuristics and domain concepts.\" Artificial Intelligence (21): pp. 61-98.</em> I couldn't find that paper available for download anywhere, and being in Russia I found it quite tricky to get a paper version. Maybe you Americans will have better luck with your local library? And to the best of my knowledge no one ever succeeded in (or even seriously tried) confirming Lenat's EURISKO results.</p>\n<p>Today in 2009 this state of affairs looks laughable. A 30-year-old pivotal breakthrough in a large and important field... that never even got <em>reproduced.</em> What if it was a gigantic case of <a href=\"http://en.wikipedia.org/wiki/Clever_Hans\">Clever Hans</a>? How do you know? You're supposed to be a <em>scientist</em>, little one.</p>\n<p>So my proposal to the LessWrong community: let's reimplement EURISKO!</p>\n<p>We have some competent programmers here, don't we? We have open source tools and languages that weren't around in 1980. We can build an open source implementation available for all to play. In my book this counts as solid progress in the AI field.</p>\n<p>Hell, I'd do it on my own if I had the goddamn paper.</p>\n<p><strong>Update</strong>: RichardKennaway has put Lenat's detailed papers up online, see the comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "QPt5ECwTCAg63mbNu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "t47TeAbBYxYgqDGQT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 23, "extendedScore": null, "score": 5.003615594618757e-07, "legacy": true, "legacyId": "1312", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 162, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rJLviHqJMTy8WQkow"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-11T18:34:35.971Z", "modifiedAt": null, "url": null, "title": "If it looks like utility maximizer and quacks like utility maximizer...", "slug": "if-it-looks-like-utility-maximizer-and-quacks-like-utility", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:57.676Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/i6npKoxQ2QALPCbcP/if-it-looks-like-utility-maximizer-and-quacks-like-utility", "pageUrlRelative": "/posts/i6npKoxQ2QALPCbcP/if-it-looks-like-utility-maximizer-and-quacks-like-utility", "linkUrl": "https://www.lesswrong.com/posts/i6npKoxQ2QALPCbcP/if-it-looks-like-utility-maximizer-and-quacks-like-utility", "postedAtFormatted": "Thursday, June 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20If%20it%20looks%20like%20utility%20maximizer%20and%20quacks%20like%20utility%20maximizer...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIf%20it%20looks%20like%20utility%20maximizer%20and%20quacks%20like%20utility%20maximizer...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi6npKoxQ2QALPCbcP%2Fif-it-looks-like-utility-maximizer-and-quacks-like-utility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=If%20it%20looks%20like%20utility%20maximizer%20and%20quacks%20like%20utility%20maximizer...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi6npKoxQ2QALPCbcP%2Fif-it-looks-like-utility-maximizer-and-quacks-like-utility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi6npKoxQ2QALPCbcP%2Fif-it-looks-like-utility-maximizer-and-quacks-like-utility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 461, "htmlBody": "<p>...it's probably adaptation executer.</p>\n<p>We often assume agents are utility maximizers. We even call this \"rationality\". On the other hand <a href=\"/lw/zv/post_your_utility_function/\">in our recent experiment</a> nobody managed to figure out even approximate shape of their utility function, and we know about large number of ways how agents deviate from utility maximization. How goes?</p>\n<p>One explanation is fairly obvious. Nature contains plenty of selection processes - evolution and markets most obviously, but plenty of others like competition between Internet forums in attracting users, and between politicians trying to get elected. In such selection processes a certain property - fitness - behaves a lot like utility function. As a good approximation a traits that give agents higher expected fitness survives and proliferates. And as a result of that agents that survive such selection processes react to inputs quite reliably as if they were optimizing some utility function - fitness of the underlying selection process.</p>\n<p>If that's the whole story, we can conclude a few things:</p>\n<ul>\n<li>Utility maximization related to some selection process - evolutionary fitness, money, chance of getting elected - is very common and quite reliable.</li>\n<li>We will not find any utility maximization related outside selection process - so we are not maximizers when it comes to happiness, global well-being, ecological responsibility, and so on.</li>\n<li>If selection process isn't strong enough, we won't see much utility maximization. So we can predict that oligopolies are pretty bad at maximizing their market success - as pressure is very low.</li>\n<li>If selection process isn't running long enough, or rules changed recently, we won't see much utility maximization. So people are really horrible at maximizing number of their offspring in our modern environment with birth control and limitless resources. And we can predict that this might change given enough time.</li>\n<li>If input is atypical, even fairly reliable utility maximizers are likely to behave badly.</li>\n<li>Agents are not utility maximizers across selection processes. So while politicians can be assumed to maximize their chance of getting elected very well, they must be pretty bad at getting as maximizing their income, or number of children.</li>\n<li>However, selection processes often correlate - historically someone who made more money could afford to have more children, richer politicians can use their wealth to increase their chance of getting elected, companies with more political power can survive in marketplace better, Internet forums need money and users to survive, so must cross-optimize for both etc. This can give an illusion of one huge cross-domain utility function, but selection across domains is often not that strong.</li>\n<li>We are naturally bad at achieving our goals and being happy in modern environment. To have any chances, we must consciously research what works and what doesn't.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1, "5f5c37ee1b5cdee568cfb16a": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "i6npKoxQ2QALPCbcP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 20, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "1313", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["MvwdPfYLX866vazFJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-12T12:28:39.826Z", "modifiedAt": null, "url": null, "title": "Typical Mind and Politics", "slug": "typical-mind-and-politics", "viewCount": null, "lastCommentedAt": "2021-05-14T04:08:12.778Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pczHfyxmnFhtKthqR/typical-mind-and-politics", "pageUrlRelative": "/posts/pczHfyxmnFhtKthqR/typical-mind-and-politics", "linkUrl": "https://www.lesswrong.com/posts/pczHfyxmnFhtKthqR/typical-mind-and-politics", "postedAtFormatted": "Friday, June 12th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Typical%20Mind%20and%20Politics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATypical%20Mind%20and%20Politics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpczHfyxmnFhtKthqR%2Ftypical-mind-and-politics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Typical%20Mind%20and%20Politics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpczHfyxmnFhtKthqR%2Ftypical-mind-and-politics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpczHfyxmnFhtKthqR%2Ftypical-mind-and-politics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1701, "htmlBody": "<p>Yesterday, in the <a href=\"/lw/10f/the_terrible_horrible_no_good_very_bad_truth/\">The Terrible, Horrible, No Good Truth About Morality</a>, Roko mentioned some good evidence that we develop an opinion first based on intuitions, and only later look for rational justifications. For example, people would claim incest was wrong because of worries like genetic defects or later harm, but continue to insist that incest was wrong even after all those worries had been taken away.</p>\r\n<p>Roko's examples take advantage of universal human feelings like the incest taboo. But if people started out with opposite intuitions, then this same mechanism would produce opinions that people hold very strongly and are happy to support with as many reasons and facts as you please, but which are highly resistant to real debate or to contradicting evidence.</p>\r\n<p>Sound familiar?</p>\r\n<p>But to explain politics with this mechanism, we'd need an explanation for why people's intuitions differed to begin with. We've already discussed some such explanations - self-serving biases, influence from family and community, et cetera - but today I want to talk about another possibility.</p>\r\n<p><a id=\"more\"></a></p>\r\n<p>A few weeks back, I was <a href=\"http://www.overcomingbias.com/2009/05/wilkinson-on-side-effects.html#more-18454\">discussing harms with Bill Swift on Overcoming Bias</a>. In particular, I was arguing that one situation in which there was an open-and-shut case for government restriction of private activity on private property was nuisance noise. I argued that if you were making noise on your property, and I could hear it on my property, that I was being harmed by your actions and that there was clearly just as much a case for government intervention here as if you were firing flaming arrows at me from your property. I fully expected Bill to agree that this was obviously true but to have some reason why he didn't think it applied to our particular disagreement.</p>\r\n<p>Instead, to my absolute astonishment, Bill said that noise wasn't really a problem. He said he lived on a noisy property and had just stopped whining and gotten on with his life. I didn't really know how to react to this<sup>1</sup>, and ended up assuming either that he'd never lived in a really noisy place like I have, or that he was such a blighted ideologue that he was willing to completely contradict common sense in order to preserve his silly argument.</p>\r\n<p>In other words, I was assuming the person I was debating was either astonishingly stupid or willfully evil. And when my thoughts tend in that direction, it <a href=\"/lw/2p/the_skeptics_trilemma/\">usually means I'm missing something</a>.</p>\r\n<p>Luckily in this case I'd already written a long essay explaining my mistake in detail. In <a href=\"/lw/dr/generalizing_from_one_example/\">Generalizing From One Example</a>,&nbsp; I warned people against assuming everyone's mind is built the same way their own mind is. One particular example I gave was:</p>\r\n<blockquote>\r\n<p>I can't deal with noise. If someone's being loud, I can't sleep, I can't study, I can't concentrate, I can't do anything except bang my head against the wall and hope they stop. I once had a noisy housemate. Whenever I asked her to keep it down, she told me I was being oversensitive and should just mellow out.</p>\r\n</blockquote>\r\n<p><br />So it seems possible to me that I have an oversensitivity to noise and Bill has an undersensitivity to it. When someone around me is being noisy, my intuitions tell me this is extremely bad and needs to be stopped by any means necessary. And maybe Bill's intuitions tell him that this is a minor non-problem. I won't say that this is actually behind our disagreement on the issue - my guess is that Bill and I would disagree about government regulation of pollution from a factory as well - but I think it contributes and it makes our debate much less productive than it would have been otherwise.</p>\r\n<p>Let me give an example of one place I think a mind difference *is* behind a political opinion. In <a href=\"/lw/65/money_the_unit_of_caring/\">Money, The Unit of Caring</a>, Eliezer complained that people were too willing to donate time to charity, and too unwilling to donate money to charity. He gave the example of his own experience, where he felt terrible every time he gave away money, but didn't mind a time committment nearly as much. I fired back <a href=\"/lw/65/money_the_unit_of_caring/4fv\">a response</a> that this was completely foreign to me, because I am happy to give money to charity and often do it before I've even fully thought about what I'm doing, but will groan and make excuses whenever I'm asked to give away time. I also mentioned that this was a general tendency of mine: I have minimal aversion to monetary loss<sup>2</sup>, but wasting time makes me angry.</p>\r\n<p>A few months ago, Barack Obama proposed a plan (which he later decided against) to make every high school and college student volunteer a certain amount of time to charity. Although I usually like Obama, I wrote an absolutely scathing essay about how unbearably bad a policy this was. It was a good essay, it convinced a number of people, and I still agree with most of the points in it. But...</p>\r\n<p>...it was completely out of character for me. I'm the sort of person who heckles libertarians with \"Stop whining and just pay your damn taxes!\" Although I acknowledge that many government policies are inefficient, I tend to just note \"Hmmm, that government policy is suboptimal, it would be an interesting mental puzzle to figure out how to fix it\" rather than actually getting angry about it. This Obama proposal was kind of unique in the amount of antipathy it got from me.</p>\r\n<p>So here's my theory. My brain is organized in such a way that I get minimal negative feelings at the idea of money being taken away from me.&nbsp;We can even localize this anatomically - studies show that <a href=\"http://www.dallasnews.com/sharedcontent/dws/dn/opinion/points/stories/DN-lehrer_01edi.State.Edition1.152824f.html \">the insula is the part responsible for sending a pain signal whenever the loss of money is considered</a>. So let's say I have a less-than-normally-active insula in this case. And I get a stronger than normal pain signal from wasted time. This explains why I prefer to donate money than time to my favorite charity.</p>\r\n<p>And it could also explain why I'm not a libertarian. One consequence of libertarianism is that you have every right to feel angry when you're taxed. But I don't feel angry, so the part of my brain that comes up with rational justifications for my feelings doesn't need to come up with a rational justification for why taxation is wrong. I do feel angry about being made to do extra work, so my brain adopted libertarian-type arguments in response to the community service proposal. I predict that if I lived in one of those feudal countries with a work levy rather than a tax, I'd be a libertarian, at least until the local knight heard my opinions and cut off my head.</p>\r\n<p>And I don't mean to pick on libertarians. I know different people have completely different emotional responses to the idea of other people suffering. For example, I can't watch documentaries on (say) the awful lives on mine workers, because they make me too upset. Other people watch them, think they're great documentaries, and then spend the next hour talking about how upset it made them. And other people watch them and then ask what's for dinner. You think that affects people's opinions on socialism much?</p>\r\n<p>Imagine a proposal to institute a tax that would raise money for some effort to help mine workers in some way. Upon hearing of it, different people would have an emotional burst of pain of a certain size at the thought of hearing of a tax, and an emotional burst of pain of a different size at the thought of considering the mine workers. Neither of these bursts of pain would be proportional to the actual size of the problem as measured in some sort of ideal utilon currency (note especially <a href=\"/lw/hw/scope_insensitivity/\">scope insensitivity</a>). But the brain very often makes decisions by comparing those two bursts of pain (see <a href=\"http://www.amazon.com/How-We-Decide-Jonah-Lehrer/dp/0618620117\">How We Decide</a> or just the insula article above) and then comes up with reasons for the decision. So all the important issues like economic freedom and labor policy and maximizing utility and suchwhat get subordinated to whether you're secreting more neurotransmitters in response to money loss or images of sad coal miners.</p>\r\n<p>If this theory were true, we would expect to find neurological differences in people of different political opinions. Ta da! A <a href=\"http://neurocritic.blogspot.com/2008/09/conservatives-are-neurotic-and-liberals.html \">long list of neurological findings that differ in liberals and conservatives</a>. Linking the startle reflex and the disgust reaction to the policies favored by these groups is left as a (very easy) exercise for the reader<sup>3</sup>.</p>\r\n<p>This may require some moderation of our political opinions on issues where we think we're far from the neurological norm. For example, I am no longer so confident that noise is such a big problem for everyone that we would all be better off if there were strict regulations on it. But I hope Bill will consider that some people may be so sensitive to noise that not everyone can just shrug it off, and so there may be a case for at least some regulation of it. Likewise, even though I don't mind taxes too much, if my goal is a society where most people are happy I need to consider that a higher tax rate will decrease other people's happiness much more quickly than it decreases mine.</p>\r\n<p>Other than that, it's just a general message of pessimism. If people's political opinions come partly from unchangeable anatomy, it makes the program of overcoming bias in politics a lot harder, and the possibility of coming up with arguments good enough to change someone else's opinion even more remote.</p>\r\n<p><strong>Footnotes</strong></p>\r\n<p>1) I am suitably ashamed of my appeal to pathos; my only defense is that it is entirely true, that I have only just finished moving, and that this post is hopefully a more&nbsp;appropriate response.</p>\r\n<p>2) Actually, it's more complicated than this, because I agonize over spending money when shopping. I seem to use different thought processes for normal budgeting, and I expect there are many processes going on more complex than just high versus low aversion to money loss.</p>\r\n<p>3) Possibly <em>too </em>easy. It's easy to go from that data to an explanation of why conservatives worry more about terrorism, but then why don't they also worry more about global warming?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FkzScn5byCs9PxGsA": 2, "YQW2DxpZFTrqrxHBJ": 1, "GPhMyXoaHBLyzibxB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pczHfyxmnFhtKthqR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 58, "extendedScore": null, "score": 9.1e-05, "legacy": true, "legacyId": "1315", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 58, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 133, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["B5K3hg8FgrMDHuXjH", "M7rwT264CSYY6EdR3", "baTWMegR42PAsH9qJ", "ZpDnRCeef2CLEFeKM", "2ftJ38y9SRBCBsCzy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-14T05:20:55.442Z", "modifiedAt": null, "url": null, "title": "Why safety is not safe", "slug": "why-safety-is-not-safe", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:32.806Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rwallace", "createdAt": "2009-03-01T16:13:25.493Z", "isAdmin": false, "displayName": "rwallace"}, "userId": "cPhXNeZvnK7LgPMnv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ro6QSQaKdhfpeeGpr/why-safety-is-not-safe", "pageUrlRelative": "/posts/Ro6QSQaKdhfpeeGpr/why-safety-is-not-safe", "linkUrl": "https://www.lesswrong.com/posts/Ro6QSQaKdhfpeeGpr/why-safety-is-not-safe", "postedAtFormatted": "Sunday, June 14th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20safety%20is%20not%20safe&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20safety%20is%20not%20safe%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRo6QSQaKdhfpeeGpr%2Fwhy-safety-is-not-safe%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20safety%20is%20not%20safe%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRo6QSQaKdhfpeeGpr%2Fwhy-safety-is-not-safe", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRo6QSQaKdhfpeeGpr%2Fwhy-safety-is-not-safe", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1404, "htmlBody": "<p><em>June 14, 3009</em><br /><br />Twilight still hung in the sky, yet the Pole Star was visible above the trees, for it was a perfect cloudless evening.<br /><br />\"We can stop here for a few minutes,\" remarked the librarian as he fumbled to light the lamp. \"There's a stream just ahead.\"<br /><br />The driver grunted assent as he pulled the cart to a halt and unhitched the thirsty horse to drink its fill.<br /><br />It was said that in the Age of Legends, there had been horseless carriages that drank the black blood of the earth, long since drained dry. But then, it was said that in the Age of Legends, men had flown to the moon on a pillar of fire. Who took such stories seriously?<br /><br />The librarian did. In his visit to the University archive, he had studied the crumbling pages of a rare book in Old English, itself a copy a mere few centuries old, of a text from the Age of Legends itself; a book that laid out a generation's hopes and dreams, of building cities in the sky, of setting sail for the very stars. Something had gone wrong - but what? That civilization's capabilities had been so far beyond those of his own people. Its destruction should have taken a global apocalypse of the kind that would leave unmistakable record both historical and archaeological, and yet there was no trace. Nobody had anything better than mutually contradictory guesses as to what had happened. The librarian intended to discover the truth.<br /><br />Forty years later he died in bed, his question still unanswered.<br /><br />The earth continued to circle its parent star, whose increasing energy output could no longer be compensated by falling atmospheric carbon dioxide concentration. Glaciers advanced, then retreated for the last time; as life struggled to adapt to changing conditions, the ecosystems of yesteryear were replaced by others new and strange - and impoverished. All the while, the environment drifted further from that which had given rise to <em>Homo sapiens</em>, and in due course one more species joined the billions-long roll of the dead. For what was by some standards a little while, eyes still looked up at the lifeless stars, but there were no more minds to wonder what might have been.<br /><a id=\"more\"></a></p>\n<hr />\n<p><br />Were I to submit the above to a science fiction magazine, it would be instantly rejected. It lacks a satisfying climax in which the hero strikes down the villain, for it has neither hero nor villain. Yet I ask your indulgence for a short time, for it may yet possess one important virtue: realism.<br /><br />The reason we relate to stories with villains is easy enough to understand. In our ancestral environment, if a leopard or an enemy tribesman escaped your attention, failure to pass on your genes was likely. Violence may or may not have been the primary cause of death, depending on time and place; but it was the primary cause <em>that you could do something about</em>. You might die of malaria, you might die of old age, but there was little and nothing respectively that you could do to avoid these fates, so there was no selection pressure to be sensitive to them. There was certainly no selection pressure to be good at explaining the distant past or predicting the distant future.<br /><br />Looked at that way, it's a miracle we possess as much general intelligence as we do; and certainly our minds have achieved a great deal, and promise more. Yet the question lurks in the background: are there phenomena, not in distant galaxies or inside the atomic nucleus beyond the reach of our eyes but in our world at the same scale we inhabit, nonetheless invisible to us because our minds are not so constructed as to perceive them?<br /><br />In search of an answer to that question, we may ask another one: why is this document written in English instead of Chinese?<br /><br />As late as the 15th century, this was by no means predictable. The great civilizations of Europe and China were roughly on par, the former having almost caught up over the previous few centuries; yet Chinese oceangoing ships were arguably still better than anything Europe could build. Fleets under Admiral Zheng He reached as far as East Africa. Perhaps China might have reached the Americas before Europeans did, and the shape of the world might have been very different.<br /><br />The centuries had brought a share of disasters to both continents. War had ravaged the lands, laying waste whole cities. Plague had struck, killing millions, men, women and children buried in mass graves. Shifts of global air currents brought the specter of famine. Civilization had endured; more, it had flourished.<br /><br />The force that put an end to the Chinese arc of progress was deadlier by far than all of these together, yet seemingly intangible as metaphysics. By the 16th century, the fleets had vanished, the proximate cause political; to this day there is no consensus on the underlying factors. It seems what saved Europe was its political disunity. Why was that lost in China? Some writers have blamed flat terrain, which others have disputed; some have blamed rice agriculture and its need for irrigation systems. Likely there were factors nobody has yet understood; perhaps we never will.<br /><br />An entire future that might have been, was snuffed out by some terrible force compared to which war, plague and famine were mere pinpricks - and yet even with the benefit of hindsight, <em>we still don't truly understand what it was</em>.<br /><br />Nor is this an isolated case. From the collapse of classical Mediterranean civilization to the divergent fates of the US and Argentina, whose prospects looked so similar as recently as the early 20th century, we find more terrible than any war or ordinary disaster are forces which operate unseen in plain sight and are only dimly understood even after the fact.<br /><br />The saving grace has always been the outside: when one nation, one civilization, faltered, another picked up the torch and carried on; but with the march of globalization, there may soon be no more outside.<br /><br />Unless of course we create a new one. Within this century, if we continue to make progress as quickly as possible, we may develop the technology to break our confinement, to colonize first the solar system and then the galaxy. And then our kind may truly be immortal, beyond the longest reach of the Grim Reaper, and love and joy and laughter be not outlived by the stars themselves.<br /><br /><em>If</em> we continue to make progress as quickly as possible.<br /><br />Yet at every turn, when risks are discussed, ten voices cry loudly about the violence that may be done with new technology for every one voice that quietly observes that we cannot afford to be without it, and we may not have as much time as we think we have. It is not that anyone is being intentionally selfish or dishonest. The critics believe what they are saying. It is that to the human mind, the dangers of progress are vivid even when imaginary; the dangers of its lack are scarcely perceptible even when real.<br /><br />There are many reasons why we need more advanced technology, and we need it as soon as possible. Every year, more than fifty million people die for its lack, most in appalling suffering. But the one reason above all others is that the window of opportunity we are currently given may be the last step in the Great Filter, that we cannot know when it will close or if it does, whether it will ever open again.<br /><br />Less Wrong is about bias, and the errors to which it leads us. I present then what may be the most lethal of all our biases: that we react instantly to the lesser death that comes in blood and fire, but the greater death that comes in the dust of time, is to our minds invisible.<br /><br />And I ask that you remember, next time you contemplate alleged dangers of technology.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sPpZRaxpNNJjw55eu": 1, "pszEEb3ctztv3rozd": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ro6QSQaKdhfpeeGpr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 81, "baseScore": 54, "extendedScore": null, "score": 8.4e-05, "legacy": true, "legacyId": "1319", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 54, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 118, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-14T22:00:28.697Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes - June 2009", "slug": "rationality-quotes-june-2009", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:59.613Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pjeby", "createdAt": "2009-02-27T23:51:22.854Z", "isAdmin": false, "displayName": "pjeby"}, "userId": "Zzxr5JZpkitaNxL4Q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QCbBEk3WKZqgvLhs2/rationality-quotes-june-2009", "pageUrlRelative": "/posts/QCbBEk3WKZqgvLhs2/rationality-quotes-june-2009", "linkUrl": "https://www.lesswrong.com/posts/QCbBEk3WKZqgvLhs2/rationality-quotes-june-2009", "postedAtFormatted": "Sunday, June 14th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20-%20June%202009&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20-%20June%202009%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQCbBEk3WKZqgvLhs2%2Frationality-quotes-june-2009%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20-%20June%202009%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQCbBEk3WKZqgvLhs2%2Frationality-quotes-june-2009", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQCbBEk3WKZqgvLhs2%2Frationality-quotes-june-2009", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 101, "htmlBody": "<div>\n<p>(Since there didn't seem to be one for this month, and I just ran across a nice quote.)</p>\n<p>A monthly thread for posting any interesting rationality-related quotes you've seen recently on the Internet, or had stored in your quotesfile for ages.</p>\n<ul>\n<li>Please post all quotes separately (so that they can be voted up (or down) separately) unless they are strongly related/ordered.</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote&nbsp;comments/posts on LW/OB - if we do this, there should be a separate thread for it.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QCbBEk3WKZqgvLhs2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 12, "extendedScore": null, "score": 5.01080340988151e-07, "legacy": true, "legacyId": "1320", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 175, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-15T01:53:48.804Z", "modifiedAt": null, "url": null, "title": "Readiness Heuristics", "slug": "readiness-heuristics", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:58.644Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RQNSKwfgzTujT3Znq/readiness-heuristics", "pageUrlRelative": "/posts/RQNSKwfgzTujT3Znq/readiness-heuristics", "linkUrl": "https://www.lesswrong.com/posts/RQNSKwfgzTujT3Znq/readiness-heuristics", "postedAtFormatted": "Monday, June 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Readiness%20Heuristics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReadiness%20Heuristics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRQNSKwfgzTujT3Znq%2Freadiness-heuristics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Readiness%20Heuristics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRQNSKwfgzTujT3Znq%2Freadiness-heuristics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRQNSKwfgzTujT3Znq%2Freadiness-heuristics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1203, "htmlBody": "<p><strong>Followup to:</strong> <a href=\"/lw/aq/how_much_thought/\" target=\"_self\">How Much Thought</a></p>\n<p>A trolley is hurtling towards three people. It will kill them, unless you pull a lever that diverts it onto a different track. However, if you do this, it will hit a small child and kill her. Do you pull the lever, and kill a child, or do nothing, and let three adults die? This question is used to test moral systems and theories; an answer reveals how you value lives and culpability. Or at least, it's supposed to. It's hard to get a straight answer, because everyone wants to take a third option. Why waste time thinking about whose life is more valuable, when you could be looking for a way to save everyone?</p>\n<p>In philosophy, decisions are <a href=\"/lw/f0/hardened_problems_make_brittle_models/\" target=\"_self\">hardened</a> by saying that there <em>are</em> no other options. The real world doesn't work that way. Every decision has an implied extra option: don't decide. Instead, put it off, gather more information, ask a friend, or think more. You might come up with new information that affects the decision or a new option that's better than the old ones. It could be that there is nothing to find, but it takes a lot of thought and investigation to be sure. Or you could find the perfect solution, if only you wait a few more seconds before deciding.</p>\n<p>We can't think about both a decision and a meta-decision at the same time, so we have a set of readiness heuristics to tell us whether we're ready to call our current-best option a final decision. Normal <a href=\"http://wiki.lesswrong.com/wiki/Heuristic\" target=\"_self\">heuristics</a> determine what we decide; if they go awry, we choose poorly. The readiness heuristics determine <em>when</em> and <em>whether</em> we decide. If they go awry, we choose hastily or not at all. Broken readiness heuristics cause decision paralysis, writer's block, and procrastination.</p>\n<p>&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<p>Given a set of known options, decision theory tells us that we can crunch some numbers to get an expected outcome value for each choice, and choose whichever gives the best expectation. We can treat putting off the decision as an extra option, and estimate its utility like we do for the other known options. However, it so different from the other options that it makes more sense to treat it separately. Instead, we split every decision into two: Decide <em>now</em>? And decide <em>what</em>? Formally, you should put off making a decision if the probability of changing your mind times the expected improvement from doing so is greater than the cost of indecision; that is, if</p>\n<p>&nbsp;&nbsp;&nbsp; P(change) * E(improvement) &gt; E(indecision cost)</p>\n<p>To decide how long to run our decision-making processes, we need to consider both the decision itself and the state of our decision-making process. We have almost as many built-in heuristics for this meta-decision as we do for actual decisions. However, note that this is a slight oversimplification; by talking about the probability of changing your mind to a different decision and the expected improvement from doing so, we presuppose that a best candidate has already been selected, which means putting the main decision strictly before the meta-decision of whether to finalize it. It's not possible to finalize a decision without selecting a best candidate, but we may decide a decision is urgent or inconsequential enough to choose a random option.</p>\n<p>&nbsp;</p>\n<p>Indecision cost is normally equal to the time spent, so it can be treated as a constant, except when something is likely to happen soon. Our heuristic representation of 'something is likely to happen soon' is urgency. Failing to finalize a decision normally means doing nothing, which would be very bad if the decision is what to do about an approaching tiger, or what to write in a paper that's coming due. The urgency heuristic is good at dealing with tigers, but bad at dealing with papers. Fortunately, we can hijack the urgency heuristic with thoughts and stimuli that we associate with urgency, such as <a href=\"/lw/fh/willpower_hax_487_execute_by_default/\" target=\"_self\">counting down</a>.</p>\n<p>Our brain's estimate of P(change), the probability that we'll think of or find something that makes us change our mind, comes out as bewilderment. We feel bewildered when a model doesn't fit in our working memory, or leaves important questions unanswered, indicating a high probability of available simplifications, confusion, and missed distinctions, all of which suggest that it's not yet time to decide. However, this heuristic keeps us from reaching any decision when we're given too many options. For example, in a <a href=\"http://www.columbia.edu/~ss957/whenchoice.html\" target=\"_self\">study</a> by Sheena Itengar and Mark Lepper, shoppers were presented with samples of 6 or 24 flavors of jam; of the shoppers who saw 6 flavors, 30% later bought one, while only 3% the shoppers who saw 24 flavors did. Many people take an extremely long time to order in restaurants, for much the same reason; and if you ever start considering the pros, cons and relative priorities of items on a lengthy todo list, then this heuristic will keep you from actually doing any of them for some time.</p>\n<p>To estimate E(improvement), the amount there is to gain by finding a new option or angle, we look at how good the currently available options are. The more readily we can generate objections to the current best option, the more room there is for improvement, and the more likely it is that we have something to gain by resolving those objections. The <a href=\"http://wiki.lesswrong.com/wiki/Affect_heuristic\" target=\"_self\">affect heuristic</a> stands in for the amount of room for improvement; negative affect yields indecision, positive affect yields decisiveness. Unlike bewilderment, however, negative affect doesn't go away with time, and this can lead to hangups. First, when all available options are genuinely bad, it becomes hard to finalize a decision. Even when the options are okay, the <a href=\"http://www.overcomingbias.com/2007/11/halo-effect.html\" target=\"_self\">halo effect</a> can contaminate our intuitive judgment; negative feelings towards the decision itself, or anything related to it, can hijack the mechanisms meant to make us keep looking for better options.</p>\n<p>&nbsp;</p>\n<p>So far, we've considered readiness heuristics in the context of decisions with multiple options, where there is some chance of finding a hidden option or decision-changing insight. For many decisions, like whether to start working on an assignment, there are only two options: the apparent best option (start now) and the default option (procrastinate). In this case, it is tempting to say that the readiness heuristics ought not to apply, since there's no possible benefit to waiting. But this is not quite true; it is possible, for example, that later events might render the assignment moot, or change its parameters. In any case, regardless of whether or not the readiness heuristics <em>ought to</em> apply, we can pretty easily observe that they <em>do</em> apply.</p>\n<p>One consequence of this is that negative affect towards a task, or anything related to it, not only induces procrastination but <em>ought to</em> induce procrastination. (This connection seemed so counterintuitive and so often harmful that I had trouble accepting that it could evolve without a <a href=\"/lw/d4/practical_advice_backed_by_deep_theories/\" target=\"_self\">deep theory</a> to explain it. The connection between affect and procrastination, and how to modify the affect heuristic's output, is the central theme of <a href=\"http://www.dirtsimple.org/\" target=\"_self\">PJ Eby</a>'s writings.)</p>\n<p>Heuristics don't just affect what we decide, but which decisions we make at all, and how long it takes us to make them. There are, almost certainly, not only more heuristics, but more heuristic <em>types</em>, specialized to particular kinds of decisions and meta-decisions, waiting to be discovered.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KoXbd2HmbdRfqLngk": 1, "4R8JYu4QF2FqzJxE5": 1, "LMFBzsJaCRADQqw3F": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RQNSKwfgzTujT3Znq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 22, "extendedScore": null, "score": 3.7e-05, "legacy": true, "legacyId": "543", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YKSwmhGJ3pY9qobnw", "JvQniHSBr6JCbTRnj", "FHukyfMagq4HrBYNt", "LqjKP255fPRY7aMzw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-15T14:30:35.501Z", "modifiedAt": null, "url": null, "title": "The two meanings of mathematical terms", "slug": "the-two-meanings-of-mathematical-terms", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:39.202Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JamesCole", "createdAt": "2009-04-16T08:24:51.843Z", "isAdmin": false, "displayName": "JamesCole"}, "userId": "RMS473ETmWQLaCtmu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bvMDbCfrX8cTs8MC9/the-two-meanings-of-mathematical-terms", "pageUrlRelative": "/posts/bvMDbCfrX8cTs8MC9/the-two-meanings-of-mathematical-terms", "linkUrl": "https://www.lesswrong.com/posts/bvMDbCfrX8cTs8MC9/the-two-meanings-of-mathematical-terms", "postedAtFormatted": "Monday, June 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20two%20meanings%20of%20mathematical%20terms&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20two%20meanings%20of%20mathematical%20terms%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbvMDbCfrX8cTs8MC9%2Fthe-two-meanings-of-mathematical-terms%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20two%20meanings%20of%20mathematical%20terms%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbvMDbCfrX8cTs8MC9%2Fthe-two-meanings-of-mathematical-terms", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbvMDbCfrX8cTs8MC9%2Fthe-two-meanings-of-mathematical-terms", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 776, "htmlBody": "<p>[edit: sorry, the formatting of links and italics in this is all screwy. &nbsp;I've tried editing both the rich-text and the HTML and either way it looks ok while i'm editing it but the formatted terms either come out with no surrounding spaces or two surrounding spaces]</p>\n<p>In the latest Rationality Quotes thread, CronoDAS &nbsp;<a href=\"/lw/10o/rationality_quotes_june_2009/uww\">quoted</a>&nbsp;&nbsp;Paul Graham:&nbsp;</p>\n<blockquote>\n<p>It would not be a bad definition of math to call it the study of terms that have precise meanings.</p>\n</blockquote>\n<div>\n<div>Sort of. I started writing a this as a reply to that comment, but it grew into a post.</div>\n<div>\n<div>We've all heard of the story of &nbsp;<a href=\"http://en.wikipedia.org/wiki/Deferent_and_epicycle\">epicycles</a>&nbsp;&nbsp;and how before Copernicus came along the movement of the stars and planets were explained by the idea of them being attached to rotating epicycles, some of which were embedded within other larger, rotating epicycles (I'm simplifying the terminology a little here).</div>\n<div>\n<div>As we now know, the Epicycles theory was completely wrong. &nbsp;The stars and planets were not at the distances from earth posited by the theory, or of the size presumed by it, nor were they moving about on some giant clockwork structure of rings. &nbsp;</div>\n<div>\n<div>In the theory of Epicycles the terms had precise mathematical meanings. &nbsp;The problem was that what the terms were meant to represent in reality were wrong. &nbsp;The theory involved applied mathematical statements, and in any such statements the terms don&rsquo;t just have their mathematical meaning -- what the equations say about them -- they also have an &lsquo;external&rsquo; meaning concerning what they&rsquo;re supposed to represent in or about reality.</div>\n<div>\n<div>Lets consider these two types of meanings. &nbsp;The mathematical, or &nbsp;&lsquo;internal&rsquo;, meaning of a statement like &lsquo;1 + 1 = 2&rsquo; is very precise. &nbsp;&lsquo;1 + 1&rsquo; is &nbsp;<em>defined&nbsp;</em> as &lsquo;2&rsquo;, so &lsquo;1 + 1 = 2&rsquo; is pretty much &nbsp;<em>the&nbsp;</em> pre-eminent fact or truth. &nbsp;This is why mathematical truth is usually given such an exhaulted place. &nbsp;So far so good with saying that mathematics is the study of terms with precise meanings.&nbsp;</div>\n<div>\n<div>But what if &lsquo;1 + 1 = 2&rsquo; happens to be used to describe something in reality? &nbsp;Each of the terms will then take on a &nbsp;<em>second&nbsp;</em>meaning -- concerning what they are meant to be representing in reality. &nbsp;This meaning lies outside the mathematical theory, and there is no guarantee that it is accurate.</div>\n<div>\n<div>The problem with saying that mathematics is the study of terms with precise meanings is that it&rsquo;s all to easy to take this as trivially true, because the terms obviously have a precise mathematical sense. &nbsp;It&rsquo;s easy to overlook the other type of meaning, to think there is just &nbsp;<em>the&nbsp;</em> meaning of the term, and that there is just <em>the</em> question of the precision of their meanings. &nbsp; This is why we get people saying \"numbers don&rsquo;t lie\". &nbsp;</div>\n<div>\n<div>&lsquo;Precise&rsquo; is a synonym for \"accurate\" and \"exact\" and it is characterized by \"perfect conformity to fact or truth\" (according to WordNet). &nbsp;So when someone says that mathematics is the study of terms with precise meanings, we have a tendancy to take it as meaning it&rsquo;s the study of things that are accurate and true. &nbsp;The problem with that is, mathematical precision clearly does <em>not</em> guarantee the precision -- the accuracy or truth -- of applied mathematical statements, which need to conform with reality.</div>\n<div>\n<div>There are quite subtle ways of falling into this trap of confusing the two meanings. &nbsp;A believer in epicycles would likely have thought that it must have been correct because it gave mathematically correct answers. &nbsp;And &nbsp;<em>it actually did&nbsp;</em>. &nbsp;Epicycles actually did precisely calculate the positions of the stars and planets (not absolutely perfectly, but in principle the theory could have been adjusted to give perfectly precise results). &nbsp;If the mathematics was right, how could it be wrong? &nbsp;</div>\n<div>\n<div>But what the theory was&nbsp;actually&nbsp;calcualting was not the movement of galactic clockwork machinery and stars and planets embedded within it, but the movement of points of light (corresponding to the real stars and planets) as those points of light moved across the sky. &nbsp;Those positions were right but they had it conceptualised all wrong. &nbsp;</div>\n<div>\n<div>Which begs the question of whether it really matters if the conceptualisation is wrong, as long as the numbers are right? &nbsp;Isn&rsquo;t instrumental correctness all that really matters? &nbsp;We might think so, but this is not true. &nbsp;How would Pluto&rsquo;s existence been&nbsp;<a href=\"http://en.wikipedia.org/wiki/Pluto#Discovery\">predicted</a>&nbsp; under an epicycles conceptualisation? &nbsp;How would we have thought about space travel under such a conceptualisation?</div>\n<div>The moral is, when we're looking at mathematical statements, numbers are representations, and representations can lie.</div>\n<div><br /></div>\n</div>\n<div>\n<hr />\n</div>\n</div>\n<div><br /></div>\n<div>If you're interested in knowing more about epicycles and how that theory was overthrown by the Copernican one, Thomas Kuhn's quite readable &nbsp;<em><a href=\"http://www.amazon.com/Copernican-Revolution-Planetary-Astronomy-Development/dp/0674171039\">The Copernican Revolution</a></em>&nbsp;&nbsp;is an excellent resource. &nbsp;</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1, "aa3Qg7Qrp9LM7QMaz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bvMDbCfrX8cTs8MC9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": -1, "extendedScore": null, "score": -4e-06, "legacy": true, "legacyId": "1322", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 80, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-15T19:13:08.743Z", "modifiedAt": null, "url": null, "title": "The Laws of Magic", "slug": "the-laws-of-magic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:32.029Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Annoyance", "createdAt": "2009-02-28T04:06:40.600Z", "isAdmin": false, "displayName": "Annoyance"}, "userId": "yEm6LWatswJYGwBFq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qK4DeEPGv848okKpd/the-laws-of-magic", "pageUrlRelative": "/posts/qK4DeEPGv848okKpd/the-laws-of-magic", "linkUrl": "https://www.lesswrong.com/posts/qK4DeEPGv848okKpd/the-laws-of-magic", "postedAtFormatted": "Monday, June 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Laws%20of%20Magic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Laws%20of%20Magic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqK4DeEPGv848okKpd%2Fthe-laws-of-magic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Laws%20of%20Magic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqK4DeEPGv848okKpd%2Fthe-laws-of-magic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqK4DeEPGv848okKpd%2Fthe-laws-of-magic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 527, "htmlBody": "<blockquote>\n<p>People are always telling you that \"we have always done thus\", and then you find that their \"always\" means a generation or two, or a century or two, at most a millennium or two.&nbsp; Cultural ways and habits are blips compared to the ways and habits of the body, of the race.&nbsp; There really is very little that human beings on our plane have always done, except find food and drink, sing, talk, procreate, nurture the children, and probably band together to some extent.</p>\n<p>- Ursula K. Le Guin, \"Seasons of the Ansarac\", <em>Changing Planes</em></p>\n</blockquote>\n<p>Human cultures vary wildly and dicursively, so it is worth noting which things all known human societies have in common.&nbsp; Several generations ago, anthropologists noted that cultures' beliefs about a suite of concepts crudely describable as 'magic' had certain principles in common.</p>\n<p><a id=\"more\"></a></p>\n<p>Humans seem to naturally generate a series of concepts known as \"Sympathetic Magic\", a host of theories and practices which have certain principles in common, two of which are of overriding importance.&nbsp; These principles can be expressed as follows:&nbsp; the Law of Contagion holds that two things which have interacted, or were once part of a single entity, retain their connection and can exert influence over each other; the Law of Similarity holds that things which are similar or treated the same establish a connection and can affect each other.</p>\n<p>These principles are grossly, obviously, in contradiction with everyday experience.&nbsp; Thusly many cultures restrict the phenomena to which the laws supposedly apply to non-standard, special cases, most especially to individuals who it is asserted have unusual powers or ritual actions that are not commonly replicated in normal life.&nbsp; Examples range from African sorcerers could supposedly bring death to their enemies by stabbing their footprints, the Imperial City of ancient China which was designed to function as a stylized representation of the whole of the country and induce peace as long as the Emperor sat in his throne facing south, and all manner of witchcraft superstitions in which a discarded body part or tiny doll could be used to work magic at a distance.&nbsp;</p>\n<p>Yet the laws themselves do not seem to be doubted, and the phenomena which they supposedly describe were historically (and often presently) widely believed despite a complete lack of actual evidence. There are even technical specialties which are not overtly \"magical\" where the laws were retained.&nbsp; An excellent example of this is herbalism, where a concept named \"The Doctrine of Signatures\" suggested that the form of a plant hinted or indicated what it was useful for.</p>\n<p>Thus the Greeks thought orchids treated impotence and infertility because they vaguely resemble testicles, the Chinese believed ginseng was a potent panacea because its forked root looked somewhat like the human form, and medieval monks thought lungwort's similarity to ulcerated lung tissue meant it was effective against respiratory ailments.&nbsp; In many cases such beliefs persisted for centuries and across civilizations, despite there being absolutely no rational reason to view the beliefs as true.</p>\n<p>Sympathetic magic is just a special case of a wider set of phenomena called <a href=\"http://en.wikipedia.org/wiki/Magical_thinking\">magical thinking</a>.&nbsp; It is important that you familiarize yourself with that collection of ideas before the next post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NSMKfa8emSbGNXRKD": 1, "bmfs4jiLaF6HiiYkC": 1, "gHCNhqxuJq2bZ2akb": 1, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qK4DeEPGv848okKpd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 20, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "1287", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-15T19:35:07.530Z", "modifiedAt": null, "url": null, "title": "Intelligence enhancement as existential risk mitigation", "slug": "intelligence-enhancement-as-existential-risk-mitigation", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:00.396Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ycr3CyrnZLFC7mb5W/intelligence-enhancement-as-existential-risk-mitigation", "pageUrlRelative": "/posts/ycr3CyrnZLFC7mb5W/intelligence-enhancement-as-existential-risk-mitigation", "linkUrl": "https://www.lesswrong.com/posts/ycr3CyrnZLFC7mb5W/intelligence-enhancement-as-existential-risk-mitigation", "postedAtFormatted": "Monday, June 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intelligence%20enhancement%20as%20existential%20risk%20mitigation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntelligence%20enhancement%20as%20existential%20risk%20mitigation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fycr3CyrnZLFC7mb5W%2Fintelligence-enhancement-as-existential-risk-mitigation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intelligence%20enhancement%20as%20existential%20risk%20mitigation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fycr3CyrnZLFC7mb5W%2Fintelligence-enhancement-as-existential-risk-mitigation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fycr3CyrnZLFC7mb5W%2Fintelligence-enhancement-as-existential-risk-mitigation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 862, "htmlBody": "<p>Here at Less Wrong, the Future of Humanity Institute and the Singularity Institute, a recurring theme is trying to steer the future of the planet away from disaster. Often, the best way to avert a particular disaster is quite hard for ordinary people to understand as it requires one to think through an argument in a cool, unemotional way; more often than not the best solution will be lost in a mass of low signal-to-noise ratio squabbling and/or emoting. Whatever the substance of the debate, the overall meta-problem is quite well captured by <a href=\"http://web.maths.unsw.edu.au/~jim/wrongthoughts.html\">this catch</a> from this month's rationality quotes:</p>\n<blockquote>\n<p>\"People are mostly sane enough, of course, in the affairs of common life: the getting of food, shelter, and so on. But the moment they attempt any depth or generality of thought, they go mad almost infallibly.</p>\n</blockquote>\n<p>Attempting to <strong><em>target the meta-problem of getting people to be slightly less mad</em></strong> when it comes to abstract or general thought, especially public policy, is a tempting option. Robin Hanson's <a href=\"http://hanson.gmu.edu/futarchy.html\">futarchy</a> proposal is one way to combat this madness (which it does by removing most people from the policymaking loop). However, another important route to combating human idiocy is to find technologies that make humans smarter. Nick Bostrom <a href=\"http://www.nickbostrom.com/views/science.pdf\">proposed that we should work hard looking for ways to enhance the cognition of research scientists</a>, because even a small increase in the average intelligence of research scientists would increase research output by a large amount, as there are lots of scientists. But improving the decisionmaking process of our society would probably have an even more profound effect; if we could improve the intelligence of the average voter by about one standard deviation, it is easy to speculate that the political decisionmaking process would work much better. For example, understanding simple logical arguments and simple quantitative analyses is stretching the capabilities of someone at IQ 100, so it seems that the marginal effect of overall IQ increases would be quite a large marginal increases in the probability that a politician was incentivized to focus on a logical argument over an emotionally appealing slander as the main focus of their campaign.<a id=\"more\"></a></p>\n<p>As a concrete example, consider the initial US reaction to rising oil prices and the need for US-produced energy: pushing corn ethanol, because a strong farming lobby liked the idea of having extra revenue. Now, if the *average voter* could understand the concept of photosynthetic efficiency, and could understand a simple numerical calculation showing how inefficient corn is at converting solar energy to stored energy in ethanol, this policy choice would have been dead in the water. But the average voter cannot do simple physics, whereas they can understand the emotional appeal of \"support our local farmers!\". Even today, there are still politicians <a href=\"http://www.startribune.com/politics/state/48032732.html?elr=KArksLckD8EQDUoaEyqyP4O:DW3ckUiD3aPc:_Yyc:aUUsA\">who defend corn ethanol because they want to pander to local interest groups</a>. Another concrete example is some of the more useless responses that the UK public has been engaging in - and being encouraged to engage in - to prevent global warming. People <a href=\"http://news.bbc.co.uk/1/hi/business/6076658.stm\">were encouraged</a> to unplug their mobile phone chargers when the chargers weren't being used. David McKay had to <a href=\"http://www.inference.phy.cam.ac.uk/sustainable/charger/\">wage a personal war against such idiocy</a> - see <a href=\"http://www.guardian.co.uk/environment/cif-green/2009/apr/29/renewable-energy-david-mackay\">this Guardian article</a>. The universal response to my criticism of people advocating this was \"it all adds up!\". I quote:</p>\n<blockquote>\n<p>There's a lack of numeracy in the public discussion of energy. Where people do use numbers, they select them to sound big and score points in arguments, rather than to aid thoughtful discussion.</p>\n</blockquote>\n<p>Toby Ord has <a href=\"http://www.givingwhatwecan.org/\">a project on efficient charity</a>, he has worked out that the difference in outcomes per dollar for alleviating human suffering in Africa can vary by 3 orders of magnitude. But most people in the developed world don't know what an \"order of magnitude\" is, or why it is a useful concept. This efficient charity concept demonstrated that the derivative</p>\n<p style=\"padding-left: 270px;\">d(Outcomes)/d(Average IQ)</p>\n<p>may be extremely large, and may be subject to powerful threshold effects. In this case, there is probably an average IQ threshold above which the average person can easily understand the concept of efficient charity, and thus all the money gets given to the most efficient charities, and the amount of suffering-alleviation in Africa goes up by a factor of 1000, even though the average IQ of the donor community may only have jumped from 100 to 140, say.</p>\n<p>It may well be the case that finding a cognitive enhancer suitable for general use is the best way to tackle the diverse array of risks we face. People with enhanced IQ would also probably find it easier (and be more willing) to absorb cognitive biases material; to see this, try and explain the concept of \"cognitive biases\" to someone who is unlucky enough to be of below average IQ, and then go an explain it to someone who is smarter than you. It is certainly the case that even people of below average IQ *do sometimes*, in favourable circumstances, take note of quantitative rational arguments, but in the maelstrom of politics such quantitative analyses get eaten alive by more emotive arguments like \"SUPPORT OUR FARMERS!\" or \"SUPPORT OUR TROOPS!\" or \"EVOLUTION IS ONLY A THEORY!\" or \"IT ALL ADDS UP!\".<a href=\"http://www.nickbostrom.com/papers/converging.pdf\"><br /></a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ac84EpK6mZbPLzmqj": 1, "xexCWMyds6QLWognu": 1, "Rz5jb3cYHTSRmqNnN": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ycr3CyrnZLFC7mb5W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 21, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "1317", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 244, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-16T17:50:07.749Z", "modifiedAt": null, "url": null, "title": "Rationalists lose when others choose", "slug": "rationalists-lose-when-others-choose", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:07.551Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YmYvZziDt3w4kaR8N/rationalists-lose-when-others-choose", "pageUrlRelative": "/posts/YmYvZziDt3w4kaR8N/rationalists-lose-when-others-choose", "linkUrl": "https://www.lesswrong.com/posts/YmYvZziDt3w4kaR8N/rationalists-lose-when-others-choose", "postedAtFormatted": "Tuesday, June 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationalists%20lose%20when%20others%20choose&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationalists%20lose%20when%20others%20choose%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYmYvZziDt3w4kaR8N%2Frationalists-lose-when-others-choose%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationalists%20lose%20when%20others%20choose%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYmYvZziDt3w4kaR8N%2Frationalists-lose-when-others-choose", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYmYvZziDt3w4kaR8N%2Frationalists-lose-when-others-choose", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1370, "htmlBody": "<p>At various times, we've argued over whether rationalists always win.&nbsp; I posed <a href=\"http://www.overcomingbias.com/2009/03/augustines-paradox.html\">Augustine's paradox of optimal repentance</a> to argue that, in some situations, rationalists lose.&nbsp; One criticism of that paradox is that its strongest forms posit a God who penalizes people for being rational.&nbsp; My response was, So what?&nbsp; Who ever said that nature, or people, don't penalize rationality?</p>\n<p>There are instances where nature penalizes the rational.&nbsp; For instance, revenge is irrational, but being thought of as someone who would take revenge gives advantages.<sup>1<a id=\"more\"></a></sup></p>\n<p>EDIT:&nbsp; Many many people immediately jumped on this, because revenge is rational in repeated interactions.&nbsp; Sure.&nbsp; Note the \"There are instances\" at the start of the sentence.&nbsp; If you admit that someone, somewhere, once faced a one-shot revenge problem, then cede the point and move on.&nbsp; It's just an example anyway.</p>\n<p>Here's another instance that more closely resembles the God who punishes rationalism, in which people deliberately punish rational behavior:</p>\n<p>If rationality means optimizing expected utility, then both social pressures and evolutionary pressures tend, on average, to bias us towards altruism.&nbsp; (I'm going to assume you know this literature rather than explain it here.)&nbsp; An employer or a lover would both rather have someone who is irrationally altruistic.&nbsp; This means that, on this particular (and important) dimension of preference, rationality correlates with undesirability.<sup>2</sup></p>\n<p>&lt;ADDED&gt;: I originally wrote \"optimizing expected selfish utility\", merely to emphasize that an agent, rational or not, tries to maximize its own utility function.&nbsp; I do not mean that a rational agent <em>appears</em> selfish by social standards.&nbsp; A utility-maximizing agent is selfish <em>by definition</em>, because its utility function is its own.&nbsp; Any altruistic behavior that results, happens only out of self-interest.&nbsp; You may argue that pragmatics argue against this use of the word \"selfish\" because it thus adds no meaning.&nbsp; Fine.&nbsp; I have removed the word \"selfish\".</p>\n<p>However, it really doesn't matter.&nbsp; Sure, it is possible to make a rational agent that acts in ways that seem unselfish. Irrelevant.&nbsp; Why would the big boss settle for \"unselfish\" when he can get \"self-sacrificing\"?&nbsp; It is often possible to find an <em>irrational </em>agent that acts more in your interests, than any rational agent will.&nbsp; The rational agent aims for equitable utility deals.&nbsp; The irrational agent can be inequitable in your favor.</p>\n<p>This whole barrage of attacks on using the world 'selfish' are yet again missing the point.&nbsp; If you read the entire post, you'll see that it doesn't matter if you think that rational agents are selfish, or that they can reciprocate.&nbsp; You just have to admit that most persons A would rather deal with an agent B having an altruistic bias, or a bias towards A's utilities, than an agent having no such bias.&nbsp; The level of selfishness/altruism of the posited rational agent is <em>irrelevant, </em>because adding a bias towards person A's utility is <em>always better</em> for person A.&nbsp; Comparing \"rational unbiased person\" to \"altruistic idiot\" is not the relevant comparison here.&nbsp; Compare instead \"person using decision function F with no bias\" vs. \"person using decision function F with excess altruism\".<sup>3</sup></p>\n<p>(Also note that, in the fMRI example, people <em>don't get to see your utility function.</em>&nbsp; They can't tell that you have a wonderful&nbsp; Yudkowskian utility function that will make you reliable.&nbsp; They can only see that you don't have the bias most people do that would make most people a better employee.)</p>\n<p>The real tricky point of this argument is whether you can define \"irrational altruism\" in a way that doesn't simply mean \"utility function that values altruism\".&nbsp; You could rephrase \"Choice by others encourages bias toward altruism\" as \"Choice by others selects for utility functions that value altruism highly\".</p>\n<p>Does an ant have an irrationally high bias towards altruism?&nbsp; It may make more sense to say that an ant is less of an invididual, and more of a subroutine, than a human is.&nbsp; So it is perfectly all right with me if you prefer to say that these forces select for valuing altruism, rather than saying that they select for bias.&nbsp; The outcome is the same either way:&nbsp; When one agent gets to choose what other agents succeed, <em>and</em> that agent can observe their biases and/or decision functions, those other agents are under selection pressure to become less like individuals and more like subroutines of the choosing agent.&nbsp; You can call this \"altruistic bias\" or you can call it \"less individuality\".</p>\n<p>&lt;/ADDED&gt;</p>\n<p>There are a lot of other situations where one person chooses another person, and they would rather choose someone who is biased, in ways encouraged by society or by genetics, than someone more rational.&nbsp; When giving a security clearance, for example, you would rather give it to someone who loved his country emotionally, than to someone who loved his country rationally; the former is more reliable, while the rational person may suddenly reach an opposite conclusion on learning one new fact.</p>\n<p>It's hard to tell how altruistic someone is.&nbsp; But the May 29, 2009 issue of Science has an article called \"The Computation of Social Behavior\".&nbsp; It's extremely skimpy on details, especially for a 5-page article; but the gist of it is that they can use functional magnetic resonance imaging to monitor someone making decisions, and extract some of that person's basic decision-making parameters.&nbsp; For example (they mention this, although it isn't clear whether they can extract this particular parameter), their degree of altruism (the value they place on someone else's utility vs. their own utility).&nbsp; Unlike a written exam, the fMRI exam can't be faked; your brain will reveal your true parameters even if you try to lie and game the exam.</p>\n<p>So, in the future, being rational may make you unemployable and unlovable, because you'll be unable to hide your rationality.</p>\n<p>Or maybe it already does?</p>\n<p>ADDED:</p>\n<p>Here is the big picture:&nbsp; The trend in the future is likely to be one of greater and greater transparency of every agent's internal operations, whether this is via fMRI or via exchanging source code.&nbsp; Rationality means acting to achieve <em>your </em>goals.&nbsp; There will almost always be other people who are more powerful than you and who have resources that you need, and they don't want you to achieve your goals.&nbsp; They want you to achieve <em>their </em>goals.&nbsp; They will have the power and the motive to select against rationality (or to avoid building it in in the first place.)</p>\n<p>All our experience is with economic and behavioral models that assume independent self-interested agents.&nbsp; In a world where powerful people can examine the utility functions of less-powerful people, and reward them for rewriting their utility functions (or just select ones with utility functions that are favorable to the powerful people, and hence irrational), then having rational, self-interested agents is not the equilibrium outcome.</p>\n<p>In a world in which agents like you or I are <em>manufactured</em> to meet the needs of more powerful agents, even more so.</p>\n<p>You may claim that an agent can be 'rational' while trying to attain the goals of another agent.&nbsp; I would instead say that it isn't an agent anymore; it's just a subroutine.</p>\n<p>The forces I am discussing in this post try to turn agents into subroutines.&nbsp; And they are getting stronger.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup> Newcomb's paradox is, strangely, more familiar to LW readers.&nbsp; I suggest replacing discussions of one-boxing by discussions of taking revenge; I think the paradoxes are very similar, but the former is more confusing and further-removed from reality.&nbsp; Its main advantage is that it prevents people from being distracted by discussing ways of fooling people about your intentions - which is not the solution evolution chose to that problem.</p>\n<p><sup>2</sup> I'm making basically the same argument that Christians make when they say that atheists can't be trusted.&nbsp; Empirical rejection of that argument does not apply to mine, for two reasons:</p>\n<ol>\n<li>Religions operate on pure rewards-based incentives, and hence destroy the altruistic instinct; therefore, I intuit that religious people have a disadvantage rather than an advantage compared to altruists WRT altruism.</li>\n<li>Religious people <em>can</em> sometimes be trusted more than atheists; the problem is that some of the things they can be trusted to do are crazy.</li>\n</ol>\n<p><sup>3</sup> This is something LW readers do all the time:&nbsp; Start reading a post, then stop in the middle and write a critical response addressing one perceived error whose truth or falsity is actually irrelevant to the logic of the post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3QnDqGSdRMA5mdMM6": 1, "fihKHQuS5WZBJgkRm": 1, "dBPou4ihoQNY4cquv": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YmYvZziDt3w4kaR8N", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": -9, "extendedScore": null, "score": -1.1e-05, "legacy": true, "legacyId": "1324", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-16T21:16:03.029Z", "modifiedAt": null, "url": null, "title": "Ask LessWrong: Human cognitive enhancement now?", "slug": "ask-lesswrong-human-cognitive-enhancement-now", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:00.547Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AQ86BPLxNXs82cxcr/ask-lesswrong-human-cognitive-enhancement-now", "pageUrlRelative": "/posts/AQ86BPLxNXs82cxcr/ask-lesswrong-human-cognitive-enhancement-now", "linkUrl": "https://www.lesswrong.com/posts/AQ86BPLxNXs82cxcr/ask-lesswrong-human-cognitive-enhancement-now", "postedAtFormatted": "Tuesday, June 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ask%20LessWrong%3A%20Human%20cognitive%20enhancement%20now%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAsk%20LessWrong%3A%20Human%20cognitive%20enhancement%20now%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAQ86BPLxNXs82cxcr%2Fask-lesswrong-human-cognitive-enhancement-now%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ask%20LessWrong%3A%20Human%20cognitive%20enhancement%20now%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAQ86BPLxNXs82cxcr%2Fask-lesswrong-human-cognitive-enhancement-now", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAQ86BPLxNXs82cxcr%2Fask-lesswrong-human-cognitive-enhancement-now", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 105, "htmlBody": "<p>Transhumanists have high hopes for enhancing human cognitive abilities in the future. But what realistic steps can we take to enhance them now? On the one hand <a href=\"http://en.wikipedia.org/wiki/Flynn_effect\">Flynn effect</a> suggests IQ (which is a major factor in human cognition) can be increased a lot with current technology, on the other hand <a href=\"http://radian.org/notebook/wp-content/uploads/2009/04/brain-botox.pdf\">review of existing drugs</a> seems rather pessimistic - they seem to have minor positive effect on low performers, and very little effect on high performers, what means they're mostly of therapeutic not enhancing use.</p>\n<p>So, fellow rationalists, how can we enhance our cognition now? Solid research especially welcome, but consistent anecdotal evidence is also welcome.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ookMdjJQMopfLG3wZ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AQ86BPLxNXs82cxcr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 16, "extendedScore": null, "score": 5.01519327820039e-07, "legacy": true, "legacyId": "1326", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 73, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-17T15:21:31.616Z", "modifiedAt": null, "url": null, "title": "Don't Count Your Chickens...", "slug": "don-t-count-your-chickens", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:02.609Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "thomblake", "createdAt": "2009-02-27T15:35:08.282Z", "isAdmin": false, "displayName": "thomblake"}, "userId": "zCHE6bXWKB6kfJsJS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GtjRZhiddn6ziTERD/don-t-count-your-chickens", "pageUrlRelative": "/posts/GtjRZhiddn6ziTERD/don-t-count-your-chickens", "linkUrl": "https://www.lesswrong.com/posts/GtjRZhiddn6ziTERD/don-t-count-your-chickens", "postedAtFormatted": "Wednesday, June 17th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Don't%20Count%20Your%20Chickens...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADon't%20Count%20Your%20Chickens...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGtjRZhiddn6ziTERD%2Fdon-t-count-your-chickens%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Don't%20Count%20Your%20Chickens...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGtjRZhiddn6ziTERD%2Fdon-t-count-your-chickens", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGtjRZhiddn6ziTERD%2Fdon-t-count-your-chickens", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 64, "htmlBody": "<p>A blog post by Derek Sivers links to evidence that stating one's goals makes one less likely to accomplish them.</p>\n<p>Excerpt:</p>\n<blockquote>\n<p>Announcing your plans to others satisfies your self-identity just enough that you're less motivated to do the hard work needed.</p>\n</blockquote>\n<p>Link: <a title=\"Shut up! Announcing your plans makes you less motivated to accomplish them.\" href=\"http://sivers.org/zipit\">Shut up! Announcing your plans makes you less motivated to accomplish them.</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4R8JYu4QF2FqzJxE5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GtjRZhiddn6ziTERD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 4, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "1327", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-17T16:08:29.757Z", "modifiedAt": "2020-10-05T16:29:04.787Z", "url": null, "title": "Applied Picoeconomics", "slug": "applied-picoeconomics", "viewCount": null, "lastCommentedAt": "2022-03-31T11:41:43.375Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NjzBrtvDS4jXi5Krp/applied-picoeconomics", "pageUrlRelative": "/posts/NjzBrtvDS4jXi5Krp/applied-picoeconomics", "linkUrl": "https://www.lesswrong.com/posts/NjzBrtvDS4jXi5Krp/applied-picoeconomics", "postedAtFormatted": "Wednesday, June 17th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Applied%20Picoeconomics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AApplied%20Picoeconomics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNjzBrtvDS4jXi5Krp%2Fapplied-picoeconomics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Applied%20Picoeconomics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNjzBrtvDS4jXi5Krp%2Fapplied-picoeconomics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNjzBrtvDS4jXi5Krp%2Fapplied-picoeconomics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1346, "htmlBody": "<p><strong>Related to: </strong><a href=\"/lw/6c/akrasia_hyperbolic_discounting_and_picoeconomics/\">Akrasia, Hyperbolic Discounting, and Picoeconomics</a>,<strong>&nbsp; </strong><a href=\"/lw/co/fix_it_and_tell_us_what_you_did/\">Fix It And Tell Us What You Did</a><br /><br />A while back, ciphergoth posted an article on \"picoeconomics\", the theory that akrasia could be partially modeled as bargaining between present and future selves. I think the model is incomplete, because it doesn't explain how the analogy is instantiated in the real world, and I'd like to investigate that further sometime<sup>1</sup> - but it's a good first-order approximation.<br /><br />For those of you too lazy to <a href=\"http://www.picoeconomics.com/breakdown.htm\">read the article</a> (come on! It has pictures of naked people! Well, one naked person. Suspended from a graph of a hyperbolic curve) Ainslie argues that \"intertemporal bargaining\" is one way to overcome preference reversal. For example, an alcoholic has two conflicting preferences: right now, he would rather drink than not drink, but next year he would rather be the sort of person who never drinks than remain an alcoholic. But because his brain uses hyperbolic discounting, a process that pays more attention to his current utility than his future utility, he's going to hit the whiskey.<br /><br />This sticks him in a sorites paradox. Honestly, it's not going to make much of a difference if he has one more drink, so why not hit the whiskey? Ainslie's answer is that he should set a hard-and-fast rule: \"I will never drink alcohol\". Following this rule will cure his alcoholism and help him achieve his dreams. He now has a very high preference for following the rule; a preference hopefully stronger than his current preference for whiskey.<br /><br />Ainslie's other point is that this rule needs to really be hard-and-fast. If his rule is \"I will drink less whiskey\", then that leaves it open for him to say \"Well, I'll drink some whiskey now, and&nbsp;none later; that counts as 'less'\", and then the whole problem comes back just as bad as before. Likewise, if he says \"It's my birthday, I'll let myself break the rule just this once,\" then soon he's likely to be saying \"It's the Sunday before Cinco de Mayo, this calls for a celebration!\" Ainslie has some much more formal and convincing ways of framing this, which is why you should read the article instead of just trusting this summary.<br /><br />The stuff by Ainslie I read (I didn't spring for any of his dead-tree books) didn't offer any specific pointers for increasing your willpower<sup>2</sup>, but it's pretty easy to read between the lines and figure out what applied picoeconomics ought to look like. In the interest of testing a scientific theory, not to mention the ongoing effort to take control of my own life, I've been testing picoeconomic techniques for the last two months.</p>\r\n<p><a id=\"more\"></a></p>\r\n<p>The essence of picoeconomics is formally binding yourself to a rule with as few loopholes as possible. So the technique I decided to test<sup>3</sup> was to write out an oath detailing exactly what I wanted to do, list in nauseating detail all of the conditions under which I could or could not be released from this oath, and then bind myself to it, with the knowledge that if I succeeded I would have a great method of self-improvement and if I failed I would be dooming myself to a life of laziness forever (Ainslie's theories suggest that exaggeration is good in this case).<br /><br />I chose a few areas of my life that I wanted to improve, of which the only one I want to mention in public is my poor study habits. I decided that I wanted to increase my current study load from practically never looking at a book after school got out, up to two hours a day.<br /><br />I wrote down - yes, literally wrote down - an oath in which I swore to study for two hours a day. I detailed exactly the conditions that would count as \"studying\" - no watching TV with an open book placed in my lap, for example.<br /><br />I also included several release valves. The theory behind this was that if I simply broke the oath outright, the oath would no longer be credible and would lose its power (again, see Ainslie's article), and there would be some point where I would be absolutely compelled to break the oath (for example, if a member of my family is in the emergency room, I refuse to read a book for an hour and a half before going to check up on them). I gave myself a whole bunch of cases in which I would be allowed to not study, guilt-free, and allowed myself five days a month when I could just take off studying for no reason (too tired, maybe). I also limited the original oath to a month, so that if it didn't work I could adjust it without completely destroying the effectiveness of the oath forever. Finally, I swore the oath in a ceremonial fashion, calling upon various fictional deities for whom I have great respect.<br /><br />One month later, I find that I kept to the terms of the oath exactly, which is no small achievement for me since my previous resolutions to study more have ended in apathy and failure. On an introspection level, the need to study each day felt exactly like the need to complete a project with a deadline, or to show up for work when the boss was expecting you. My brain clearly has different procedures for dealing with vague responsibilities it can weasel out of, and serious responsibilities it can't, and the oath served to stick studying on the \"serious\" side of the line.<br /><br />I am suitably cautious about <a href=\"/lw/9v/beware_of_otheroptimizing/\">other-optimizing</a> and <a href=\"/lw/dr/generalizing_from_one_example/\">the typical mind fallacy</a>, so I don't promise the same method will work for you. But I'd be interested to see if it did<sup>4</sup>. I'd be especially interested if everyone who tried it would post, right now, what they're trying so that in a month or so we can come back and see how many people kept their oath without having too much response bias.</p>\r\n<p>&nbsp;</p>\r\n<p><strong>Footnotes</strong></p>\r\n<p><strong>1: </strong>I'm split on the value of picoeconomic theory. A lot of it seems either common-sense if taken as a vague model or metaphor, or obviously false if taken literally. But sometimes it's very good to have a formal model for common sense, and I'm optimistic about someone developing a more literal version of it that explains what's actually going on inside someone's head.<br /><br /><strong>2: </strong>Ciphergoth, as far as you know does Ainslie ever start making practical suggestions based on his theory anywhere, or does he leave it entirely as an exercise for the reader?</p>\r\n<p><strong>3: </strong>I don't read a lot of stuff on productivity, so I might be reinventing the wheel here.</p>\r\n<p><strong>4: </strong>For people trying this, a few suggestions and caveats from my experience:</p>\r\n<ol>\r\n<li>Do NOT make the oath open-ended. Set a time limit, and if you're happy at the end of that time limit, set another time limit.</li>\r\n<li>Don't overdo it; this only works if you really do want the goal you're after more than you want momentary pleasure, people are notoriously bad at knowing what they want, and if you break an oath once you've set a precedent and it'll be harder to keep a better-crafted oath next time. If I'd sworn six hours of studying a day, no way I'd have been able to keep it.</li>\r\n<li>Set release valves.</li>\r\n<li>Do something extremely measurable in which success or failure is a very yes-or-no affair, like how much time you do something for. Saying \"study more\" or \"eat better\" will be completely useless.</li>\r\n<li>Read the article so you know the theory behind it and especially why it's important to always keep the rules.</li>\r\n<li>Don't just think up the oath and figure it's in effect. Write it down and swear it aloud, more or less ceremonially, depending on your taste for drama and ritual.</li>\r\n<li>Seriously, don't overdo it. <a href=\"http://en.wikipedia.org/wiki/Ego_depletion\">Ego depletion</a> and all that.</li>\r\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YrLoz567b553YouZ2": 1, "r7qAjcbfhj2256EHH": 2, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NjzBrtvDS4jXi5Krp", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 52, "baseScore": 60, "extendedScore": null, "score": 0.0001, "legacy": true, "legacyId": "529", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 60, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 87, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["geNZ6ZpfFce5intER", "N75n9scjdvvvMN627", "6NvbSwuSAooQxxf7f", "baTWMegR42PAsH9qJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2009-06-17T16:08:29.757Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-18T03:02:37.973Z", "modifiedAt": null, "url": null, "title": "Representative democracy awesomeness hypothesis", "slug": "representative-democracy-awesomeness-hypothesis", "viewCount": null, "lastCommentedAt": "2017-06-17T03:51:59.474Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/T56E8MHqpcuc696Dx/representative-democracy-awesomeness-hypothesis", "pageUrlRelative": "/posts/T56E8MHqpcuc696Dx/representative-democracy-awesomeness-hypothesis", "linkUrl": "https://www.lesswrong.com/posts/T56E8MHqpcuc696Dx/representative-democracy-awesomeness-hypothesis", "postedAtFormatted": "Thursday, June 18th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Representative%20democracy%20awesomeness%20hypothesis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARepresentative%20democracy%20awesomeness%20hypothesis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT56E8MHqpcuc696Dx%2Frepresentative-democracy-awesomeness-hypothesis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Representative%20democracy%20awesomeness%20hypothesis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT56E8MHqpcuc696Dx%2Frepresentative-democracy-awesomeness-hypothesis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT56E8MHqpcuc696Dx%2Frepresentative-democracy-awesomeness-hypothesis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 487, "htmlBody": "<p>I have hypothesis that <a href=\"/lw/10h/if_it_looks_like_utility_maximizer_and_quacks/\">utility maximization is always a second order process</a> - there's always some underlying selection process with its fitness, and only because it promotes traits that make agents agents act in a way that best approximates utility maximizing, adaptation executers seem to us like utility maximizers.</p>\n<p>Now let's apply this to political systems:</p>\n<ul>\n<li>In most political systems ruling elites could only be removed by either military intervention from the outside, or revolution from inside. That's the fitness function. So traits that would be promoted were those related to having strong and effective military, and skillful diplomacy to avoid external threats; keeping population in conditions bearable enough to prevent a revolt; strong policing and effective divide&amp;conquer approach to keep any revolt from succeeding.</li>\n<li>In modern two-party (or multi-party with two big possible coalitions) representative democracy economic performance of the last term can predict voting patterns very well - if people's incomes and well-being are improving fast enough, people will vote for the ruling party. If they're deteriorating, or not improving fast enough, people will vote for the opposition (<a href=\"http://douglas-hibbs.com/Elections2004-00-96-92/election2004.pdf\">Bread and Peace Model</a>). This means parties' fitness function is strongly linked to short term economic performance, and therefore policies that improve people's well-being will be selected.</li>\n<li>In direct democracy with most important decisions being taken directly by voters, there's no selection process - so they will be as unsuccessful as dictatorships. This is extremely surprising prediction to me, but remarkably bad economic performance of California seems to confirm this.</li>\n<li>I guess that in single party regimes, like those of post-Stalin communist countries party leaders would be removed in case of highly unsuccessful performance. This would produce weaker selection than in two-party representative democracies, but stronger selection than in pure dictatorships, or direct democracies.</li>\n<li>Libertarian idea of free competition between political systems never existed, so there is no need to discuss it.</li>\n</ul>\n<p>There are also some hints how to design better representative democracy:</p>\n<ul>\n<li>The closest a representative democracy is to proportional one-person one-vote system, be stronger the signal. So countries with highly distorting electoral systems like US would have weaker selection than countries with straight-forward electoral systems like Israel.</li>\n<li>If people vote for reasons not related to improvement in their well-being, like for ethnicity, religion, or ideology, it weakens the signal.</li>\n<li>A mix of direct democracy to resolve non-economical issues (like gay marriage, abortion), and representative democracy to actually run the country (and politicians wouldn't need to bother with ideology) might work even better. It seems vaguely like what Ireland is doing, and they also seem to be performing very well economically, is it just a coincidence?</li>\n</ul>\n<p>I used to think that direct democracy would be a major improvement relative to what we have now, but this analysis suggests that representative democracy (with small bits of direct democracy thrown in) should work much better.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FkzScn5byCs9PxGsA": 1, "5f5c37ee1b5cdee568cfb16a": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "T56E8MHqpcuc696Dx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 4, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "1328", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["i6npKoxQ2QALPCbcP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-18T04:11:52.445Z", "modifiedAt": null, "url": null, "title": "The Physiology of Willpower", "slug": "the-physiology-of-willpower", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:39.402Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pjeby", "createdAt": "2009-02-27T23:51:22.854Z", "isAdmin": false, "displayName": "pjeby"}, "userId": "Zzxr5JZpkitaNxL4Q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ob6FdjoXnirRkodNs/the-physiology-of-willpower", "pageUrlRelative": "/posts/ob6FdjoXnirRkodNs/the-physiology-of-willpower", "linkUrl": "https://www.lesswrong.com/posts/ob6FdjoXnirRkodNs/the-physiology-of-willpower", "postedAtFormatted": "Thursday, June 18th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Physiology%20of%20Willpower&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Physiology%20of%20Willpower%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fob6FdjoXnirRkodNs%2Fthe-physiology-of-willpower%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Physiology%20of%20Willpower%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fob6FdjoXnirRkodNs%2Fthe-physiology-of-willpower", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fob6FdjoXnirRkodNs%2Fthe-physiology-of-willpower", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 226, "htmlBody": "<p><a href=\"http://psr.sagepub.com/cgi/reprint/11/4/303.pdf\">This paper (PDF)</a><sub>1</sub> looks more than a little interesting:</p>\n<blockquote>\n<p>Past research indicates that self-control relies on some sort of limited energy source. This review suggests that blood glucose is one important part of the energy source of selfcontrol. Acts of self-control deplete relatively large amounts of glucose. Self-control failures are more likely when glucose is low or cannot be mobilized effectively to the brain (i.e., when insulin is low or insensitive). Restoring glucose to a sufficient level typically improves self-control. Numerous self-control behaviors fit this pattern, including controlling attention, regulating emotions, quitting smoking, coping with stress, resisting impulsivity, and refraining from criminal and aggressive behavior. Alcohol reduces glucose throughout the brain and body and likewise impairs many forms of self-control. Furthermore, self-control failure is most likely during times of the day when glucose is used least effectively. Self-control thus appears highly susceptible to glucose. Self-control benefits numerous social and interpersonal processes. Glucose might therefore be related to a broad range of social behavior.</p>\n</blockquote>\n<p>I find this interesting, in that the days I get less work done (due to e.g. spending more time on Less Wrong) are often days when I don't eat breakfast right away, and am generally undereating (like today).</p>\n<p><strong>References</strong></p>\n<p>1. Matthew T. Gailliot, Roy F. Baumeister. (2007) <a href=\"http://psr.sagepub.com/cgi/content/abstract/11/4/303\">The Physiology of Willpower: Linking Blood Glucose to Self-Control</a>. Personality and Social Psychology Review, Vol. 11, No. 4, 303-327</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ksdiAMKfgSyEeKMo6": 1, "YrLoz567b553YouZ2": 1, "rfZ6DY88ApBDXFpyW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ob6FdjoXnirRkodNs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 25, "extendedScore": null, "score": 4e-05, "legacy": true, "legacyId": "1329", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-18T10:06:12.174Z", "modifiedAt": null, "url": null, "title": "Time to See If We Can Apply Anything We Have Learned", "slug": "time-to-see-if-we-can-apply-anything-we-have-learned", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:00.445Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelVassar", "createdAt": "2009-02-28T07:34:32.206Z", "isAdmin": false, "displayName": "MichaelVassar"}, "userId": "PvhrBWHzCGKRkwKcd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7Em9eRh9Cwphg3qhS/time-to-see-if-we-can-apply-anything-we-have-learned", "pageUrlRelative": "/posts/7Em9eRh9Cwphg3qhS/time-to-see-if-we-can-apply-anything-we-have-learned", "linkUrl": "https://www.lesswrong.com/posts/7Em9eRh9Cwphg3qhS/time-to-see-if-we-can-apply-anything-we-have-learned", "postedAtFormatted": "Thursday, June 18th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Time%20to%20See%20If%20We%20Can%20Apply%20Anything%20We%20Have%20Learned&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATime%20to%20See%20If%20We%20Can%20Apply%20Anything%20We%20Have%20Learned%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Em9eRh9Cwphg3qhS%2Ftime-to-see-if-we-can-apply-anything-we-have-learned%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Time%20to%20See%20If%20We%20Can%20Apply%20Anything%20We%20Have%20Learned%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Em9eRh9Cwphg3qhS%2Ftime-to-see-if-we-can-apply-anything-we-have-learned", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Em9eRh9Cwphg3qhS%2Ftime-to-see-if-we-can-apply-anything-we-have-learned", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 323, "htmlBody": "<p><span style=\"font-family: Arial; font-size: 12px; white-space: pre-wrap;\">It seems to me that this blog has just reached it's first real crisis. </span></p>\n<p>&nbsp;</p>\n<p><span style=\"font-family: Arial; font-size: 12px; white-space: pre-wrap;\">Three people are announcing three apparently opposed beliefs with substantial real expected consequences and yet no-one has yet spoken, or it seems to me implied, the key slogan... \"LETS USE SCIENCE!\" or, as hubristic Bayesian wannabes, not invoked Bayes as an idol to swear by, but rather said \"LETS USE HUMANE REFLECTIVE DECISION THEORY, THE QUANTITATIVELY UNKNOWN BUT QUALITATIVELY INTUITED POWER DEEPER THAN SCIENCE FROM WHICH IT STEMS AND TO WHICH OUR COMMUNITY IS DEVOTED\". </span></p>\n<p><span style=\"font-family: Arial; font-size: 12px; white-space: pre-wrap;\">IF RDS was applied to our current situation, people would be analyzing Yvain's, Davis' and Eby's proposals, working out exactly what their implications are, and trying to propose, in the name of SCIENCE, hypotheses which will distinguish between them, and in the name of BAYES, confidence estimates of their analyses and of the quality with which the denotations of their words have cleaved reality at the joints enabling an odds ratio of updating to be extracted from a single data point. People would be working out what features of which of the models used by Yvain, Davis and Eby constitute evidence against what other features. They would be trying to evaluate non-verbally, through subjectively opaque but known-to-be-informative processes vulnerable to verbal overshadowing, what relative odds to place on those different features of the models. Finally, they would be examining the expected costs entailed by experiments being proposed and selecting those experiments which promise to provide the most information for the least cost be performed. The cost estimate would include both the effort required to perform the experiments, probably best assessed with an outside view in most cases like these, and the dangers to the minds of the participants from possible adverse outcomes, taking into account, as well as possible, the structural uncertainty of the models. </span></p>\n<p><span style=\"font-family: Arial; font-size: 12px; white-space: pre-wrap;\">I sincerely hope to see some of that in the comments section soon, either under this post or the \"Applied Picoeconomics\" post.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7Em9eRh9Cwphg3qhS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 1, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "1330", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-18T15:09:57.954Z", "modifiedAt": null, "url": null, "title": "Cascio in The Atlantic, more on cognitive enhancement as existential risk mitigation", "slug": "cascio-in-the-atlantic-more-on-cognitive-enhancement-as", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:01.466Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ymRZAN6c2sozFJZvm/cascio-in-the-atlantic-more-on-cognitive-enhancement-as", "pageUrlRelative": "/posts/ymRZAN6c2sozFJZvm/cascio-in-the-atlantic-more-on-cognitive-enhancement-as", "linkUrl": "https://www.lesswrong.com/posts/ymRZAN6c2sozFJZvm/cascio-in-the-atlantic-more-on-cognitive-enhancement-as", "postedAtFormatted": "Thursday, June 18th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cascio%20in%20The%20Atlantic%2C%20more%20on%20cognitive%20enhancement%20as%20existential%20risk%20mitigation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACascio%20in%20The%20Atlantic%2C%20more%20on%20cognitive%20enhancement%20as%20existential%20risk%20mitigation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FymRZAN6c2sozFJZvm%2Fcascio-in-the-atlantic-more-on-cognitive-enhancement-as%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cascio%20in%20The%20Atlantic%2C%20more%20on%20cognitive%20enhancement%20as%20existential%20risk%20mitigation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FymRZAN6c2sozFJZvm%2Fcascio-in-the-atlantic-more-on-cognitive-enhancement-as", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FymRZAN6c2sozFJZvm%2Fcascio-in-the-atlantic-more-on-cognitive-enhancement-as", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 854, "htmlBody": "<p>Jamais Cascio <a href=\"http://www.theatlantic.com/doc/200907/intelligence\">writes in the atlantic</a>:</p>\n<blockquote>\n<p>Pandemics. Global warming. Food shortages. No more fossil fuels. What are humans to do? The same thing the species has done before: evolve to meet the challenge. But this time we don&rsquo;t have to rely on natural evolution to make us smart enough to survive. We can do it ourselves, right now, by harnessing technology and pharmacology to boost our intelligence. Is Google actually making us smarter? ...</p>\n<p>&nbsp;... Modafinil isn&rsquo;t the only example; on college campuses, the use of ADD drugs (such as Ritalin and Adderall) as study aids has become almost ubiquitous. But these enhancements are primitive. As the science improves, we could see other kinds of cognitive-modification drugs that boost recall, brain plasticity, even empathy and emotional intelligence. They would start as therapeutic treatments, but end up being used to make us &ldquo;better than normal.&rdquo;</p>\n</blockquote>\n<p>Read the whole article <a href=\"http://www.theatlantic.com/doc/200907/intelligence\">here</a>.</p>\n<p>This relates to <a href=\"/lw/10l/intelligence_enhancement_as_existential_risk/\">cognitive enhancement as existential risk mitigation</a>, where Anders Sandberg wrote:</p>\n<blockquote>\n<p>Would it actually reduce existential risks? I do not know. But given correlations between long-term orientation, cooperation and intelligence, it seems likely that it might help not just to discover risks, but also in ameliorating them. It might be that other noncognitive factors like fearfulness or some innate discounting rate are more powerful.</p>\n</blockquote>\n<p>The main criticisms of this idea generated in the Less Wrong comments were:</p>\n<blockquote>\n<p>The problem is not that people are stupid. The problem is that people simply don't give a damn. If you don't fix that, I doubt raising IQ will be anywhere near as helpful as you may think. (Psychohistorian)</p>\n</blockquote>\n<blockquote>\n<p>Yes, this is the key problem that people don't really want to understand. (Robin Hanson)</p>\n</blockquote>\n<blockquote>\n<p>Making people more rational and aware of cognitive biases material would help much more (many people)<a id=\"more\"></a></p>\n</blockquote>\n<p>These criticisms really boil down to the same thing: people love their cherished falsehoods! Of course, I cannot disagree with this statement. But it seems to me that smarter people have a lower tolerance for making utterly ridiculous claims in favour of their cherished falsehood, and will (to some extent) be protected from believing silly things that make them (individually) feel happier, but are highly unsupported by evidence. Case in point: religion. This study<sup>1</sup> states that</p>\n<blockquote>\n<p>Evidence is reviewed pointing to a negative relationship between intelligence and religious belief in the United States and Europe. It is shown that intelligence measured as psychometric <em>g</em> is negatively related to religious belief. We find that in a sample of 137 countries the correlation between national IQ and disbelief in God is 0.60.</p>\n</blockquote>\n<p>Many people in the comments made the claim that making people more intelligent will, due to human self-deceiving tendencies, make people more deluded about the nature of the world. The data concerning religion detracts support from this hypothesis. There is also direct evidence to show that a whole list of human cognitive biases are more likely to be avoided by being more intelligent - though far from all (perhaps even far from most?) of them. This paper<sup>2</sup> states:</p>\n<blockquote>\n<p>In a further experiment, the authors nonetheless showed that cognitive ability does correlate with the tendency to avoid some rational thinking biases, specifically the tendency to display denominator neglect, probability matching rather than maximizing, belief bias, and matching bias on the 4-card selection task. The authors present a framework for predicting when cognitive ability will and will not correlate with a rational thinking tendency.</p>\n</blockquote>\n<p>Anders Sandberg also suggested the following piece of evidence<sup>3</sup> in favour of the hypothesis that increased intelligence leads to more rational political decisions:</p>\n<blockquote>\n<p>Political theory has described a positive linkage between education, cognitive ability and democracy. This assumption is confirmed by positive correlations between education, cognitive ability, and positively valued political conditions (N=183&minus;130). Longitudinal studies at the country level (N=94&minus;16) allow the analysis of causal relationships. It is shown that in the second half of the 20th century, education and intelligence had a strong positive impact on democracy, rule of law and political liberty independent from wealth (GDP) and chosen country sample. One possible mediator of these relationships is the attainment of higher stages of moral judgment fostered by cognitive ability, which is necessary for the function of democratic rules in society. <em><strong>The other mediators for citizens as well as for leaders could be the increased competence and willingness to process and seek information necessary for political decisions due to greater cognitive ability</strong></em>. There are also weaker and less stable reverse effects of the rule of law and political freedom on cognitive ability.</p>\n</blockquote>\n<p>Thus the hypothesis that increasing peoples' intelligence will make them believe fewer falsehoods and will make them vote for more effective government has at least two pieces of empirical evidence on its side.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<hr />\n<p>1. <a href=\"http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6W4M-4SD1KNR-1&amp;_user=10&amp;_coverDate=04%2F29%2F2008&amp;_alid=759868596&amp;_rdoc=1&amp;_fmt=high&amp;_orig=search&amp;_cdi=6546&amp;_sort=d&amp;_docanchor=&amp;view=c&amp;_ct=1&amp;_acct=C000050221&amp;_version=1&amp;_urlVersion=0&amp;_userid=10&amp;md5=bdb3ca48b21fdb2959f6f8ce4b6001de\">Average intelligence predicts atheism rates across 137 nations</a>, Richard Lynn,&nbsp; John Harvey and Helmuth Nyborg, Intelligence Volume 37, Issue 1,</p>\n<p>2. <a href=\"http://web.mac.com/kstanovich/iWeb/Site/Research%20on%20Reasoning_files/JPSP08.pdf\">On the Relative Independence of Thinking Biases and Cognitive Ability</a>, Keith E. Stanovich, Richard F. West, Journal of Personality and Social Psychology, 2008, Vol. 94, No. 4, 672&ndash;695</p>\n<p>3. <a href=\"http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6W4M-4R70VV6-1&amp;_user=809099&amp;_rdoc=1&amp;_fmt=&amp;_orig=search&amp;_sort=d&amp;view=c&amp;_acct=C000043939&amp;_version=1&amp;_urlVersion=0&amp;_userid=809099&amp;md5=f2764e696268d7c262a2da6840546401\">Relevance of education and intelligence for the political development of nations: Democracy, rule of law and political liberty</a>, Heiner Rindermann, Intelligence, Volume 36, Issue 4</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"pGqRLe9bFDX2G2kXY": 1, "ookMdjJQMopfLG3wZ": 1, "jiuackr7B5JAetbF6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ymRZAN6c2sozFJZvm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 23, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "1331", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 92, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ycr3CyrnZLFC7mb5W"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-20T00:16:29.923Z", "modifiedAt": null, "url": null, "title": "ESR's comments on some EY:OB/LW posts", "slug": "esr-s-comments-on-some-ey-ob-lw-posts", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:14.438Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GGEoygaLHvZCg5LBi/esr-s-comments-on-some-ey-ob-lw-posts", "pageUrlRelative": "/posts/GGEoygaLHvZCg5LBi/esr-s-comments-on-some-ey-ob-lw-posts", "linkUrl": "https://www.lesswrong.com/posts/GGEoygaLHvZCg5LBi/esr-s-comments-on-some-ey-ob-lw-posts", "postedAtFormatted": "Saturday, June 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20ESR's%20comments%20on%20some%20EY%3AOB%2FLW%20posts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AESR's%20comments%20on%20some%20EY%3AOB%2FLW%20posts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGGEoygaLHvZCg5LBi%2Fesr-s-comments-on-some-ey-ob-lw-posts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=ESR's%20comments%20on%20some%20EY%3AOB%2FLW%20posts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGGEoygaLHvZCg5LBi%2Fesr-s-comments-on-some-ey-ob-lw-posts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGGEoygaLHvZCg5LBi%2Fesr-s-comments-on-some-ey-ob-lw-posts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 255, "htmlBody": "<p><a href=\"http://esr.ibiblio.org/?p=1068\">Eric S. Raymond's comments</a> on some of my <em>Overcoming Bias</em> posts.<a id=\"more\"></a></p>\n<p>In his reply to my <a href=\"/lw/mc/to_lead_you_must_stand_up/%22%22\">To Lead, You Must Stand Up</a>, he writes:</p>\n<blockquote>\n<p>\"I think your exhortations here are nearly useless. Experience I&rsquo;ve collected over the last ten years suggests to me that the kind of immunity to stage fright you and I have is a function of basic personality type at the neurotransmitter-balance level, and not really learnable by most people.\"</p>\n</blockquote>\n<p>This is a particularly interesting observation if combined with Hanson's hypothesis that people <a href=\"http://www.overcomingbias.com/2009/04/choke-to-submit.html\">choke to submit</a>.</p>\n<p>\"I disagree with <a href=\"/lw/iv/the_futility_of_emergence\">The Futility of Emergence</a>,\" says ESR.&nbsp; Yea, many have said this to me.&nbsp; And they go on to say:&nbsp; <em>Emergence has the useful meaning that...</em>&nbsp; And it's a different meaning every time.&nbsp; In ESR's case it's:</p>\n<blockquote>\n<p>\"The word 'emergent' is a signal that we believe a very specific thing about the relationship between 'neurons firing' and 'intelligence', which is that there is no possible account of intelligence in which the only explanatory units are neurons or subsystems of neurons.\"</p>\n</blockquote>\n<p>Let me guess, <em>you </em>think the word \"emergence\" means something useful but that's not <em>exactly </em>it, although ESR's definition does aim in the rough general direction of what <em>you</em> think is the right definition...</p>\n<p>So-called \"words\" like this should not be actually spoken from one human to another.&nbsp; It is tempting fate.&nbsp; It would be like trying to have a serious discussion between two theologians if both of them were <a href=\"http://wiki.lesswrong.com/wiki/Rationalist_taboo\">allowed to say</a> the word \"God\" directly, instead of always having to <a href=\"/lw/nv/replace_the_symbol_with_the_substance/\">say whatever they meant by the word</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"aa3Qg7Qrp9LM7QMaz": 1, "FtT2T9bRbECCGYxrL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GGEoygaLHvZCg5LBi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 7, "extendedScore": null, "score": 5.022174595934908e-07, "legacy": true, "legacyId": "1335", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8QzZKw9WHRxjR4948", "GKfPL6LQFgB49FEnv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-20T11:31:59.758Z", "modifiedAt": null, "url": null, "title": "Nonparametric Ethics", "slug": "nonparametric-ethics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:29.018Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eMSoo6izTTrL9j6iZ/nonparametric-ethics", "pageUrlRelative": "/posts/eMSoo6izTTrL9j6iZ/nonparametric-ethics", "linkUrl": "https://www.lesswrong.com/posts/eMSoo6izTTrL9j6iZ/nonparametric-ethics", "postedAtFormatted": "Saturday, June 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Nonparametric%20Ethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANonparametric%20Ethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeMSoo6izTTrL9j6iZ%2Fnonparametric-ethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Nonparametric%20Ethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeMSoo6izTTrL9j6iZ%2Fnonparametric-ethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeMSoo6izTTrL9j6iZ%2Fnonparametric-ethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1373, "htmlBody": "<p>(Inspired by a recent conversation with Robin Hanson.)</p>\n<p>Robin Hanson, in his essay on \"<a href=\"http://www.overcomingbias.com/2009/05/minimal-morals.html\">Minimal Morality</a>\", suggests that the unreliability of our moral reasoning should lead us to seek <em>simple</em> moral principles:</p>\n<blockquote>\n<p>\"In the ordinary practice of fitting a curve to a set of data points, the more noise one expects in the data, the simpler a curve one fits to that data.&nbsp; Similarly, when fitting moral principles to the data of our moral intuitions, the more noise we expect in those intuitions, the simpler a set of principles we should use to fit those intuitions.&nbsp; (<a href=\"http://hanson.gmu.edu/bioerr.pdf\">This</a> paper elaborates.)\"</p>\n</blockquote>\n<p>In \"the limit of expecting very large errors of our moral intuitions\", says Robin, we should follow an extremely simple principle - the simplest principle we can find that seems to compress as much morality as possible.&nbsp; And that principle, says Robin, is that <em>it is usually good for people to get what they want, if no one else objects.</em></p>\n<p>Now I myself carry on something of a crusade <em>against</em> trying to compress morality down to One Great Moral Principle.&nbsp; I have developed at some length <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">the thesis that human values are, in actual fact, complex, but that numerous biases lead us to underestimate and overlook this complexity</a>.&nbsp; From a <a href=\"http://wiki.lesswrong.com/wiki/Friendly_AI\">Friendly AI</a> perspective, the word \"want\" in the English sentence above is a <a href=\"http://wiki.lesswrong.com/wiki/Magical_categories\">magical category</a>.</p>\n<p>But Robin wasn't making an argument in Friendly AI, but in human ethics: he's proposing that, in the presence of probable errors in moral reasoning, we should look for principles that seem simple to us, to carry out at the end of the day.&nbsp; The more we distrust ourselves, the simpler the principles.</p>\n<p>This argument from fitting noisy data, is a kind of logic that can apply even when you have prior reason to believe the underlying generator is in fact complicated.&nbsp; You'll still get better predictions from the simpler model, because it's less sensitive to noise.</p>\n<p>Even so, my belief that human values are in fact complicated, leads me to two objections and an alternative proposal:<a id=\"more\"></a></p>\n<p>The first objection is that we do, in fact, have enough data to support moral models that are more complicated than a small set of short English sentences.&nbsp; If you have a thousand data points, even noisy data points, it may be a waste of evidence to try to fit them to a straight line, especially if you have prior reason to believe the true generator is not linear.</p>\n<p>And my second fear is that people underestimate the complexity and error-proneness of the reasoning they do to <em>apply</em> their Simple Moral Principles.&nbsp; If you try to reduce morality to the Four Commandments, then people are going to end up doing elaborate, error-prone rationalizations in the course of <a href=\"http://wiki.lesswrong.com/wiki/Magical_categories\">shoehorning their real values</a> into the Four Commandments.</p>\n<p>But in the ordinary practice of machine learning, there's a different way to deal with noisy data points besides trying to <em>fit simple models</em>.&nbsp; You can use <em>nonparametric methods.</em>&nbsp; The classic example is k-nearest-neighbors:&nbsp; To predict the value at a new point, use the average of the 10 nearest points previously observed.</p>\n<p>A line has two parameters - slope and intercept; to fit a line, we try to pick values for the slope and intercept that well-match the data.&nbsp; (Minimizing squared error corresponds to maximizing the likelihood of the data given Gaussian noise, for example.)&nbsp; Or we could fit a cubic polynomial, and pick four parameters that best-fit the data.</p>\n<p>But the nearest-neighbors estimator doesn't assume a particular shape of underlying curve - not even that the curve is a polynomial.&nbsp; Technically, it doesn't even assume continuity.&nbsp; It just says that we think that the true values at nearby positions are likely to be similar.&nbsp; (If we furthermore believe that the underlying curve is likely to have continuous first and second derivatives, but don't want to assume anything else about the shape of that curve, then we can use cubic splines to fit an arbitrary curve with a smoothly changing first and second derivative.)</p>\n<p>And in terms of machine learning, it works.&nbsp; It is done rather less often in science papers - for various reasons, some good, some bad; e.g. academics may prefer models with simple extractable parameters that they can hold up as the triumphant fruits of their investigation:&nbsp; <em>Behold, this is the slope!</em>&nbsp; But if you're trying to win the Netflix Prize, and you find an algorithm that seems to do well by fitting a line to a thousand data points, then yes, one of the next things you try is substituting some nonparametric estimators of the same data; and yes, this often greatly improves the estimates in practice.&nbsp; (Added:&nbsp; And conversely there are plenty of occasions where ridiculously simple-seeming parametric fits to the same data turn out to yield surprisingly good predictions.&nbsp; And <em>lots</em> of occasions where added complexity for tighter fits buys you very little, or even makes predictions worse.&nbsp; In machine learning this is usually something you find out by playing around, AFAICT.)</p>\n<p>It seems to me that concepts like <em>equality before the law,</em> or even the notion of writing down stable laws in the first place, reflect a <em>nonparametric</em> approach to the ethics of error-prone moral reasoning.</p>\n<p>We don't suppose that society can be governed by only four laws.&nbsp; In fact, we don't even need to suppose that the 'ideal' morality (obtained as the limit of perfect knowledge and reflection, etc.) <em>would in fact</em> subject different people and different occasions to the same laws.&nbsp; We need only suppose that we believe, a priori, that <em>similar</em> moral dilemmas are likely ceteris paribus to have <em>similar</em> resolutions, and that <em>moral reasoning about adjustment to specific people is highly error-prone</em> - that, given unlimited flexibility to 'perfectly fit' the solution to the person, we're likely to favor our friends and relatives too much.&nbsp; (And not in an explicit, internally and externally visible way, that we could correct just by having a new rule not to favor friends and relatives.)</p>\n<p>So instead of trying to recreate, each time, the judgment that is the perfect fit to the situation and the people, we try to use the ethical equivalent of a cubic spline - have underlying laws that are allowed to be complicated, but have to be written down for stability, and are supposed to treat neighboring points similarly.</p>\n<p>Nonparametric ethics says:&nbsp; \"Let's reason about which moral situations are at least rough neighbors so that an acceptable solution to one should be at least mostly-acceptable to another; and let's reason about where people are likely to be highly biased in their attempt to adjust to specifics; and then, to reduce moral error, let's enforce similar resolutions across neighboring cases.\"&nbsp; If you think that good moral codes will treat different people similarly, and/or that people are highly biased in how they adjust their judgments to different people, then you will come up with the ethical solution of <em>equality before the law.</em></p>\n<p>Now of course you can still have laws that are too complicated, and that try to sneak in too much adaptation to particular situations.&nbsp; This would correspond to a nonparametric estimator that <em>doesn't smooth enough</em>, like using 1-nearest-neighbor instead of 10-nearest-neighbors, or like a cubic spline that tried to exactly fit every point without trying to minimize the absolute value of third derivatives.</p>\n<p>And of course our society may not <em>succeed </em>at similarly treating different people in similar situations - people who can afford lawyers experience a different legal system.</p>\n<p>But if nothing else, coming to grips with the concept of nonparametric ethics helps us see the way in which our society is failing to deal with the error-proneness of its own moral reasoning.</p>\n<p>You can interpret a fair amount of my <a href=\"http://wiki.lesswrong.com/wiki/Yudkowsky%27s_Coming_of_Age\">coming-of-age</a> as my switch from parametric ethics to nonparametric ethics - from the pre-2000 search for simple underlying morals and my attempts to therefore reject values that seemed complicated; to my later acceptance that my values were actually going to be <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">complicated</a>, and that both I and my AI designs needed to come to terms with that.&nbsp; <a href=\"http://wiki.lesswrong.com/wiki/Friendly_AI\">Friendly AI</a> can be viewed as the problem of coming up with - not the Three Simple Laws of Robotics that are all a robot needs - but rather a regular and stable method for learning, predicting, and renormalizing human values that <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">are and should be complicated</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 1, "ouT6wKhACJRouGokM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eMSoo6izTTrL9j6iZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 30, "extendedScore": null, "score": 5.023223944772051e-07, "legacy": true, "legacyId": "1336", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 60, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-21T17:57:09.235Z", "modifiedAt": null, "url": null, "title": "Shane Legg on prospect theory and computational finance", "slug": "shane-legg-on-prospect-theory-and-computational-finance", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:01.176Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PnhpMqMP75Dxpvar5/shane-legg-on-prospect-theory-and-computational-finance", "pageUrlRelative": "/posts/PnhpMqMP75Dxpvar5/shane-legg-on-prospect-theory-and-computational-finance", "linkUrl": "https://www.lesswrong.com/posts/PnhpMqMP75Dxpvar5/shane-legg-on-prospect-theory-and-computational-finance", "postedAtFormatted": "Sunday, June 21st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Shane%20Legg%20on%20prospect%20theory%20and%20computational%20finance&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShane%20Legg%20on%20prospect%20theory%20and%20computational%20finance%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPnhpMqMP75Dxpvar5%2Fshane-legg-on-prospect-theory-and-computational-finance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Shane%20Legg%20on%20prospect%20theory%20and%20computational%20finance%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPnhpMqMP75Dxpvar5%2Fshane-legg-on-prospect-theory-and-computational-finance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPnhpMqMP75Dxpvar5%2Fshane-legg-on-prospect-theory-and-computational-finance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 240, "htmlBody": "<blockquote>\n<p>People are <em>not </em>rational expected utility maximisers.&nbsp; When we have to make decisions, all sorts of cognitive biases and distortions come into play.&nbsp; Seminal work in this area was done by Kahneman and Tversky.&nbsp; They produced a model of human decision making known as <em>prospect theory</em>, work that Kahneman later won a Noble prize ...</p>\n<p>... I looked at these investors and thought, &ldquo;Hey, they&rsquo;re just like reinforcement learning agents. No big deal. If I want to know what investors with probability weighting and a curved value function do, I can just brute force compute their optimal policy by writing down their Bellman equation and using dynamic programming. Easy!&rdquo; It was a mystery to me why, seemingly, nobody else was doing that. So off I went to build software to do just this, starting with a simple Merton model&hellip;</p>\n<p>... When we fired up my simulator and gave this distribution to an investor that had probability weighting: the investor took one look at that scary negative tail and didn&rsquo;t want to invest in the stock. This is exactly what the model should predict.&nbsp; In short, we took realistic stock returns, and presented this to an investor with a realistic decision making process complete with a bunch of parameters that have been empirically estimated by others in previous work, and what we got out the other end was realistic investor behaviour!</p>\n</blockquote>\n<p>Read the whole article <a href=\"http://www.vetta.org/2009/06/prospect_theory_investors/\">here</a> at Vetta Project.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PnhpMqMP75Dxpvar5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 16, "extendedScore": null, "score": 6e-06, "legacy": true, "legacyId": "1337", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-23T04:58:55.550Z", "modifiedAt": null, "url": null, "title": "The Domain of Your Utility Function", "slug": "the-domain-of-your-utility-function", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:29.933Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Peter_de_Blanc", "createdAt": "2009-02-27T14:15:28.882Z", "isAdmin": false, "displayName": "Peter_de_Blanc"}, "userId": "vRvaAqR5tcjGEWaoC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xgicQnkrdA5FehhnQ/the-domain-of-your-utility-function", "pageUrlRelative": "/posts/xgicQnkrdA5FehhnQ/the-domain-of-your-utility-function", "linkUrl": "https://www.lesswrong.com/posts/xgicQnkrdA5FehhnQ/the-domain-of-your-utility-function", "postedAtFormatted": "Tuesday, June 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Domain%20of%20Your%20Utility%20Function&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Domain%20of%20Your%20Utility%20Function%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxgicQnkrdA5FehhnQ%2Fthe-domain-of-your-utility-function%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Domain%20of%20Your%20Utility%20Function%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxgicQnkrdA5FehhnQ%2Fthe-domain-of-your-utility-function", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxgicQnkrdA5FehhnQ%2Fthe-domain-of-your-utility-function", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 486, "htmlBody": "<p><strong>Unofficial Followup to:</strong> <a href=\"/lw/kx/fake_selfishness/\">Fake Selfishness</a>, <a href=\"/lw/zv/post_your_utility_function/\">Post Your Utility Function</a></p>\n<p>A perception-determined utility function is one which is determined only by the perceptual signals your mind receives from the world; for instance, <em>pleasure minus pain</em>. A noninstance would be <em>number of living humans</em>. There's an argument in favor of perception-determined utility functions which goes like this: clearly, the state of your mind screens off the state of the outside world from your decisions. Therefore, the argument to your utility function is not a world-state, but a mind-state, and so, when choosing between outcomes, you can only judge between anticipated experiences, and not external consequences. If one says, \"I would willingly die to save the lives of others,\" the other replies, \"that is only because you anticipate great satisfaction in the moments before death - enough satisfaction to outweigh the rest of your life put together.\"</p>\n<p>Let's call this dogma <em>perceptually determined utility</em>. PDU can be criticized on both descriptive and prescriptive grounds. On descriptive grounds, we may observe that it is psychologically unrealistic for a human to experience a lifetime's worth of satisfaction in a few moments. (I don't have a good reference for this, but) I suspect that our brains count pain and joy in something like unary, rather than using a place-value system, so it is not possible to count very high.</p>\n<p>The argument I've outlined for PDU is prescriptive, however, so I'd like to refute it on such grounds. To see what's wrong with the argument, let's look at some diagrams. Here's a picture of you doing an expected utility calculation - using a perception-determined utility function such as <em>pleasure minus pain</em>.<a id=\"more\"></a></p>\n<p><img src=\"http://images.lesswrong.com/t3_116_0.png\" alt=\"\" width=\"478\" height=\"411\" /></p>\n<p>Here's what's happening: you extrapolate several (preferably <em>all</em>) possible futures that can result from a given plan. In each possible future, you extrapolate what would happen to you personally, and calculate the <em>pleasure minus pain</em> you would experience. You call this the utility of that future. Then you take a weighted average of the utilities of each future &mdash; the weights are probabilities. In this way you calculate the expected utility of your plan.</p>\n<p>But this isn't the <em>most general</em> possible way to calculate utilities.</p>\n<p><img src=\"http://images.lesswrong.com/t3_116_2.png?v=5b039b6b66c78adf2d8da719aa3327f2\" alt=\"\" width=\"478\" height=\"411\" /></p>\n<p>Instead, we could calculate utilities based on <em>any</em> properties of the extrapolated futures &mdash; anything at all, such as how many people there are, how many of those people have ice cream cones, etc. Our preferences over lotteries will be consistent with the <a href=\"http://en.wikipedia.org/wiki/Utility#Additive_von_Neumann-Morgenstern_Utility\">Von Neumann-Morgenstern axioms</a>. The basic error of PDU is to confuse the big box (labeled \"your mind\") with the tiny boxes labeled \"Extrapolated Mind A,\" and so on. The inputs to your utility calculation exist inside your mind, but that does not mean they have to come from your extrapolated future mind.</p>\n<p>So that's it! You're free to care about family, friends, humanity, fluffy animals, and all the wonderful things in the universe, and decision theory won't try to stop you &mdash; in fact, it will help.</p>\n<p>Edit: Changed \"PD\" to \"PDU.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1, "3uE2pXvbcnS9nnZRE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xgicQnkrdA5FehhnQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 42, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "1338", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 99, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Masoq4NdmmGSiq2xw", "MvwdPfYLX866vazFJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-24T05:30:16.760Z", "modifiedAt": null, "url": null, "title": "The Monty Maul Problem", "slug": "the-monty-maul-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:03.930Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JGWeissman", "createdAt": "2009-04-01T04:43:56.740Z", "isAdmin": false, "displayName": "JGWeissman"}, "userId": "Mw8rsM7m7E8nnEFEp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XrKpemoWiZ2w9HuZB/the-monty-maul-problem", "pageUrlRelative": "/posts/XrKpemoWiZ2w9HuZB/the-monty-maul-problem", "linkUrl": "https://www.lesswrong.com/posts/XrKpemoWiZ2w9HuZB/the-monty-maul-problem", "postedAtFormatted": "Wednesday, June 24th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Monty%20Maul%20Problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Monty%20Maul%20Problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXrKpemoWiZ2w9HuZB%2Fthe-monty-maul-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Monty%20Maul%20Problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXrKpemoWiZ2w9HuZB%2Fthe-monty-maul-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXrKpemoWiZ2w9HuZB%2Fthe-monty-maul-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 454, "htmlBody": "<p>In his Coding Horror Blog, Jeff Atwood <a title=\"Monty Hall, Monty Fall, Monty Crawl\" href=\"http://www.codinghorror.com/blog/archives/001278.html\">writes</a> about the Monty Hall Problem and some variants. The classic problem presents the situation in which the game show host allows a contestant to choose one of three doors, one of which opens to reveal a prize while the other two reveals goats. The host then opens one of the other doors, reliably choosing one that has a goat, and invites the contestant to switch to the remaining unopened door. The problem is to determine the probability of winning the prize by switching and staying. The variants deal with cases in which the host does not reliably choose a door with a goat, but happens to do so.</p>\n<p>Jeff cites <a href=\"http://www.probability.ca/jeff/writing/montyfall.pdf\">Monty Hall, Monty Fall, Monty Crawl</a> (PDF) by Jeff Rosenthal, which explains why the variants have different probabilities in terms of the \"Proportionality Principle\", which the appendix acknowledges to be a special case of <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a>.</p>\n<p>One of Jeff's anonymous commenters presented the Monty Maul Problem:</p>\n<blockquote>\n<p>Hypothetical Situation:</p>\n<p>The Monty Maul problem. There are 1 million doors. You pick one, and the shows host goes on a bloodrage fueled binge of insane violence, knocking open doors at random with no knowledge of which door has the car. He knocks open 999,998 doors, leaving your door and one unopened door. None of the opened doors contains the car.</p>\n<p>Are your odds of winning if you switch still 50/50, as outlined by the linked Rosenthal paper? It seems counter-intuitive even for people who've wrapped their head around the original problem.</p>\n</blockquote>\n<p>If you take as absolute the problem's statement the host is randomly knocking doors open, then yes, the fact that only goats were revealed is strong evidence that only goats were available because you picked the door with the prize, which, when combined with the low prior probability that you picked the door with the prize, gives equal probability to either of the unopened doors having the prize.</p>\n<p>However, the fact that only goats were revealed is also strong evidence that the host deliberately avoided opening the door with the prize, and therefor switching is a winning strategy. After all, the probability of this happening if the host really is choosing doors randomly is 2 in a million, but it is guaranteed if the host deliberately opened only doors with goats.</p>\n<p>Note that this principal still applies in variants with fewer doors. Unless there is an actual penalty for switching doors (which could happen if the host only sometimes offers the opportunity to switch, and is more likely to do so when the contestant chooses the winning door), any uncertainty about the host choosing doors randomly implies that it is a good strategy to switch.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XrKpemoWiZ2w9HuZB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 7, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "1340", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-24T17:29:10.462Z", "modifiedAt": null, "url": null, "title": "Guilt by Association", "slug": "guilt-by-association", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:00.892Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Annoyance", "createdAt": "2009-02-28T04:06:40.600Z", "isAdmin": false, "displayName": "Annoyance"}, "userId": "yEm6LWatswJYGwBFq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nhfHxMbapMa5GacGA/guilt-by-association", "pageUrlRelative": "/posts/nhfHxMbapMa5GacGA/guilt-by-association", "linkUrl": "https://www.lesswrong.com/posts/nhfHxMbapMa5GacGA/guilt-by-association", "postedAtFormatted": "Wednesday, June 24th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Guilt%20by%20Association&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGuilt%20by%20Association%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnhfHxMbapMa5GacGA%2Fguilt-by-association%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Guilt%20by%20Association%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnhfHxMbapMa5GacGA%2Fguilt-by-association", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnhfHxMbapMa5GacGA%2Fguilt-by-association", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 651, "htmlBody": "<p>The formal concept of the fallacious argument was born as the twin of logic itself.&nbsp; When the ancient Greeks first began to systematically examine the natural arguments people made as they sought to demonstrate the truth of propositions, they noted that certain types of arguments were vulnerable to counterexamples while others were not.&nbsp; The vulnerable were not true - when it was claimed that they justified a conclusion they could not rule out the alternative - and so were identified as <em>fallacious</em>.</p>\n<p>Although the validity of logical arguments can be determined through logic, that doesn't particularly distinguish one fallacy from another.&nbsp; It is a curious fact that, despite this, some fallacies are more frequently made by human beings than others.&nbsp; Much more.</p>\n<p><a id=\"more\"></a>For example, \"<strong>If P, then Q.&nbsp; P.&nbsp; Therefore, Not-Q.</strong>\" is just as basic and elemental an error as \"<strong>If P, then Q.&nbsp; Q.&nbsp; Therefore, P.</strong>\" is.&nbsp; But the first fallacy is hardly ever found (humans being what they are, there's probably no mistake within our reach that is <em>never</em> made) while the second is extraordinarily common.</p>\n<p>It is in fact generally true that we often confuse a unidirectional implication with the bidirectional.&nbsp; If something implies another thing, we leap to the conclusion that the second also implies the first, that the connection is equally strong each way, even though it is fairly trivial to demonstrate that this isn't necessarily the case.&nbsp; This error seems to be inherent to human intuition, as it occurs across contexts and subjects quite regularly, even when people are aware that it's logically invalid; only careful examination counters this tendency.</p>\n<p>Much later, Sigmund Freud began to identify ways that people would deny assertions that they found emotionally threatening, what we now call 'psychological defense mechanisms'.&nbsp; The flaws in Freud's work as a whole are not directly relevant to this discussion and are beyond the scope of this site in any case.&nbsp; Suffice it to say that therapists and psychologists do not consider his theories to be either true or useful, that they do consider them to be unscientific and a self-reinforcing belief system, and that many of the concepts which he introduced and have been taken up into the culture at large are invalid.&nbsp; Not all of his work is so flawed, though - particularly his early ideas.</p>\n<p>There is a peculiar relationship between the nature of those defense mechanisms and the intuitive fallacies.</p>\n<p>When confronted with a contradiction in their emotionally-charged arguments, people who normally reasoned quite appropriately would suddenly begin to fall into fallacies.&nbsp; What's more, they would be unable to see the errors in their reasoning.&nbsp; Even more extraordinarily, they would often reach conclusions which were superficially related to the correct ones, but which were applied to the wrong concepts, situations, and individuals.&nbsp; In <em>projection</em>, for example, motives and traits belonging to the patients are instead asserted to belong to others or even the world itself; properties within the psyche are \"projected\" outward.&nbsp; So when a therapist demonstrated to a patient that some of their beliefs were incompatible or their arguments were contradictory, the patient might assert that the therapist was the one who had the irrational concern or obsession.</p>\n<p>In such cases it seems clear that there is an awareness of some kind that the unpleasant conclusion must be reached, but not of where the property must be attributed.&nbsp; Accusing the therapist of possessing the unacceptable property seems to reduce tension of some sort - it's a relief that people actively seek out and will vehemently, even violently, defend.</p>\n<p>Guilt, hate, fear, forbidden joys and loves - there are countless ways people will deny that they possess them.&nbsp; But they all tend to follow into certain predictable patterns, as the wild diversity of snowflakes still showcases repeating and similar forms.</p>\n<p>Why is this the case?&nbsp; Ultimately, it took research into concept formation before psychology could really produce an answer to that question.</p>\n<p>Next time:&nbsp; associational thought and the implications for rationality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nhfHxMbapMa5GacGA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 1, "extendedScore": null, "score": 5.032742763340565e-07, "legacy": true, "legacyId": "1333", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-24T21:56:15.638Z", "modifiedAt": null, "url": null, "title": "Lie to me?", "slug": "lie-to-me-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:40.303Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pwno", "createdAt": "2009-02-27T06:17:31.584Z", "isAdmin": false, "displayName": "pwno"}, "userId": "SCgoHNxqc2agmDWEg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CQQS8CvLH3Y7kJbEQ/lie-to-me-1", "pageUrlRelative": "/posts/CQQS8CvLH3Y7kJbEQ/lie-to-me-1", "linkUrl": "https://www.lesswrong.com/posts/CQQS8CvLH3Y7kJbEQ/lie-to-me-1", "postedAtFormatted": "Wednesday, June 24th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lie%20to%20me%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALie%20to%20me%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCQQS8CvLH3Y7kJbEQ%2Flie-to-me-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lie%20to%20me%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCQQS8CvLH3Y7kJbEQ%2Flie-to-me-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCQQS8CvLH3Y7kJbEQ%2Flie-to-me-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 421, "htmlBody": "<p>I used to think that, given two equally capable individuals, the person with more true information can always do at least as good as the other person. And hence, one can only gain from having true information. There is one implicit assumption that makes this line of reason not true in all cases. We are not perfectly rational agents; our mind isn&rsquo;t stored in a vacuum, but in a Homo sapien brain.There are certain false beliefs that benefit you by exploiting your primitive mental warehouse, e.g., self-fulfilling prophecies.</p>\n<p>Despite the benefits, adopting false beliefs is an irrational practice. If people never acquire the maps that correspond the best to the territory, they won&rsquo;t have the most accurate cost-benefit analysis for adopting false beliefs. Maybe, in some cases, false beliefs make you better off. The problem is you'll have a wrong or sub-optimal cost-benefit analysis, unless you first adopt reason.</p>\n<p>Also, it doesn&rsquo;t make sense to say that the rational decision could be to &ldquo;have a false belief&rdquo; because in order to make that decision, you would have to compare that outcome against &ldquo;having a true belief.&rdquo; But in order for a false belief to work, you must truly believe in it &mdash; you cannot deceive yourself into believing the false belief after knowing the truth! It&rsquo;s like figuring out that taking a placebo leads to the best outcome, yet knowing it&rsquo;s a placebo no longer makes it the best outcome.</p>\n<p>Clearly, it is not in your best interest to choose to believe in a falsity&mdash;but what if someone else did the choosing? Can&rsquo;t someone whose advice you rationally trust be the decider of whether to give you false information or not (e.g. a doctor deciding whether you receive a placebo or not)? They could perform a cost-benefit analysis without diluting the effects of the false belief. We only want to know the truth, but prefer to be unknowingly lied to in some cases.</p>\n<p>Which brings me to my question: do we program an AI to only tell us the truth or to lie when the AI believes (with high certainty) the lie will lead us to a net benefit over our expected lifetime?</p>\n<p><strong>Added</strong>: Keep in mind that knowledge of the truth, even for a truth-seeker, is finite in value. The AI can believe that the benefit of a lie would outweigh a truth-seeker's cost of being lied to. So unless someone values the truth above anything else (which I highly doubt), would a truth-seeker ever choose only to be told the truth from the AI?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YTCrHWYHAsAD74EHo": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CQQS8CvLH3Y7kJbEQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -1, "extendedScore": null, "score": 5.033160748987844e-07, "legacy": true, "legacyId": "1343", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-25T00:27:23.325Z", "modifiedAt": null, "url": null, "title": "Richard Dawkins TV - Baloney Detection Kit video", "slug": "richard-dawkins-tv-baloney-detection-kit-video", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:00.990Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/58RrdgwdzYRN6Co9b/richard-dawkins-tv-baloney-detection-kit-video", "pageUrlRelative": "/posts/58RrdgwdzYRN6Co9b/richard-dawkins-tv-baloney-detection-kit-video", "linkUrl": "https://www.lesswrong.com/posts/58RrdgwdzYRN6Co9b/richard-dawkins-tv-baloney-detection-kit-video", "postedAtFormatted": "Thursday, June 25th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Richard%20Dawkins%20TV%20-%20Baloney%20Detection%20Kit%20video&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARichard%20Dawkins%20TV%20-%20Baloney%20Detection%20Kit%20video%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F58RrdgwdzYRN6Co9b%2Frichard-dawkins-tv-baloney-detection-kit-video%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Richard%20Dawkins%20TV%20-%20Baloney%20Detection%20Kit%20video%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F58RrdgwdzYRN6Co9b%2Frichard-dawkins-tv-baloney-detection-kit-video", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F58RrdgwdzYRN6Co9b%2Frichard-dawkins-tv-baloney-detection-kit-video", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 27, "htmlBody": "<p>See this great little rationalist video <a href=\"http://www.youtube.com/watch?v=eUB4j0n2UDU\">here</a>.</p>\n<blockquote>\n<p>Well, if I am pro-business, I have to be skeptical about global warming. Wait! How about just following the data?</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZzxvopS4BwLuQy42n": 1, "BtQRRKTPxagBH6KrG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "58RrdgwdzYRN6Co9b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 2, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "1344", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-25T12:09:18.790Z", "modifiedAt": null, "url": null, "title": "Requests to the Right Ear Are More Successful Than to the Left", "slug": "requests-to-the-right-ear-are-more-successful-than-to-the", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:00.582Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CannibalSmith", "createdAt": "2009-02-27T07:32:08.507Z", "isAdmin": false, "displayName": "CannibalSmith"}, "userId": "4DedYkNap2GW8X79T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vicBmmMgMKnf3apKK/requests-to-the-right-ear-are-more-successful-than-to-the", "pageUrlRelative": "/posts/vicBmmMgMKnf3apKK/requests-to-the-right-ear-are-more-successful-than-to-the", "linkUrl": "https://www.lesswrong.com/posts/vicBmmMgMKnf3apKK/requests-to-the-right-ear-are-more-successful-than-to-the", "postedAtFormatted": "Thursday, June 25th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Requests%20to%20the%20Right%20Ear%20Are%20More%20Successful%20Than%20to%20the%20Left&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARequests%20to%20the%20Right%20Ear%20Are%20More%20Successful%20Than%20to%20the%20Left%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvicBmmMgMKnf3apKK%2Frequests-to-the-right-ear-are-more-successful-than-to-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Requests%20to%20the%20Right%20Ear%20Are%20More%20Successful%20Than%20to%20the%20Left%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvicBmmMgMKnf3apKK%2Frequests-to-the-right-ear-are-more-successful-than-to-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvicBmmMgMKnf3apKK%2Frequests-to-the-right-ear-are-more-successful-than-to-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 26, "htmlBody": "<p><a href=\"http://www.wired.com/wiredscience/2009/06/earcigarette/\">Talk into the right ear and you send your words into a slightly more amenable part of the brain.</a></p>\n<p>I urge you to try this at home.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XYHzLjwYiqpeqaf4c": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vicBmmMgMKnf3apKK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 4, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "1345", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-25T16:16:47.650Z", "modifiedAt": null, "url": null, "title": "Coming Out", "slug": "coming-out", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:01.894Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eyjeuQc2F95yJXJo6/coming-out", "pageUrlRelative": "/posts/eyjeuQc2F95yJXJo6/coming-out", "linkUrl": "https://www.lesswrong.com/posts/eyjeuQc2F95yJXJo6/coming-out", "postedAtFormatted": "Thursday, June 25th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Coming%20Out&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AComing%20Out%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeyjeuQc2F95yJXJo6%2Fcoming-out%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Coming%20Out%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeyjeuQc2F95yJXJo6%2Fcoming-out", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeyjeuQc2F95yJXJo6%2Fcoming-out", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 679, "htmlBody": "<p>It's 11 p.m. on a Friday night and I'm heading home from work, shivering in the cold wind. My mind's divided on what to do after getting home: practice my guitar stuff and go to sleep, or head to a nightclub and get the fatigue out of my system? Given two nearly identical emotional assessments and honestly not knowing in advance which to pick, I suddenly recall LessWrong and decide to use logic. Trying to apply logic in real life is an unfamiliar sensation but I prime my mind for recognizing accurate arguments and drift away for a moment, a proven technique from my mathematician days...</p>\n<p>By the time I reach my front door, the solution for discriminating between two outcomes of equal utility has arrived and it's a logical slam dunk. Tonight is Friday night. There will be ample opportunity to practice music tomorrow and the day after that. So get dressed and go dance now.</p>\n<p>It worked.</p>\n<p>&nbsp;</p>\n<p><a id=\"more\"></a>You might be tempted to dismiss my little exercise as rationalization and I can't really convince you otherwise. But the specific tool used for rationalization does matter. What if I'd made that decision based on some general ethical principle or a pithy quotation I'd heard somewhere? Success or failure would have prompted an equally ill-specified update of my held beliefs, the same way they have meandered aimlessly all my life. A single human lifetime sees a precious few belief-changing events&nbsp;&mdash;&nbsp;definitely not enough bits to select an adequate emotional framework, unless you started out with a good one. Not so with logic. Any vague verbal principle, however much you admire it, is always allowed to give way under you if you base your decisions on it... but logic is the<em> ground floor</em>.</p>\n<p>The above episode has hit me hard for reasons I can't quite verbalize. For the past few days I've been obsessively going over Eliezer's <a href=\"http://www.cs.auckland.ac.nz/~andwhay/postlist.html\">old posts</a>. No longer viewing them in the mindset of \"articulate dissent\": this time I went searching for useful stuff. Turns out that most of his concrete recommendations are either way beyond my current abilities, or solving problems I haven't reached yet. Forget expected utility calculations, I don't even have the basics down!</p>\n<p>Logic is the ground floor. Disregard the defense of biases. If you find yourself falling, use logic.</p>\n<p>I remember a <a href=\"http://lambda-the-ultimate.org/classic/message2305.html\">quote</a> from functional programming researcher Frank Atanassow about his formal semantics enlightenment, similar in spirit:</p>\n<blockquote>\n<p>When I graduated, I was as pure a corporate hacker as you can imagine: I wanted to be a corporate salaryman; I wanted to \"architecture\" object-oriented programs in the Gang-of-Four style; and above all, I was convinced that CS is a completely new field unto itself and had no relation at all to all that mathematical crap they were trying to stuff down our throats in school...</p>\n<p>What happened next, I remember very clearly. I bought a book, \"Lambda Calculus\" by Chris Hankin. It's a thin (&lt;200 pages) yellow book, mostly about untyped lambda-calculus. It has a fair amount of equational stuff in it, but I took it on my trip to Korea (to get my Japanese visa---I was working in Tokyo, and you have to leave the country once for some reason to get a working visa) and was amazed that I could understand it...</p>\n<p>After that, I was able to read most of the programming language theory literature, and I read TONS, and tons of it. I seriously couldn't get enough, and what I marvelled at was that you could actually make indisputable statements about programs this way, provided your language had a formal semantics. It was no longer a fuzzy thing, you see; it relieved me of a great burden, in a way, because I no longer had to subscribe to a \"methodology\" which relied on some unsubstantiated relation between real world-\"objects\" and programs. I came to understand that certain things <em>have</em> to be true, <em>provably</em>, incontrovertibly and forever, if your programs can be translated easily into mathematical terms. This was a huge step for me.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>There's much work ahead, but at least now I know what to do.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KoXbd2HmbdRfqLngk": 1, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eyjeuQc2F95yJXJo6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 12, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "1346", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-25T22:29:32.820Z", "modifiedAt": null, "url": null, "title": "The Great Brain is Located Externally", "slug": "the-great-brain-is-located-externally", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:05.654Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h7NkpER4Jo8BLWgPD/the-great-brain-is-located-externally", "pageUrlRelative": "/posts/h7NkpER4Jo8BLWgPD/the-great-brain-is-located-externally", "linkUrl": "https://www.lesswrong.com/posts/h7NkpER4Jo8BLWgPD/the-great-brain-is-located-externally", "postedAtFormatted": "Thursday, June 25th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Great%20Brain%20is%20Located%20Externally&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Great%20Brain%20is%20Located%20Externally%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh7NkpER4Jo8BLWgPD%2Fthe-great-brain-is-located-externally%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Great%20Brain%20is%20Located%20Externally%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh7NkpER4Jo8BLWgPD%2Fthe-great-brain-is-located-externally", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh7NkpER4Jo8BLWgPD%2Fthe-great-brain-is-located-externally", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 481, "htmlBody": "<p><img style=\"vertical-align: middle;\" src=\"http://i166.photobucket.com/albums/u108/Alicorn24/23733stripprint.gif\" alt=\"Dilbert cartoon\" width=\"560\" height=\"170\" /></p>\n<p>How many of the things you \"know\" do you have <em>memorized</em>?</p>\n<p>Do you remember how to spell all of those words you let the spellcheck catch?&nbsp; Do you remember what fraction of a teaspoon of salt goes into that one recipe, or would you look at the list of ingredients to be sure?&nbsp; Do you remember what kinds of plastic they recycle in your neighborhood, or do you delegate that task to a list attached with a magnet to the fridge?</p>\n<p>If I asked you what day of the month it is today, would you know, or would you look at your watch/computer clock/the posting date of this post?</p>\n<p>Before I lost my Palm Pilot, I called it my \"external brain\".&nbsp; It didn't really fit the description; with no Internet access, it mostly held my contact list, class schedule, and grocery list.&nbsp; And a knockoff of Minesweeper.&nbsp; Still, in a real enough sense, it remembered things for me.<a id=\"more\"></a>The vast arena of knowledge at our fingertips in the era of constant computing has, ironically, brought it farther away.&nbsp; It seems nearer: after all, now, if you are curious about Zanzibar, Wikipedia is a few keystrokes away.&nbsp; Before the Internet, you'd probably have been looking at a trip to the library and a while wrestling with the card catalog; and that would be if you lived in an affluent, literate society.&nbsp; If you didn't, good luck knowing Zanzibar exists in the first place!</p>\n<p>But if you were an illiterate random peasant farmer in some historical venue, and you needed to know the growing season of taro or barley or insert-your-favorite-staple-crop-here, Wikipedia would have been superfluous: you would already know it.&nbsp; It would be unlikely that you would find a song lyrics website of any use, because all of the songs you'd care about would be ones you really <em>knew</em>, in the sense of having heard them sung by real people who could clarify the words on request, as opposed to the \"I think I heard half of this on the radio at the dentist's office last month\" sense.</p>\n<p>Everything you would need to know would be important enough to warrant - and keep - a spot in your memory.</p>\n<p>So in a sense, propositional knowledge is being gradually supplanted by the procedural.&nbsp; You need only know <em>how to find </em>information, to be able to use it after a trivial delay.&nbsp; This requires some snippet of propositional data - to find a song lyric, you need a long enough string that you won't turn up 99% noise when you try to Google it! - but mostly, it's a skill, not a fact, that you need to act like you knew the fact.</p>\n<p>It's not clear to me whether this means that we should be alarmed and seek to hone our factual memories... or whether we should devote our attention to honing our Google-fu, as our minds gradually become server-side operations.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TkZ7MFwCi4D63LJ5n": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h7NkpER4Jo8BLWgPD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 30, "extendedScore": null, "score": 6.2e-05, "legacy": true, "legacyId": "1347", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-26T17:57:56.343Z", "modifiedAt": null, "url": null, "title": "Controlling your inner control circuits", "slug": "controlling-your-inner-control-circuits", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:16.762Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fGzPFwAosXXBcv5Jc/controlling-your-inner-control-circuits", "pageUrlRelative": "/posts/fGzPFwAosXXBcv5Jc/controlling-your-inner-control-circuits", "linkUrl": "https://www.lesswrong.com/posts/fGzPFwAosXXBcv5Jc/controlling-your-inner-control-circuits", "postedAtFormatted": "Friday, June 26th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Controlling%20your%20inner%20control%20circuits&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AControlling%20your%20inner%20control%20circuits%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfGzPFwAosXXBcv5Jc%2Fcontrolling-your-inner-control-circuits%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Controlling%20your%20inner%20control%20circuits%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfGzPFwAosXXBcv5Jc%2Fcontrolling-your-inner-control-circuits", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfGzPFwAosXXBcv5Jc%2Fcontrolling-your-inner-control-circuits", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2909, "htmlBody": "<p>On the topic of: <a href=\"http://wiki.lesswrong.com/wiki/Control_theory\">Control theory</a></p>\r\n<p>Yesterday, PJ Eby sent the subscribers of his mailing list a link to an <a href=\"http://thinkingthingsdone.com/signup/TheSelfHelpMyth.pdf\">article</a> describing a control theory/mindhacking insight he'd had. With his permission, here's a summary of that article. I found it potentially life-changing. The article seeks to answer the question, \"why is it that people often stumble upon great self-help techniques or productivity tips, find that they work great, and then after a short while the techniques either become ineffectual or the people just plain stop using them anyway?\", but I found it to have far greater applicability than just that.</p>\r\n<p>Richard Kennaway <a href=\"/lw/dj/what_is_control_theory_and_why_do_you_need_to/\">already mentioned</a> the case of driving a car as an example where the human brain uses control systems, and Eby mentioned another: <em>ask a friend to hold their arm out straight, and tell them that when you push down on their hand, they should lower their arm. And what you&rsquo;ll generally find is that when you push down on their hand, the arm will spring back up before they lower it... and the harder you push down on the hand, the harder the arm will pop back up!&nbsp;</em>That's because the control system in charge of maintaining the arm's position will try to keep up the old position, until one consciously realizes that the arm has been pushed and changes the setting.</p>\r\n<p>Control circuits aren't used just for guiding physical sequences of actions, they also regulate the workings of our mind. A few hours before typing out a previous version of this post, I was starting to feel restless because I hadn't accomplished any work that morning. This has often happened to me in the past - if, at some point during the day, I haven't yet gotten started on doing anything, I begin to feel anxious and restless. In other words, in my brain there's a control circuit monitoring some estimate of \"accomplishments today\". If that value isn't high enough, it starts sending an error signal - creating a feeling of anxiety - in an attempt to bring that value into the desired range.</p>\r\n<p>The problem with this is that more often than not, that anxiety doesn't push me into action. Instead I become paralyzed and <em>incapable</em> of getting anything started. Eby proposes that this is because of two things: one, the control circuits are dumb and don't actually realize what they're doing, so they may actually take counter-productive action. Two, there may be several control circuits in the brain which are actually opposed to each other.</p>\r\n<p>Here we come to the part about productivity techniques often not working. We also have higher-level controllers - control circuits influencing other control circuits. Eby's theory is that many of us have circuits that try to <em>prevent us from doing the things we want to do</em>. When they notice that we've found a method to actually accomplish something we've been struggling with for a long time, they start sending an error signal... causing neural reorganization, eventually ending up at a stage where we don't use those productivity techniques anymore and solving the \"crisis\" of us actually accomplishing things. Moreover, these circuits are to a certain degree predictive, and they can start firing when they pick up on a behavior that only even <em>possibly</em> leads to success - that's when we hear about a great-sounding technique and for some reason never even try it. A higher-level circuit, or a lower-level one set up by the higher-level circuit, actively suppresses the \"let's try that out\" signals sent by the other circuits.<br /><a id=\"more\"></a>But why would we have such self-sabotaging circuits? This ties into Eby's more general theory of the hazards of some kinds of self-motivation. He uses the example of a predator who's chased a human up to a tree. The human, sitting on a tree branch, is in a safe position now, so circuits developed to protect his life send signals telling him to stay there and not to move until the danger is gone. Only if the predator actually starts climbing the tree does the danger become more urgent and the human is pushed to actively flee.<br /><br />Eby then extends this example into a social environment. In a primitive, tribal culture, being seen as useless to the tribe could easily be a death sentence, so we evolved mechanisms to avoid giving the impression of being useless. A good way to avoid showing your incompetence is to simply not do the things you're incompetent at, or things which you suspect you might be incompetent at and that have a great associated cost for failure. If it's important for your image within the tribe that you do not fail at something, then you attempt to avoid doing that.<br /><br />You might already be seeing where this is leading. The things many of us procrastinate on are exactly the kinds of things that are important to us. We're deathly afraid of the consequences of what might happen if we fail at them, so there are powerful forces in play trying to make us not work on them at all. Unfortunately, for beings living in modern society, this behavior is maladaptive and buggy. It leads to us having control circuits which try to keep us unproductive, and when they pick up on things that might make us more productive, they start suppressing our use of those techniques.<br /><br />Furthermore, the control circuits are stupid. They are occasionally capable of being somewhat predictive, but they are fundamentally just doing some simple pattern-matching, oblivious to deeper subtleties. They may end up reacting to wholly wrong inputs. Consider the example of developing a phobia for a particular place, or a particular kind of environment. Something very bad happens to you in that place once, and as a result, a circuit is formed in your brain that's designed to keep you out of such situations in the future. Whenever it detects that you are in a place resembling the one where the incident happened, it starts sending error signals to get you away from there. Only that this is a very crude and unoptimal way of keeping you out of trouble - if a car hit you while you were crossing the road, you might develop a phobia for crossing the road. Needless to say, this is more trouble than it's worth.<br /><br />Another common example might be a musician learning to play an instrument. Learning musicians are taught to practice their instrument in a variety of postures, for otherwise a flutist who's always played his flute sitting down may realize he can't play it while standing up! The reason being that while practicing, he's been setting up a number of control circuits designed to guide his muscles the right way. Those control circuits have no innate knowledge of what muscle postures are integral for a good performance, however. As a result, the flutist may end up with circuits that try to make sure they are sitting down when playing.<br /><br />This kind of malcalibration extends to higher-level circuits as well. Eby writes:</p>\r\n<blockquote>\r\n<p>I know this now, because in the last month or so, I&rsquo;ve been struggling to identify my &ldquo;top-level&rdquo; master control circuits.<br /><br />And you know what I found they were controlling for? Things like:<br /><br />* Being &ldquo;good&rdquo;<br />* Doing the &ldquo;right&rdquo; thing<br />* &ldquo;Fairness&rdquo;<br /><br />But don&rsquo;t be fooled by how harmless or even &ldquo;good&rdquo; these phrases <em>sound</em>.<br /><br />Because, when I broke them down to what subcontrollers they were actually driving, it turned out that &ldquo;being good&rdquo; meant &ldquo;do things for others while ignoring your own needs and being resentful&rdquo;!<br /><br />&ldquo;Fairness&rdquo;, meanwhile, meant, &ldquo;accumulate resentment and injustices in order to be able to justify being selfish later.&rdquo;<br /><br />And &ldquo;doing the right thing&rdquo; translated to, &ldquo;don&rsquo;t do anything unless you can come up with a logical justification for why it&rsquo;s right, so you don&rsquo;t get in trouble, and no-one can criticize you.&rdquo;<br /><br />Ouch!<br /><br />Now, if you look at that list, nowhere on there is something like, &ldquo;go after what I really want and make it happen&rdquo;. Actually doing anything &ndash; in fact, even <em>deciding</em> to do anything! &ndash; was <em>entirely</em> conditional on being able to justify my decisions as &ldquo;fair&rdquo; or &ldquo;right&rdquo; or &ldquo;good&rdquo;, within some extremely <strong>twisted</strong> definitions of those words!</p>\r\n</blockquote>\r\n<p>So that's the crux of the issue. We are wired with a multitude of circuits designed for controlling our behavior... but because those circuits are often stupid, they end up in conflict with each other, and end up monitoring values that don't actually represent the things they ought to.<br /><br />While Eby provides few references and no peer-reviewed experimental work to support his case of motivation systems being controlled in this way, I find it to mesh very well with everything I know about the brain. I took the phobia example from a textbook on biological psychology, while the flutist example came from a lecture by a neuroscientist emphasizing the stupidity of the cerebellum's control systems. Building on systems that were originally developed to control motion and hacking them to also control higher behavior is a very evolution-like thing to do. We already develop control systems for muscle behavior starting from the time when we first learn to control our body as infants, so it's very plausible that we'd also develop such mechanisms for all kinds of higher cognition. The mechanism by they work is also fundamentally very simple, making it easy for new circuits to form: a person ends up in an unpleasant situation, causing an emotional subsystem to flood the whole brain with negative feedback, leading to pattern recognizers which were active at the time to start activating the same kind of negative feedback the next time when they pick up on the same input. (At its simplest, it's probably a case of simple <a href=\"http://en.wikipedia.org/wiki/Hebbian_learning\">Hebbian learning</a>.)<br /><br />Furthermore, since reading his text, I have noticed several things in myself which could only be described as control circuits. After reading Overcoming Bias and Less Wrong for a long time, I've found myself noticing whenever I have a train of thought that seems to be indicative of a number of certain kinds of cognitive biases. In retrospect, that is probably a control circuit that has developed to detect the general appearance of a biased thought and to alert me about it. The anxiety circuit I already mentioned. A closely related circuit is one that causes me to need plenty of time to accomplish whatever it is that I'm doing - if I only have a couple of hours before a deadline, I often freeze up and end up unable of doing anything. This leads to me being at my most productive in the mornings, when I have a feeling of having the whole day for myself and of not being in any rush. That's easily interpreted as a circuit that looks at the remaining time and sends sending an alarm when the time runs low. Actually, the circuit in question is probably even stupid than that, as the feeling of not having any time is often tied only what the clock is, not to the time when I'll be going to bed. If I get up at 2 PM and go to bed at 4 AM, I have just as much time as if I'd get up at 9 AM and went to bed at 11 PM, but the circuit in question doesn't recognize this.<br /><br />So, what can we do about conflicting circuits? Simply recognizing them for what they are is already a big step forward, one which I feel has already helped me overcome some of their effects. Some of them can probably be dismantled simply by identifying them, working out their purpose and deciding it to be unnecessary. (I suspect that this process might actually set up new circuits whose function is to counteract the signals sent by the harmful ones. Maybe. I'm not very sure of what the actual neural mechanism might be.) Eby writes:</p>\r\n<blockquote>\r\n<p>So, you want to build Desire and Awareness by tuning in to the right qualities to perceive. Then, you need to eliminate any conflicts that come up.<br /><br />Now, a lot of times, you can do this by simple negotiation with yourself. Just sit and write down all your objections or issues about something, and then go through them one at a time, to figure out how you can either work around the problem, or find another way to get your other needs met.<br /><br />Of course, you have to enter this process in good faith; if you judge yourself for say, wanting lots of chocolate, and decide that you shouldn&rsquo;t want it, that&rsquo;s not going to work.<br /><br />But it might work, to be willing to give up chocolate for a while, in order to lose weight. The key is that you need to actually imagine what it would be like to give it up, and then find out whether you can be &ldquo;okay&rdquo; with that.<br /><br />Now, sadly, about 97% of the people who read this are going to take that last paragraph and go, &ldquo;yeah, sure, I&rsquo;m going to give up [whatever]&rdquo;, but without actually considering what it would be like to do so.<br /><br />And those people are going to fail.<br /><br />And I kind of debated whether or not I should even mention this method here, because frankly, I don&rsquo;t trust most people&rsquo;s controllers any further than I can reprogram them (so to speak).<br /><br />See, I know from bitter experience that my own controllers for things like &ldquo;being smart&rdquo; used to make me rationalize this sort of thing, skipping the actual mental work involved in a technique, because &ldquo;clearly I&rsquo;m smart enough not to need to do all that.&rdquo;<br /><br />And so I&rsquo;d assume that just &ldquo;thinking&rdquo; about it was enough, without really going through the mental experience needed to make it work. So, most of the people who read this, are going to take that paragraph above where I explained the deep, dark, master-level mindhacking secret, and find a way to ignore it.<br /><br />They&rsquo;re going to say things like, &ldquo;Is that all?&rdquo; &ldquo;Oh, I already knew that.&rdquo; And they&rsquo;re not going to really sit down and consider all the things that might conflict with what they say they want.<br /><br />If they want to be wealthy, for example, they&rsquo;re almost certainly not going to sit down and consider whether they&rsquo;ll lose their friends by doing so, or end up having strained family relations. They&rsquo;re not considering whether they&rsquo;re going to feel guilty for making a lot of money when other people in the world don&rsquo;t have any, or for doing it easily when other people are working so hard.<br /><br />They&rsquo;re not going to consider whether being wealthy or fit or confident will make them like the people they hate, or whether maybe they&rsquo;re really only afraid of being broke!<br /><br />But all of them will read everything I&rsquo;ve just written, and assume it doesn&rsquo;t apply to them, or that they&rsquo;ve already taken all that into account.<br /><br />Only they haven&rsquo;t.<br /><br />Because if they had, they would have already changed.</p>\r\n</blockquote>\r\n<p>That's a pretty powerful reminder not to ignore your controllers. When you've been reading this, some controller that tries to keep you from doing things has probably already picked up on the excitement some emotional system might now be generating... meaning that you might be about to stumble upon a technique that might actually make you more productive... causing signals to be sent out to suppress attempts to even try it out. Simply acknowleding its existence isn't going to be enough - you need to actively think things out, identify different controllers within you, and dismantle them.<br /><br />I feel I've managed to avoid the first step, of not doing anything even after becoming aware of the problem. I've been actively looking at different control circuits, some of which have plagued me for quite a long time, and I at least seem to have managed to overcome them. My worry is that there might be some high-level circuit which is even now coming online to prevent me from using this technique - to make me forget about the whole thing, or to simply not use it even though I know of it. It feels that the best way to counteract that is to try to consciously set up new circuits dedicated to the task of monitoring for the presence of new circuits, and alarming me of their presence. In other words, keep actively looking for anything that might be a mental control circuit, and teach myself to notice them.</p>\r\n<p>(And now, Eby, please post any kind of comment here so that we can vote it up and give you your fair share of this post's karma. :))</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"9mShmhRFzBat3523A": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fGzPFwAosXXBcv5Jc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 65, "baseScore": 53, "extendedScore": null, "score": 8.3e-05, "legacy": true, "legacyId": "1349", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 53, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 159, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fJKbCXrCPwAR5wjL8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-06-29T12:54:11.922Z", "modifiedAt": "2021-07-26T20:30:53.583Z", "url": null, "title": "What's In A Name?", "slug": "what-s-in-a-name", "viewCount": null, "lastCommentedAt": "2019-08-27T04:51:29.061Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eRhFaibbTeGbjdaaf/what-s-in-a-name", "pageUrlRelative": "/posts/eRhFaibbTeGbjdaaf/what-s-in-a-name", "linkUrl": "https://www.lesswrong.com/posts/eRhFaibbTeGbjdaaf/what-s-in-a-name", "postedAtFormatted": "Monday, June 29th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What's%20In%20A%20Name%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat's%20In%20A%20Name%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeRhFaibbTeGbjdaaf%2Fwhat-s-in-a-name%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What's%20In%20A%20Name%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeRhFaibbTeGbjdaaf%2Fwhat-s-in-a-name", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeRhFaibbTeGbjdaaf%2Fwhat-s-in-a-name", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 768, "htmlBody": "<p><em><strong>&nbsp;&nbsp; Marge:</strong> You changed your name&nbsp;without consulting me?<br /><strong>&nbsp;&nbsp; Homer: </strong>That's the way Max Power is, Marge.&nbsp; Decisive.<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; --</em>The Simpsons</p>\r\n<p>In honor of <a href=\"/lw/11h/controlling_your_inner_control_circuits/vw6\">Will Powers and his theories about self-control</a>, today I would like to talk about my favorite bias ever, the name letter effect. The name letter effect doesn't cause global existential risk or stock market crashes, and it's pretty far down on the list of things to compensate for. But it's a good example of just how insidious biases can be and of the egoism that permeates every level of the mind.</p>\r\n<p>The name letter effect is your subconscious preference for things that sound like your own name. This might be expected to mostly apply to small choices like product brand names, but it's been observed in choices of spouse, city of residence, and even career.&nbsp;Some evidence comes from Pelham et al's <a href=\"http://www.stat.columbia.edu/~gelman/stuff_for_blog/susie.pdf\">Why Susie Sells Seashells By The Seashore</a>:</p>\r\n<p>The paper's first few studies investigate the relationship between a person's name and where they live. People named Phil were found more frequently than usual in Philadelphia, people named Jack in Jacksonville, people named George in Georgia, and so on with p &lt; .001. To eliminate the possibility of the familiarity effect causing parents to subconsciously name their children after their place of residence, further studies were done with surnames and with people who moved later in life, both with the same results. The results held across US and Canadian city names as well as US state names, and were significant both for first name and surname.</p>\r\n<p>In case that wasn't implausible enough, the researchers also looked at association between birth date and city of residence: that is, were people born on 2/02 more likely to live in the town of Two Harbors, and 3/03 babies more likely to live in Three Forks? With p = .003, yes, they are.</p>\r\n<p>The researchers then moved on to career choices. They combed the records of the American Dental Association and the American Bar association looking for people named either Dennis, Denice, Dena, Denver, et cetera, or Lawrence, Larry, Laura, Lauren, et cetera. That is: were there more dentists named Dennis and lawyers named Lawrence than vice versa? Of the various statistical analyses they performed, most said yes, some at &lt; .001 level. Other studies determined that there was a suspicious surplus of geologists named Geoffrey, and that hardware store owners were more likely to have names starting with 'H' compared to roofing store owners, who were more likely to have names starting with 'R'.</p>\r\n<p>Some other miscellaneous findings: people are more likely to donate to Presidential candidates whose names begin with the same letter as their own, people are more likely to marry spouses whose names begin with the same letter as their own, that women are more likely to show name preference effects than men (but why?), and that batters with names beginning in 'K' are more likely than others to strike out (strikeouts being symbolized by a 'K' on the records).</p>\r\n<p>If you have any doubts about the validity of the research, I urge you to read the linked paper. It's a great example of researchers who go above and beyond the call of duty to eliminate as many confounders as possible.</p>\r\n<p>The name letter effect&nbsp;is a great addition to any list of psychological curiosities, but it does have some more solid applications. I often use it as my first example when I'm introducing the idea of subconscious biases to people, because it's clear, surprising, and has major real-world effects. It also tends to shut up people who don't believe there are subconscious influences on decision-making, and who are always willing to find some excuse for why a supposed \"bias\" could actually be an example of legitimate decision-making.</p>\r\n<p>And it introduces the concept of implicit egoism, the tendency to prefer something just because it's associated with you. It's one possible explanation for the endowment effect, and if it applies to <em>my</em> beliefs as strongly as to <em>my </em>personal<em> </em>details or <em>my </em>property, it's yet another mechanism by which opinions become calcified.</p>\r\n<p>This is also an interesting window onto the complex and important world of self-esteem. Jones, Pelham et al suggest that the name preference effect is either involved in or a byproduct of some sort of self-esteem regulatory system. <a href=\"http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6WJB-45B58TX-8&amp;_user=10&amp;_coverDate=03%2F31%2F2002&amp;_rdoc=1&amp;_fmt=&amp;_orig=search&amp;_sort=d&amp;view=c&amp;_acct=C000050221&amp;_version=1&amp;_urlVersion=0&amp;_userid=10&amp;md5=17a8092fdf8cdf31d85dc23926ba54e1\">They find</a> that name preferences are most common among high self-esteem people who have just experienced threats to their self-esteem, almost as if it is a reactive way of saying \"No, you really are that great.\" I think an examination of how different biases interact with self-esteem would be a profitable direction for future research.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4R8JYu4QF2FqzJxE5": 1, "gHCNhqxuJq2bZ2akb": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eRhFaibbTeGbjdaaf", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 47, "extendedScore": null, "score": 0.00010026057662193974, "legacy": true, "legacyId": "1352", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 139, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2009-06-29T12:54:11.922Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-01T02:19:18.764Z", "modifiedAt": null, "url": null, "title": "Atheism = Untheism + Antitheism", "slug": "atheism-untheism-antitheism", "viewCount": null, "lastCommentedAt": "2016-02-16T02:07:49.293Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PYtus925Gcg7cqTEq/atheism-untheism-antitheism", "pageUrlRelative": "/posts/PYtus925Gcg7cqTEq/atheism-untheism-antitheism", "linkUrl": "https://www.lesswrong.com/posts/PYtus925Gcg7cqTEq/atheism-untheism-antitheism", "postedAtFormatted": "Wednesday, July 1st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Atheism%20%3D%20Untheism%20%2B%20Antitheism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAtheism%20%3D%20Untheism%20%2B%20Antitheism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPYtus925Gcg7cqTEq%2Fatheism-untheism-antitheism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Atheism%20%3D%20Untheism%20%2B%20Antitheism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPYtus925Gcg7cqTEq%2Fatheism-untheism-antitheism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPYtus925Gcg7cqTEq%2Fatheism-untheism-antitheism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1053, "htmlBody": "<p>One occasionally sees such remarks as, \"What good does it do to go around being angry about the nonexistence of God?\" (on the one hand) or \"Babies are natural atheists\" (on the other).&nbsp; It seems to me that such remarks, and the rather silly discussions that get started around them, show that the concept \"Atheism\" is really made up of two distinct components, which one might call \"untheism\" and \"antitheism\".</p>\n<p>A pure \"untheist\" would be someone who grew up in a society where the concept of God had simply never been invented - where writing was invented before agriculture, say, and the first plants and animals were domesticated by early scientists.&nbsp; In this world, superstition never got past the hunter-gatherer stage - a world seemingly haunted by mostly amoral spirits - before coming into conflict with Science and getting slapped down.</p>\n<p>Hunter-gatherer superstition isn't much like what we think of as \"religion\".&nbsp; Early Westerners often derided it as not really being religion at all, and they were right, in my opinion.&nbsp; In the hunter-gatherer stage the supernatural agents aren't particularly moral, or charged with enforcing any rules; they may be placated with ceremonies, but not <em>worshipped.</em>&nbsp; But above all - they haven't yet<em> split their epistemology.</em>&nbsp; Hunter-gatherer cultures don't have special rules for <em>reasoning about</em> \"supernatural\" entities, or indeed an explicit distinction between supernatural entities and natural ones; the thunder spirits are just out there in the world, as evidenced by lightning, and the rain dance is supposed to manipulate them - it may not be perfect but it's the best rain dance developed so far, there was that famous time when it worked...</p>\n<p>If you could show hunter-gatherers a raindance that called on a different spirit and worked with perfect reliability, or, equivalently, a desalination plant, they'd probably chuck the <em>old </em>spirit right out the window.&nbsp; Because there are no <a href=\"http://wiki.lesswrong.com/wiki/Anti-Epistemology\">special rules for reasoning about it</a> - nothing that denies the validity of the <a href=\"/lw/i8/religions_claim_to_be_nondisprovable/\">Elijah Test</a> that the previous rain-dance just failed.&nbsp; Faith is a post-agricultural concept.&nbsp; Before you have chiefdoms where the priests are a branch of government, the gods aren't <em>good</em>, they don't enforce the chiefdom's rules, and there's no penalty for questioning them.</p>\n<p>And so the Untheist culture, when it invents science, simply concludes in a very ordinary way that rain turns out to be caused by condensation in clouds rather than rain spirits; and at once they say \"Oops\" and chuck the old superstitions out the window; because they <em>only</em> got as far as superstitions, and not as far as <a href=\"http://wiki.lesswrong.com/wiki/Anti-Epistemology\">anti-epistemology</a>.</p>\n<p>The Untheists don't know they're \"atheists\" because no one has ever told them what they're supposed to not believe in - nobody has invented a \"high god\" to be chief of the pantheon, let alone monolatry or monotheism.<a id=\"more\"></a></p>\n<p>However, the Untheists do know that they don't believe in tree spirits.&nbsp; And we shall even suppose that the Untheists don't believe in tree spirits, because they have a sophisticated and <em>good </em>epistemology - they understand why it is in general a bad idea to <a href=\"/lw/tv/excluding_the_supernatural/\">postulate ontologically basic mental entities</a>.</p>\n<p>So if you come up to the Untheists and say:</p>\n<p style=\"padding-left: 30px;\">\"The universe was created by God -\"</p>\n<p style=\"padding-left: 30px;\">\"By what?\"</p>\n<p style=\"padding-left: 30px;\">\"By a, ah, um, God is the Creator - the Mind that chose to make the universe -\"</p>\n<p style=\"padding-left: 30px;\">\"So the universe was created by an intelligent agent.&nbsp; Well, that's the standard Simulation Hypothesis, but do you have actual evidence confirming this?&nbsp; You sounded very certain -\"</p>\n<p style=\"padding-left: 30px;\">\"No, not like the Matrix!&nbsp; God isn't in another universe simulating this one, God just... is.&nbsp; He's indescribable.&nbsp; He's the First Cause, the Creator of <em>everything </em>-\"</p>\n<p style=\"padding-left: 30px;\">\"Okay, that sounds like you just postulated an ontologically basic mental entity.&nbsp; <em>And </em>you offered a <a href=\"/lw/iu/mysterious_answers_to_mysterious_questions/\">mysterious answer to a mysterious question</a>.&nbsp; Besides, where are you <em>getting</em> all this stuff?&nbsp; Could you maybe start by telling us about your evidence - the new observation you're trying to interpret?\"</p>\n<p style=\"padding-left: 30px;\">\"I don't need any evidence!&nbsp; I have faith!\"</p>\n<p style=\"padding-left: 30px;\">\"You have <em>what?</em>\"</p>\n<p>And at this very moment the Untheists have become, for the first time, Atheists.&nbsp; And what they just acquired, between the two points, was Antitheism - explicit arguments against explicit theism.&nbsp; You can be an Untheist without ever having heard of God, but you can't be an Antitheist.</p>\n<p>Of course the Untheists are not inventing <em>new </em>rules to refute God, just <em>applying </em>their standard epistemological guidelines that their civilization developed in the course of rejecting, say, <a href=\"/lw/iu/mysterious_answers_to_mysterious_questions/\">vitalism</a>.&nbsp; But then that's just what we rationalist folk claim antitheism is <em>supposed</em> to be, in our own world: a strictly standard analysis of religion which <em>turns out</em> to issue a strong rejection - both epistemically and morally, and not after too much time.&nbsp; Every antitheist argument is supposed to be a special case of <em>general </em>rules of epistemology and morality which ought to have applications beyond religion - visible in the encounters of science with vitalism, say.</p>\n<p>With this distinction in hand, you can make a bit more sense of some modern debates - for example, \"Why care so much about God not existing?\" could become \"What is the public benefit from publicizing antitheism?\"&nbsp; Or \"What good is it to just be against something?&nbsp; Where is the positive agenda?\" becomes \"Less antitheism and more untheism in our atheism, please!\"&nbsp; And \"Babies are born atheists\", which sounds a bit odd, is now understood to sound odd because babies have no grasp of antitheism.</p>\n<p>And as for the claim that religion is compatible with Reason - well, is there a single religious claim that a well-developed, sophisticated Untheist culture would <em>not</em> reject?&nbsp; When they have no reason to suspend judgment, and no anti-epistemology of separate magisteria, and no established religions in their society to avoid upsetting?</p>\n<p>There's nothing inherently fulfilling about arguing against Goddism - in a society of Untheists, no one would ever give the issue a second thought.&nbsp; But in this world, at least, insanity is not a good thing, and sanity is worth defending, and explicit antitheism by the likes of Richard Dawkins would surely be a public service conditioning on the proposition of it actually working.&nbsp; (Which it may in fact be doing; the next generation is growing up increasingly atheist.)&nbsp; Yet in the long run, the goal is an Untheistic society, not an Atheistic one - one in which the question \"What's left, when God is gone?\" is greeted by a puzzled look and \"What exactly is missing?\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NSMKfa8emSbGNXRKD": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PYtus925Gcg7cqTEq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 120, "baseScore": 130, "extendedScore": null, "score": 0.0002, "legacy": true, "legacyId": "1354", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 131, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 179, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fAuWLS7RKWD2npBFR", "u6JzcFtPGiznFgDxP", "6i3zToomS86oj9bS6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-01T18:39:03.138Z", "modifiedAt": null, "url": null, "title": "Book Review: Complications", "slug": "book-review-complications", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:03.214Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Annoyance", "createdAt": "2009-02-28T04:06:40.600Z", "isAdmin": false, "displayName": "Annoyance"}, "userId": "yEm6LWatswJYGwBFq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yXvDCDDvuJdharpCF/book-review-complications", "pageUrlRelative": "/posts/yXvDCDDvuJdharpCF/book-review-complications", "linkUrl": "https://www.lesswrong.com/posts/yXvDCDDvuJdharpCF/book-review-complications", "postedAtFormatted": "Wednesday, July 1st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Book%20Review%3A%20Complications&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABook%20Review%3A%20Complications%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyXvDCDDvuJdharpCF%2Fbook-review-complications%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Book%20Review%3A%20Complications%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyXvDCDDvuJdharpCF%2Fbook-review-complications", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyXvDCDDvuJdharpCF%2Fbook-review-complications", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 516, "htmlBody": "<p>Atul Gawande's <strong>Complications:&nbsp; A Surgeon's Notes on an Imperfect Science</strong><em> </em>is a mixed bag for rationalists. Written as a series of essays organized into three sections entitled <em>Fallibility</em>, <em>Mystery</em>, and <em>Uncertainty</em>, the book as a whole is of questionable value, but the sections need to be considered individually for their worth to be accurately recognized.<a id=\"more\"></a></p>\n<p><em>Fallibility</em> examines a number of issues involving error and how it can be avoided in medicine, including but not limited to:&nbsp; how computers are better than humans at making diagnoses, the factors that are known to negatively influence human reasoning when applied to diagnosis and treatment, how and why doctors make errors, what happens when doctors \"go bad\", and ways that the system fails and succeeds at curbing irresponsible physicians.&nbsp; The section dealing with the field of anesthesiology is especially interesting, as Gawande recounts the changes that made it possible for the death rate due to general anesthesia to be reduced to a twentieth of its previous value midway through this century.</p>\n<p>What is distressing is that <em>Mystery</em> and <em>Uncertainty </em>do not represent good examples of the principles that Gawande demonstrates he understands in <em>Fallibility</em>, supporting his assertions with vivid anecdotes instead of reasoning from principles.&nbsp; One story is especially obnoxious, as Gawande recounts how a \"gut feeling\" about a case of infection causes him to persuade a patient to undergo an extensive biopsy which reveals she has life-threatening necrotizing fasciitis, caused by the popularly-known \"flesh-eating bacteria\".&nbsp; He explicitly acknowledges that he had no logical reason for regarding the case as unusual, that all appearances indicated a routine cellulitis which was approximately three thousand times more likely than necrotizing fasciitis was - and that he had recently taken care of a patient who had died agonizingly from a massive infection of the flesh-eating bacteria.&nbsp; And yet the story is presented as an example of how to deal with uncertainty.</p>\n<p>If a normal infection had been present, the incident would likely not have been recounted and possibly not even remembered with clarity by Gawande.&nbsp; We don't know - and likely Gawande doesn't know either - whether his hunches are more likely than not to be accurate.&nbsp; As he discusses himself earlier in the book, such hunches are known to be usually wrong, but they make vivid anecdotes - which renders them potent and dangerous lures for erroneous thinking.</p>\n<p>On other problematic topic, he decries the relative rarity of autopsies and notes that they demonstrate that doctors are wrong about the cause of death and diagnosis of roughly one-third of patients who die while in care.&nbsp; Yet despite also noting that this fraction has not changed since the 1930s, he implies that the reduction is due to the unwillingness of doctors to trouble grieving family members rather than drawing the obvious conclusion that doctors don't utilize or care about autopsy results.</p>\n<p>The book is worth reading, both to gain knowledge about the troubling subject of medical error and how it is and isn't reduced, and as an object lesson in how difficult it is for even intelligent and informed people to actually apply principles of proper reasoning.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4Kcm4etxAJjmeDkHP": 2, "xHjy88N2uJvGdgzfw": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yXvDCDDvuJdharpCF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 13, "extendedScore": null, "score": 3e-05, "legacy": true, "legacyId": "1357", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-02T04:00:53.593Z", "modifiedAt": null, "url": null, "title": "Open Thread: July 2009", "slug": "open-thread-july-2009", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:17.678Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "odm3FHPzgbiGpWsSg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HCTzukWgCtdgtJjRS/open-thread-july-2009", "pageUrlRelative": "/posts/HCTzukWgCtdgtJjRS/open-thread-july-2009", "linkUrl": "https://www.lesswrong.com/posts/HCTzukWgCtdgtJjRS/open-thread-july-2009", "postedAtFormatted": "Thursday, July 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20July%202009&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20July%202009%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHCTzukWgCtdgtJjRS%2Fopen-thread-july-2009%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20July%202009%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHCTzukWgCtdgtJjRS%2Fopen-thread-july-2009", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHCTzukWgCtdgtJjRS%2Fopen-thread-july-2009", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 29, "htmlBody": "<p>Here's our place to discuss Less Wrong topics that have not appeared in recent posts. Have fun building smaller brains inside of your brains (or not, as you please).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HCTzukWgCtdgtJjRS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 5.04949250508276e-07, "legacy": true, "legacyId": "1358", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 251, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-02T09:56:03.799Z", "modifiedAt": null, "url": null, "title": "Fourth London Rationalist Meeting?", "slug": "fourth-london-rationalist-meeting", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:02.365Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DHbJk5uDXz2vW56wq/fourth-london-rationalist-meeting", "pageUrlRelative": "/posts/DHbJk5uDXz2vW56wq/fourth-london-rationalist-meeting", "linkUrl": "https://www.lesswrong.com/posts/DHbJk5uDXz2vW56wq/fourth-london-rationalist-meeting", "postedAtFormatted": "Thursday, July 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fourth%20London%20Rationalist%20Meeting%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFourth%20London%20Rationalist%20Meeting%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDHbJk5uDXz2vW56wq%2Ffourth-london-rationalist-meeting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fourth%20London%20Rationalist%20Meeting%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDHbJk5uDXz2vW56wq%2Ffourth-london-rationalist-meeting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDHbJk5uDXz2vW56wq%2Ffourth-london-rationalist-meeting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<p>It's been the first Sunday of the month so far, but I haven't seen any announcement for this month yet. There was <a href=\"/lw/106/london_rationalist_meetups_bikeshed_painting/\">a discussion</a>, but no conclusion. Is anything happening?</p>\n<p>ETA: This would have appeared a day and a half ago, but I did not notice that it had only been stored as a draft and not published. When logged in, it was impossible to notice that I was the only person seeing this. Feature request for this site: add a visual indication that something is only a draft, e.g. a \"Publish\" link, perhaps with the words somewhere, <em>Unpublished draft</em>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DHbJk5uDXz2vW56wq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "1355", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iQi7roeLMEriey3ty"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-02T18:35:19.802Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes - July 2009", "slug": "rationality-quotes-july-2009", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:01.996Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SilasBarta", "createdAt": "2009-03-01T00:03:34.864Z", "isAdmin": false, "displayName": "SilasBarta"}, "userId": "zDPSZfarhLM7Gehug", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/S8ysxzgraSeuBXnpk/rationality-quotes-july-2009", "pageUrlRelative": "/posts/S8ysxzgraSeuBXnpk/rationality-quotes-july-2009", "linkUrl": "https://www.lesswrong.com/posts/S8ysxzgraSeuBXnpk/rationality-quotes-july-2009", "postedAtFormatted": "Thursday, July 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20-%20July%202009&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20-%20July%202009%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS8ysxzgraSeuBXnpk%2Frationality-quotes-july-2009%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20-%20July%202009%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS8ysxzgraSeuBXnpk%2Frationality-quotes-july-2009", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS8ysxzgraSeuBXnpk%2Frationality-quotes-july-2009", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 101, "htmlBody": "<p>(Last month's started a little late, I thought I'd bring it back to its original schedule.)</p>\n<p>A monthly thread for posting any interesting rationality-related quotes you've seen recently on the Internet, or had stored in your quotesfile for ages.</p>\n<ul>\n<li>Please post all quotes separately (so that they can be voted up (or down) separately) unless they are strongly related/ordered. </li>\n<li>Do not quote yourself (or your&nbsp;sockpuppets). </li>\n<li>Do not quote&nbsp;comments/posts on LW/OB - if we do this, there should be a separate thread for it. </li>\n<li>No more than 5 quotes per person per monthly thread, please. </li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "S8ysxzgraSeuBXnpk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 7, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "1359", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 185, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-02T20:45:28.821Z", "modifiedAt": null, "url": null, "title": "Harnessing Your Biases", "slug": "harnessing-your-biases", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:02.312Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "swestrup", "createdAt": "2009-02-28T21:56:25.664Z", "isAdmin": false, "displayName": "swestrup"}, "userId": "od7qdMLon3iWYxetg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Sq87zh3ZGPwShKDEJ/harnessing-your-biases", "pageUrlRelative": "/posts/Sq87zh3ZGPwShKDEJ/harnessing-your-biases", "linkUrl": "https://www.lesswrong.com/posts/Sq87zh3ZGPwShKDEJ/harnessing-your-biases", "postedAtFormatted": "Thursday, July 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Harnessing%20Your%20Biases&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHarnessing%20Your%20Biases%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSq87zh3ZGPwShKDEJ%2Fharnessing-your-biases%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Harnessing%20Your%20Biases%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSq87zh3ZGPwShKDEJ%2Fharnessing-your-biases", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSq87zh3ZGPwShKDEJ%2Fharnessing-your-biases", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 930, "htmlBody": "<p>Theoretically, my 'truth' function, the amount of evidence I need to cache something as 'probably true and reliable' should be a constant. I find, however, that it isn't. I read a large amount of scientific literature every day, and only have time to investigate a scant amount of it in practice. So, typically I rely upon science reporting that I've found to be accurate in the past, and only investigate the few things that have direct relevance to work I am doing (or may end up doing).</p>\n<p>Today I noticed something about my habits. I saw an article on <a href=\"http://www.newscientist.com/article/mg20227101.300-what-string-theory-is-really-good-for.html?full=true\">how string theory was making testable predictions in the realm of condensed matter physics</a>, and specifically about room-temperature superconductors. While a pet interest of mine, this is not an area that I'm ever likely to be working in, but the article seemed sound and so I decided it was an interesting fact, and moved on, not even realizing that I had cached it as probably true.</p>\n<p>A few minutes later it occurred to me that some of my friends might also be interested in the article. I have a Google RSS feed that I use to republish occasional articles that I think are worth reading. I have a known readership of all of 2. Suddenly, I discovered that what I had been willing to accept as 'probably true' on my own behalf was no longer good enough. Now I wanted to look at the original paper itself, and to see if I could find any learn&eacute;d refutations or comments.</p>\n<p>This seems to be because my reputation was now, however tangentially, \"on the line\" since I have a reputation in my circle of friends as the science geek and would not want to damage it by steering someone wrong. Now, clearly this is wrong headed. My theory of truth should be my theory of truth, period.</p>\n<p>One could argue, I suppose, that information that I store internally can only affect my own behavior while information that I disseminate can affect the behaviour of an arbitrarily large group of people, and so a more stringent standard should apply to things I tell others. In fact that was the first justification that sprang to mind when I noticed my double standard.</p>\n<p>Its a bogus argument though, as none of my friends are likely to repeat the article or post it in their blogs and so the dissemination has only a tiny probability of propagating by that route. However, once its in my head and I'm treating it as true, I'm very likely to trot it out as an interesting fact when I'm talking at Science Fiction conventions or to groups of interested geeks. If anything, the standard for my believing something should be <strong>more</strong> stringent than my standard for repeating it, not the other way around.</p>\n<p>But, the title of this post is \"Harnessing Your Biases\" and it seems to me that if I am going to have this strange predisposition to check more carefully if I am going to publish something, then maybe I need to set up a blog of things I have read that I think are true. It can just be an edited feed of my RSS stream, since this is simple to put together. Then I may find myself being more careful in what I accept as true. The mere fact that I have the feed and that its public (although I doubt that anyone would, in fact, read it), would make me more careful. Its even possible that it will contain very few articles as I would find I don't have time to investigate interesting claims well enough to declare them true, but this will have the positive side effect that I won't go around caching them internally as true either.</p>\n<p>I think that, in many ways, this is why, in the software field, code reviews are universally touted as an extraordinarily cheap and efficient way of improving code design and documentation while decreasing bugs, and yet is very hard to get put into practice. The idea is that after you've written any piece of code, you give it to a coworker to critique before you put it in the code base. If they find too many things to complain about, it goes back for revision before being given to yet another coworker to check. This continues until its deemed acceptable.</p>\n<p>In practice, the quality of work goes way up and the speed of raw production goes down marginally. The end result is code that needs far less debugging and so the number of working lines of code produced per day goes way up. I think this is because programmers in such a regime quickly find that the testing and documenting that they think is 'good enough' when their work is not going to be immediately reviewed is far less than the testing and documenting they do when they know they have to hand it to a coworker to criticize. The downside, of course, is that they are now opening themselves up for criticism on a daily basis, and this is something that few folks enjoy no matter how good it is for them, and so the practice continues to be quite rare due to programmer resistance to the idea.</p>\n<p>This appears to be two different ways in which to harness the bias that folks have to do better (or more careful) work when it is going to be examined, to achieve better results. Can anyone else here think of other biases that can be exploited in useful ways to leverage greater productivity or reliability in projects?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XYHzLjwYiqpeqaf4c": 1, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Sq87zh3ZGPwShKDEJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 13, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "1295", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-03T13:01:55.583Z", "modifiedAt": null, "url": null, "title": "The Fixed Sum Fallacy", "slug": "the-fixed-sum-fallacy", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:02.218Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7T5J3WM5zqcnTPofS/the-fixed-sum-fallacy", "pageUrlRelative": "/posts/7T5J3WM5zqcnTPofS/the-fixed-sum-fallacy", "linkUrl": "https://www.lesswrong.com/posts/7T5J3WM5zqcnTPofS/the-fixed-sum-fallacy", "postedAtFormatted": "Friday, July 3rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Fixed%20Sum%20Fallacy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Fixed%20Sum%20Fallacy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7T5J3WM5zqcnTPofS%2Fthe-fixed-sum-fallacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Fixed%20Sum%20Fallacy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7T5J3WM5zqcnTPofS%2Fthe-fixed-sum-fallacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7T5J3WM5zqcnTPofS%2Fthe-fixed-sum-fallacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 350, "htmlBody": "<p>(<strong style=\"font-weight: bold;\">Update:</strong>&nbsp;Patrick points out the subject of this post is already well-known as the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Gambler's_fallacy\">gambler's fallacy</a>. I really should have read Tversky and Kahneman before posting.)</p>\n<p>You're flipping a coin 100 times, the first five throws came up heads, what do you expect on the next throw? If you believe the coin to be fair, you allocate 0.5 credence to each face coming up. If your Bayesian prior allowed for biased coins, you update and answer something like 0.6 to 0.4. So far it's all business as usual.</p>\n<p>There exists, however, a truly bizarre third possibility that assigns <em>reduced</em> credence to heads. The reasoning goes like this: at the outset we expected about 50 heads and 50 tails. Your first five throws have <em>used up</em> some of the available heads, while all 50 tails are still waiting for us ahead. When presented so starkly, the reasoning sounds obviously invalid, but here's the catch: people use it a lot, especially when thinking about stuff that matters to them. Happy days viewed as payback for sad days, rich times for poor times, poor people suffering because rich people wallow, and of course all of that vice versa.</p>\n<p>I initially wanted to dub this the \"fallacy of fate\" but decided to leave that lofty name available for some equally lofty concept. \"Fallacy of scarcity\", on the other hand, is actively used but doesn't quite cover all the scenarios I had in mind. So let's call this way of thinking the \"fixed sum fallacy\", or maybe \"counterbalance bias\".</p>\n<p>Now contrarians would point out that some things in life <em>are</em> fixed-sum, e.g. <a href=\"http://www.overcomingbias.com/2009/05/mea-cupla-positionality-data.html\">highly positional values</a>. But other things aren't. Your day-to-day happiness obviously resembles repeatedly throwing a biased coin more than it resembles withdrawing value from a fixed pool: being happy today doesn't decrease your average happiness over all future days. (I have no sources on that besides my common sense; if I'm wrong, call me out.) So we could naturally hypothesize that fixed-sum thinking, when it arises, serves as some kind of coping mechanism. Maybe the economists or psychologists among us could say more; sounds like a topic for Robin?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1, "4R8JYu4QF2FqzJxE5": 1, "ZzxvopS4BwLuQy42n": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7T5J3WM5zqcnTPofS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 7e-06, "legacy": true, "legacyId": "1361", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-03T17:59:38.239Z", "modifiedAt": null, "url": null, "title": "Avoiding Failure: Fallacy Finding", "slug": "avoiding-failure-fallacy-finding", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:07.129Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Patrick", "createdAt": "2009-02-27T08:09:44.663Z", "isAdmin": false, "displayName": "Patrick"}, "userId": "KC7mjSorWj2XsdL3v", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hA2q5nbvxxPrpZGgR/avoiding-failure-fallacy-finding", "pageUrlRelative": "/posts/hA2q5nbvxxPrpZGgR/avoiding-failure-fallacy-finding", "linkUrl": "https://www.lesswrong.com/posts/hA2q5nbvxxPrpZGgR/avoiding-failure-fallacy-finding", "postedAtFormatted": "Friday, July 3rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Avoiding%20Failure%3A%20Fallacy%20Finding&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAvoiding%20Failure%3A%20Fallacy%20Finding%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhA2q5nbvxxPrpZGgR%2Favoiding-failure-fallacy-finding%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Avoiding%20Failure%3A%20Fallacy%20Finding%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhA2q5nbvxxPrpZGgR%2Favoiding-failure-fallacy-finding", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhA2q5nbvxxPrpZGgR%2Favoiding-failure-fallacy-finding", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 262, "htmlBody": "<p>When I was in high school, one of the exercises we did was to take a newspaper column, and find all of the fallacies it employed. It was a fun thing to do, and is good awareness raising for critical thinking, but it probably wouldn't be enough to stave off being deceived by an artful propagandist unless I did it until it was reflexive. To catch the fallacy being, I usually have to read a sentence three or four times to see the underlying logic behind it and remember why the logic is invalid, when I'm confronted by something as fallacy ridden as an ad for the Love Calculator, I just give up in exhaustion. Worse, when I'm watching television, I can't even rewind to see what they said (I suspect the fallacy count is higher too).</p>\n<p>To counter this, (and to further hone my fallacy finding skills), I've extended the fallacy finding exercise to work on video. Take a video from a genre that generally has a high fallacy per minute ratio (e.g. Campaign ads, political debates, speeches, regular ads, Oprah) and edit the video to play a klaxon sound whenever someone commits a logical fallacy or gets a fact wrong, followed by the name of the fallacy they committed flashing on screen.</p>\n<p>EDIT: I've made one of these and&nbsp;<a href=\"http://www.youtube.com/watch?v=AJBeZw487ec\" target=\"_blank\">uploaded it to Youtube.</a> Thank you Eliezer and CannibalSmith for the encouragement. You can find other debates at CNN, and youtube lets you do annotations so no editing software is technically required. I'll be posting further videos to this post as I make/find them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dJ6eJxJrCEget7Wb6": 1, "Ng8Gice9KNkncxqcj": 1, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hA2q5nbvxxPrpZGgR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 14, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "1362", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-04T18:40:02.830Z", "modifiedAt": null, "url": null, "title": "Not Technically Lying", "slug": "not-technically-lying", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:01.962Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Psychohistorian", "createdAt": "2009-04-05T18:10:22.976Z", "isAdmin": false, "displayName": "Psychohistorian"}, "userId": "mAQgf4N8jv2jTMK5B", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PrXR66hQcaJXsgWsa/not-technically-lying", "pageUrlRelative": "/posts/PrXR66hQcaJXsgWsa/not-technically-lying", "linkUrl": "https://www.lesswrong.com/posts/PrXR66hQcaJXsgWsa/not-technically-lying", "postedAtFormatted": "Saturday, July 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Not%20Technically%20Lying&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANot%20Technically%20Lying%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPrXR66hQcaJXsgWsa%2Fnot-technically-lying%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Not%20Technically%20Lying%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPrXR66hQcaJXsgWsa%2Fnot-technically-lying", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPrXR66hQcaJXsgWsa%2Fnot-technically-lying", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1299, "htmlBody": "<p>I'm sorry I took so long to post this. My computer broke a little while ago. I promise this will be relevant later.</p>\n<p>A surgeon has to perform emergency surgery on a patient. No painkillers of any kind are available. The surgeon takes an inert saline IV and hooks it up to the patient, hoping that the illusion of extra treatment will make the patient more comfortable. The patient asks, \"What's in that?\" The doctor has a few options:</p>\n<ol>\n<li>\"It's a saline IV. It shouldn't do anything itself, but if you believe it's a painkiller, it'll make this less painful.</li>\n<li>\"Morphine.\"</li>\n<li>\"The strongest painkiller I have.\"</li>\n</ol>\n<p>-The first explanation is not only true, but maximizes the patient's understanding of the world. <br />-The second is obviously a lie, though, in this case, it is a lie with a clear intended positive effect: if the patient thinks he's getting morphine, then, due to the placebo effect, there is a very real chance he will experience less subjective pain. <br />-The third is, in a sense, both true and a lie. It is technically true. However, it's somewhat arbitrary; the doctor could have easily have said \"It's the weakest painkiller I have,\" or \"It's the strongest sedative I have,\" or any other number of technically true but misleading statements. This statement is clearly intended to mislead the hearer into thinking it is a potent painkiller; it promotes false beliefs while not quite being a false statement. It's Not Technically Lying. It seems that it deserves most, if not almost all, the disapproval that actually lying does; the truth does not save it. Because language does not specify single, clear meanings we can often use language where the obvious meaning is false and the non-obvious true, intentionally promoting false beliefs without false statements.</p>\n<p><a id=\"more\"></a>Another, perhaps more practical example: the opening two sentences of this post. I have been meaning to write this for a couple weeks, and have failed mostly due to akrasia. My computer broke a few months ago. Both statements are technically true,<sup>1</sup> but the implied \"because\" is not just false, but completely opposite the truth - it's complex, but if my computer had not broken, I would probably never have written this post. I've created the impression of a quasi-legitimate excuse without actually saying anything false, because our conventional use of language filled in the gaps that would have been lies.</p>\n<p>The distinction between telling someone a falsehood with the intention of promoting false beliefs and telling them a truth with the intention of promoting false beliefs seems razor-thin. In general, you're probably not justified in deceiving someone, but if you are justified, I hardly see how one form of deception is totally OK and the other is totally wrong. If, and I stress <em>if</em>, your purpose is justified, it seems you should choose whichever will fulfill it more effectively. I'd imagine the balance generally favors NTL, because there are often negative consequences associated with lies, but I doubt that the balance <em>strictly</em> favors NTL; the above doctor hypothetical is an example where the lie seems better than the truth (absent malpractice concerns).</p>\n<p>For what common sentiment is worth, people often see little distinction between lies and NTLs. If I used my computer excuse with a boss or professor, and she later found out my computer actually broke before the paper was even assigned, my saying, \"Well, I didn't claim there was a causal connection; you made that leap yourself! I was telling the truth (technically)!\" is unlikely to fix the damage to her opinion of me. From the perspective of the listener, the two are about equally wrong. Indeed, at least in my experience, some listeners view NTL as worse because you don't even think you're lying to them.</p>\n<p>Lying does admittedly have its own special problems, though I think the big one, deception of others, is clearly shared. There is the risk of lies begetting further lies, as <a href=\"/lw/uw/entangled_truths_contagious_lies/\">the truth is entangled.</a>&nbsp; This may be true, but it is unclear how Not Technically Lying resolves this; if you are entirely honest, the moment your claim is questioned seriously, you either admit you were misleading someone, or you have to continue misleading them in a very clever manner. If you were actually justified in misleading them, failing to do so does not appear to be an efficient outcome. If you're able to mislead them further, then you've further separated their mind from reality, even if, had they <em>really</em> understood what you said, you wouldn't have. And, of course, there's the risk that you will come to believe your own lies, which is serious.</p>\n<p>Not Technically Lying poses a few problems that lying does not. For one, if I fill in the <a href=\"/lw/js/the_bottom_line/\">bottom line</a> and then fill in my premises with NTL's, omitting or rephrasing difficult facts, I can potentially create an excellent argument, an investigation of which will show all my premises are true. If I lied, this could be spotted by fact-checking and my argument largely dismissed as a result. Depending on the context (for example, if I know there are fact-checkers) either one may be more efficient at confounding the truth.</p>\n<p>While it may be a risk that one believes their own lies, if you are generally honest, you will at least be aware when you are lying, and it will likely be highly infrequent. NTL, by contrast, may be too cheap. If I lie about something, I realize that I'm lying and I feel bad that I have to. I may change my behaviour in the future to avoid that. I may realize that it reflects poorly on me as a person. But if I don't technically lie, well, hey! I'm still an honest, upright person and I can thus justify visciously misleading people because at least I'm not technically dishonest. I can easily overvalue the technical truth if I don't worry about promoting true beliefs. Of course, this will vary by individual; if you think lying is generally pretty much OK, you're probably doomed. You'd have to have a pretty serious attachment to the truth. But if you have that attachment, NTL seems that much more dangerous.</p>\n<p>I'm not trying to spell out a moral argument for why we should all lie; if anything, I'm spelling out an argument for why we shouldn't all Not Technically Lie. Where one is immoral, in most if not all cases, so is the other, though where one is justified, the other is likely justified as well, though perhaps not <em>more </em>justified. If lying is never justified because of its effect on the listener, then neither is NTL. If lying is never justified because of its effect on the speaker, well, NTL may or may not be justified; its effects on the speaker don't seem so good, either.</p>\n<p>To tie this into AI (definitely not my field, so I'll be quite brief), it seems a true superintelligence would be unbelievably good at promoting false beliefs with true statements if it really understood the beings it was speaking to. Imagine how well a person could mislead you if they knew beforehand exactly how you would interpret any statement they made. If our concern is the effect on the listener, rather than the effect on the speaker, this is a problem to be concerned with. A Technically Honest AI could probably get away with more deception than we can imagine.</p>\n<p>1-Admittedly this depends on your value of a \"little while,\" but this is sufficiently subjective that I find it reasonable to call both statements true.</p>\n<p>As a footnote, I realize that this topic has been done a lot, but I do not recall seeing this angle (or, actually, this distinction) discussed; it's always been truth vs. falsity, so hopefully this is an interesting take on a thoroughly worn subject.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"cHoCqtfE9cF7aSs9d": 1, "cRaweRcZcXnb9Qryt": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PrXR66hQcaJXsgWsa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 49, "extendedScore": null, "score": 8e-05, "legacy": true, "legacyId": "1366", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 49, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 85, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wyyfFfaRar2jEdeQK", "34XxbRFe54FycoCDw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-05T15:08:05.874Z", "modifiedAt": null, "url": null, "title": "The enemy within", "slug": "the-enemy-within", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:03.139Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RDABLCYLaKzrTEPe6/the-enemy-within", "pageUrlRelative": "/posts/RDABLCYLaKzrTEPe6/the-enemy-within", "linkUrl": "https://www.lesswrong.com/posts/RDABLCYLaKzrTEPe6/the-enemy-within", "postedAtFormatted": "Sunday, July 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20enemy%20within&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20enemy%20within%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRDABLCYLaKzrTEPe6%2Fthe-enemy-within%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20enemy%20within%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRDABLCYLaKzrTEPe6%2Fthe-enemy-within", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRDABLCYLaKzrTEPe6%2Fthe-enemy-within", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 937, "htmlBody": "<p>I read an article from the economist subtitled \"The evolutionary origin of depression\" which puts forward the following hypothesis:</p>\n<blockquote>\n<p>As pain stops you doing damaging physical things, so low mood stops you doing damaging mental ones&mdash;in particular, pursuing unreachable goals. Pursuing such goals is a waste of energy and resources. Therefore, he argues, there is likely to be an evolved mechanism that identifies certain goals as unattainable and inhibits their pursuit&mdash;and he believes that low mood is at least part of that mechanism. ...</p>\n</blockquote>\n<p><a href=\"http://www.economist.com/sciencetechnology/displaystory.cfm?story_id=13899022\">[Read the whole article]</a></p>\n<p>This ties in with Kaj and PJ Eby's idea that our brain has a collection of primitive, evolved mechanisms that control us via our mood. <a href=\"/lw/11h/controlling_your_inner_control_circuits/\">Eby's theory is that many of us have circuits that try to <em>prevent us from doing the things we want to do</em></a>.</p>\n<p>Eliezer has already told us about <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">Adaptation-Executers, not Fitness-Maximizers</a>; evolution mostly created animals which excecuted certain adaptions without really understanding how or why they worked - such as mating at a certain time or eating certain foods over others.</p>\n<p>But, in humans, evolution didn't create the perfect the perfect consequentialist straight off. It seems that evolution combined an explicit goal-driven propositional system with a dumb pattern recognition algorithm for identifying the pattern of \"pursuing an unreachable goal\". It then played with a parameter for balance of power between the goal-driven propositional system and the dumb pattern recognition algorithms until it found a level which was optimal in the human EEA. So blind idiot god bequeathed us a legacy of depression and akrasia - it gave us an enemy within.</p>\n<p>Nowadays, it turns out that that parameter is best turned by giving all the power to the goal-driven propositional system because the modern environment is far more complex than the EEA and requires long-term plans like founding a high-technology startup in order to achieve extreme success. These long-term plans do not immediately return a reward signal, so they trip the \"unreachable goal\" sensor inside most people's heads, causing them to completely lose motivation.</p>\n<p>However, some people seem to be naturally very determined; perhaps their parameter is set slightly more towards the goal-driven propositional system than average. These people rise up from council flats to billionaire-dom and celebrity status. People like <a href=\"http://en.wikipedia.org/wiki/Alan_Sugar#Personal_life\">Alan Sugar</a>. Of course this is mere hypothesis; I cannot find good data to back up the claim that certain people succeed for this reason, but I think we all have a lot of personal evidence that suggests that if we could just work harder, we could do much better. It is now well accepted that getting into a positive mood counteracts ego depletion, see, for example, <a href=\"http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6WJB-4NBR8XV-1&amp;_user=809099&amp;_rdoc=1&amp;_fmt=&amp;_orig=search&amp;_sort=d&amp;_docanchor=&amp;view=c&amp;_searchStrId=948528483&amp;_rerunOrigin=scholar.google&amp;_acct=C000043939&amp;_version=1&amp;_urlVersion=0&amp;_userid=809099&amp;md5=6eab116894358ba5d1f45dd8fc40bd01\">this paper</a><sup>1 </sup>. One might ask why on earth evolution designed the power-balance parameter to vary with your mood; but suppose that the mechanism is that the \"unreachable goal\" sensor works as follows:</p>\n<p>{pursuing goal} + {sad} = {current goal is unachievable} ==&gt; decrease motivation</p>\n<p>{pursuing goal} + {happy} = {current goal is being achieved} ==&gt; increase motivation</p>\n<p>And the \"mood\" setting takes a number of inputs to determine whether to go into the \"happy\" state or the \"sad\" state, such as whether you have recently laughed, whether you received praise or a gift recently, and whether your conscious, deliberative mind has registered the \"subgoal achieved\" signal.</p>\n<p>In our EEA, all of the above probably correlated well with being in pursuit of a goal that you are succeeding at: since the EEA seems to be mostly about getting food and status in the tribe, receiving a gift, laughing or getting more food probably all correlated with with doing something that was good - such as making allies who would praise you and laugh and socialize with you. Conversely, being hungry and lonely and frustrated indicate that you are trying something that isn't working, and that the best course of action for your genes is to hit you with a big dose of depression so that you stop doing whatever you were doing.</p>\n<p>Following PJ Eby's idea of the brain as a lot of PID feedback controller circuits, we can see what might happen in the case of someone who \"makes it\": they try something which works, and people praise them and give them gifts (e.g. money, business competition prizes, corporate hospitality gifts, attention, status), which increases their motivation because it sets their \"goal attainability\" sensor to \"attainable\". This creates a positive feedback loop. Conversely, if someone does badly and then gets criticism for that bad performance, their \"unreachable goal\" sensor will trip out and remove their will to continue, creating a downward spiral of ever diminishing motivation. This downward spiral failure mode wouldn't have happened in the EEA, because the long-term planning aspect of our cognition was probably useful much more occasionally in the EEA than it is today, hence it was no bad thing for your brain to be quite eager to switch it off.</p>\n<p>So what are we to do? Powerful anti-depressants would seem to be your friend here, as they might \"fool\" your unreachable goal sensor into not tripping out. In a comments thread on Sentient Developments, David Pearce and another commenter <a href=\"https://www.blogger.com/comment.g?blogID=6753820&amp;postID=7471817285634912837&amp;pli=1\">claimed</a> that there are some highly motivating antidepressants which could help. Laughing and socializing in a positive, fun way also seem like good ideas, or even just watching a funny video on youtube. But we should definitely think about developing much more effective ways to defeat that enemy within; I have my eye on hypnosis, meditation and antidepressants as big potential contributors, as well as spending time with a mutually praising community.</p>\n<p>&nbsp;</p>\n<hr />\n<p>1. <em>Restoring the self: Positive affect helps improve self-regulation following ego depletion</em>, Ticea et al, Journal of Experimental Social Psychology</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb16a": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RDABLCYLaKzrTEPe6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 22, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "1368", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fGzPFwAosXXBcv5Jc", "XPErvb8m9FapXCjhA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-05T16:54:37.819Z", "modifiedAt": null, "url": null, "title": "Media bias", "slug": "media-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:17.979Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/koHaGLxsurYadru49/media-bias", "pageUrlRelative": "/posts/koHaGLxsurYadru49/media-bias", "linkUrl": "https://www.lesswrong.com/posts/koHaGLxsurYadru49/media-bias", "postedAtFormatted": "Sunday, July 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Media%20bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMedia%20bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkoHaGLxsurYadru49%2Fmedia-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Media%20bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkoHaGLxsurYadru49%2Fmedia-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkoHaGLxsurYadru49%2Fmedia-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 415, "htmlBody": "<p>No, I don't mean that the news media are biased politically.&nbsp; I mean that authors are biased by the media they use.</p>\n<p>I'm learning about support vector machines (SVMs).&nbsp; There are a lot of books and articles written on SVMs.&nbsp; There are also a whole lot of video lectures on SVMs at videolectures.net (see \"kernel methods\").</p>\n<p>People go into much greater detail in lectures than in text.&nbsp; I like to work with text.&nbsp; I'd like to have a text on SVMs that goes into as much detail as videos on SVMs usually do, and works out the ideas behind the concepts as thoroughly, but no such text exists.&nbsp; For some reason, giving a 5-hour lecture series in which you describe the motivations, applications, and work out the mathematical details is acceptable; but writing a text of the same level of detail, which might take only 2 hours to read, is not.</p>\n<p>Perhaps this is because writers are motivated to keep pagecounts low.&nbsp; But pagecount no longer matters with electronic articles.&nbsp; Yet writers still don't want to explain things thoroughly.&nbsp; They certainly aren't saving their readers any time by leaving out intermediate steps.&nbsp; A longer article would take less time to read (and possibly less time to write).&nbsp; Another problem with the pagecount theory is that texts routinely include footnotes and appendices, contributing to the pagecount; yet relegate them to the back of the book, as if embarassed of them, despite the fact that this makes them very difficult to use.</p>\n<p>It's especially bad in math, in which writers have a long tradition of deliberately concealing difficult steps and leaving them \"as an exercise to the reader\".&nbsp; For some reason it is considered bad form to write out <em>all</em> of the steps in a proof, even if adding one or two lines could save the reader five minutes of puzzling.&nbsp; I read an electronic article yesterday where the author said, \"These two equations are actually equivalent.&nbsp; Do you see why?\"</p>\n<p>I think people have adopted a set of cultural biases about what is appropriate in lectures vs. in writing by simply counting observations, without thinking about the systematic sample bias.&nbsp; Speakers speak the way they've seen other speakers speak, without recollecting that most of those speakers were instructors.&nbsp; Technical writers, meanwhile, are picking up their cues from authors of textbooks, which are written with the assumption that a person will be on hand to take you through the details; and applying them in situations where no such person will be available.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"7mTviCYysGmLqiHai": 1, "EdDGrAxYcrXnKkDca": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "koHaGLxsurYadru49", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 39, "extendedScore": null, "score": 0.000550405687542528, "legacy": true, "legacyId": "1369", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-06T03:08:32.930Z", "modifiedAt": null, "url": null, "title": "Can chess be a game of luck?", "slug": "can-chess-be-a-game-of-luck", "viewCount": null, "lastCommentedAt": "2021-02-21T05:47:51.859Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Rune", "createdAt": "2009-03-21T18:16:37.470Z", "isAdmin": false, "displayName": "Rune"}, "userId": "fvDuey7i36uP3fg5u", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J3Ch3sHCytXRQRo9L/can-chess-be-a-game-of-luck", "pageUrlRelative": "/posts/J3Ch3sHCytXRQRo9L/can-chess-be-a-game-of-luck", "linkUrl": "https://www.lesswrong.com/posts/J3Ch3sHCytXRQRo9L/can-chess-be-a-game-of-luck", "postedAtFormatted": "Monday, July 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Can%20chess%20be%20a%20game%20of%20luck%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACan%20chess%20be%20a%20game%20of%20luck%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ3Ch3sHCytXRQRo9L%2Fcan-chess-be-a-game-of-luck%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Can%20chess%20be%20a%20game%20of%20luck%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ3Ch3sHCytXRQRo9L%2Fcan-chess-be-a-game-of-luck", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ3Ch3sHCytXRQRo9L%2Fcan-chess-be-a-game-of-luck", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 52, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/Gil_Kalai\">Gil Kalai</a>, a well known mathematician, has this to say on the topic of chess and luck:</p>\n<p>http://gilkalai.wordpress.com/2009/07/05/chess-can-be-a-game-of-luck/</p>\n<p>I didn't follow his argument at all, but it seems like something other LW posters may understand, so I decided to post it here. Do comment on his arguments if you agree or disagree with him.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nwcnHxrxcgnwJ878t": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J3Ch3sHCytXRQRo9L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -3, "extendedScore": null, "score": -4e-06, "legacy": true, "legacyId": "1370", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-06T15:16:41.776Z", "modifiedAt": null, "url": null, "title": "The Dangers of Partial Knowledge of the Way:  Failing in School", "slug": "the-dangers-of-partial-knowledge-of-the-way-failing-in", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.902Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gworley", "createdAt": "2009-03-26T17:18:20.404Z", "isAdmin": false, "displayName": "G Gordon Worley III"}, "userId": "gjoi5eBQob27Lww62", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/68e9tsKyrtvoy78Cx/the-dangers-of-partial-knowledge-of-the-way-failing-in", "pageUrlRelative": "/posts/68e9tsKyrtvoy78Cx/the-dangers-of-partial-knowledge-of-the-way-failing-in", "linkUrl": "https://www.lesswrong.com/posts/68e9tsKyrtvoy78Cx/the-dangers-of-partial-knowledge-of-the-way-failing-in", "postedAtFormatted": "Monday, July 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Dangers%20of%20Partial%20Knowledge%20of%20the%20Way%3A%20%20Failing%20in%20School&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Dangers%20of%20Partial%20Knowledge%20of%20the%20Way%3A%20%20Failing%20in%20School%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F68e9tsKyrtvoy78Cx%2Fthe-dangers-of-partial-knowledge-of-the-way-failing-in%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Dangers%20of%20Partial%20Knowledge%20of%20the%20Way%3A%20%20Failing%20in%20School%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F68e9tsKyrtvoy78Cx%2Fthe-dangers-of-partial-knowledge-of-the-way-failing-in", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F68e9tsKyrtvoy78Cx%2Fthe-dangers-of-partial-knowledge-of-the-way-failing-in", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1388, "htmlBody": "<p>I lost the Way, even when I didn't know what I was trying to follow, when I learned a Dangerous Truth about school.&nbsp; This is a brief recounting of how I lost the Way, came back, and what I believe we can learn from this.&nbsp; I hope you find it instructive.</p>\n<p>I suffered from insomnia as a child.&nbsp; Not because of any traumatic events or abuse (if anything I had an exceptionally comfortable childhood), but because I would lay awake in bed, staring into the darkness and worrying about school.&nbsp; I worried that I wouldn't complete assignments, that I would fail in subjects, and that terrible things would happen if I didn't make straight As.&nbsp; None of my fears were well founded, though:&nbsp; I was never at any serious risk of failing to complete an assignment, getting anything worse than a B for a quarter grade in a subject, or having indeterminate terrible things happen to me.&nbsp; I experienced this anxiety in part because I was suffering from undiagnosed obsessive-compulsive disorder and in part because I believed that the most important thing in life was to succeed in school.&nbsp; If school was a game, I was playing to win.<a id=\"more\"></a></p>\n<p>I continued in this mode all the way until I entered high school (9th grade in Florida, USA).&nbsp; Since it's not significant to this post I'll omit most of the details, but the short story is that I did not enjoy high school for a multitude of reasons, not least of which was that I felt held back by teachers and administrators who didn't want me advancing my learning too far past the level they were trying to teach.<sup>1</sup>&nbsp; I don't think most of them were being malicious, but had their hands forced by an educational system that is more concerned with average performance and improving the results of underperforming students than allowing exceptional students to succeed.&nbsp; Looking back, if I'd had more guts I would have pushed to take my GED and gone to early college, but instead I wallowed in my high school's educational mire until I had wasted nearly all of four years.</p>\n<p>Having finished my four year sentence to secondary education and graduating with high marks and my belief in the educational system still intact, I began college hopeful that I would find a better learning environment, and with only a few exceptions, I did.&nbsp; I enjoyed most of my classes, even if I did sleep through the middle 2/3rds of nearly ever class meeting I attended, because I was learning things I was interested in:&nbsp; math, physics, anthropology, and computer science, my major.&nbsp; By my second year I was taking all upper level computer science classes, and by taking classes over the summer I was able to graduate at the end of my third year with a BS.</p>\n<p>I don't recall what I had wanted to do with my degree when I started college, but by the end I knew that I wanted to join the ranks of academe.&nbsp; Both of my parents are teachers, and I have always loved to think about better ways to teach what I've learned, so combined with the promise of discovering new knowledge, the academic career path seemed ideal to me.&nbsp; So in my last year of undergraduate education I made arrangements to stay on at my university to earn a PhD in computer science and work as a teaching assistant for the department.</p>\n<p>It was during this time that I <a href=\"/lw/le/lost_purposes/\">lost the Way</a>.&nbsp; I don't recall the exact day, but somewhere within 2004 it happened.&nbsp; While researching teaching methods I might find useful, I stumbled upon the writings of <a href=\"http://www.alfiekohn.org/index.html\">Alfie Kohn</a> and <a href=\"http://www.paulgraham.com/articles.html\">Paul Graham's essays</a>.&nbsp; Between the two of them, combined with my experiences in high school, <a href=\"/tag/akrasia/\">my willpower broke</a>.&nbsp; Not because they said anything that made me feel as though I shouldn't try, but because they made it painfully obvious that the whole point of school isn't really learning, and in some cases school's attempt to make it appear as though learning is going on actually hurts your ability to really learn.&nbsp; Seeing this, I came to the great realization that how I did in school didn't really matter.</p>\n<p>As you might have guessed, this is the <a href=\"http://www.overcomingbias.com/2009/06/the-best-big-lies.html\">Dangerous Truth</a> that I learned.&nbsp; It's a dangerous one because I only learned one truth, that school isn't really about learning but that you have to go through it if you want people to believe that you are as good as you claim to be.&nbsp; What happened to me from there should be obvious:&nbsp; I stopped devoting myself to my school work, worrying about its quality, and trying to be the best.&nbsp; The only way I stayed in graduate school was a combination of raw intelligence and and an ability to finish work to a good enough quality when under the pressure of a deadline.</p>\n<p>After two years, though, I was pushed out of the computer science program with my MS.&nbsp; My university requires us to pass a qualifying exam, essentially a test of knowledge in a variety of subjects from undergraduate computer science education.&nbsp; Passing is more a feat of memory and determination than intelligence or ability to do academic research, so given my feelings about educational hoops at the time, I failed all of my attempts (though, I will note, I actually passed if you were to superimpose all of my attempts over one another, but I was never able to do it all in one sitting).&nbsp; Not knowing what else to do, and with my research already tending towards combinatorics, I switched to the mathematics department.</p>\n<p>I'm now at the end of my third year in the mathematics PhD program.&nbsp; I still haven't passed my qualifiers because I am being much more cautious, maybe even too cautious, with my attempts.&nbsp; But I anticipate a better outcome for me this time, and that's because I <a href=\"/lw/31/what_do_we_mean_by_rationality/\">found the Way again</a>.</p>\n<p>Between the posts on Overcoming Bias and Less Wrong, I finally got a missing piece of the truth.&nbsp; No matter if the intermediate steps seem dumb, if you want to <a href=\"/lw/9r/playing_to_win/\">play to win</a>, if you want to follow the Way, you have to push through it if the end result is worthwhile. &nbsp;As it applies to my life, if I want to get that PhD, even though the qualifying exam is dumb because it doesn't test anything I haven't already demonstrated competence in and doesn't demonstrate my ability to complete academic research, it's still something I have to get through. &nbsp;If I want to play to win, I have to pass the qualifying exam. &nbsp;No more <a href=\"http://www.sirlin.net/ptw-book/intermediates-guide.html\">placing extra restrictions on myself</a> so that if I fail <a href=\"http://www.sirlin.net/ptw-book/more-on-losing.html\">I have an excuse</a>: &nbsp;I will pass or fail by my own best efforts.</p>\n<p>As this post suggests, partial knowledge of a powerful set of techniques can be very dangerous. &nbsp;It's a motif that appears in many arts. &nbsp;To me the most salient examples are in martial arts training and chi cultivation, but other good ones include physics, biology, AI, and economics, where applying partial knowledge could lead to real danger for the applier and the world. &nbsp;The Art of Rationality is similarly dangerous when only partial knowledge is obtained. &nbsp;It's almost too bad we can't send aspiring rationalists off to monasteries where, like ancient martial arts students, they can train and learn and keep themselves separated from the world until they have reached sufficient mastery to be safe to return to wider society.</p>\n<p>But today most people who learn martial arts spend only a few hours a week in the dojo and maybe several more hours at home training. &nbsp;The rest of the time they are in normal society, possessing dangerous, partial knowledge of their martial art. &nbsp;Yet few of them kill anyone accidentally because the first thing they learn is where and how they are allowed to use their art. &nbsp;Even when tempted, it's important that the martial arts student not use what they know until they have reached a sufficient level of mastery and maturity to use their abilities responsibly in the wider world. &nbsp;We may need a similar approach for training in the Winning Way.</p>\n<h2>Footnotes</h2>\n<p><sup>1</sup> And this was even with me being in the International Baccalaureate Program, an internationally recognized college prep program that allows the transfer of course credits from high school to many universities around the world.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fF9GEdWXKJ3z73TmB": 1, "WqLn4pAWi5hn6McHQ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "68e9tsKyrtvoy78Cx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 14, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "519", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sP2Hg6uPwpfp3jZJN", "RcZCwxFiZzE6X7nsv", "Zrg8zohTAJKGAo5if"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-07T07:09:42.080Z", "modifiedAt": null, "url": null, "title": "An interesting speed dating study", "slug": "an-interesting-speed-dating-study", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:03.792Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CronoDAS", "createdAt": "2009-02-27T04:42:19.587Z", "isAdmin": false, "displayName": "CronoDAS"}, "userId": "Q2oaNonArzibx5cQN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AR8PnNkhTzP22Eygf/an-interesting-speed-dating-study", "pageUrlRelative": "/posts/AR8PnNkhTzP22Eygf/an-interesting-speed-dating-study", "linkUrl": "https://www.lesswrong.com/posts/AR8PnNkhTzP22Eygf/an-interesting-speed-dating-study", "postedAtFormatted": "Tuesday, July 7th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20interesting%20speed%20dating%20study&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20interesting%20speed%20dating%20study%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAR8PnNkhTzP22Eygf%2Fan-interesting-speed-dating-study%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20interesting%20speed%20dating%20study%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAR8PnNkhTzP22Eygf%2Fan-interesting-speed-dating-study", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAR8PnNkhTzP22Eygf%2Fan-interesting-speed-dating-study", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 485, "htmlBody": "<p>I recently found <a href=\"http://www.nytimes.com/2009/07/07/health/07dating.html?ref=health\">an article in the New York Times</a> that talks about a speed dating study that is going to be published in an upcoming issue of the journal <em>Psychological Science</em>. Given the usual state of science journalism, the fact that the article includes links that let me find <a href=\"http://www.psychologicalscience.org/media/releases/2009/finkel.cfm\">a press release about the upcoming paper</a> and <a href=\"http://faculty.wcas.northwestern.edu/eli-finkel/documents/RotationMS_FINAL.pdf\">a 20-page PDF file containing the paper itself</a> was very helpful.</p>\n<p>According to most studies and in accordance with popular stereotypes, men are normally less selective than women when it comes to evaluating potential romantic partners - in general, it appears that men are more likely to want to date any given woman than women are to want to date any given man. In a typical speed dating experiment, men and women rate potential partners as either a \"yes\" or a \"no\" depending on whether or not they want to see that person again. Men almost always rate a larger percentage of women as a \"yes\" than women do men, and, according to this paper, this is a fairly robust finding that generalizes over many different contexts. The usual explanation of this phenomena is based on evolutionary psychology: a female has a lot more to lose from a bad mate choice than a male does. If there were a biological, genetic basis for this tendency, it should be difficult to come up with an experimental setup in which women are less selective and men are more selective.</p>\n<p>However, that's not the case at all. This study demonstrates that a small, seemingly trivial change in the speed dating ritual results in a (partial) reversal of the normal results. You see, in practically every speed dating setup, when it is time to interact with a new partner, men physically leave their seat and move to the table where the next woman is sitting, while the women remain seated and wait for the men to approach them. The authors of this study had the men remain still and had the women change seats, and found that this was all it took to wipe away the usual pattern: when the women were required to physically approach while the men remained still, the women became less selective then the men, reporting greater romantic interest and \"yes\"ing partners at a higher rate. \"Rotaters\" also reported greater self-confidence than \"sitters\", regardless of gender.</p>\n<p>I suggest that you go read the paper, or at least the press release, yourself; my summary doesn't really do it justice, and I'm leaving the implications for the evolutionary psychology-based analysis of gender as an exercise for the reader.</p>\n<p>EDIT: Having had some more time to look over the study, I think I should point out that it wasn't a <em>complete</em> reversal of the usual gender behavior: female rotators were only moderately less selective than male sitters, while male rotators were significantly less selective than female sitters. (Sitters of both genders were equally selective.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"W9aNkPwtPhMrcfgj7": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AR8PnNkhTzP22Eygf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 20, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "1375", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-07T20:40:44.330Z", "modifiedAt": null, "url": null, "title": "Can self-help be bad for you?", "slug": "can-self-help-be-bad-for-you", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.838Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tom_Talbot", "createdAt": "2009-03-01T16:19:24.634Z", "isAdmin": false, "displayName": "Tom_Talbot"}, "userId": "3kMLvQM3hA4tnMBNr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j3spwkpm6MEA86aPS/can-self-help-be-bad-for-you", "pageUrlRelative": "/posts/j3spwkpm6MEA86aPS/can-self-help-be-bad-for-you", "linkUrl": "https://www.lesswrong.com/posts/j3spwkpm6MEA86aPS/can-self-help-be-bad-for-you", "postedAtFormatted": "Tuesday, July 7th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Can%20self-help%20be%20bad%20for%20you%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACan%20self-help%20be%20bad%20for%20you%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj3spwkpm6MEA86aPS%2Fcan-self-help-be-bad-for-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Can%20self-help%20be%20bad%20for%20you%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj3spwkpm6MEA86aPS%2Fcan-self-help-be-bad-for-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj3spwkpm6MEA86aPS%2Fcan-self-help-be-bad-for-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 361, "htmlBody": "<p>From the NHS <a href=\"http://www.nhs.uk/news/2009/07July/Pages/SelfHelpCanBeBad.aspx\"><em>Behind the Headlines</em> blog</a>:</p>\n<p>&nbsp;</p>\n<blockquote>&ldquo;Self help makes you feel worse,&rdquo; BBC News has reported. It says that the growing trend of using self-help mantras to boost your spirits may actually have a detrimental effect. The news comes from Canadian research, which found that people with low self-esteem felt worse after repeating positive statements about themselves.</blockquote>\n<p>&nbsp;</p>\n<blockquote>Although positive self-statements are widely believed to boost mood and self-esteem, they have not been widely studied, and their effectiveness has not been demonstrated. This experimental study sought to investigate the contradictory theory that these statements can be harmful.\n<p>The researchers had a theory that when a person feels deficient in some way, making positive self-statements to improve that aspect of their life may highlight the discrepancy between their perceived deficiency and the standard they would like to achieve. The researchers carried out three studies in which they manipulated positive self-statements and examined their effects on mood and self-esteem.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Something about the hypothesis sounds familiar:</p>\n<blockquote>This experimental research among a group of Canadian university students has found that positive statements may reinforce that positivity among those with high self-esteem, and make them feel even better. But it causes those with low self-esteem to feel worse and to have lower self-esteem.\n<p>The researchers say that this theory is based on the idea of &lsquo;latitudes of acceptance&rsquo;, i.e. <strong>messages that reinforce a position close to one&rsquo;s own are more likely to be persuasive than messages that reinforce a position far from one&rsquo;s own</strong>. As they suggest, if a person believes that they are unlovable and keeps repeating, \"I&rsquo;m a lovable person\", they may dismiss this statement and possibly reinforce their conviction that they are unlovable.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>What do you think? Is this plausible, or is it an attempt to shoehorn one of those trendy heuristics-and-biases-related hypotheses into a study on self-esteem? If you accept the validity of the study and its conclusion, does it influence LW's Rationalists Should Win self-help philosophy? What if it is literally true that some people are more lovable and some less, and that this has unavoidable effects on self-esteem? Do low self-esteem rationalists need different techniques from those with high self-esteem?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"WqLn4pAWi5hn6McHQ": 1, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j3spwkpm6MEA86aPS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 4, "extendedScore": null, "score": 5.062380021492278e-07, "legacy": true, "legacyId": "1377", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-08T00:52:28.329Z", "modifiedAt": null, "url": null, "title": "Causality does not imply correlation", "slug": "causality-does-not-imply-correlation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:05.576Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CXcYoRMyMcs7zyknh/causality-does-not-imply-correlation", "pageUrlRelative": "/posts/CXcYoRMyMcs7zyknh/causality-does-not-imply-correlation", "linkUrl": "https://www.lesswrong.com/posts/CXcYoRMyMcs7zyknh/causality-does-not-imply-correlation", "postedAtFormatted": "Wednesday, July 8th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Causality%20does%20not%20imply%20correlation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACausality%20does%20not%20imply%20correlation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCXcYoRMyMcs7zyknh%2Fcausality-does-not-imply-correlation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Causality%20does%20not%20imply%20correlation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCXcYoRMyMcs7zyknh%2Fcausality-does-not-imply-correlation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCXcYoRMyMcs7zyknh%2Fcausality-does-not-imply-correlation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1059, "htmlBody": "<p>It is a commonplace that correlation does not imply causality, however <a href=\"/lw/bs/rationality_quotes_april_2009/87a\">eyebrow-wagglingly suggestive</a>&nbsp;it may be of causal hypotheses. It is less commonly noted that causality does not imply correlation either. It is quite possible for two variables to have zero correlation, and yet for one of them to be completely determined by the other.</p>\n<div>The causal analysis of statistical information is the subject of several major books, including Judea Pearl's <a href=\"http://bayes.cs.ucla.edu/BOOK-2K/index.html\">Causality</a>&nbsp;and&nbsp;<a href=\"http://books.google.com/books?id=AvNID7LyMusC\">Probabilistic reasoning in intelligent systems</a>, and Spirtes et al's <a href=\"http://mitpress.mit.edu/catalog/item/default.asp?tid=4087&amp;ttype=2\">Causation, Prediction, and Search</a>. One of the axioms used in the last-mentioned is the Faithfulness Axiom. See the book for the precise formulation; informally put it amounts to saying that if two variables are uncorrelated, then they are causally independent. As support for this, the book offers a theorem to the effect that while counterexamples are theoretically possible, they have measure zero in the space of causal systems, and anecdotal evidence that people find fault with causal explanations violating the axiom.</div>\n<div>The purpose of this article is to argue that this is not the case.</div>\n<div>\n<div><a id=\"more\"></a>The counterexample consists of just two variables A and B. The time series data can be found&nbsp;<a href=\"http://www.cmp.uea.ac.uk/~jrk/temp/ab.txt\">here</a>, a text file in which each line contains a pair of values for A and B. Here is a scatter-plot:</div>\n<div><img src=\"http://www2.cmp.uea.ac.uk/~jrk/temp/ab.png\" alt=\"scatter plot of 1000 points\" width=\"414\" height=\"397\" /><br /></div>\n<div>The correlation is not significantly different from zero. Consider the possible causal relationships there might be between two variables, assuming there are no other variables involved. A causes B; B causes A; each causes the other; neither causes the other. Which of these describes the relationship between A and B for the above data?</div>\n<div>The correct answer is that none of the four hypotheses can be rejected by these data alone. The actual relationship is: A causes B. Furthermore, there is no noise in the process. A is varying randomly, but B is deterministically caused by A and nothing else, and not by a complex process either. The process is robust: it is not by an accident that the correlation is zero. Every physical process that is modelled by the very simple mathematical relation at work here (to be revealed below) has the same property.</div>\n<div>\n<div>Because I know the process that generated these data, I can confidently predict that it is not possible for anyone to discover from them the true dynamical relation between A and B. So I'll make it a little easier to guess what is going on before I tell you a few paragraphs down. &nbsp;<a href=\"http://www.cmp.uea.ac.uk/~jrk/temp/abfull.txt\">Here</a>&nbsp;(warning: large file) is another time series for the same variables, sampled at 1000 times the frequency (but only 1/10 the total time). Just by plotting these a certain regularity may become evident to the eyes, and it should be quite easy for anyone so inclined to discover the mathematical relationship between A and B.</div>\n<div>So what are these variables, that are tightly causally connected yet completely uncorrelated?</div>\n<div>Consider a signal generator. It generates a voltage that varies with time. Most signal generators can generate square waves or sine waves, sometimes sawtooths as well. This signal generator generates a random waveform. Not white noise -- it meanders slowly up and down without pattern, and in the long run the voltage is normally distributed.</div>\n<div>Connect the output across a capacitor. The current through the capacitor is proportional to the rate of change of the voltage. Because the voltage is bounded and differentiable, the correlation with its first derivative is zero. That is what A and B are: a randomly wandering variable A and its rate of change B.</div>\n<div><strong>Theorem:</strong>&nbsp;&nbsp;<em>In the long run, a bounded, differentiable real function has zero correlation with its first derivative.</em></div>\n<div>The proof is&nbsp;<a href=\"/lw/121/media_bias/\">left as an exercise</a>.</div>\n<div>Notice that unlike the case that Spirtes considers, where the causal connections between two variables just happen to have multiple effects that exactly cancel, the lack of correlation between A and B is robust. It does not matter what smooth waveform the signal generator puts out, it will have zero correlation with the current that it is the sole cause of. I chose a random waveform because it allows any value and any rate of change of that value to exist simultaneously, rather than e.g. a sine wave, where each value implies at most two possible rates of change. But if your data formed neat sine waves you wouldn't be resorting to statistics. The problem here is that they form a cloud of the sort that people immediately start doing statistics on, but the statistics tells you nothing. I could have arranged that A and B had a modest positive correlation, by taking for B a linear combination of A and dA/dt, but the seductive exercise of drawing a regression line through the cloud would be meaningless.</div>\n<div>In some analyses of causal networks (for example&nbsp;<a href=\"http://www.optimizelife.com/cyclic-discovery.pdf\">here</a>, which tries, but I think unsuccessfully, to handle cyclic causal graphs), an assumption is made that the variables are at equilibrium, i.e. that observations are made at intervals long enough to ignore transient temporal effects. As can be seen by comparing the two time series for A and B, or by considering the actual relation between the variables, this procedure guarantees to hide, not reveal, the relationship between these variables.</div>\n<div>If anyone tackled and solved the exercise of studying the detailed time series to discover the relationship before reading the answer, I doubt that you did it by any statistical method.</div>\n<div>Some signal generators can be set to generate a current instead of a voltage. In that case the current through the capacitor would cause the voltage across it, reversing the mathematical relationship. So even detailed examination of the time series will not distinguish between the voltage causing the current and the current causing the voltage.</div>\n<hr />\n<div>In a further article I will exhibit time series for three variables, A, B, and C, where the joint distribution is multivariate normal, the correlation of A with C is below -0.99, and each has zero correlation with B. Some causal information is also given: A is exogenous (i.e. is not causally influenced by either B or C), and there are no confounding variables (other variables correlating with more than one of A, B, or C). This means that there are four possible causal arrows you might draw between the variables: A to B, A to C, B to C, and C to B, giving 16 possible causal graphs. Which of these graphs are consistent with the distribution?</div>\n</div>\n</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"cq69M9ceLNA35ShTR": 1, "6nS8oYmSMuFMaiowF": 1, "bh7uxTTqmsQ8jZJdB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CXcYoRMyMcs7zyknh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 18, "extendedScore": null, "score": 3e-05, "legacy": true, "legacyId": "1378", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["koHaGLxsurYadru49"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-08T11:04:39.325Z", "modifiedAt": null, "url": null, "title": "Revisiting torture vs. dust specks", "slug": "revisiting-torture-vs-dust-specks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:34.337Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zA7ryCXAQ3wrSrKLz/revisiting-torture-vs-dust-specks", "pageUrlRelative": "/posts/zA7ryCXAQ3wrSrKLz/revisiting-torture-vs-dust-specks", "linkUrl": "https://www.lesswrong.com/posts/zA7ryCXAQ3wrSrKLz/revisiting-torture-vs-dust-specks", "postedAtFormatted": "Wednesday, July 8th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Revisiting%20torture%20vs.%20dust%20specks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARevisiting%20torture%20vs.%20dust%20specks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzA7ryCXAQ3wrSrKLz%2Frevisiting-torture-vs-dust-specks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Revisiting%20torture%20vs.%20dust%20specks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzA7ryCXAQ3wrSrKLz%2Frevisiting-torture-vs-dust-specks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzA7ryCXAQ3wrSrKLz%2Frevisiting-torture-vs-dust-specks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 472, "htmlBody": "<p>In line with my fine tradition of <a href=\"/lw/7v/formalizing_newcombs/\">beating</a> <a href=\"/lw/do/reformalizing_pd/\">old</a> <a href=\"/lw/10g/lets_reimplement_eurisko/\">horses</a>, in this post I'll try to summarize some arguments that people proposed in the ancient puzzle of <a href=\"/lw/kn/torture_vs_dust_specks/\">Torture vs. Dust Specks</a> and add some of my own. Not intended as an endorsement of either side. (I do have a preferred side, but don't know exactly why.)</p>\n<ul>\n<li>The people saying one dust speck is \"zero disutility\" or \"incommensurable utilities\" are being naive. Just pick the smallest amount of suffering that in your opinion is non-zero or commensurable with the torture and restart.</li>\n<li>Escalation argument: go from dust specks to torture in small steps, slightly increasing the suffering and massively decreasing the number of people at each step. If each individual change increases utility, so does the final result.</li>\n<li>Fluctuation argument: the probability that the universe randomly subjects you to the torture scenario is considerably higher than 1/3^^^3 anyway, so choose torture without worries even if you're in the affected set. (This doesn't assume the <a href=\"/lw/2k/the_least_convenient_possible_world/\">least convenient possible world</a>, so fails.)</li>\n<li>Proximity argument: don't ask me to value strangers equally to friends and relatives. If each additional person matters 1% less than the previous one, then even an infinite number of people getting dust specks in their eyes adds up to a finite and not especially large amount of suffering. (This assumption negates the escalation argument once you do the math.)</li>\n<li>Real-world analogy: we don't decide to pay one penny each to collectively save one starving African child, so choose torture. (This is resolved by the proximity argument.)</li>\n<li>Observer splitting: if you split into 3^^^3 people tomorrow, would you prefer all of you to get dust specks, or one of you to be tortured for 50 years? (This neutralizes the proximity argument, but the escalation argument also becomes non-obvious.)</li>\n</ul>\n<p>Oh what a tangle. I guess Eliezer is too altruistic to give up torture no matter what we throw at him; others will adopt excuses to choose specks; still others will stay gut-convinced but logically puzzled, like me. The right answer, or the right theory to guide you to the answer, no longer seems so inevitable and mathematically certain.</p>\n<p><strong>Edit:</strong> I submitted this post to LW by mistake, then deleted it which turned out to be the real mistake. Seeing the folks merrily discussing away in the comments <em>long after the deletion</em>, I tried to undelete the post somehow, but nothing worked. All right; let this be a sekrit area. A shame, really, because I just thought of a scenario that might have given even Eliezer cause for self-doubt:</p>\n<ul>\n<li>Observer splitting with a twist: instead of you, one of your <em>loved ones</em> will be split into 3^^^3 people tomorrow. Torture a single branch for 50 years, or give every branch a dust speck?</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb187": 1, "ouT6wKhACJRouGokM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zA7ryCXAQ3wrSrKLz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "1379", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GZ8t3uJRPSQb2sAH3", "5iK6rsa3MSrMhHQyf", "t47TeAbBYxYgqDGQT", "3wYTFWY3LKQCnAptN", "neQ7eXuaXpiYw7SBy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-09T19:47:02.279Z", "modifiedAt": null, "url": null, "title": "Recommended reading for new rationalists", "slug": "recommended-reading-for-new-rationalists", "viewCount": null, "lastCommentedAt": "2020-04-17T17:55:11.005Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XFrequentist", "createdAt": "2009-03-22T17:06:22.991Z", "isAdmin": false, "displayName": "XFrequentist"}, "userId": "zfW5w3TbDWjRW3YaD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wfJebLTPGYaK3Gr8W/recommended-reading-for-new-rationalists", "pageUrlRelative": "/posts/wfJebLTPGYaK3Gr8W/recommended-reading-for-new-rationalists", "linkUrl": "https://www.lesswrong.com/posts/wfJebLTPGYaK3Gr8W/recommended-reading-for-new-rationalists", "postedAtFormatted": "Thursday, July 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Recommended%20reading%20for%20new%20rationalists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARecommended%20reading%20for%20new%20rationalists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwfJebLTPGYaK3Gr8W%2Frecommended-reading-for-new-rationalists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Recommended%20reading%20for%20new%20rationalists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwfJebLTPGYaK3Gr8W%2Frecommended-reading-for-new-rationalists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwfJebLTPGYaK3Gr8W%2Frecommended-reading-for-new-rationalists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 297, "htmlBody": "<p>This has been discussed in passing several times, but I thought it might be worthwhile to collect a list of recommended reading for new members and/or aspiring rationalists. There's&nbsp;probably going to be&nbsp;plenty of overlap with the <a href=\"http://intelligence.org/reading/corereading/\">SingInst reading list</a>, but I think the purposes of the two are sufficiently distinct that a separate list is appropriate.</p>\r\n<p>Some requests:</p>\r\n<ul>\r\n<li>A list of blog posts can be collected at&nbsp;another point in spacetime; for now, please stick to books, book sections, or essays<sup>1</sup>.</li>\r\n<li>Please post a single suggestion per comment, so upvoting can determine the final list for the&nbsp;eternal fame of&nbsp;wikihood.</li>\r\n<li>Please limit yourself to no more than 3-5 suggestions. We could probably&nbsp;all think of dozens, try and think what would actually be the best for the purposes of this site.</li>\r\n<li>Please only suggest an entry if you've read it. <a href=\"http://www.amazon.com/Judgment-under-Uncertainty-Heuristics-Biases/dp/0521284147/\">Judgement Under Uncertainty</a>, while certain to make the list, should be put there by someone who has invested the time and waded through it (i.e. someone other than me).</li>\r\n<li>Please say why you're suggesting it. What did you learn from it? What is its specific relevance to rationality? (ETA)</li>\r\n</ul>\r\n<p>&nbsp;Happy posting!</p>\r\n<p>PS - Is there a \"New Readers Start Here\" page, or something&nbsp;similar (aside from \"About\")? I seem to remember someone talking about one, but I can't find it.<a id=\"more\"></a></p>\r\n<p><sup>1</sup>\"Everything Eliezer has ever written (since 2001)... twice!\" while likely a highly beneficial suggestion for&nbsp;every single&nbsp;human being in existence, is not an acceptable entry. <a href=\"http://yudkowsky.net/rational/technical\">A Technical Explanation of Technical Explanation</a>&nbsp;is fine. If you're not sure whether to classify something as \"an essay\" or \"a blog post\", there is a little-known trick to distinguish the two: <a href=\"/lw/nm/disguised_queries/\">essays contain small nuggets of vanadium ore, and blog posts contain shreds of palladium</a>. Alternatively, just use your best judgement.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fF9GEdWXKJ3z73TmB": 1, "GQyPQcdEQF4zXhJBq": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wfJebLTPGYaK3Gr8W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 37, "extendedScore": null, "score": 6.1e-05, "legacy": true, "legacyId": "1381", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 168, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4FcxgdvdQP45D6Skg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-09T20:51:04.087Z", "modifiedAt": null, "url": null, "title": "Formalized math: dream vs reality", "slug": "formalized-math-dream-vs-reality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:06.545Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/P9r9fECxxnWW566oE/formalized-math-dream-vs-reality", "pageUrlRelative": "/posts/P9r9fECxxnWW566oE/formalized-math-dream-vs-reality", "linkUrl": "https://www.lesswrong.com/posts/P9r9fECxxnWW566oE/formalized-math-dream-vs-reality", "postedAtFormatted": "Thursday, July 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Formalized%20math%3A%20dream%20vs%20reality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFormalized%20math%3A%20dream%20vs%20reality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP9r9fECxxnWW566oE%2Fformalized-math-dream-vs-reality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Formalized%20math%3A%20dream%20vs%20reality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP9r9fECxxnWW566oE%2Fformalized-math-dream-vs-reality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP9r9fECxxnWW566oE%2Fformalized-math-dream-vs-reality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 638, "htmlBody": "<p><em>(Disclaimer: this post is intended as an enthusiastic introduction to a topic I knew absolutely nothing about till yesterday. Correct me harshly as needed.)</em></p>\n<p>We programmers <a href=\"http://fishbowl.pastiche.org/2007/07/17/understanding_engineers_feasibility/\">like</a> to instantly pigeonhole problems as \"trivial\" and \"impossible\". The <a href=\"/lw/up/shut_up_and_do_the_impossible/\">AI-Box</a> has been proposed as candidate for the simplest \"impossible\" problem. Today I'd like to talk about a genuinely hard problem that many people quickly file as \"trivial\": formalizing mathematics.&nbsp;Not as in, teach the computer to devise and prove novel conjectures. More like, type in a proof for some simple mathematical fact, e.g. the irrationality of the square root of 2, and get the computer to verify it.</p>\n<p>Now, if you're unfamiliar with the entire field, what's your best guess as to the length of the proof? I'll grant you the generous assumption that the verifier already has an extensive library of axioms, lemmas and theorems in elementary math, just not this particular fact. Can you do it in one line? Two lines? (Here's a one-liner for humans: when p/q is in lowest terms, p<sup>2</sup>/q<sup>2</sup>&nbsp;is also in lowest terms and hence cannot reduce to 2.)</p>\n<p>While knowledgeable readers chuckle to themselves and the rest frantically calibrate, I will take a moment to talk about the dream...<a id=\"more\"></a></p>\n<p>Since the cutting edge of math will obviously always outrun any codification effort, why formalize proofs at all? Robert S. Boyer (of <a href=\"http://en.wikipedia.org/wiki/Boyer-Moore_string_search_algorithm\">Boyer-Moore</a> fame) spelled out his vision of the benefits in <a href=\"http://www.rbjones.com/rbjpub/logic/qedres01.htm\">the QED Manifesto</a>&nbsp;in 1993: briefly, a machine-checkable repository of humanity's mathematical knowledge could help math progress in all sorts of interesting ways, from educating kids to publishing novel results. It would be like a good twin of <a href=\"http://www.cyc.com/\">Cyc</a> where the promises actually make sense.</p>\n<p>But there could be more than that. If you're a programmer like me, what word first comes to your mind upon imagining such a repository? Duh, \"refactoring\". Our industry has a long and successful history of using tools for <a href=\"http://en.wikipedia.org/wiki/Refactoring\">automated refactorings</a>, mechanically identifying opportunities to extract common code and then shove it around with guaranteed conservation of program behavior. Judging from our experiences in shortening large ad-hoc codebases, this could imply a radical simplification in all areas of math at once!</p>\n<p><strong>Import Group Theory. Analyze. Duplicate Code Spotted: 119 Instances. Extract? Yes or No.</strong></p>\n<p>...Or so goes the dream.</p>\n<p>You ready with that estimate? Here's a <a href=\"http://www.cs.ru.nl/~freek/comparison/index.html\">page</a> comparing formal proofs that sqrt(2) is irrational for 17 different proof checkers. The typical short proof (e.g. Mizar) runs about a page long, but a lot of its complexity is hidden in innocent-looking \"by\" clauses or \"tactics\" that are, in effect, calls to a sophisticated formula-matching algorithm \"prove this from that\", so you can't exactly verify this kind of proof by hand the way the software does it. The more explicit proofs are many times longer. And yes, all of those were allowed to use the verifiers' standard libraries, weighing in at several megabytes in some cases.</p>\n<p>Things aren't so gloomy. We haven't won, but we're making definite progress. We don't have <a href=\"http://en.wikipedia.org/wiki/Atiyah&ndash;Singer_index_theorem\">Atiyah-Singer</a> yet, but the <a href=\"http://en.wikipedia.org/wiki/Prime_number_theorem\">Prime number theorem</a> is done and so is the <a href=\"http://en.wikipedia.org/wiki/Jordan_curve_theorem\">Jordan curve</a>, so the world's formalized math knowledge already contains proofs I can't recite offhand. Still, the situation looks as if a few wannabe AI researchers could substantially help humanity by putting their mind to the proof-encoding problem instead; it's obviously much easier than general AI, but lies in the same rough direction.</p>\n<p>In conclusion, ponder this: human beings have no evolutionary reason to be good at math. (As opposed to, say, hunting or deceiving each other.) Chances are that, on an absolute scale, we suck about as hard as it's possible to suck and still be called \"mathematicians\": what do you mean, you can't do 32-bit multiplication in your head? It's still quite conceivable that a relatively dumb prune-search program, not your ominous paperclipper, can beat humans at math as decisively as they beat us at chess.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1, "FJ3MGb684F88BoN2o": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "P9r9fECxxnWW566oE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 19, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "1382", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nCvvhFBaayaXyuBiD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-10T08:38:23.873Z", "modifiedAt": null, "url": null, "title": "Causation as Bias (sort of)", "slug": "causation-as-bias-sort-of", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:06.039Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "spuckblase", "createdAt": "2009-05-28T03:41:38.723Z", "isAdmin": false, "displayName": "spuckblase"}, "userId": "n2eBfBJrfmdoMBzrp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QCgboeRgsjvtCK5Hb/causation-as-bias-sort-of", "pageUrlRelative": "/posts/QCgboeRgsjvtCK5Hb/causation-as-bias-sort-of", "linkUrl": "https://www.lesswrong.com/posts/QCgboeRgsjvtCK5Hb/causation-as-bias-sort-of", "postedAtFormatted": "Friday, July 10th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Causation%20as%20Bias%20(sort%20of)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACausation%20as%20Bias%20(sort%20of)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQCgboeRgsjvtCK5Hb%2Fcausation-as-bias-sort-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Causation%20as%20Bias%20(sort%20of)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQCgboeRgsjvtCK5Hb%2Fcausation-as-bias-sort-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQCgboeRgsjvtCK5Hb%2Fcausation-as-bias-sort-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 616, "htmlBody": "<p class=\"MsoNormal\"><span lang=\"EN-GB\">David Hume called causation the &ldquo;cement of the universe&rdquo;, and he was convinced that psychologically and in our practices, we can&rsquo;t do without it.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Yet he was famously sceptical of any attempt to analyze causation in terms of necessary connections. For him, causation can only be defined in terms of a constant conjunction in space and time, and that is, I would add, no causation at all, but correlation. For every two events that seem causally connected can also, and without loss of the phenomenon, be described as just the first event, followed by the second. It&rsquo;s really &ldquo;just one damn thing after another&rdquo;. </span><span lang=\"EN-GB\"> It seems to me we still cannot, will not and need not make sense of the notion of causation (virtually no progress has been made since Hume's time). </span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">There seems no need for another sort connection besides the spatio-temporal one, nor do we perceive any. In philosophy,<span> </span>a <em>Hume world</em> is a possible world defined in this way. All the phenomena are the same, but no necessary connections hold between the supposed relata. Maybe one should best imagine such a world as a game of life-world, but without a fundamental level governed by laws and forces; or as a movie, made of frames that are not intrinsically connected to each other.&nbsp;</span><span lang=\"EN-GB\">So, however strong the psychological forces that drive humans to accept further mysterious connections: Shouldn't we just stop worrying and accept living in a Hume world? Or are there actual arguments in favour of \"real\" causation?</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><a id=\"more\"></a><br /></span><span lang=\"EN-GB\">Yes. There's the problem of order. What accounts for all the order in the world?It is remarkably ordered. If no special connections hold between events, why isn&rsquo;t the world pure chaos? Or at least much more disordered? When two billard balls collide, never does one turn into an pink elephant.To explain this, men came up with laws of nature (self-sustained or enforced by a higher being). </span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><br />So, there's the paradox: On the one hand, we have to postulate special connections to account for an orderly world like ours; on the other, we cannot give a proper account of these connections.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">&nbsp;</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><em>Inflationary cosmology</em> to the rescue.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">I won't go into the details (but see the nontechnical explanation and some further philosophical implications <a href=\"http://www.unc.edu/%7Eknobe/BJPS.pdf\">here</a></span><a href=\"http://www.unc.edu/%7Eknobe/BJPS.pdf\"></a><span lang=\"EN-GB\">). </span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Suffice it to say that </span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">1) inflationary cosmology is mainstream physics, and</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">2) it postulates a spatially infinite universe in which every event with nonzero probability is realized infinitely many times.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">&nbsp;</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">How does this help to solve our paradox? The solution seems straightforward:</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">In an infinite universe of the right kind, order can locally emerge out of random events.&nbsp; Our universe is of the right kind.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">So, we can account for the order in our observed (local) part of the universe.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Random events just happen, one after another, there is no need for mysterious causal connections. We throw them out but keep the order.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Problem solved.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">&nbsp;<br />Q: But if this is true, <a href=\"http://www.questiaschool.com/read/110006145?title=6%3A%20Mental%20Causation%20and%20Free%20Will\">it&rsquo;s the end of the world</a>. Thinking, action, science, biases and many, many more concepts are causal ones. How can we do without them?</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">A: life is hard, get over it.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Q: But the theory is untestable?! </span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">A: Falsificationism is dead; we have other evidences in favour (see below).</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Q: But isn&rsquo;t the theory self-defeating?</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">A: It is certainly odd to have a theory informed by experiences and high-level physics that tells us that, strictly speaking, there are no experiences or sciences. But it doesn&rsquo;t seem incoherent to me climb the ladder and then throw it away.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">And, looking at the bright side:</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">In addition to being non-mysterious and conceptually sparse, this might allow to solve some additional (would-be?) hard problems: </span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">qualia, clustering of tropes, time travel-paradoxes, indeterministic processes: All easy or trivial when a thouroughly indeterministic universe is considered. </span></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">So. What do you think &ndash; if you can?</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QCgboeRgsjvtCK5Hb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -13, "extendedScore": null, "score": 5.068052763765942e-07, "legacy": true, "legacyId": "1383", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 92, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-11T16:34:36.230Z", "modifiedAt": null, "url": null, "title": "Debate: Is short term planning in humans due to a short life or due to bias?", "slug": "debate-is-short-term-planning-in-humans-due-to-a-short-life", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:04.828Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whpearson", "createdAt": "2009-02-28T00:34:00.976Z", "isAdmin": false, "displayName": "whpearson"}, "userId": "bq8qsRbPNvFihHxgi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mp5Lhd8EiDwC6kBTW/debate-is-short-term-planning-in-humans-due-to-a-short-life", "pageUrlRelative": "/posts/mp5Lhd8EiDwC6kBTW/debate-is-short-term-planning-in-humans-due-to-a-short-life", "linkUrl": "https://www.lesswrong.com/posts/mp5Lhd8EiDwC6kBTW/debate-is-short-term-planning-in-humans-due-to-a-short-life", "postedAtFormatted": "Saturday, July 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Debate%3A%20Is%20short%20term%20planning%20in%20humans%20due%20to%20a%20short%20life%20or%20due%20to%20bias%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADebate%3A%20Is%20short%20term%20planning%20in%20humans%20due%20to%20a%20short%20life%20or%20due%20to%20bias%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmp5Lhd8EiDwC6kBTW%2Fdebate-is-short-term-planning-in-humans-due-to-a-short-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Debate%3A%20Is%20short%20term%20planning%20in%20humans%20due%20to%20a%20short%20life%20or%20due%20to%20bias%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmp5Lhd8EiDwC6kBTW%2Fdebate-is-short-term-planning-in-humans-due-to-a-short-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmp5Lhd8EiDwC6kBTW%2Fdebate-is-short-term-planning-in-humans-due-to-a-short-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 112, "htmlBody": "<p>One of the proposed benefits of life extension is that it will help us long term plan as we will be around in the future, so we will be more likely to care about the long term future of the world if we live longer.</p>\n<p>So is this true? Are we rational in this respect or will the mind recoil from thinking in time scales longer than 40-60 years even when we are living hundreds of years, due to biases intrinsic to the mammalian brain.</p>\n<p>I don't have time to research this question right now, so I thought I would experiment by throwing out this question to lesswrong and see how people treat it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mp5Lhd8EiDwC6kBTW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 5.071078804772066e-07, "legacy": true, "legacyId": "1387", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-11T21:13:19.421Z", "modifiedAt": null, "url": null, "title": "Jul 12 Bay Area meetup - Hanson, Vassar, Yudkowsky", "slug": "jul-12-bay-area-meetup-hanson-vassar-yudkowsky", "viewCount": null, "lastCommentedAt": "2017-06-17T03:55:53.642Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Aty8uRt4j6v6aQzbp/jul-12-bay-area-meetup-hanson-vassar-yudkowsky", "pageUrlRelative": "/posts/Aty8uRt4j6v6aQzbp/jul-12-bay-area-meetup-hanson-vassar-yudkowsky", "linkUrl": "https://www.lesswrong.com/posts/Aty8uRt4j6v6aQzbp/jul-12-bay-area-meetup-hanson-vassar-yudkowsky", "postedAtFormatted": "Saturday, July 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Jul%2012%20Bay%20Area%20meetup%20-%20Hanson%2C%20Vassar%2C%20Yudkowsky&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJul%2012%20Bay%20Area%20meetup%20-%20Hanson%2C%20Vassar%2C%20Yudkowsky%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAty8uRt4j6v6aQzbp%2Fjul-12-bay-area-meetup-hanson-vassar-yudkowsky%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Jul%2012%20Bay%20Area%20meetup%20-%20Hanson%2C%20Vassar%2C%20Yudkowsky%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAty8uRt4j6v6aQzbp%2Fjul-12-bay-area-meetup-hanson-vassar-yudkowsky", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAty8uRt4j6v6aQzbp%2Fjul-12-bay-area-meetup-hanson-vassar-yudkowsky", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 49, "htmlBody": "<p>Just a reminder that the <a href=\"http://www.meetup.com/Bay-Area-Overcoming-Bias-Meetup/calendar/10780424/\">July 12th Overcoming Bias / Less Wrong meetup</a> in Santa Clara, CA @7pm will feature Robin Hanson, Michael Vassar, and the Singularity Institute summer interns.&nbsp; This meetup will also take place in a larger house - there should be plenty of room to mingle.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Aty8uRt4j6v6aQzbp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 5.071519220892418e-07, "legacy": true, "legacyId": "1390", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-12T09:26:23.365Z", "modifiedAt": null, "url": null, "title": "Our society lacks good self-preservation mechanisms", "slug": "our-society-lacks-good-self-preservation-mechanisms", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:06.387Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rGY5i4FGwutouMvWM/our-society-lacks-good-self-preservation-mechanisms", "pageUrlRelative": "/posts/rGY5i4FGwutouMvWM/our-society-lacks-good-self-preservation-mechanisms", "linkUrl": "https://www.lesswrong.com/posts/rGY5i4FGwutouMvWM/our-society-lacks-good-self-preservation-mechanisms", "postedAtFormatted": "Sunday, July 12th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Our%20society%20lacks%20good%20self-preservation%20mechanisms&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOur%20society%20lacks%20good%20self-preservation%20mechanisms%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrGY5i4FGwutouMvWM%2Four-society-lacks-good-self-preservation-mechanisms%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Our%20society%20lacks%20good%20self-preservation%20mechanisms%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrGY5i4FGwutouMvWM%2Four-society-lacks-good-self-preservation-mechanisms", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrGY5i4FGwutouMvWM%2Four-society-lacks-good-self-preservation-mechanisms", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 832, "htmlBody": "<p>The prospect of a dangerous collection of existential risks and risks of major civilizational-level catastrophes in the 21st century, combined with a distinct lack of agencies whose job it is to mitigate against such risks probably indicates that the world might be in something of an emergency at the moment. Firstly, what do we mean by risks? Well, Bostrom has a paper on <a href=\"http://www.nickbostrom.com/existential/risks.html\">existential risks</a>, and he lists the following risks as being \"<span style=\"font-family: Times New Roman,Times,serif;\"><span style=\"font-size: 12pt;\">most likely</span></span>\":</p>\n<ul>\n<li>Deliberate misuse of nanotechnology, </li>\n<li>Nuclear holocaust, </li>\n<li>Badly programmed superintelligence, </li>\n<li>Genetically engineered biological agent, </li>\n<li>Accidental misuse of nanotechnology (&ldquo;gray goo&rdquo;), </li>\n<li>Physics disasters, </li>\n<li>Naturally occurring disease, </li>\n<li>Asteroid or comet impact, </li>\n<li>Runaway global warming, </li>\n<li>Resource depletion or ecological destruction, </li>\n<li>Misguided world government or another static social equilibrium stops technological progress, </li>\n<li>&ldquo;Dysgenic&rdquo; pressures (We might evolve into a less brainy but more fertile species, homo philoprogenitus &ldquo;lover of many offspring&rdquo;)</li>\n<li>Our potential or even our core values are eroded by evolutionary development,</li>\n<li>Technological arrest, </li>\n<li>Take-over by a transcending upload, </li>\n<li>Flawed superintelligence, </li>\n<li>[Stable] Repressive totalitarian global regime, </li>\n<li><a href=\"http://hanson.gmu.edu/filluniv.pdf\">Hanson's cosmic locusts scenario</a> [Added by author]</li>\n</ul>\n<p>To which I would add various possibilities for major civilization-level disasters that aren't existential risks, such as milder versions of all of the above, or the following:</p>\n<ul>\n<li>convergence of computer viruses and cults/religions,</li>\n<li>advanced personal weapons or surveillance devices such as nanotech, micro-UAV bugs (cyberpunk dystopia), </li>\n<li>erosion of privacy and freedom through massively oppressive government, </li>\n<li>highly effective meta-religions such as Scientology or a much more virulent version of modern evangelical Christianity</li>\n</ul>\n<p>This collection is daunting, especially given that the human race doesn't have any official agency dedicated to mitigating risks to its own medium-long term survival. We face a long list of challenges, and we aren't even formally trying to mitigate many of them in advance, and in many past cases, mitigation of risks occurred on a last-minute, ad-hoc basis, such as individuals in the cold war making the decision not to initiate a nulcear exchange, particularly in the Cuban missile crisis.</p>\n<p>So, a small group of people have realized that the likely outcome of a large and dangerous collection of risks combined with a haphazard, informal methodology for dealing with risks (driven by the efforts of individuals, charities and public opinion) is that one of these potential risks will actually be realized - killing many or all of us or radically reducing our quality of life. This coming disaster is ultimately <em>not the result of any one particular risk</em>, but the result of the lack of a powerful defence against risks.</p>\n<p>One could argue that I [and Bostrom, Rees, etc] are blowing the issue out of proportion. We have survived so far, right? (Wrong, actually - anthropic considerations indicate that survival so far is not evidence that we will survive for a lot longer, and technological progress indicates that risks in the future are worse than risks in the past). Major civilizational disasters have already happened many, many times over.</p>\n<p>Most ecosystems that ever existed were wiped out by natural means, almost all species that have ever existed have gone extinct, and without human intervention most existing ecosystems will probably be wiped out within a 100 million year timescale. Most civilizations that ever existed, collapsed. Some went really badly wrong, like communist Russia. Complex, homeostatic objects that don't have extremely effective self-preservation systems empirically tend to get wiped by the churning of the universe.</p>\n<p>Our western civilization lacks an effective long-term (order of 50 years plus) self-preservation system. Hence we should reasonably expect to either build one, or get wiped out, because we observe that complex systems which seem similar to societies today - such as past societies - collapsed.</p>\n<p>And even though our society does have short-term survival mechanisms such as governments and philanthropists, they often behave in superbly irrational, myopic or late-responding ways. It seems that the response to the global warming problem (late-responding, weak, still failing to overcome co-ordination problems) or the invasion of Iraq (plain irrational) are cases in point from recent history, and that there are numerous examples from the past, such as close calls in the cold war, and the spectacular chain of failures that led from world war I to world war II and the rise of Hitler.</p>\n<p>This article could be summarized as follows:</p>\n<p>The systems we have for preserving the values and existence of our western society, and the human race as a whole are weak, and the challenges of the 21st-22nd century seem likely to overwhelm them.</p>\n<p>I originally wanted to write an article about ways to mitigate existential risks and major civilization-level catastrophes, but I decided to first establish that there are actually such things as serious existential risks and major civilization-level catastrophes, and <em>that we haven't got them handled yet</em>. My next post will be about ways to mitigate existential risks.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Rz5jb3cYHTSRmqNnN": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rGY5i4FGwutouMvWM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 17, "extendedScore": null, "score": 6e-06, "legacy": true, "legacyId": "1385", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 135, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-14T09:53:11.871Z", "modifiedAt": null, "url": null, "title": "Good Quality Heuristics", "slug": "good-quality-heuristics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:59.702Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CannibalSmith", "createdAt": "2009-02-27T07:32:08.507Z", "isAdmin": false, "displayName": "CannibalSmith"}, "userId": "4DedYkNap2GW8X79T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ge7a2d9rhnqyiEkGY/good-quality-heuristics", "pageUrlRelative": "/posts/Ge7a2d9rhnqyiEkGY/good-quality-heuristics", "linkUrl": "https://www.lesswrong.com/posts/Ge7a2d9rhnqyiEkGY/good-quality-heuristics", "postedAtFormatted": "Tuesday, July 14th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Good%20Quality%20Heuristics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGood%20Quality%20Heuristics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGe7a2d9rhnqyiEkGY%2Fgood-quality-heuristics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Good%20Quality%20Heuristics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGe7a2d9rhnqyiEkGY%2Fgood-quality-heuristics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGe7a2d9rhnqyiEkGY%2Fgood-quality-heuristics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 291, "htmlBody": "<p>We use heuristics when we don't have the time to <a href=\"/lw/aq/how_much_thought/\">think more</a>, which is almost all the time. So why don't we compile a big list of good quality heuristics that we can trust? (Insert eloquent analogy with mathematical theorems and proofs.) Here are some heuristics to kick things off:</p>\n<p><strong>Make important decisions in a quiet, featureless room.</strong> [<a href=\"/lw/3b/never_leave_your_room/\">1</a>]</p>\n<p><strong>Apply deodorant before going to bed rather than any other time.</strong> [<a href=\"http://www.overcomingbias.com/2009/05/sweat-intuition.html\">1</a>]</p>\n<p><strong>Avoid counterfactuals and thought experiments in when talking to other people.</strong> [Because they don't happen in real life. Not in mine at least (anecdotal evidence). For example with the trolley, I would not push the fat man because I'd be frozen in horror. But what if you wouldn't be? But I would! And all too often the teller of a counterfactual abuses it by crafting it so that the other person has to give either an inconsistent or unsavory answer. (This proof is a stub. You can improve it by commenting.)]</p>\n<p><strong>If presented with a Monty Hall problem, switch.</strong> [<a href=\"http://en.wikipedia.org/wiki/Monty_Hall_problem#Popular_solution\">1</a>]</p>\n<p><strong>Sign up for cryonics.</strong> [There are so many. Which ones to link? Wait, didn't Eliezer promise us some cryonics articles here in LW?]</p>\n<p><strong>In chit-chat, ask questions and avoid assertions.</strong> [How to Win Friends and Influence People by Dale Carnegie]</p>\n<p><strong>When in doubt, think what your past and future selves would say.</strong> [<a href=\"http://www.picoeconomics.com/\">1</a>, also there was an LW article with the prince with multiple personality disorder chaining himself to his throne that I can't find. Also, I'm not sure if I should include this because it's almost Think More.]</p>\n<p>I urge you to comment my heuristics and add your own. One heuristic per comment. Hopefully this takes off and turns into a series if wiki pages. Edit: We should concentrate on heuristics that save time, effort, and thought.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ge7a2d9rhnqyiEkGY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 14, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "1394", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 113, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YKSwmhGJ3pY9qobnw", "ZmQv4DFx6y4jFbhLy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-15T00:01:28.640Z", "modifiedAt": null, "url": null, "title": " How likely is a failure of nuclear deterrence?", "slug": "how-likely-is-a-failure-of-nuclear-deterrence", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:05.353Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2Ef3a3FNHXjbuTwsn/how-likely-is-a-failure-of-nuclear-deterrence", "pageUrlRelative": "/posts/2Ef3a3FNHXjbuTwsn/how-likely-is-a-failure-of-nuclear-deterrence", "linkUrl": "https://www.lesswrong.com/posts/2Ef3a3FNHXjbuTwsn/how-likely-is-a-failure-of-nuclear-deterrence", "postedAtFormatted": "Wednesday, July 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%20How%20likely%20is%20a%20failure%20of%20nuclear%20deterrence%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%20How%20likely%20is%20a%20failure%20of%20nuclear%20deterrence%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Ef3a3FNHXjbuTwsn%2Fhow-likely-is-a-failure-of-nuclear-deterrence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%20How%20likely%20is%20a%20failure%20of%20nuclear%20deterrence%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Ef3a3FNHXjbuTwsn%2Fhow-likely-is-a-failure-of-nuclear-deterrence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Ef3a3FNHXjbuTwsn%2Fhow-likely-is-a-failure-of-nuclear-deterrence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 287, "htmlBody": "<blockquote>\n<p>Last month I asked Robert McNamara, the secretary of defense during the Kennedy and Johnson administrations, what he believed back in the 1960s was the status of technical locks on the Minuteman intercontinental missiles. ... McNamara replied ... that he personally saw to it that these [PAL's] ... were installed on the Minuteman force, and that he regarded them as essential to strict central control and preventing unauthorized launch. ... What I then told McNamara about his vitally important locks elicited this response: &ldquo;I am shocked, absolutely shocked and outraged. Who the hell authorized that?&rdquo; What he had just learned from me was that the locks had been installed, but everyone knew the combination. The Strategic Air Command (SAC) in Omaha quietly decided to set the &ldquo;locks&rdquo; to all zeros in order to circumvent this safeguard. During the early to mid-1970s, during my stint as a Minuteman launch officer, they still had not been changed. Our launch checklist in fact instructed us, the firing crew, to double-check the locking panel in our underground launch bunker to ensure that no digits other than zero had been inadvertently dialed into the panel. SAC remained far less concerned about unauthorized launches than about the potential of these safeguards to interfere with the implementation of wartime launch orders. And so the &ldquo;secret unlock code&rdquo; during the height of the nuclear crises of the Cold War remained constant at 00000000.</p>\n</blockquote>\n<blockquote>\n<p>Training exercises can be mistaken for the real thing. In 1979, a test tape, simulating a Russian attack was mistakenly fed into a NORAD computer connected to the operational missile alert system, resulting in an alert and the launching of American aircraft [Borning 1988].</p>\n</blockquote>\n<p><a href=\"http://nuclearrisk.org/3likely.php\">Read the rest here.</a></p>\n<h2 id=\"pageName\" style=\"font-size: 125%; padding-left: 0px;\"><br /></h2>\n<h2 style=\"font-size: 125%; padding-left: 0px;\"><br /></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2Ef3a3FNHXjbuTwsn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 10, "extendedScore": null, "score": 3e-06, "legacy": true, "legacyId": "1395", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-15T02:27:37.651Z", "modifiedAt": null, "url": null, "title": "The Strangest Thing An AI Could Tell You", "slug": "the-strangest-thing-an-ai-could-tell-you", "viewCount": null, "lastCommentedAt": "2021-09-27T02:42:18.044Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/t2NN6JwMFaqANuLqH/the-strangest-thing-an-ai-could-tell-you", "pageUrlRelative": "/posts/t2NN6JwMFaqANuLqH/the-strangest-thing-an-ai-could-tell-you", "linkUrl": "https://www.lesswrong.com/posts/t2NN6JwMFaqANuLqH/the-strangest-thing-an-ai-could-tell-you", "postedAtFormatted": "Wednesday, July 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Strangest%20Thing%20An%20AI%20Could%20Tell%20You&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Strangest%20Thing%20An%20AI%20Could%20Tell%20You%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft2NN6JwMFaqANuLqH%2Fthe-strangest-thing-an-ai-could-tell-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Strangest%20Thing%20An%20AI%20Could%20Tell%20You%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft2NN6JwMFaqANuLqH%2Fthe-strangest-thing-an-ai-could-tell-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft2NN6JwMFaqANuLqH%2Fthe-strangest-thing-an-ai-could-tell-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 653, "htmlBody": "<p>Human beings are all crazy.&nbsp; And if you tap on our brains just a little, we get so crazy that even other humans notice.&nbsp; <a href=\"/lw/20/the_apologist_and_the_revolutionary/\">Anosognosics</a> are one of my favorite examples of this; people with right-hemisphere damage whose left arms become paralyzed, and who <em>deny that their left arms are paralyzed,</em> coming up with excuses whenever they're asked why they can't move their arms.</p>\n<p>A truly wonderful form of brain damage - it disables your <em>ability to notice or accept</em> the brain damage.&nbsp; If you're told outright that your arm is paralyzed, you'll deny it.&nbsp; All the marvelous excuse-generating rationalization faculties of the brain will be mobilized to mask the damage from your own sight.&nbsp; As Yvain summarized:</p>\n<blockquote>\n<p>After a right-hemisphere stroke, she lost movement in her left arm but continuously denied it. When the doctor asked her to move her arm, and she observed it not moving, she claimed that it wasn't actually her arm, it was her daughter's. Why was her daughter's arm attached to her shoulder? The patient claimed her daughter had been there in the bed with her all week. Why was her wedding ring on her daughter's hand? The patient said her daughter had borrowed it. Where was the patient's arm? The patient \"turned her head and searched in a bemused way over her left shoulder\".</p>\n</blockquote>\n<p>I find it disturbing that the brain has such a simple macro for absolute denial that it can be invoked as a <em>side effect </em>of paralysis.&nbsp; That a single whack on the brain can both disable a left-side motor function, and disable our ability to recognize or <em>accept</em> the disability.&nbsp; Other forms of brain damage also seem to both cause insanity and disallow recognition of that insanity - for example, when people insist that their friends have been replaced by exact duplicates after damage to face-recognizing areas.</p>\n<p>And it really makes you wonder...</p>\n<p>...what if we <em>all</em> have some form of brain damage in common, so that <em>none</em> of us notice some simple and obvious fact?&nbsp; As blatant, perhaps, as our left arms being paralyzed?&nbsp; Every time this fact intrudes into our universe, we come up with some ridiculous excuse to dismiss it - as ridiculous as \"It's my daughter's arm\" - only there's no sane doctor watching to pursue the argument any further.&nbsp; (Would we all come up with the <em>same</em> excuse?)</p>\n<p>If the \"absolute denial macro\" is that simple, and invoked that easily...<a id=\"more\"></a></p>\n<p>Now, suppose you built an AI.&nbsp; You wrote the source code yourself, and so far as you can tell by inspecting the AI's thought processes, it has no equivalent of the \"absolute denial macro\" - there's no point damage that could inflict on it the equivalent of anosognosia.&nbsp; It has redundant differently-architected systems, defending in depth against cognitive errors.&nbsp; If one system makes a mistake, two others will catch it.&nbsp; The AI has no functionality at all for deliberate rationalization, let alone the doublethink and denial-of-denial that characterizes anosognosics or humans thinking about politics.&nbsp; Inspecting the AI's thought processes seems to show that, in accordance with your design, the AI has no intention to deceive you, and an explicit goal of telling you the truth.&nbsp; And in your experience so far, the AI has been, inhumanly, <em>well-calibrated;</em> the AI has assigned 99% certainty on a couple of hundred occasions, and been wrong exactly twice that you know of.</p>\n<p>Arguably, you now have far better reason to trust what the AI says to you, than to trust your own thoughts.</p>\n<p>And now the AI tells you that it's 99.9% sure - having seen it with its own cameras, and confirmed from a hundred other sources - even though (it thinks) the human brain is built to invoke the absolute denial macro on it - that...</p>\n<p>...what?</p>\n<p>What's the craziest thing the AI could tell you, such that you would be willing to believe that the AI was the sane one?</p>\n<p>(Some of my own answers appear in the comments.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "YTCrHWYHAsAD74EHo": 1, "hNFdS3rRiYgqqD8aM": 1, "zCYXpx33wq8chGyEz": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "t2NN6JwMFaqANuLqH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 105, "baseScore": 109, "extendedScore": null, "score": 0.000169, "legacy": true, "legacyId": "1396", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 109, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 605, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZiQqsgGX6a42Sfpii"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-15T09:52:57.503Z", "modifiedAt": null, "url": null, "title": "\"Sex Is Always Well Worth Its Two-Fold Cost\"", "slug": "sex-is-always-well-worth-its-two-fold-cost", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:08.941Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yKfmsyKZhzAGZBAwt/sex-is-always-well-worth-its-two-fold-cost", "pageUrlRelative": "/posts/yKfmsyKZhzAGZBAwt/sex-is-always-well-worth-its-two-fold-cost", "linkUrl": "https://www.lesswrong.com/posts/yKfmsyKZhzAGZBAwt/sex-is-always-well-worth-its-two-fold-cost", "postedAtFormatted": "Wednesday, July 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Sex%20Is%20Always%20Well%20Worth%20Its%20Two-Fold%20Cost%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Sex%20Is%20Always%20Well%20Worth%20Its%20Two-Fold%20Cost%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyKfmsyKZhzAGZBAwt%2Fsex-is-always-well-worth-its-two-fold-cost%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Sex%20Is%20Always%20Well%20Worth%20Its%20Two-Fold%20Cost%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyKfmsyKZhzAGZBAwt%2Fsex-is-always-well-worth-its-two-fold-cost", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyKfmsyKZhzAGZBAwt%2Fsex-is-always-well-worth-its-two-fold-cost", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 264, "htmlBody": "<p>This might be of interest to the evo bio and game theory wannabes here: <a href=\"http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2701646\">\"Sex Is Always Well Worth Its Two-Fold Cost\"</a> by Alexander Feigel, Avraham Englander and Assaf Engel.</p>\n<p>Abstract:</p>\n<blockquote>\n<p>Sex is considered as an evolutionary paradox, since its positive contribution to Darwinian fitness remains unverified for some species. Defenses against unpredictable threats (parasites, fluctuating environment and deleterious mutations) are indeed significantly improved by wider genetic variability and by positive epistasis gained by sexual reproduction. The corresponding evolutionary advantages, however, do not overcome universally the barrier of the two-fold cost for sharing half of one's offspring genome with another member of the population. Here we show that sexual reproduction emerges and is maintained even when its Darwinian fitness is twice as low as the fitness of asexuals. We also show that more than two sexes (inheritance of genetic material from three or even more parents) are always evolutionary unstable. Our approach generalizes the evolutionary game theory to analyze species whose members are able to sense the sexual state of their conspecifics and to adapt their own sex consequently, either by switching or by taxis towards the highest concentration of the complementary sex. The widespread emergence and maintenance of sex follows therefore from its co-evolution with the even more widespread environmental sensing abilities.</p>\n</blockquote>\n<p>I'm currently trying to parse the article, and on first reading could only see a disguised form of the old familiar argument about the stability of sex ratios. It still doesn't seem to answer why females don't switch to parthenogenesis and block all male advances. But maybe you can detect something I missed?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yKfmsyKZhzAGZBAwt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 5.079557621402347e-07, "legacy": true, "legacyId": "1397", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-15T17:58:44.128Z", "modifiedAt": null, "url": null, "title": "The Dirt on Depression", "slug": "the-dirt-on-depression", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:01.680Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pjeby", "createdAt": "2009-02-27T23:51:22.854Z", "isAdmin": false, "displayName": "pjeby"}, "userId": "Zzxr5JZpkitaNxL4Q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wMwuXbXeq8YZhWFHg/the-dirt-on-depression", "pageUrlRelative": "/posts/wMwuXbXeq8YZhWFHg/the-dirt-on-depression", "linkUrl": "https://www.lesswrong.com/posts/wMwuXbXeq8YZhWFHg/the-dirt-on-depression", "postedAtFormatted": "Wednesday, July 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Dirt%20on%20Depression&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Dirt%20on%20Depression%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwMwuXbXeq8YZhWFHg%2Fthe-dirt-on-depression%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Dirt%20on%20Depression%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwMwuXbXeq8YZhWFHg%2Fthe-dirt-on-depression", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwMwuXbXeq8YZhWFHg%2Fthe-dirt-on-depression", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 269, "htmlBody": "<p>(From the \"humans are crazy\" and \"truth is stranger than fiction\" departments...)</p>\n<p>Want to be happy?&nbsp; Try eating dirt... or at least dirty plants.</p>\n<p>Seriously.</p>\n<p>From an article in Discover magazine, <a href=\"http://discovermagazine.com/2007/jul/raw-data-is-dirt-the-new-prozac\">\"Is Dirt The New Prozac?\"</a>:</p>\n<blockquote>\n<p>The results so far suggest that simply inhaling <em>M. vaccae</em>&mdash;you get a dose just by taking a walk in the wild or rooting around in the garden&mdash;could help elicit a jolly state of mind. &ldquo;You can also ingest mycobacteria either through water sources or through eating plants&mdash;lettuce that you pick from the garden, or carrots,&rdquo; Lowry says.</p>\n<p>Graham Rook, an immunologist at University College London and a coauthor of the paper, adds that depression itself may be in part an inflammatory disorder. By triggering the production of immune cells that curb the inflammatory reaction typical of allergies, <em>M. vaccae</em> may ease that inflammation and hence depression. Therapy with <em>M. vaccae</em>&mdash;or with drugs based on the bacterium&rsquo;s molecular components&mdash;might someday be used to treat depression. &ldquo;It&rsquo;s not clear to me whether the way ahead will be drugs that circumvent the use of these bugs,&rdquo; Rook says, &ldquo;or whether it will be easier to say, &lsquo;The hell with it, let&rsquo;s use the bugs.&rsquo;&rdquo;</p>\n</blockquote>\n<p>Given the way the industry works, we'll probably either see drugs, or somebody will patent the bacteria.&nbsp; But that's sort of secondary.&nbsp; The real point is that to the extent our current environment doesn't match our ancestral one, there are likely to be \"bugs\", no pun intended.</p>\n<p>(<a href=\"http://www.pubmedcentral.nih.gov/botrender.fcgi?blobtype=html&amp;artid=1868963\"><strong>The original study</strong></a>: &ldquo;Identification of an Immune-Responsive Mesolimbocortical Serotonergic System: Potential Role in Regulation of Emotional Behavior,&rdquo; by Christopher Lowry et al., published online on March 28 in <em>Neuroscience</em>.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wMwuXbXeq8YZhWFHg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 5, "extendedScore": null, "score": 7e-06, "legacy": true, "legacyId": "1398", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-16T04:17:44.081Z", "modifiedAt": null, "url": null, "title": "Fair Division of Black-Hole Negentropy: an Introduction to Cooperative Game Theory", "slug": "fair-division-of-black-hole-negentropy-an-introduction-to", "viewCount": null, "lastCommentedAt": "2019-03-08T18:54:18.812Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z3W8PRHJM9ZanTDcx/fair-division-of-black-hole-negentropy-an-introduction-to", "pageUrlRelative": "/posts/z3W8PRHJM9ZanTDcx/fair-division-of-black-hole-negentropy-an-introduction-to", "linkUrl": "https://www.lesswrong.com/posts/z3W8PRHJM9ZanTDcx/fair-division-of-black-hole-negentropy-an-introduction-to", "postedAtFormatted": "Thursday, July 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fair%20Division%20of%20Black-Hole%20Negentropy%3A%20an%20Introduction%20to%20Cooperative%20Game%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFair%20Division%20of%20Black-Hole%20Negentropy%3A%20an%20Introduction%20to%20Cooperative%20Game%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz3W8PRHJM9ZanTDcx%2Ffair-division-of-black-hole-negentropy-an-introduction-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fair%20Division%20of%20Black-Hole%20Negentropy%3A%20an%20Introduction%20to%20Cooperative%20Game%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz3W8PRHJM9ZanTDcx%2Ffair-division-of-black-hole-negentropy-an-introduction-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz3W8PRHJM9ZanTDcx%2Ffair-division-of-black-hole-negentropy-an-introduction-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 631, "htmlBody": "<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>ZH-CN</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> <w:UseFELayout /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]-->Non-cooperative game theory, as exemplified by the Prisoner&rsquo;s Dilemma and commonly referred to by just \"game theory\", is well known in this community. But cooperative game theory seems to be much less well known. <span>&nbsp;</span>Personally, I had barely heard of it until a few weeks ago. Here&rsquo;s my attempt to give a taste of what cooperative game theory is about, so you can decide whether it might be worth your while to learn more about it.</p>\n<p class=\"MsoNormal\">The example I&rsquo;ll use is the fair division of black-hole negentropy. It seems likely that for an advanced civilization, the main constraining resource in the universe is negentropy. Every useful activity increases entropy, and since entropy of the universe as a whole never decreases, the excess entropy produced by civilization has to be dumped somewhere. A black hole is the only physical system we know whose entropy grows quadratically with its mass, which makes it ideal as an entropy dump. (See <a href=\"http://weidai.com/black-holes.txt\">http://weidai.com/black-holes.txt</a> where I go into a bit more detail about this idea.)</p>\n<p class=\"MsoNormal\">Let&rsquo;s say there is a civilization consisting of a number of individuals, each the owner of some matter with mass m<sub>i</sub>. They know that their civilization can&rsquo;t produce more than (&sum; m<sub>i</sub>)<sup>2</sup> bits of total entropy over its entire history, and the only way to reach that maximum is for every individual to cooperate and eventually contribute his or her matter into a common black hole. A natural question arises: what is a fair division of the (&sum; m<sub>i</sub>)<sup>2</sup> bits of negentropy among the individual matter owners?</p>\n<p class=\"MsoNormal\">Fortunately, Cooperative Game Theory provides a solution, known as the <a href=\"http://en.wikipedia.org/wiki/Shapley_value\">Shapley Value</a>. There are other proposed solutions, but the Shapley Value is well accepted due to its desirable properties such as &ldquo;symmetry&rdquo; and &ldquo;additivity&rdquo;. Instead of going into the theory, I&rsquo;ll just show you how it works. The idea is, we take a sequence of players, and consider the marginal contribution of each player to the total value as he or she joins the coalition in that sequence. Each player is given an allocation equal to his or her average marginal contribution over all possible sequences.<a id=\"more\"></a></p>\n<p class=\"MsoNormal\">So in the black-hole negentropy game, suppose there are two players, Alice and Bob, with masses A and B. There are then two possible sequences, {Alice, Bob} and {Bob, Alice}. In {Alice, Bob}, Alice&rsquo;s marginal contribution is just A<sup>2</sup>. When Bob joins, the total value becomes (A+B)<sup>2</sup>, so his marginal contribution is (A+B)<sup>2</sup>-A<sup>2</sup> = B<sup>2</sup>+2AB. Similarly, in {Bob, Alice}, Bob&rsquo;s MC is B<sup>2</sup>, and Alice&rsquo;s is A<sup>2</sup>+2AB. Alice&rsquo;s average marginal contribution, and hence her allocation, is therefore A<sup>2</sup>+AB, and Bob&rsquo;s is B<sup>2</sup>+AB.</p>\n<p class=\"MsoNormal\">What happens when there are N players? The math is not hard to work out, and the result is that player i gets an allocation equal to m<sub>i</sub> (m<sub>1</sub> + m<sub>2</sub> + &hellip; + m<sub>N</sub>). Seems fair, right?</p>\n<p class=\"MsoNormal\"><strong>ETA:</strong> At this point, the interested reader can pursue two paths to additional knowledge. You can learn more about the rest of cooperative game theory, or compare other approaches to the problem of fair division, for example welfarist and voting-based. Unfortunately, I don't know of a good online resource or textbook for systematically learning cooperative game theory. If anyone does, please leave a comment. For the latter path, a good book is <span class=\"addmd\">Herv&eacute; Moulin's</span> <a href=\"http://books.google.com/books?id=qQXtEnb2B2cC\">Fair Division and Collective Welfare</a>, which includes a detailed discussion of the Shapley Value in chapter 5.</p>\n<p class=\"MsoNormal\"><strong>ETA2</strong>: I found that <a href=\"http://theory.economics.utoronto.ca/books\">a website of Martin J. Osborne and Ariel Rubinstein</a> offers their game theory textbook for free (after registration), and it contains several chapters on cooperative game theory. The site also has several other books that might be of relevance to this community. A more comprehensive textbook on cooperative game theory seems to be <a href=\"http://books.google.com/books?id=f4htgCcYESIC\">Introduction to the Theory of Cooperative Games</a>. A good reference is <a href=\"http://www.amazon.com/Handbook-Economic-Applications-Handbooks-Economics/dp/0444894284\">Handbook of Game Theory with Economic Applications</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 3, "5A5ZGTQovxbay6fpr": 6}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z3W8PRHJM9ZanTDcx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 55, "extendedScore": null, "score": 8.6e-05, "legacy": true, "legacyId": "1399", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-16T15:41:02.412Z", "modifiedAt": null, "url": null, "title": "Absolute denial for atheists", "slug": "absolute-denial-for-atheists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:08.348Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/32RBBkFSMtvXtQDe2/absolute-denial-for-atheists", "pageUrlRelative": "/posts/32RBBkFSMtvXtQDe2/absolute-denial-for-atheists", "linkUrl": "https://www.lesswrong.com/posts/32RBBkFSMtvXtQDe2/absolute-denial-for-atheists", "postedAtFormatted": "Thursday, July 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Absolute%20denial%20for%20atheists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAbsolute%20denial%20for%20atheists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F32RBBkFSMtvXtQDe2%2Fabsolute-denial-for-atheists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Absolute%20denial%20for%20atheists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F32RBBkFSMtvXtQDe2%2Fabsolute-denial-for-atheists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F32RBBkFSMtvXtQDe2%2Fabsolute-denial-for-atheists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 324, "htmlBody": "<p>This article is a deliberate meta-troll. To be successful I need your trolling cooperation. Now hear me out.</p>\n<p>In <a href=\"/lw/12s/the_strangest_thing_an_ai_could_tell_you/\">The Strangest Thing An AI Could Tell You</a> Eliezer talks about asognostics, who have one of their arm paralyzed, and what's most interesting are in absolute denial of this - in spite of overwhelming evidence that their arm is paralyzed they will just come with new and new rationalizations proving it's not.</p>\n<p>Doesn't it sound like someone else we know? Yes, religious people! In spite of heaps of empirical evidence against existence of their particular flavour of the supernatural, internal inconsistency of their beliefs, and perfectly plausible alternative explanations being well known, something between 90% and 98% of humans believe in the supernatural world, and is in a state of absolute denial not too dissimilar to one of asognostics. Perhaps as many as billions of people in history have even been willing to die for their absurd beliefs.</p>\n<p>We are mostly atheists here - we happen not to share this particular delusion. But please consider an outside view for a moment - how likely is it that unlike almost everyone else we don't have any other such delusions, for which we're in absolute denial of truth in spite of mounting heaps of evidence?</p>\n<p>If the delusion is of the kind that all of us share it, we won't be able to find it without building an AI. We might have some of those - it's not too unlikely as we're a small and self-selected group.</p>\n<p>What I want you to do is try to trigger absolute denial macro in your fellow rationalists! Is there anything that you consider proven beyond any possibility of doubt by both empirical evidence and pure logic, and yet saying it triggers automatic stream of rationalizations in other people? Yes, I pretty much ask you to troll, but it's a good kind of trolling, and I cannot think of any other way to find our delusions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YTCrHWYHAsAD74EHo": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "32RBBkFSMtvXtQDe2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 51, "extendedScore": null, "score": 8.5e-05, "legacy": true, "legacyId": "1400", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 51, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 605, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["t2NN6JwMFaqANuLqH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-16T21:51:57.422Z", "modifiedAt": null, "url": null, "title": "Causes of disagreements", "slug": "causes-of-disagreements", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:19.905Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JustinShovelain", "createdAt": "2009-06-10T00:56:47.112Z", "isAdmin": false, "displayName": "JustinShovelain"}, "userId": "LEeresErqn3BpWrwG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DYWXntS3ybp8x3cKq/causes-of-disagreements", "pageUrlRelative": "/posts/DYWXntS3ybp8x3cKq/causes-of-disagreements", "linkUrl": "https://www.lesswrong.com/posts/DYWXntS3ybp8x3cKq/causes-of-disagreements", "postedAtFormatted": "Thursday, July 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Causes%20of%20disagreements&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACauses%20of%20disagreements%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDYWXntS3ybp8x3cKq%2Fcauses-of-disagreements%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Causes%20of%20disagreements%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDYWXntS3ybp8x3cKq%2Fcauses-of-disagreements", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDYWXntS3ybp8x3cKq%2Fcauses-of-disagreements", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1089, "htmlBody": "<p>You have a disagreement before you. How do you handle it? <br /><strong><br />Causes of fake disagreements:</strong><br />Is the disagreement real? The trivial case is an apparent disagreement occuring over a noisy or low information channel. Internet chat is especially liable to fail this way because of the lack of tone, body language, and relative location cues. People can also disagree through the use of differing definitions with corresponding denotations and connotations. Fortunately, when recognized this cause of disagreement rarely produces problems; the topic at issue rarely is the definitions themselves. If there is a game theoretic reason the agents may also give the appearance of disagreement even though they might well agree in private. The agents could also disagree if they are victims of a Man-in-the-middle attack where someone is intercepting and altering the messages passed between the two parties. Finally, the agents could disagree simply because they are in different contexts. Is the sun yellow I ask? Yes, say you. No, say the aliens at Eta Carinae. <br /><strong><br />Causes of disagreements about predictions:</strong><br />&nbsp; <em>Evidence</em><br />Assuming the disagreement is real what does that give us? Most commonly the disagreement is about the facts that predicate our actions. To handle these we must first consider our relationship to the other person and how they think (a la superrationality); observations made by others may not be given the same weight we would give those observations if we had made them ourselves. After considering this we must then merge their evidence with our own <a href=\"/lw/z/information_cascades/\">in a controlled way</a>. With people this gets a bit tricky. Rarely do people give us information we can handle in a cleanly Bayesian way (a la Aumann's agreement theorem). Instead we must merge our explicit evidence sets along with vague abstracted probabilistic intuitions that are half speculation and half partially forgotten memories.<a id=\"more\"></a><br /><br />&nbsp; <em>Priors</em><br />If we still have a disagreement after considering the evidence what now? The agents could have \"started\" at different locations in <a href=\"/lw/s0/where_recursive_justification_hits_bottom/\">prior or induction space</a>. While it is true that a persons \"starting\" point and what evidence they've seen can be conflated, it is also possible that they really did start at different locations. <br /><br />&nbsp; <em>Resource limitations</em><br />The disagreement could also be caused by resource limitations and implementation details. Cognition could have sensitive dependence on initial conditions. For instance, when answering the question \"is this red?\" slight variations in lighting conditions can make people respond differently on boundary cases. This illustrates both sensitive dependence on initial conditions and also the fact that some types of information (what you saw exactly) just cannot be communicated effectively. Our mental processes are also inherently noisy leading to differing errors in processing the evidence and increasing the need to rehash an argument multiple times. We suffer from computational space and time limitations making computational approximations necessary. We learn these approximations slowly across varying situations and so may disagree with someone even if the prediction relevant evidence is on hand, our other \"evidence\" used to develop these approximations may vary and inadvertently leak into our answers. Our approximation methods may differ. Finally, it takes time to integrate all of the evidence at hand and people differ on the amount of time and resources they have to do so.<br /><br />&nbsp; <em>Systematic errors</em><br />Sadly, it is also possible that one or the other party could simply have a deeply flawed prediction system. They could make systematic errors and have broken or missing corrective feedback loops. They could have disruptive feedback loops that drain the truth from predictions. Their methods of prediction may invalidly vary with what is being considered; their thoughts may shy away from subjects such as death or disease or flaws in their favorite theory and their thoughts may be attracted to what will happen after they win the lottery. Irrationality and biases; emotions and inability to abstract. Or even worse, how is it possible to eliminate a disagreement with someone who disagrees with himself and presents an inconsistent opinion?<br /><br /><strong>Other causes of disagreement:</strong><br />&nbsp; <em>Goals</em><br />I say that dogs are interesting, you say they are boring and yet we both agree on our predictions. How is this possible? This type of disagreement would fall under disagreement about what utility function to apply and between utilitarian goal-preserving agents it is irresolvable in a direct manner; however, indirect ways such as trading boring dogs for interesting cats works much of the time.&nbsp; Plus, we are not utilitarian agents (e.g. circular preferences) ; perhaps there are strategies available to us for resolving conflicts of this form that are not available to utilitarian ones?<br /><br />&nbsp; <em>Epiphenomenal</em><br />Lastly, it is possible for agents to agree on all observable predictions and yet disagree on unobservable predictions. Predictions without consequences aren't predictions at all, how could they be? If the disagreement still exists after realizing that there are no observable consequences look elsewhere for the cause, it cannot be here. Why disagree over things of no value? The disagreement must be caused by something; look there not here.<br /><br /><br /><strong>How to use this taxonomy:</strong><br />I tried to list the above sections in the order one should check for each type of cause if you were to use the sections as a decision tree (ease of checking and fixing, fit to definition, probability of occurrence). This taxonomy is symmetric between the disagreeing parties and many of the sections lend themselves naturally to looping; merging evidence piece by piece, refining calculations iteration by iteration, .... This taxonomy can also be applied recursively to meta disagreements and disagreements found in the process of analyzing the original one. What are the termination conditions for analyzing a disagreement? They come in five forms: complete agreement, satisfying agreement, impossible to agree, acknowledgment of conflict, and dissolving the question. Being a third party to a disagreement changes the analysis only in that you are no longer doing the symmetric self analysis but rather looking in upon a disagreement with that additional distance that entails. <br /><br /><br /><br />Many thanks to Eliezer Yudkowsky, Robin Hanson, and the LessWrong community for much thought provoking material.<br /><br />(ps This is my first post and I would appreciate any feedback: what I did well, what I did badly, and what I can do to improve.)<br /><br />Links:<br />1. http://lesswrong.com/lw/z/information_cascades/<br />2. http://lesswrong.com/lw/s0/where_recursive_justification_hits_bottom/</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"wzgcQCrwKfETcBpR9": 4, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DYWXntS3ybp8x3cKq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 27, "extendedScore": null, "score": 6.5e-05, "legacy": true, "legacyId": "1401", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["DNQw596nPCX4x7xT9", "C8nEXTcjZb9oauTCW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-17T15:43:30.338Z", "modifiedAt": null, "url": null, "title": "The Popularization Bias", "slug": "the-popularization-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:26.053Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/57YjxPCRfZpXNJw49/the-popularization-bias", "pageUrlRelative": "/posts/57YjxPCRfZpXNJw49/the-popularization-bias", "linkUrl": "https://www.lesswrong.com/posts/57YjxPCRfZpXNJw49/the-popularization-bias", "postedAtFormatted": "Friday, July 17th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Popularization%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Popularization%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F57YjxPCRfZpXNJw49%2Fthe-popularization-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Popularization%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F57YjxPCRfZpXNJw49%2Fthe-popularization-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F57YjxPCRfZpXNJw49%2Fthe-popularization-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 390, "htmlBody": "<p>I noticed that most recommendations in the recent <a href=\"/lw/12d/recommended_reading_for_new_rationalists/\">recommended readings thread</a> consist of either fiction or popularizations of specific scientific disciplines. This introduces a potential bias: aspiring rationalists may never learn about some fields or ideas that are important for the art of rationality, just because they've never been popularized.</p>\n<p>In my recent post on the fair division of black-hole negentropy, I tried to introduce two such ideas/fields (which may be one too many for a single post :). One is that black holes have entropy quadratic in mass, and therefore are ideal entropy dumps (or equivalently, negentropy mines). This is a well-known result in thermodynamics, plus an obvious application of it. Some have complained that the idea is too sci-fi, but actually the opposite is true. Unlike other perhaps equally obvious futuristic ideas such as cryonics, AI and the Singularity, I've never read or watched a piece of science fiction that explorered this one. (BTW, in case it's not clear why black-hole negentropy is important for <em>rationality</em>, it implies that value probably scales superlinearly with material and that huge gains from cooperation can be directly derived from the fundamental laws of physics.)</p>\n<p>Similarly, there are many popularizations of topics such as the Prisoner's Dilemma and the Nash Equilibrium in non-cooperative game theory (and even a blockbuster <a href=\"http://en.wikipedia.org/wiki/A_Beautiful_Mind_%28film%29\">movie</a> about John Nash!), but I'm not aware of any for cooperative game theory.</p>\n<p>Much of Less Wrong, and Overcoming Bias before it, can be seen as an attempt to correct this bias. Eliezer's posts have provided fictional treatments or popular accounts of probability theory, decision theory, MWI, algorithmic information theory, Bayesian networks, and various ethical theories, to name a few, and others have continued the tradition to some extent. But since popularization and writing fiction are <em>hard</em>, and not many people have both the skills and the motivation to do them, I wonder if there are still other important ideas/fields that most of us don't know about yet.</p>\n<p>So here's my request: if you know of such a field or idea, just name it in a comment and give a reference for it, and maybe say a few words about why it's important, if that's not obvious. Some of us may be motivated to learn about it for whatever reason, even from a textbook or academic article, and may eventually produce a popular account for it.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fF9GEdWXKJ3z73TmB": 1, "MAp6Ft8b3s7kJdrQ9": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "57YjxPCRfZpXNJw49", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 30, "extendedScore": null, "score": 6.2e-05, "legacy": true, "legacyId": "1402", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wfJebLTPGYaK3Gr8W"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-17T16:59:41.901Z", "modifiedAt": null, "url": null, "title": "Zwicky's Trifecta of Illusions", "slug": "zwicky-s-trifecta-of-illusions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:07.009Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "thomblake", "createdAt": "2009-02-27T15:35:08.282Z", "isAdmin": false, "displayName": "thomblake"}, "userId": "zCHE6bXWKB6kfJsJS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SZkdkbWemx8kM5tDY/zwicky-s-trifecta-of-illusions", "pageUrlRelative": "/posts/SZkdkbWemx8kM5tDY/zwicky-s-trifecta-of-illusions", "linkUrl": "https://www.lesswrong.com/posts/SZkdkbWemx8kM5tDY/zwicky-s-trifecta-of-illusions", "postedAtFormatted": "Friday, July 17th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Zwicky's%20Trifecta%20of%20Illusions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AZwicky's%20Trifecta%20of%20Illusions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSZkdkbWemx8kM5tDY%2Fzwicky-s-trifecta-of-illusions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Zwicky's%20Trifecta%20of%20Illusions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSZkdkbWemx8kM5tDY%2Fzwicky-s-trifecta-of-illusions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSZkdkbWemx8kM5tDY%2Fzwicky-s-trifecta-of-illusions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 258, "htmlBody": "<p>Linguist <a title=\"Arnold Zwicky's home page\" href=\"http://www.stanford.edu/~zwicky/\">Arnold Zwicky</a> has named three linguistic 'illusions' which seem relevant to cognitive bias. They are:</p>\n<ol>\n<li><a title=\"Word Spy - Frequency Illusion\" href=\"http://www.wordspy.com/words/frequencyillusion.asp\">Frequency Illusion</a> - Once you've noticed a phenomenon, it seems to happen a lot.</li>\n<li><a title=\"Wikipedia - Recency Illusion\" href=\"http://en.wikipedia.org/wiki/Recency_illusion\">Recency Illusion</a> - The belief that something is a recent phenomenon, when it has actually existed a long time.</li>\n<li><a title=\"Wikipedia - Adolescent Illusion\" href=\"http://en.wikipedia.org/wiki/Adolescent_illusion\">Adolescent Illusion</a> - The belief that adolescents are the cause of undesirable language trends.</li>\n</ol>\n<p>Zwicky talks about them <a title=\"Language Log\" href=\"http://itre.cis.upenn.edu/~myl/languagelog/archives/002407.html\">here</a>, and in not so many words links them to the standard bias of <a title=\"Wikipedia - Selective Perception\" href=\"http://en.wikipedia.org/wiki/Selective_perception\">selective perception</a>.</p>\n<p>As an example, here is an exerpt via <a title=\"Jerz's Literacy Weblog\" href=\"http://jerz.setonhill.edu/weblog/2008/09/shattering_the_illusions_of_te/\">Jerz's Literacy Weblog</a> (originally via <a title=\"David Crystal on Texting\" href=\"http://david-crystal.blogspot.com/2008/08/on-txtng-reactions.html\">David Crystal</a>), regarding text messages:</p>\n<blockquote>\n<div><span style=\"font-family: 'trebuchet ms'; color: #333333;\"> \n<ul style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0.75em; margin-left: 30px; list-style-type: disc; list-style-position: outside; list-style-image: initial; background-repeat: repeat-y; padding: 0px;\">\n<li style=\"padding: 0px; margin: 0px;\">Text messages aren't full of abbreviations - typically less than ten percent of the words use them. [Frequency Illusion]</li>\n<li style=\"padding: 0px; margin: 0px;\">These abbreviations aren't a new language - they've been around for decades. [Recency Illusion]</li>\n<li style=\"padding: 0px; margin: 0px;\">They aren't just used by kids - adults of all ages and institutions are the leading texters these days. [Adolescent Illusion]</li>\n</ul>\n</span></div>\n</blockquote>\n<p>It is my conjecture that these illusions are notable in areas other than linguistics. For example, history is rife with allusions that the younger generation is corrupt, and such speakers are not merely referring to their use of language. Could this be the adolescent illusion in action?</p>\n<p>So, are these notable biases to watch out for, or are they merely obvious instances of standard biases?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4R8JYu4QF2FqzJxE5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SZkdkbWemx8kM5tDY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 23, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "1403", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-19T04:35:05.961Z", "modifiedAt": null, "url": null, "title": "Are You Anosognosic?", "slug": "are-you-anosognosic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:02.312Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xWfAvTmCBtoqhHR7Q/are-you-anosognosic", "pageUrlRelative": "/posts/xWfAvTmCBtoqhHR7Q/are-you-anosognosic", "linkUrl": "https://www.lesswrong.com/posts/xWfAvTmCBtoqhHR7Q/are-you-anosognosic", "postedAtFormatted": "Sunday, July 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20You%20Anosognosic%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20You%20Anosognosic%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxWfAvTmCBtoqhHR7Q%2Fare-you-anosognosic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20You%20Anosognosic%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxWfAvTmCBtoqhHR7Q%2Fare-you-anosognosic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxWfAvTmCBtoqhHR7Q%2Fare-you-anosognosic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 259, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/12s/the_strangest_thing_an_ai_could_tell_you/\">The Strangest Thing An AI Could Tell You</a></p>\n<p>Brain damage patients with <a href=\"/lw/20/the_apologist_and_the_revolutionary/\">anosognosia</a> are incapable of considering, noticing, admitting, or realizing even after being argued with, that their left arm, left leg, or left side of the body, is paralyzed.&nbsp; Again I'll quote <a href=\"/lw/20/the_apologist_and_the_revolutionary/\">Yvain's summary</a>:</p>\n<blockquote>\n<p>After a right-hemisphere stroke, she lost movement in her left arm but continuously denied it. When the doctor asked her to move her arm, and she observed it not moving, she claimed that it wasn't actually her arm, it was her daughter's. Why was her daughter's arm attached to her shoulder? The patient claimed her daughter had been there in the bed with her all week. Why was her wedding ring on her daughter's hand? The patient said her daughter had borrowed it. Where was the patient's arm? The patient \"turned her head and searched in a bemused way over her left shoulder\".</p>\n</blockquote>\n<p>A brief search didn't turn up a base-rate frequency in the population for left-arm paralysis with anosognosia, but let's say the base rate is 1 in 10,000,000 individuals (so around 670 individuals worldwide).</p>\n<p>Supposing this to be the <a href=\"http://wiki.lesswrong.com/wiki/Prior\">prior</a>, what is your estimated probability that your left arm is currently paralyzed?<a id=\"more\"></a></p>\n<p><strong>Added</strong>:&nbsp; This interests me because it seems to be a special case of the same general issue discussed in <a href=\"/lw/gr/the_modesty_argument/\">The Modesty Argument</a> and Robin's reply <a href=\"http://www.overcomingbias.com/2008/05/sleepy-fools.html\">Sleepy Fools</a> - when pathological minds roughly similar to yours update based on fabricated evidence to conclude they are not pathological, under what circumstances can you update on different-seeming evidence to conclude that <em>you </em>are not pathological?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"rWzGNdjuep56W5u2d": 1, "YPZCAs9Axp9PtrF22": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xWfAvTmCBtoqhHR7Q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 20, "extendedScore": null, "score": 3.2e-05, "legacy": true, "legacyId": "1406", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 67, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["t2NN6JwMFaqANuLqH", "ZiQqsgGX6a42Sfpii", "NKECtGX4RZPd7SqYp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-19T17:03:06.180Z", "modifiedAt": null, "url": null, "title": "Article upvoting", "slug": "article-upvoting", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:33.993Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qK2bro89zYwXii6gM/article-upvoting", "pageUrlRelative": "/posts/qK2bro89zYwXii6gM/article-upvoting", "linkUrl": "https://www.lesswrong.com/posts/qK2bro89zYwXii6gM/article-upvoting", "postedAtFormatted": "Sunday, July 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Article%20upvoting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArticle%20upvoting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqK2bro89zYwXii6gM%2Farticle-upvoting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Article%20upvoting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqK2bro89zYwXii6gM%2Farticle-upvoting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqK2bro89zYwXii6gM%2Farticle-upvoting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<p>Except in rare cases (like Wei Dai's <a href=\"/lw/12v/fair_division_of_blackhole_negentropy_an/\">Fair Division of Black-Hole Negentropy</a>) I'm still using article upvotes to partially determine whether to promote articles to the front page - some informal mixture of \"number of upvotes\" + \"editor's judgment\".&nbsp; I mention this because while comment voting is still healthy, the amount of article voting seems to be dropping off.&nbsp; As of now I'm still drawing the inference that no one thinks \"<a href=\"/lw/132/are_you_anosognosic/\">Are You Anosognosic?</a>\" worthy of promotion, or wants to see similar articles from me in the future - since other articles have at least gotten more votes than 0.&nbsp; But as the amount of article voting diminishes, it becomes harder to trust such inferences.&nbsp; Maybe people liked that article (or others I haven't promoted) and just didn't bother to upvote.</p>\n<p>I'm posting this observation just in case people figure that upvoting articles doesn't make a difference.&nbsp; It does.&nbsp; It also encourages authors to write similar posts in the future, or alternatively not.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qK2bro89zYwXii6gM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 18, "extendedScore": null, "score": 5.089383895288101e-07, "legacy": true, "legacyId": "1407", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["z3W8PRHJM9ZanTDcx", "xWfAvTmCBtoqhHR7Q"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-19T22:24:32.947Z", "modifiedAt": null, "url": null, "title": "Sayeth the Girl", "slug": "sayeth-the-girl", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:26.096Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gsL6CLqjujPNSLL2o/sayeth-the-girl", "pageUrlRelative": "/posts/gsL6CLqjujPNSLL2o/sayeth-the-girl", "linkUrl": "https://www.lesswrong.com/posts/gsL6CLqjujPNSLL2o/sayeth-the-girl", "postedAtFormatted": "Sunday, July 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sayeth%20the%20Girl&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASayeth%20the%20Girl%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgsL6CLqjujPNSLL2o%2Fsayeth-the-girl%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sayeth%20the%20Girl%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgsL6CLqjujPNSLL2o%2Fsayeth-the-girl", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgsL6CLqjujPNSLL2o%2Fsayeth-the-girl", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 825, "htmlBody": "<p><strong>Disclaimer:</strong> If you are prone to dismissing women's complaints of gender-related problems as the women being whiny, emotionally unstable girls who see sexism where there is none, this post is unlikely to interest you.</p>\n<p><strong>For your convenience, links to followup posts:</strong> <a href=\"/lw/139/being_saner_about_gender_and_rationality/\">Roko says</a>; <a href=\"/lw/13g/outside_analysis_and_blind_spots/\">orthonormal says</a>; <a href=\"/lw/13j/of_exclusionary_speech_and_gender_politics/\">Eliezer says</a>; <a href=\"/lw/13k/missing_the_trees_for_the_forest/\">Yvain says</a>; <a href=\"/lw/13s/the_nature_of_offense/\">Wei_Dai says</a></p>\n<p>As far as I can tell, I am the most active female poster on Less Wrong.&nbsp; (AnnaSalamon has higher karma than I, but she hasn't commented on anything for two months now.)&nbsp; There are <a href=\"/lw/fk/survey_results/\">not many of us</a>.&nbsp; This is usually immaterial.&nbsp; Heck, sometimes people don't even <a href=\"/lw/12b/revisiting_torture_vs_dust_specks/wsz?context=2#comments\">notice</a> in spite of my <a href=\"/lw/ap/of_gender_and_rationality/7v4\">girly username</a>, my <a href=\"/lw/b9/welcome_to_less_wrong/7o9\">self-introduction</a>, and the fact that I'm now apparently the <a href=\"/lw/fo/religion_mystery_and_warm_soft_fuzzies/cdu\">feminism</a> <a href=\"/lw/12w/absolute_denial_for_atheists/xua\">police</a> <a href=\"/lw/da/wheres_your_sense_of_mystery/9se\">of Less Wrong</a>.</p>\n<p>My life is not <em>about</em> being a girl.&nbsp; In fact, I'm less preoccupied with feminism and women's special interest issues than most of the women I know, and some of the men.&nbsp; It's not my pet topic.&nbsp; I do not focus on feminist philosophy in school.&nbsp; I took an \"Early Modern Women Philosophers\" course because I needed the history credit, had room for a suitable class in a semester when one was offered, and heard the teacher was nice, and I was pretty bored.&nbsp; I wound up doing my midterm paper on Malebranche in that class because we'd covered him to give context to Mary Astell, and he was more interesting than she was.&nbsp; I didn't vote for Hilary Clinton in the primary.&nbsp; Given the choice, I have lots of things I'd rather be doing than ferreting out hidden or less-than-hidden sexism on one of my favorite websites.</p>\n<p>Unfortunately, nobody else seems to want to do it either, and I'm not content to leave it undone.&nbsp; I suppose I could abandon the site and leave it even more masculine so the guys could all talk in <a href=\"/lw/12w/absolute_denial_for_atheists/xq8\">their own language</a>, unimpeded by stupid chicks being stupidly offended by completely unproblematic things like <a href=\"/lw/12w/absolute_denial_for_atheists/xpv\">objectification</a> and just plain <a href=\"/lw/cn/instrumental_vs_epistemic_a_bardic_perspective/9r6\">jerkitude</a>.&nbsp; I would almost certainly have vacated the site already if feminism <em>were</em> my pet issue, or if I were more easily offended.&nbsp; (In general, I'm very hard to offend.&nbsp; The fact that people here have succeeded in doing so anyway without even, apparently, going out of their way to do it should be a great big red flag that something's up.)&nbsp; If you're wondering why half of the potential audience of the site seems to be conspicuously <a href=\"/lw/ap/of_gender_and_rationality/\">not here</a>, this may have something to do with it.</p>\n<p><a id=\"more\"></a></p>\n<p>So can I get some help?&nbsp; <a href=\"/lw/12w/absolute_denial_for_atheists/xr1\">Some</a> <a href=\"/lw/12w/absolute_denial_for_atheists/xug\">lovely</a> <a href=\"/lw/fo/religion_mystery_and_warm_soft_fuzzies/cgk\">people</a> have thrown in their support, but usually after I or, more rarely, <a href=\"/lw/fo/religion_mystery_and_warm_soft_fuzzies/cdk\">someone else</a> sounds the alarm, and usually without much persistence or apparent investment.&nbsp; There is still conspicuous <a href=\"/lw/12w/absolute_denial_for_atheists/xg9\">karmic support</a> for some comments that perpetuate the problems, which does nothing to disincentivize being piggish around here - some people seem to <a href=\"/lw/12w/absolute_denial_for_atheists/xqq\">earnestly care</a> about the problem, but this isn't enforced by the community at large, it's just a preexisting disposition (near as I can tell).</p>\n<p>I would like help reducing the incidence of:</p>\n<ul>\n<li>Comments and posts that casually objectify women or encourage the objectification of women.&nbsp; \"Objectification\" is what happens when a person is treated or discussed as an object, not as an autonomous being.&nbsp; (Non-women can also be objectified, and that too should be stopped.)</li>\n<li>Casual use of masculine and/or heteronormative examples in posts and comments that aren't explicitly about gender.&nbsp; It's just not that hard to come up with an unsexed example.&nbsp; Be especially careful when using the second person.&nbsp; If you <em>need</em> to use an example with a gender, there's no reason to consider male the default - consider choosing <a href=\"/lw/3k/how_to_not_lose_an_argument/wsr\">randomly</a>, or you could use a real person as an example (who isn't presumed to archetypically represent anyone in the audience) instead of a hypothetical one (who might be).</li>\n<li>Sweeping generalizations about women, if they are not backed up by <em>overwhelming</em> hard data (responsibly gathered and interpreted).&nbsp; The cost of being wrong about this sort of thing is high, even if the culprits don't bear it themselves, and extreme care should be taken.</li>\n<li>Fawning admiration of pickup artists who attain their fame by the systematic manipulation of women.&nbsp; If it is necessary to refer admiringly to a pickup artist or pickup strategy (I'm not sure why it would be, but if), care should be taken to choose one whose methods are explicitly non-depersonalizing, and disclaim that specifically in the comment.</li>\n</ul>\n<p>We could use more of the following:</p>\n<ul>\n<li>Thoughtful use of qualifiers and disclaimers in talk about sex and gender.&nbsp; <a href=\"http://www.overcomingbias.com/2008/06/against-disclai.html\">Robin</a> is <a href=\"/lw/dr/generalizing_from_one_example/af7\">not right</a>.</li>\n<li>Attention to the <a href=\"http://www.amptoons.com/blog/the-male-privilege-checklist/\">privileges</a> of masculinity and attempts to reduce that disparity.&nbsp; (Note that of course there are also female privileges, but until Less Wrong hosts custody battles or we start suspecting that some of us might be violent criminals, they are unlikely to come into play nearly so much in this location.)</li>\n</ul>\n<p>Thank you for your attention and, hopefully, your assistance.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"W9aNkPwtPhMrcfgj7": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gsL6CLqjujPNSLL2o", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 162, "baseScore": 66, "extendedScore": null, "score": 0.000104, "legacy": true, "legacyId": "1408", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 66, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 503, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["MswXnxRX5xN4tNjzC", "oZb6AqZKBG4gmLdAZ", "MyqGb24pM54rJhDpb", "MtNnFg4uN32YPoKNa", "QPqm5aj2meRmE7kR8", "ZWC3n9c6v4s35rrZ3", "xsyG7PkMekHud2DMK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-20T00:02:59.721Z", "modifiedAt": null, "url": null, "title": "Timeless Decision Theory: Problems I Can't Solve", "slug": "timeless-decision-theory-problems-i-can-t-solve", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:36.365Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c3wWnvgzdbRhNnNbQ/timeless-decision-theory-problems-i-can-t-solve", "pageUrlRelative": "/posts/c3wWnvgzdbRhNnNbQ/timeless-decision-theory-problems-i-can-t-solve", "linkUrl": "https://www.lesswrong.com/posts/c3wWnvgzdbRhNnNbQ/timeless-decision-theory-problems-i-can-t-solve", "postedAtFormatted": "Monday, July 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Timeless%20Decision%20Theory%3A%20Problems%20I%20Can't%20Solve&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATimeless%20Decision%20Theory%3A%20Problems%20I%20Can't%20Solve%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc3wWnvgzdbRhNnNbQ%2Ftimeless-decision-theory-problems-i-can-t-solve%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Timeless%20Decision%20Theory%3A%20Problems%20I%20Can't%20Solve%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc3wWnvgzdbRhNnNbQ%2Ftimeless-decision-theory-problems-i-can-t-solve", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc3wWnvgzdbRhNnNbQ%2Ftimeless-decision-theory-problems-i-can-t-solve", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1687, "htmlBody": "<p>Suppose you're out in the desert, running out of water, and soon to die - when someone in a motor vehicle drives up next to you.&nbsp; Furthermore, the driver of the motor vehicle is a perfectly selfish ideal game-theoretic agent, and even further, so are you; and what's more, the driver is Paul Ekman, who's really, really good at reading facial microexpressions.&nbsp; The driver says, \"Well, I'll convey you to town if it's in my interest to do so - so will you give me $100 from an ATM when we reach town?\"</p>\n<p>Now of course you wish you could answer \"Yes\", but as an ideal game theorist yourself, you realize that, once you actually reach town, you'll have no further motive to pay off the driver.&nbsp; \"Yes,\" you say.&nbsp; \"You're lying,\" says the driver, and drives off leaving you to die.</p>\n<p><a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">If only you weren't so rational!</a></p>\n<p>This is the dilemma of Parfit's Hitchhiker, and the above is the standard resolution according to mainstream philosophy's causal decision theory, which also <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">two-boxes on Newcomb's Problem</a> and <a href=\"http://wiki.lesswrong.com/wiki/PD\">defects in the Prisoner's Dilemma</a>.&nbsp; Of course, any <em>self-modifying</em> agent who expects to face such problems - in general, or in particular - will soon self-modify into an agent that doesn't regret its \"rationality\" so much.&nbsp; So from the perspective of a self-modifying-AI-theorist, classical causal decision theory is a wash.&nbsp; And indeed I've worked out a theory, tentatively labeled \"timeless decision theory\", which covers these three Newcomblike problems and delivers a first-order answer that is already reflectively consistent, without need to explicitly consider such notions as \"precommitment\".&nbsp; Unfortunately this \"timeless decision theory\" would require a long sequence to write up, and <a href=\"/lw/7v/formalizing_newcombs/5ml\">it's not my current highest writing priority unless someone offers to let me do a PhD thesis on it</a>.</p>\n<p>However, there are some other timeless decision problems for which I do <em>not</em> possess a general theory.</p>\n<p>For example, there's a problem introduced to me by Gary Drescher's marvelous <em>Good and Real</em> (OOPS: The below formulation was independently invented by <a href=\"/lw/3l/counterfactual_mugging/\">Vladimir Nesov</a>; Drescher's book actually contains a related dilemma in which box B is transparent, and only contains $1M if Omega predicts you will one-box whether B appears full or empty, and Omega has a 1% error rate) which runs as follows:</p>\n<p>Suppose Omega (the same superagent from Newcomb's Problem, who is known to be honest about how it poses these sorts of dilemmas) comes to you and says:</p>\n<p>\"I just flipped a fair coin.&nbsp; I decided, before I flipped the coin, that if it came up heads, I would ask you for $1000.&nbsp; And if it came up tails, I would give you $1,000,000 if and only if I predicted that you would give me $1000 if the coin had come up heads.&nbsp; The coin came up heads - can I have $1000?\"<a id=\"more\"></a></p>\n<p>Obviously, the only reflectively consistent answer in this case is \"Yes - here's the $1000\", because if you're an agent who expects to encounter many problems like this in the future, you will self-modify to be the sort of agent who answers \"Yes\" to this sort of question - just like with Newcomb's Problem or Parfit's Hitchhiker.</p>\n<p>But I don't have a general theory which replies \"Yes\".&nbsp; At the point where Omega asks me this question, I already know that the coin came up heads, so I already know I'm not going to get the million.&nbsp; It seems like I want to decide \"as if\" I don't know whether the coin came up heads or tails, and then implement that decision even if I know the coin came up heads.&nbsp; But I don't have a good formal way of talking about how my decision in one state of knowledge has to be determined by the decision I would make if I occupied a different epistemic state, conditioning using the probability <em>previously </em>possessed by events I have <em>since </em>learned the outcome of...&nbsp; Again, it's easy to talk informally about why you have to reply \"Yes\" in this case, but that's not the same as being able to exhibit a general algorithm.</p>\n<p>Another stumper was presented to me by Robin Hanson at an OBLW meetup.&nbsp; Suppose you have ten ideal game-theoretic selfish agents and a pie to be divided by <em>majority vote</em>.&nbsp; Let's say that six of them form a coalition and decide to vote to divide the pie among themselves, one-sixth each.&nbsp; But then two of them think, \"Hey, this leaves four agents out in the cold.&nbsp; We'll get together with those four agents and offer them to divide half the pie among the four of them, leaving one quarter apiece for the two of us.&nbsp; We get a larger share than one-sixth that way, and they get a larger share than zero, so it's an improvement from the perspectives of all six of us - they should take the deal.\"&nbsp; And those six then form a new coalition and redivide the pie.&nbsp; Then another two of the agents think:&nbsp; \"The two of us are getting one-eighth apiece, while four other agents are getting zero - we should form a coalition with them, and by majority vote, give each of us one-sixth.\"</p>\n<p>And so it goes on:&nbsp; Every majority coalition and division of the pie, is <em>dominated </em>by another <em>majority</em> coalition in which each agent of the new majority gets <em>more</em> pie.&nbsp; There does not appear to be any such thing as a dominant majority vote.</p>\n<p>(Robin Hanson actually used this to suggest that if you set up a Constitution which governs a society of humans and AIs, the AIs will be unable to conspire among themselves to change the constitution and leave the humans out in the cold, because then the new compact would be dominated by yet other compacts and there would be chaos, and therefore any constitution stays in place forever.&nbsp; Or something along those lines.&nbsp; Needless to say, I do not intend to rely on such, but it would be nice to have a formal theory in hand which shows how ideal reflectively consistent decision agents will act in such cases (so we can <em>prove</em> they'll shed the old \"constitution\" like used snakeskin.))</p>\n<p>Here's yet another problem whose proper <em>formulation</em> I'm still not sure of, and it runs as follows.&nbsp; First, consider the Prisoner's Dilemma.&nbsp; Informally, two timeless decision agents with common knowledge of the other's timeless decision agency, but no way to communicate or make binding commitments, will both Cooperate because they know that the other agent is in a similar epistemic state, running a similar decision algorithm, and will end up doing the same thing that they themselves do.&nbsp; In general, on the <a href=\"/lw/tn/the_true_prisoners_dilemma/\">True Prisoner's Dilemma</a>, facing an opponent who can accurately predict your own decisions, you want to cooperate only if the other agent will cooperate if and only if they predict that you will cooperate.&nbsp; And the other agent is reasoning similarly:&nbsp; They want to cooperate only if you will cooperate if and only if you accurately predict that they will cooperate.</p>\n<p>But there's actually an infinite regress here which is being glossed over - you won't cooperate <em>just</em> because you predict that they will cooperate, you will only cooperate if you predict <em>they</em> will cooperate <em>if and only if</em> you cooperate.&nbsp; So the other agent needs to cooperate if they predict that you will cooperate <em>if </em>you predict that they will cooperate... (...only if they predict that you will cooperate, etcetera).</p>\n<p>On the Prisoner's Dilemma in <em>particular</em>, this infinite regress can be cut short by expecting that the other agent is doing symmetrical reasoning on a symmetrical problem and will come to a symmetrical conclusion, so that you can expect their action to be the symmetrical analogue of your own - in which case (C, C) is preferable to (D, D).&nbsp; But what if you're facing a more general decision problem, with many agents having asymmetrical choices, and everyone wants to have their decisions depend on how they predict that other agents' decisions depend on their own predicted decisions?&nbsp; Is there a general way of resolving the regress?</p>\n<p>On Parfit's Hitchhiker and Newcomb's Problem, we're <em>told</em> how the other behaves as a <em>direct </em>function of our own predicted decision - Omega rewards you if you (are predicted to) one-box, the driver in Parfit's Hitchhiker saves you if you (are predicted to) pay $100 on reaching the city.&nbsp; My timeless decision theory only functions in cases where the other agents' decisions can be viewed as functions of one argument, that argument being your own choice in that particular case - either by specification (as in Newcomb's Problem) or by symmetry (as in the Prisoner's Dilemma).&nbsp; If their decision is allowed to depend on how your decision <em>depends on</em> their decision - like saying, \"I'll cooperate, not 'if the other agent cooperates', but <em>only </em>if the other agent cooperates <em>if and only if I cooperate</em> - if I predict the other agent to cooperate <em>unconditionally</em>, then I'll just defect\" - then in general I do not know how to resolve the resulting infinite regress of conditionality, except in the special case of predictable symmetry.</p>\n<p>You perceive that there is a definite note of \"timelessness\" in all these problems.</p>\n<p>Any offered solution may assume that a timeless decision theory for direct cases already exists - that is, if you can reduce the problem to one of \"I can predict that if (the other agent predicts) I choose strategy X, then the other agent will implement strategy Y, and my expected payoff is Z\", then I already have a reflectively consistent solution which this margin is unfortunately too small to contain.</p>\n<p>(In case you're wondering, I'm writing this up because one of the SIAI Summer Project people asked if there was any Friendly AI problem that could be modularized and handed off and potentially written up afterward, and the answer to this is almost always \"No\", but this is actually the one exception that I can think of.&nbsp; (Anyone actually taking a shot at this should probably familiarize themselves with the existing literature on Newcomblike problems - the edited volume \"Paradoxes of Rationality and Cooperation\" should be a sufficient start (and I believe there's a copy at the SIAI Summer Project house.)))</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 2, "5f5c37ee1b5cdee568cfb1b6": 2, "5f5c37ee1b5cdee568cfb1db": 7}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c3wWnvgzdbRhNnNbQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 53, "baseScore": 55, "extendedScore": null, "score": 9.1e-05, "legacy": true, "legacyId": "1409", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 156, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6ddcsdA2c2XpNpE5x", "mg6jDEuQEjBGtibX7", "HFyWNBnDNEDsDNLrZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-20T03:10:40.135Z", "modifiedAt": null, "url": null, "title": "An Akrasia Anecdote", "slug": "an-akrasia-anecdote", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:14.070Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Matt_Simpson", "createdAt": "2009-03-05T21:06:45.432Z", "isAdmin": false, "displayName": "Matt_Simpson"}, "userId": "v4krJe8Qa4jnhPTmd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/u4zwGST5Nc7eqfQv6/an-akrasia-anecdote", "pageUrlRelative": "/posts/u4zwGST5Nc7eqfQv6/an-akrasia-anecdote", "linkUrl": "https://www.lesswrong.com/posts/u4zwGST5Nc7eqfQv6/an-akrasia-anecdote", "postedAtFormatted": "Monday, July 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20Akrasia%20Anecdote&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20Akrasia%20Anecdote%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu4zwGST5Nc7eqfQv6%2Fan-akrasia-anecdote%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20Akrasia%20Anecdote%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu4zwGST5Nc7eqfQv6%2Fan-akrasia-anecdote", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu4zwGST5Nc7eqfQv6%2Fan-akrasia-anecdote", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1687, "htmlBody": "<p>About a month ago I committed myself to an <a href=\"/lw/ep/applied_picoeconomics/vbv\">anti-akrasia resolution</a>, inspired by <a href=\"/lw/ep/applied_picoeconomics/\">Yvain</a> and <a href=\"/user/Z_M_Davis/\">ZM</a>. &nbsp;I won't repost the resolution here, but if you want to see it, click the first link. &nbsp;The essence of my resolution was to commit myself to practice math to prepare for graduate school in the fall. &nbsp;The resolution was valid from June 18 to 28 - I ended it there because the next day I was on a plane to Orange, CA to start an internship (in experimental economics, if anyone's interested). &nbsp;Now, I'm going to provide a lot of anecdotal evidence and speculation. &nbsp;Please do not fall prey to the <a href=\"http://wiki.lesswrong.com/wiki/Typical_mind_fallacy\">typical mind fallacy</a>. &nbsp;The structure of my brain may be sufficiently different from yours that nothing I say here generalizes. &nbsp;I also could be deluding myself. &nbsp;This is only anecdotal evidence, and I have no real way of knowing whether or not my perception of what happened is biased.<a id=\"more\"></a></p>\n<p>With that being said, there is <em>some</em>&nbsp;objective evidence, namely how much I deviated from my resolution. &nbsp;There is also what happened <em>after </em>my resolution while here at Chapman University. &nbsp;Here, again, the evidence is mostly subjective with the exception of how much I stuck to the old resolution.</p>\n<p>To start with the beginnning, the first day my resolution was in effect I worked diligently for well over two hours - probably closer to 4 - working through multiple chapters of my old probability text... actually, that was a lie. &nbsp;I had planned a date with my girlfriend that day, and she assumed that the date day continued on through the evening. &nbsp;It was a fair assumption on her part, so I used my one free day on the first day of my resolution. &nbsp;Great start, eh?</p>\n<p>From that day on, I did pretty well. &nbsp;It wasn't until near the end that I finally broke the resolution. &nbsp;Twice I did it because I realized I needed a second loophole - a way to trade an extra two hours one day for a free day at a later date. &nbsp;On the last day I broke the resolution because I didn't have the foresight to plan a full day with my girlfriend before leaving for six weeks. &nbsp;So I broke my resolution 33% of the non-free days (total days minus the one free day I gave myself). &nbsp;If we count the extra long sessions as compensating for the first two days, that reduces to 11%. &nbsp;Not bad, considering that this summer I had opened a textbook up exactly twice before my resolution.</p>\n<p>When I originally posted my resolution, I said I would make a new one once I made it here to Chapman and got a feel for how much time I had to spare. &nbsp;Well, I never got around to that (Akrasia applies to plans to fight akrasia. &nbsp;It really is vicious). &nbsp;The fact that I didn't put that in my resolution may or may not be significant. &nbsp;I don't think it is. &nbsp;I did continue to do math, at first. &nbsp;My first two mornings here I spent a solid two hours apiece reviewing some analysis. &nbsp;There were a couple more days where I might have worked for about an hour. &nbsp;But for the most part, I haven't don't much studying in the past three weeks.</p>\n<p>This meager data suggests that the resolution worked for the specific duties I mentioned, but there could be counfounders. &nbsp;I may have been gung ho about it because I had just worked myself up. &nbsp;As time went on, this might have waned, and caused me to not renew my resolution as well as not do as much math. &nbsp;This doesn't seem like what happen to me, but the data doesn't rule it out.</p>\n<p>As far as the \"maximize my utility function\" clause, i have no objective data. &nbsp;From here on out, it's just my impressions. &nbsp;Take them for what they are worth.</p>\n<p>It did seem like I did a better job of doing what I really wanted to do (i.e. maximizing my utility function) even when taking into account that my resolution was already binding me to do better for two hours out of the day. &nbsp;I spent less of my free time playing Settlers of Catan online<sup>1</sup>&nbsp;and more hanging out with friends, getting paperwork filled out for graduate school, reading books, etc. &nbsp;I seemed to spend much less time, well, killing time. &nbsp;It wasn't perfect though, I still killed time doing mostly worthless things. &nbsp;I did much better with the specified things<sup>2</sup> in my resolution than with the utility maximization clause. &nbsp;It helped, but not as much.</p>\n<p>After the resolution expired, however, it was back to my old ways. &nbsp;Lots of flash games, little productivity. &nbsp;In fact, this is largely the reason that I haven't posted about <a href=\"http://wiki.lesswrong.com/wiki/Simple_math_of_everything\">The Simple Math of Everything</a> yet (it's coming...) and why it took so long to write this followup. &nbsp;If MichaelHoward hadn't <a href=\"/lw/ep/applied_picoeconomics/xwl\">asked</a> me how everything panned out, I may not have even written this.</p>\n<p>I have an idea<sup>3</sup> which I think explains why there was a difference at all and why the resolution worked at all. &nbsp;The resolution, being so formal, was in the forefront of my consciousness. &nbsp;Normally, it seems, my mind would query \"what should I do\" and faced with the daunting task of actually computing this, a null would pop out. &nbsp;Basically akrasia was preventing my efforts to stop akrasia. And so I would go on doing whatever worthless nonsense I was doing the day before. &nbsp;Playing cheesy computer games, watching sportscenter, etc. &nbsp;I <em>wanted</em>&nbsp;to do what I should do, but when I actually sat down at my desk and saw my computer in front of me, I wasn't even thinking about what I should do anymore. &nbsp;My resolution, in effect, kept this in my consciousness, which made it easier for me to stop akrasia before it starts. &nbsp;I would think, \"what should I do?\" and my mind would immediately go to my resolution.</p>\n<p>This explains why it worked at all, but why the difference between specific activities and the utility maximization clause? &nbsp;The difference seems to lie what had to be computed after I thought of my resolution. &nbsp;With specific activities, I could just go down a checklist. &nbsp;When the checklist was completed, I actually had to think about what I really wanted to do, which just left the door open for akrasia to rear its ugly head again. &nbsp;I suspect I can improve my ability to just figure out what I want to do and then do it, but for now having specific things outlined in a resolution is more effective.</p>\n<p>This also seems consistent with my previous experience. &nbsp;When a deadline is nearing, I simply think about whatever I should be doing more which makes it easier to conclude \"I'm going to do that\" and then actually go and do it. &nbsp;At work, I know I have specific duties, again seemingly with the same effect. &nbsp;Far away from a deadline, I'm simply not thinking about it. &nbsp;I'm not really sure why - it seems to be a subconscious process that I can't really control - but it happens. &nbsp;It might be related to stress - the resolution might be creating a mild amount of stress which, in turn, motivates me. &nbsp;It didn't <em>feel</em>&nbsp;like that - I really enjoyed the math I was doing, but it's worth looking into.</p>\n<p>One other thing I should mention about my resolution. &nbsp;I used a green dry erase marker to write LW in large capital letters on the resolution as it hung in front of my desk. &nbsp;This is close to the symbol for LessWrong in Google Chrome (the W is lowercase), so it grabbed my attention and reminded me of what I should be doing. &nbsp;Again, it seemed to keep me conscious of what I needed to be doing. &nbsp;I probably should have made my computer's wallpaper a collage of these symbols or something to help keep it in my mind. &nbsp;(This may be part of the psychology behind <a href=\"/lw/3b/never_leave_your_room/2ei\">chaos magic</a>). &nbsp;After rereading the thread about chaos magic I linked to, I immediately thought \"so I guess <em>that's</em>&nbsp;what <a href=\"http://wiki.lesswrong.com/wiki/Priming\">priming</a> feels like.\" &nbsp;Priming is probably a big part of the explanation I have above, but I don't think it is everything. &nbsp;Sometimes I was aware of what I should do seemingly without being primed - like when I'm away from home, or immediately when I wake up. &nbsp;This is testable though; perhaps I'll do a run without having the resolution in sight and check if the effects change.</p>\n<p>Now take all of this with a grain of salt. &nbsp;Your mind is not my mind, so nothing I said above need apply to you. &nbsp;For example, not all akrasia has to occur because you simply aren't concsiously thinking \"I should do X.\" &nbsp;Some people may be constantly thinking that and still do Y, but that doesn't seem to be the issue with me. &nbsp;I could also be very deluded about what actually happened. &nbsp;These are, after all, largely my subjective impressions. &nbsp;However, I intend to renew my resolution and monitor the results more closely. &nbsp;It did seem to work even if I'm completely wrong about why. &nbsp;Perhaps you have a better explanation? &nbsp;I'm listening.</p>\n<p><span style=\"font-weight: bold;\">Footnotes</span></p>\n<p><strong>1.</strong>&nbsp;&nbsp;It's my own solitare. &nbsp;The company that hosts the servers uses a different name for the game to avoid a lawsuit by the manufacturer. &nbsp;I'm not telling you what they call it because it really is a productivity buster. &nbsp;Google at your own risk.</p>\n<p><strong>2.</strong>&nbsp;&nbsp;Shortly after I posted my resolution, I ammeded it to add a few more specific duties which I'd rather not mention in such a public medium. &nbsp;Suffice to say that results were similar to that of the math duties.</p>\n<p><strong>3. &nbsp;</strong>I don't think this explanation and the reasons that the resolution should work that Yvain gave (and linked to) in his original post are mutually exclusive. &nbsp;Hyperbolic discounting might be the reason I was less conscious of what I should be doing, or the fact that I was less conscious might be the reason hyperbolic discountings works as a good model. &nbsp;Or neither, I'm not sure. &nbsp;But I see no reason why they are fundamentally incompatible.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "u4zwGST5Nc7eqfQv6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 12, "extendedScore": null, "score": 5.090350161156574e-07, "legacy": true, "legacyId": "1410", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NjzBrtvDS4jXi5Krp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-20T07:17:13.855Z", "modifiedAt": null, "url": null, "title": "Being saner about gender and rationality", "slug": "being-saner-about-gender-and-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:17.423Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MswXnxRX5xN4tNjzC/being-saner-about-gender-and-rationality", "pageUrlRelative": "/posts/MswXnxRX5xN4tNjzC/being-saner-about-gender-and-rationality", "linkUrl": "https://www.lesswrong.com/posts/MswXnxRX5xN4tNjzC/being-saner-about-gender-and-rationality", "postedAtFormatted": "Monday, July 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Being%20saner%20about%20gender%20and%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABeing%20saner%20about%20gender%20and%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMswXnxRX5xN4tNjzC%2Fbeing-saner-about-gender-and-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Being%20saner%20about%20gender%20and%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMswXnxRX5xN4tNjzC%2Fbeing-saner-about-gender-and-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMswXnxRX5xN4tNjzC%2Fbeing-saner-about-gender-and-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 667, "htmlBody": "<p>It seems that LessWrong has a <a href=\"/lw/134/sayeth_the_girl/\">nascent political problem brewing</a>. Firstly, let me re-iterate why politics is bad for our rationality:</p>\n<blockquote>\n<p>People go funny in the head when talking about politics.&nbsp; The evolutionary reasons for this are so obvious as to be worth belaboring:&nbsp; In the ancestral environment, politics was a matter of life and death.&nbsp; And sex, and wealth, and allies, and reputation...&nbsp; When, today, you get into an argument about whether \"we\" ought to raise the minimum wage, you're executing adaptations for an ancestral environment where being on the wrong side of the argument could get you killed.&nbsp; Being on the <em>right</em> side of the argument could let <em>you</em> kill your hated rival!</p>\n<p>Politics is an extension of war by other means.&nbsp; Arguments are soldiers.&nbsp; Once you know which side you're on, you must support all arguments of that side, and attack all arguments that appear to favor the enemy side; otherwise it's like stabbing your soldiers in the back - providing aid and comfort to the enemy.&nbsp; People who would be level-headed about evenhandedly weighing all sides of an issue in their professional life as scientists, can suddenly turn into slogan-chanting zombies when there's a <a href=\"/lw/gt/a_fable_of_science_and_politics\">Blue or Green</a> position on an issue.</p>\n</blockquote>\n<p>Politics is especially bad for the community if people begin to form political factions within the community. Specifically, if LessWrong starts to polarize along a \"feminist/masculinist\" fault-line, then every subsequent debate will become a proxy war for the crusade between the masculinist jerks and the femenazis.<a id=\"more\"></a></p>\n<p>Alicorn has contributed in several ways to the emerging politicization of LessWrong. She has started name-calling against the other side (\"Jerkitude\" \"disincentivize being piggish\"), started to attempt to form a political band of feminist allies (\"So can I get some help?&nbsp; <a href=\"/lw/12w/absolute_denial_for_atheists/xr1\">Some</a> <a href=\"/lw/12w/absolute_denial_for_atheists/xug\">lovely</a> <a href=\"/lw/fo/religion_mystery_and_warm_soft_fuzzies/cgk\">people</a> have thrown in their support,\"), implicitly asked these new allies to downvote anyone who disagrees with her position (\"There is still conspicuous <a href=\"/lw/12w/absolute_denial_for_atheists/xg9\">karmic support</a> for some comments that perpetuate the problems\"), and asks her faction to begin enforcing her ideas, specifically by criticising, ostracizing or downvoting anyone who engages in a perfectly standard use of langage and thought: modeling the generic human female as a mechanical system and using that model to make predictions about reality. She has billed this effort as a moral crusade (\"unethical\"). I am sure she isn't doing this on purpose: like all humans, her brain is hard-wired to see any argument as a moral crusade where she is objectively right, and to seek allies within the tribe to move against and oppress the enemy. [notice how I <em>objectified </em>her there, leaving behind the language of a unified self or person in favour of a collection of mechanical motivations and processes whose dynamics are partially determined by evolutionary pressures, and what a useful exercise this can be for making sense of reality]</p>\n<p>We should expend extreme effort to nip this problem in the bud. As part of this effort, I will delete my account and re-register under a different username. I would recommend that Alicorn do the same. I would also recommend that anyone who feels that they have played a particularly large part in the debate on either side do the same, for example PJeby. That way, when we talk to each other next in a comment thread, we won't be treating the interaction as a proxy war in the great feminist/masculinist crusade, because we will be anonymous again.</p>\n<p>I would also implore everyone to just not bring this issue up again. If someone uses language in a way that mildly annoys you (hint: they probably didn't do this on purpose), rather than precipitating a major community feud over it, just ignore it. The epistemic rationality of LessWrong is worth more than the gender ratio we have. A 95% male community that manages to overcome a whole host of problems in instrumental and epistemic rationality is worth more to the world than a 80% male community that is crippled by a blood-feud between a feminist faction and a masculinist faction.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"W9aNkPwtPhMrcfgj7": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MswXnxRX5xN4tNjzC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 72, "baseScore": 14, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "1413", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 98, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gsL6CLqjujPNSLL2o", "6hfGNLf4Hg5DXqJCF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-20T16:27:14.524Z", "modifiedAt": null, "url": null, "title": "Are you crazy?", "slug": "are-you-crazy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:05.636Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gworley", "createdAt": "2009-03-26T17:18:20.404Z", "isAdmin": false, "displayName": "G Gordon Worley III"}, "userId": "gjoi5eBQob27Lww62", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QK9ArXuRyE2vsPC43/are-you-crazy", "pageUrlRelative": "/posts/QK9ArXuRyE2vsPC43/are-you-crazy", "linkUrl": "https://www.lesswrong.com/posts/QK9ArXuRyE2vsPC43/are-you-crazy", "postedAtFormatted": "Monday, July 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20you%20crazy%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20you%20crazy%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQK9ArXuRyE2vsPC43%2Fare-you-crazy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20you%20crazy%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQK9ArXuRyE2vsPC43%2Fare-you-crazy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQK9ArXuRyE2vsPC43%2Fare-you-crazy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 540, "htmlBody": "<p><span style=\"font-weight: bold;\">Followup To</span>:&nbsp; <a href=\"/lw/132/are_you_anosognosic/\">Are You Anosognosic?</a>, <a href=\"/lw/12s/the_strangest_thing_an_ai_could_tell_you\">The Strangest Thing An AI Could Tell You</a></p>\n<p>Over this past weekend I listened to an episode of <a href=\"http://www.thisamericanlife.org/\">This American Life</a> titled <a href=\"http://www.thisamericanlife.org/Radio_Episode.aspx?sched=1306\">Pro Se</a>.&nbsp; Although the episode is nominally about people defending themselves in court, the first act of the episode was about a man who pretended to act insane in order to get out of a prison sentence for an assault charge.&nbsp; There doesn't appear to be a transcript, so I'll summarize here first.</p>\n<blockquote>\n<p>A man, we'll call him John, was arrested in the late 1990s for assaulting a homeless man.&nbsp; Given that there was plenty of evidence to prove him guilty, he was looking for a way to avoid the likely jail sentence of five to seven years.&nbsp; The other prisoners he was being held with suggested that he plead insanity:&nbsp; he'd be put up at a hospital for several months with hot food and TV and released once they considered him \"rehabilitated\".&nbsp; So he took bits and pieces about how insane people are supposed to act from movies he had seen and used them to form a case for his own insanity.&nbsp; The court believed him, but rather than sending him to a cushy hospital, they sent him to a maximum security asylum for the criminally insane.</p>\n<p>Within a day of arriving, John realized the mistake he had made and sought to find a way out.&nbsp; He tries a variety of techniques:&nbsp; engaging in therapy, not engaging in therapy, dressing like a sane person, acting like a sane person, acting like an incurably insane person, but none of it works.&nbsp; Over a decade later he is still being held.</p>\n<p>As the story unravels, we learn that although John makes a convincing case that he faked his way in and is being held unjustly, the psychiatrists at the asylum know that he faked his way in and continue to hold him anyway, though John is not aware of this.&nbsp; The reason:&nbsp; through his long years of documented behavior John has made it clear to the psychiatrists that he is a psychopath/sociopath and is not safe to return to society without therapy.&nbsp; John is aware that this is his diagnosis, but continues to believe himself sane.</p>\n</blockquote>\n<p>Similar to trying to determine if you are anosognosic, how do you determine if you are insane?&nbsp; Some kinds of insanity can be self diagnosed, but in John's case he has lots of evidence (he has access to read all of his own medical records) that he is insane, yet continues to believe himself not to be.&nbsp; To me this seems a level trickier than anosognosis, since there's no physical tests you can make, but perhaps it's only a level of difference significant to people but not to an AI.</p>\n<p><span style=\"font-weight: bold;\">Edited to add a footnote: &nbsp;</span>By \"sane\" I simply mean normative human reasoning: &nbsp;the way you expect, all else being equal, a human to think about things. &nbsp;While the discussion in the comments about how to define sanity might be of some interest, it really gets away from the point of the post unless you want to argue that \"sanity\" is creating a question here that is best solved by dissolving the question (as at least one commenter does).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QK9ArXuRyE2vsPC43", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 2, "extendedScore": null, "score": 5.091617514444635e-07, "legacy": true, "legacyId": "1414", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xWfAvTmCBtoqhHR7Q", "t2NN6JwMFaqANuLqH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-20T16:31:55.512Z", "modifiedAt": null, "url": null, "title": "Counterfactual Mugging v. Subjective Probability", "slug": "counterfactual-mugging-v-subjective-probability", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:14.649Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yzKSQ6P6XcrbALawX/counterfactual-mugging-v-subjective-probability", "pageUrlRelative": "/posts/yzKSQ6P6XcrbALawX/counterfactual-mugging-v-subjective-probability", "linkUrl": "https://www.lesswrong.com/posts/yzKSQ6P6XcrbALawX/counterfactual-mugging-v-subjective-probability", "postedAtFormatted": "Monday, July 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Counterfactual%20Mugging%20v.%20Subjective%20Probability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACounterfactual%20Mugging%20v.%20Subjective%20Probability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyzKSQ6P6XcrbALawX%2Fcounterfactual-mugging-v-subjective-probability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Counterfactual%20Mugging%20v.%20Subjective%20Probability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyzKSQ6P6XcrbALawX%2Fcounterfactual-mugging-v-subjective-probability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyzKSQ6P6XcrbALawX%2Fcounterfactual-mugging-v-subjective-probability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 353, "htmlBody": "<p>This <a href=\"/lw/c0/the_ideas_youre_not_ready_to_post/8jj\">has been in my drafts folder for ages</a>, but in light of Eliezer's post yesterday, I thought I'd see if I could get some comment on it:</p>\n<p>&nbsp;</p>\n<p>A couple weeks ago, Vladimir Nesov stirred up the biggest hornet's nest I've ever seen on LW by introducing us to the <a href=\"/lw/3l/counterfactual_mugging/\">Counterfactual Mugging</a> scenario.</p>\n<p>If you didn't read it the first time, please do -- I don't plan to attempt to summarize. &nbsp;Further, if you don't think you would give Omega the $100 in that situation, I'm afraid this&nbsp;article&nbsp;will mean next to nothing to you.</p>\n<p>So, those still reading, you would give Omega the $100. &nbsp;You would do so because if someone told you about the problem now, you could do the expected utility calculation 0.5*U(-$100)+0.5*U(+$10000)&gt;0. &nbsp;Ah, but where did the 0.5s come from in your calculation? &nbsp;Well, Omega told you he flipped a fair coin. &nbsp;Until he did, there existed a 0.5 probability of either outcome. &nbsp;Thus, for you, hearing about the problem, there is a 0.5 probability of your encountering the problem as stated, and a 0.5 probability of your encountering the&nbsp;corresponding&nbsp;situation, in which Omega either hands you $10000 or doesn't, based on his prediction. &nbsp;This is all very fine and rational. &nbsp;</p>\n<p>So, new problem.&nbsp; Let's leave money out of it, and assume Omega hands you 1000 utilons in one case, and asks for them in the other -- exactly equal utility.&nbsp; What if there is an urn, and it contains either a red or a blue marble, and Omega looks, maybe gives you the utility if the marble is red, and asks for it if the marble is blue?&nbsp; What if you have devoted considerable time to determining whether the marble is red or blue, and your subjective probability has fluctuated over the course of you life? What if, unbeknownst to you, a rationalist community has been tracking evidence of the marble's color (including your own probability estimates), and running a prediction market, and Omega now shows you a plot of the prices over the past few years?</p>\n<p>In short, what information do you use to calculate the probability you plug into the EU calculation?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YpHkTW27iMFR2Dkae": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yzKSQ6P6XcrbALawX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 4, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "262", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mg6jDEuQEjBGtibX7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-20T22:45:31.971Z", "modifiedAt": null, "url": null, "title": "Creating The Simple Math of Everything", "slug": "creating-the-simple-math-of-everything", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:39.086Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Matt_Simpson", "createdAt": "2009-03-05T21:06:45.432Z", "isAdmin": false, "displayName": "Matt_Simpson"}, "userId": "v4krJe8Qa4jnhPTmd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rXbgabdnmxd3jdY7w/creating-the-simple-math-of-everything", "pageUrlRelative": "/posts/rXbgabdnmxd3jdY7w/creating-the-simple-math-of-everything", "linkUrl": "https://www.lesswrong.com/posts/rXbgabdnmxd3jdY7w/creating-the-simple-math-of-everything", "postedAtFormatted": "Monday, July 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Creating%20The%20Simple%20Math%20of%20Everything&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACreating%20The%20Simple%20Math%20of%20Everything%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrXbgabdnmxd3jdY7w%2Fcreating-the-simple-math-of-everything%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Creating%20The%20Simple%20Math%20of%20Everything%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrXbgabdnmxd3jdY7w%2Fcreating-the-simple-math-of-everything", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrXbgabdnmxd3jdY7w%2Fcreating-the-simple-math-of-everything", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 618, "htmlBody": "<p>Eliezer once proposed an Idea for a book, <a href=\"/lw/l7/the_simple_math_of_everything/\">The Simple Math of Everything</a>. &nbsp;The basic idea is to compile articles on the basic mathematics of a wide variety of fields, but nothing too complicated.</p>\n<blockquote>\n<p>Not Jacobean matrices for frequency-dependent gene selection; just Haldane's calculation of time to fixation. &nbsp;Not quantum physics; just the wave equation for sound in air. &nbsp;Not the maximum entropy solution using Lagrange Multipliers; just <a href=\"http://yudkowsky.net/rational/bayes\">Bayes's Rule</a>.</p>\n</blockquote>\n<p>Now, writing a book is a pretty daunting task. &nbsp;Luckily <a href=\"/lw/zj/open_thread_june_2009/rxe\">brian_jaress</a> had the idea of creating an index of links to already available online articles. &nbsp;XFrequentist pointed out that something like this has been done before over at <a href=\"http://scienceblogs.com/evolvingthoughts/2008/01/basic_concepts_in_science_a_li.php\">Evolving Thoughts</a>. &nbsp;This initially discourage me, but it eventually helped me refine what I thought the index should be. &nbsp;A key characteristic of Eliezer's idea is that it should be worthwhile for someone who doesn't know the material to read the entire index. &nbsp;Many of the links at evolving thoughts point to rather narrow topics that might not be very interesting to a generalist. &nbsp;Also there is just plain a ton of stuff to read over there - at least 100 articles.</p>\n<p>So we should come up with some basic criteria for the articles. &nbsp;Here is what I suggest (let me know what you think):<a id=\"more\"></a></p>\n<ol>\n<li>The index must be short: say 10 - 20 links. &nbsp;Or rather, the core of the index must be short. &nbsp;We can have longer lists of narrower and more in depth articles for people who want to get into more detail about, say, quantum physics or economic growth. &nbsp;But these should be separate from the main index.</li>\n<li>Each article must meet minimum requirements in terms of how interesting the topic is and how important it is. Remember, this is an index for the reader to gain a general understanding of many fields</li>\n<li>The article must include some math - at minimum, some basic algebra. &nbsp;Calculus is good as long as it significantly adds to the article. &nbsp;In fact, this should probably be the basic rule for all additions of complex math. &nbsp;Modularization also helps - i.e., if the relatively complicated math is in a clearly visible section that can be skipped without losing anything significant from the rest of the article, it should be ok.</li>\n</ol>\n<div>This list of criteria isn't meant to be exhaustive either. &nbsp;If there is anything you guys think should be added, by all means, suggest it and we can debate it. &nbsp;Also, an article shouldn't have to perfectly fit our criteria in order to qualify for the index, as long as it's at least close and is an improvement over what we have in its place.</div>\n<div>I should also mention that there is no problem with linking to Lesswrong. &nbsp;So if you see major problem with the article we have on, say, the ideal gas law, then write a better version. &nbsp;If we gradually replace offsite links to LW links, we could even publish an ebook or something.</div>\n<div>We should also hash out exactly which topics deserve to be represented, and furthermore the number of topics. &nbsp;I'll suggest some from the fields I'm most familiar with (you should do the same):</div>\n<div>\n<ul>\n<li>Baye's Rule</li>\n<li>Supply and Demand (probably with effects of price controls, incidence of tax, etc., and limitations)</li>\n<li>Economic Growth (Solow Growth model with limitations/implications)</li>\n</ul>\n</div>\n<p>If you do happen to come across something worth considering for the index, by all means, update the <a href=\"http://wiki.lesswrong.com/wiki/Simple_math_of_everything\">wiki</a>.&nbsp;&nbsp;(a good place to start looking would be at <a href=\"http://scienceblogs.com/evolvingthoughts/2008/06/basic_concepts_in_science_a_li.php\">Evolving Thoughts</a>...)&nbsp;&nbsp;Perhaps we should add a section to the wiki for articles that we think are worth consideration so we can differentiate them from the main list. &nbsp;What thoughts do you have (about all of this)?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rXbgabdnmxd3jdY7w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 16, "extendedScore": null, "score": 5.092217960350633e-07, "legacy": true, "legacyId": "1419", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HnPEpu5eQWkbyAJCT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-20T23:31:27.817Z", "modifiedAt": null, "url": null, "title": "Joint Distributions and the Slow Spread of Good Ideas", "slug": "joint-distributions-and-the-slow-spread-of-good-ideas", "viewCount": null, "lastCommentedAt": "2011-08-21T23:57:12.579Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_J_Balan", "createdAt": "2009-06-21T18:40:19.064Z", "isAdmin": false, "displayName": "David_J_Balan"}, "userId": "LkRB9E4GWd7hRa7zt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vX3EjNHR387p3GzCN/joint-distributions-and-the-slow-spread-of-good-ideas", "pageUrlRelative": "/posts/vX3EjNHR387p3GzCN/joint-distributions-and-the-slow-spread-of-good-ideas", "linkUrl": "https://www.lesswrong.com/posts/vX3EjNHR387p3GzCN/joint-distributions-and-the-slow-spread-of-good-ideas", "postedAtFormatted": "Monday, July 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Joint%20Distributions%20and%20the%20Slow%20Spread%20of%20Good%20Ideas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJoint%20Distributions%20and%20the%20Slow%20Spread%20of%20Good%20Ideas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvX3EjNHR387p3GzCN%2Fjoint-distributions-and-the-slow-spread-of-good-ideas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Joint%20Distributions%20and%20the%20Slow%20Spread%20of%20Good%20Ideas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvX3EjNHR387p3GzCN%2Fjoint-distributions-and-the-slow-spread-of-good-ideas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvX3EjNHR387p3GzCN%2Fjoint-distributions-and-the-slow-spread-of-good-ideas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 740, "htmlBody": "<p>A few years ago a well-known economist named David Romer published a <a href=\"http://elsa.berkeley.edu/~dromer/\">paper</a> in a top economics journal* arguing that professional football teams don't \"go for it\" nearly often enough on fourth down. The question, of course, is how this can persist in equilibrium. If Romer is correct, wouldn't teams have a strong incentive to change their strategies? Of course it's possible that he is correct, but that no one ever knew it before the paper was published. But then would the fact that the recommendation has not been widely adopted** constitute strong evidence that he is not correct? The paper points out two possible reasons why not. First, the objective function of the decision-makers may not be to maximize the probability of winning the game. Second and more relevant for our purposes, there may be some biases at work. The key point is this quote from the article (page 362):</p>\n<blockquote>\n<p>\"Many skills are more important to running a football team than a command of mathematical and statistical tools. And it would hardly be obvious to someone without knowledge of those tools that they could have any significant value in football.\"</p>\n</blockquote>\n<p><a id=\"more\"></a>Romer's point is that what's relevant is the <em>joint</em> distribution of attributes in the pool of potential football coaches (or other decision-makers). Even in something like professional football where there is a very strong incentive to get better results, it may take a long time for coaches who are willing/able to adopt a good new idea to out-compete and displace those who continue to use the bad old idea if there are very few potential coaches who <em>both</em> have the conventional talents <em>and</em> understand the new idea.<br /><br />I think Romer is right about this, and his point is the main take-away point of this post. But I don't think the main \"joint distribution\" problem is a paucity of would-be coaches who understand both conventional football stuff and math: math talent can be hired to work under a head coach who doesn't understand it, just like medical talent is. Rather, it needs to be the case that the joint distribution is unfavorable <em>and</em> that this can't be gotten around by just adding math talent as necessary. Maybe the problem is a paucity of potential coaches who have both conventional coaching skills and also the attitude that nerds are to be listened to rather than beaten up. This may explain why baseball seems to have been much more accepting of statistical analysis than has football.<br /><br />The point is this: an unfavorable joint distribution of attributes in the pool of potential decision-makers can greatly retard the adoption of good ideas, even when incentives to adopt are very strong, which means that the fact of non-adoption is not decisive evidence that an idea is bad. But for this to be true, there must be some reason why the people with the principal attribute cannot simply seek and incorporate the advice of the people with the secondary attribute. This will often be because the very acculturation process that produced the people who have the principal attribute creates some barrier to them making use of the secondary one.<br /><br />\"Do Firms Maximize? Evidence from Professional Football.\" by David Romer, Journal of Political Economy (2006). It is a sad commentary on the state of the economics profession that some journal editor seems to have made him change the title from the much cooler: \"It's Fourth Down and What Does the Bellman Equation Say? A Dynamic-Programming Analysis of Football Strategy.\"<br />**I think this is a true statement, but I could be wrong. Please correct me if I am.<br />***Another possibility is that the problem is not the coaches, but the fans. A coach who sticks with the conventional strategy is protected by a \"nobody ever got fired for buying IBM\" attitude, whereas a coach who does something unconventional (but probabilistically correct) runs the risk of getting fired if it doesn't work out. This just pushes the irrationality from the coaches to the fans, but that might be more plausible: they have access to less resources to figure out what is and is not a good idea, and have much less of an incentive to try to get it right. Then the problem would be a paucity of fans who have the attribute \"really care about football\" and \"understand and are willing to support good ideas, even from nerds.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vX3EjNHR387p3GzCN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 16, "extendedScore": null, "score": 5.092292681425274e-07, "legacy": true, "legacyId": "1412", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-21T00:57:19.823Z", "modifiedAt": null, "url": null, "title": "Chomsky, Sports Talk Radio, Media Bias, and Me", "slug": "chomsky-sports-talk-radio-media-bias-and-me", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:12.364Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_J_Balan", "createdAt": "2009-06-21T18:40:19.064Z", "isAdmin": false, "displayName": "David_J_Balan"}, "userId": "LkRB9E4GWd7hRa7zt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/95ieojrogPEiAygeZ/chomsky-sports-talk-radio-media-bias-and-me", "pageUrlRelative": "/posts/95ieojrogPEiAygeZ/chomsky-sports-talk-radio-media-bias-and-me", "linkUrl": "https://www.lesswrong.com/posts/95ieojrogPEiAygeZ/chomsky-sports-talk-radio-media-bias-and-me", "postedAtFormatted": "Tuesday, July 21st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Chomsky%2C%20Sports%20Talk%20Radio%2C%20Media%20Bias%2C%20and%20Me&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChomsky%2C%20Sports%20Talk%20Radio%2C%20Media%20Bias%2C%20and%20Me%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F95ieojrogPEiAygeZ%2Fchomsky-sports-talk-radio-media-bias-and-me%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Chomsky%2C%20Sports%20Talk%20Radio%2C%20Media%20Bias%2C%20and%20Me%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F95ieojrogPEiAygeZ%2Fchomsky-sports-talk-radio-media-bias-and-me", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F95ieojrogPEiAygeZ%2Fchomsky-sports-talk-radio-media-bias-and-me", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 552, "htmlBody": "<p>Just about everyone knows that one of Noam Chomsky's big things is that he thinks the media are badly distorted away from the truth and toward the interests of the wealthy and powerful. Once a long time ago I read or heard either this quote from this <a href=\"http://www.chomsky.info/interviews/1992----02.htm\">interview</a> or something like it:</p>\n<blockquote>\n<p>\"Take, say, sports -- that's another crucial example of the indoctrination system, in my view. For one thing because it -- you know, it offers people something to pay attention to that's of no importance. [audience laughs] That keeps them from worrying about -- [applause] keeps them from worrying about things that matter to their lives that they might have some idea of doing something about. And in fact it's striking to see the intelligence that's used by ordinary people in [discussions of] sports [as opposed to political and social issues]. I mean, you listen to radio stations where people call in -- they have the most exotic information [more laughter] and understanding about all kind of arcane issues. And the press undoubtedly does a lot with this.\"</p>\n</blockquote>\n<p>Taking this quote along with the rest of the interview, the idea seems to be that that the default condition of most people is to have decent critical faculties unless someone takes the trouble to actively screw them up. So in contrast to their badly distorted ideas about politics, people tend to have sensible ideas about sports, since the powerful have no particular motive to distort those ideas (though they do have a motive to get people to think about sports instead of things that are important). <br /><br />Leaving completely aside the merits of Chomsky's critique of the media or any of his other views, I have logged a pretty decent number of hours listening to sports talk radio and folks, I'm here to tell you that the quality of the discourse is generally mighty low. <a id=\"more\"></a>It might not be as low as on political talk radio (people aren't as hate-filled), but it's low. I sent an email to Chomsky pointing this out (which I cannot locate as it was many years ago), and to his credit he wrote me back. I don't think I am revealing any confidences by saying that, to the best of my recollection, he admitted that it had been many years since he had really paid any attention to sports or sports talk or anything like that. The point is not (or at least not mostly) to ding Chomsky for being a little loose with the facts behind his just-so story. The point is that whatever you think about the media and the powerful and all that (see <a href=\"http://www.overcomingbias.com/2007/01/effects_of_ideo.html\">here</a> for some of my views), it is <em>not</em> true that reasoned discussion is the default condition of humanity and prevails unless someone comes along and screws it up. I think it's closer to the truth to say that natural irrationality is the raw material that manipulation by the powerful has to act upon.<br /><br />And that is the story of one of my very few brushes with fame (I also once saw Mike Ditka on a plane, once saw Al Sharpton on a plane, once shook hands with Paul Krugman and knew a very lucky someone who had actually personally laid eyes on Gwyneth Paltrow).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "95ieojrogPEiAygeZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 17, "extendedScore": null, "score": 5.092429361355367e-07, "legacy": true, "legacyId": "1421", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-21T01:00:18.458Z", "modifiedAt": null, "url": null, "title": "Outside Analysis and Blind Spots", "slug": "outside-analysis-and-blind-spots", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:38.880Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oZb6AqZKBG4gmLdAZ/outside-analysis-and-blind-spots", "pageUrlRelative": "/posts/oZb6AqZKBG4gmLdAZ/outside-analysis-and-blind-spots", "linkUrl": "https://www.lesswrong.com/posts/oZb6AqZKBG4gmLdAZ/outside-analysis-and-blind-spots", "postedAtFormatted": "Tuesday, July 21st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Outside%20Analysis%20and%20Blind%20Spots&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOutside%20Analysis%20and%20Blind%20Spots%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoZb6AqZKBG4gmLdAZ%2Foutside-analysis-and-blind-spots%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Outside%20Analysis%20and%20Blind%20Spots%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoZb6AqZKBG4gmLdAZ%2Foutside-analysis-and-blind-spots", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoZb6AqZKBG4gmLdAZ%2Foutside-analysis-and-blind-spots", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1359, "htmlBody": "<p>(I originally tried to make this a comment, but it kept on growing.)</p>\n<p>I was looking through the Google results for \"Less Wrong\" when I found the blog of a rather intelligent Leon Kass acolyte, who's written a critique of our community.&nbsp; While it's a bit of a caricature, it's not entirely off the mark.&nbsp; For example:</p>\n<blockquote>\n<p>Trying to think more like a mathematician, whose empiricism resides in the realm of pure thought, does not predispose these 'rationalists' to collect evidence from the real world. Neither does the downplaying of personal experiences. Many are computer science majors, used to being in the comfortable position of being capable of testing their hypotheses without needing to leave their office. It is, then, an easy temptation for them to come up with a nice-sounding theory which appears to explain the facts, and then consider the question solved. Reason must reign supreme, must it not?</p>\n</blockquote>\n<p>How seriously do you take this critique?&nbsp; Do you wonder why I'm bothering with this straw-man criticism of Less Wrong?</p>\n<p><a id=\"more\"></a>Actually, I've deceived you; there's no such Leon Kass devotee.&nbsp; The quote above is a very minor adaptation of <a href=\"/lw/8z/rationalists_should_beware_rationalism/\" target=\"_self\">this Kaj Sotala post</a>, which I've changed from the first person plural to the third person plural.&nbsp; Read it if you like, and then reevaluate the critique.&nbsp; (Yes, I know it's less coherent out of its actual context.)&nbsp; Does it seem to be less of a caricature now that you've read a version in which you identify with the writer, rather than one in which the writer is analyzing and criticizing you from outside?</p>\n<p>Now I hope that this little trick (which people are starting to expect around here) caught your attention.&nbsp; We really do seem to react differently to an outside analysis of a person or group in <em>very different</em> ways, depending on whether we've been primed to identify ourselves more with the author or with the object of analysis.&nbsp; One might say that we strongly object to being modeled as a more or less predictable agent in someone else's scheme: that we instinctively reject any out-group analysis of our own personality and cognition.&nbsp; (Compare people's reluctance to trust the <a href=\"http://wiki.lesswrong.com/wiki/Outside_view\" target=\"_self\">Outside View</a> even when they know it's more reliable.)</p>\n<p>In fact, in some ways this is reminiscent of <a href=\"/lw/20/the_apologist_and_the_revolutionary/\" target=\"_self\">anosognosia</a>: take for instance the <a href=\"http://bps-research-digest.blogspot.com/2009/06/were-unable-to-read-our-own-body.html\" target=\"_blank\">recent study on body language</a>, which discovered that</p>\n<ol>\n<li>We can predict a stranger's extraversion scores on the Implicit Association Test quite well by watching them act out a one-minute commercial.</li>\n<li>We can improve our predictions further if we're first told about a few nonverbal tics to watch for.</li>\n<li>We're bad at predicting <em>our own</em> extraversion scores on the IAT, even if we're given the video of ourselves acting out the commercial and told about the nonverbal tics.</li>\n</ol>\n<p>This screams out \"blind spot\" to me, and one with a nice evolutionary reason to boot: a blind spot toward one's own patterns of action would make it possible to sincerely promise something we'll probably fail to do, which is a pretty nice trick to have up one's sleeve in a social environment.&nbsp; (Anyhow, the cute ev-psych story can be discarded without incident to the rest of the evidence.)</p>\n<p>If it's true that we instinctively react to an out-group analysis of our actions, what might we expect would happen when we're faced with one?&nbsp; The most likely reaction would be a knee-jerk dismissal of the model, with <a href=\"http://wiki.lesswrong.com/wiki/Rationalization\" target=\"_self\">justification to be constructed after the fact</a>; or perhaps we'd take offense.&nbsp; If that were so, then we might expect the following kinds of results:</p>\n<ul>\n<li>We often analyze our friends when they're absent, in ways that would offend them if they were present; and yet we don't feel we're being dishonest or unfair.</li>\n<li>Political groups have explanations for why other people believe the way they do, and no group ever accepts a different group's proposed explanation for them.</li>\n<li>An analysis of behavior that puts us in the group being analyzed, and the speaker outside it, never sits right with us.&nbsp; We can, however, consider it more openly if the speaker argues that this is true for a class including themselves; this, after all, is an Inside View.</li>\n</ul>\n<p>And if a fourth obvious case doesn't occur to you, you must have been somewhere else for the past week.</p>\n<p>Fortunately, it seems to me that a fix is available: if the analysis is set up so that the (intended) readers identify more with the analyst than with the objects of analysis, they seem to avoid that blind spot.&nbsp; (They don't have to share all the characteristics of the analyst to do that; I would bet that female readers didn't have a moment of difficulty identifying with Eliezer rather than the woman on the panel in <a href=\"/lw/i6/professing_and_cheering/\" target=\"_self\">this anecdote</a>, where the implied divide was \"rationalists versus irrationalists\" rather than \"men versus women\".)&nbsp; Keeping your readers with you is usually not that hard to do in practice; it's what writers call \"knowing your audience\", and if you imagine delivering your statement to the proper audience, you should (subconsciously, even) do better at avoiding that split between yourself and them.</p>\n<p>The key thing, though, is that readers don't seem to do this on their own, not even rationalists.&nbsp; <em>This is not a failing of one part of this community or another;</em> this seems to be part of the current human condition, and it behooves a good communicator to avoid implicit \"Us/Them\" splits that leave a good part of the intended readership in \"Them\".&nbsp; In particular, writing with more care on gender is worth the cost in extra words and thought: gender-specific pronouns really do seem to cause distraction and dis-identification with the author, and I'd predict that the difference between</p>\n<blockquote>\n<p>most people here don't value social status enough and (especially the men) don't value having sex with extremely attractive women that money and status would get them</p>\n</blockquote>\n<p>and, say,</p>\n<blockquote>\n<p>money and status would make a man more attractive to many women; men who really value a better romantic or sexual life should thus put more priority on money and status</p>\n</blockquote>\n<p>is pretty significant to a female reader (please correct me if I'm wrong).&nbsp; In the first, it's generally just the male readers who can easily take it as an analysis from their perspective, while female readers identify themselves with the thing being (very crudely) modeled from outside.&nbsp; In the second, female and male readers can identify themselves with someone making an actual choice or observation, or equally well envision themselves looking at the whole situation from outside.</p>\n<p>Therefore, I suggest that when your post or comment touches on a subject that divides the Less Wrong community into identifiable groups (transhumanism or PCT or libertarianism, not just gender), it's good practice to imagine reading your contribution out loud to members of the various subgroups, and edit if you feel it might go over badly.&nbsp; This goes double if you're analyzing a general tendency in a group you don't belong to.&nbsp; (<strong>ETA:</strong> Sometimes it might be necessary to go ahead and damn the torpedoes, but I think that on some subjects we're being far too lax in this respect.)&nbsp; On the other hand, if someone analyzes you or your group from outside (and, needless to say, gets it wrong), I'd suggest you show a little extra patience with them; neither of you need be exceptionally irrational/sensitive/insensitive for this kind of impasse to arise.</p>\n<p><strong>P.S.</strong> I've hung back from the Less Wrong Gender Wars for a while, in part because I wanted to observe it a bit <a href=\"http://en.wikipedia.org/wiki/Attitude_polarization\" target=\"_blank\">before committing myself to a position</a>, and in part because everything I had to say seemed wrong somehow.&nbsp; I finally started writing out a comment listing several hypotheses for how we could have a situation where one good rationalist feels that a way of speaking is clearly unethical (while not necessarily incorrect in substance), and another good rationalist appears to be, not just disagreeing, but actively mystified about what could be wrong with it.&nbsp; Then I realized that one of my hypotheses was much better supported than the others.</p>\n<p><strong>EDIT:</strong> At the request of several, I've stopped diluting the term \"Outside View\" and called this particular thing \"out-group analysis.\"</p>\n<ol> </ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"gHCNhqxuJq2bZ2akb": 2, "x6evH6MyPK3nxsoff": 2, "FkzScn5byCs9PxGsA": 2, "izp6eeJJEg9v5zcur": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oZb6AqZKBG4gmLdAZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 72, "baseScore": 81, "extendedScore": null, "score": 0.00015419375418611802, "legacy": true, "legacyId": "1420", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 71, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["b88EtWvjyRQc89XzT", "ZiQqsgGX6a42Sfpii", "RmCjazjupRGcHSm5N"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-21T04:04:12.836Z", "modifiedAt": null, "url": null, "title": "Shut Up And Guess", "slug": "shut-up-and-guess", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:40.232Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/znBJwbuT3f5eWgM4E/shut-up-and-guess", "pageUrlRelative": "/posts/znBJwbuT3f5eWgM4E/shut-up-and-guess", "linkUrl": "https://www.lesswrong.com/posts/znBJwbuT3f5eWgM4E/shut-up-and-guess", "postedAtFormatted": "Tuesday, July 21st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Shut%20Up%20And%20Guess&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShut%20Up%20And%20Guess%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FznBJwbuT3f5eWgM4E%2Fshut-up-and-guess%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Shut%20Up%20And%20Guess%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FznBJwbuT3f5eWgM4E%2Fshut-up-and-guess", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FznBJwbuT3f5eWgM4E%2Fshut-up-and-guess", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1611, "htmlBody": "<p><strong>Related to: </strong><a href=\"/lw/9p/extreme_rationality_its_not_that_great/\">Extreme Rationality: It's Not That Great</a></p>\n<p>A while back, I <a href=\"/lw/9p/extreme_rationality_its_not_that_great/\">said</a> provocatively that the rarefied sorts of rationality we study at Less Wrong hadn't helped me in my everyday life and probably hadn't helped you either. I got a lot of controversy but not a whole lot of good clear examples of getting some use out of rationality.<br /><br />Today I can share one such example.<br /><br />Consider a set of final examinations based around tests with the following characteristics:</p>\n<p>* Each test has one hundred fifty true-or-false questions.<br />* The test is taken on a scan-tron which allows answers of \"true\", \"false\", and \"don't know\".<br />* Students get one point for each correct answer, zero points for each \"don't know\", and minus one half point for each incorrect answer.<br />* A score of &gt;50% is \"pass\", &gt;60% is \"honors\", &gt;70% is \"high honors\".<br />* The questions are correspondingly difficult, so that even a very intelligent student is not expected to get much above 70. All students are expected to encounter at least a few dozen questions which they can answer only with very low confidence, or which they can't answer at all.<br /><br />At what confidence level do you guess? At what confidence level do you answer \"don't know\"?</p>\n<p><a id=\"more\"></a></p>\n<p>I took several of these tests last month, and the first thing I did was some quick mental calculations. If I have zero knowledge of a question, my expected gain from answering is 50% probability of earning one point and 50% probability of losing one half point. Therefore, my expected gain from answering a question is .5(1)-.5(.5)= +.25 points. Compare this to an expected gain of zero from not answering the question at all. Therefore, I ought to guess on every question, even if I have zero knowledge. If I have some inkling, well, that's even better.<br /><br />You look disappointed. This isn't a very exciting application of arcane Less Wrong knowledge. Anyone with basic math skills should be able to calculate that out, right?<br /><br />I attend a pretty good university, and I'm in a postgraduate class where most of us have at least a bachelor's degree in a hard science, and a few have master's degrees. And yet, talking to my classmates in the cafeteria after the first test was finished, I started to realize I was the only person in the class who hadn't answered \"don't know\" to any questions.<br /><br />I have several friends in the class who had helped me with difficult problems earlier in the year, so I figured the least I could do for them was to point out that they could get several free points on the exam by guessing instead of putting \"don't know\". I got a chance to talk to a few people between tests, and I explained the argument to them using exactly the calculation I gave above. My memory's not perfect, but I think I tried it with about five friends.<br /><br />Not one of them was convinced. I see that while I've been off studying and such, you've been talking about <a href=\"/lw/12s/the_strangest_thing_an_ai_could_tell_you/\">macros of absolute denial </a>and such, and while I'm not sure I like the term, this almost felt like coming up against a macro of absolute denial.<br /><br />I had people tell me there must be some flaw in my math. I had people tell me that math doesn't always map to the real world. I had people tell me that no, I didn't understand, they <em>really</em> didn't have any <em>idea </em>of the answer to that one question. I had people tell me they were so baffled by the test that they expected to consistently get significantly more than fifty percent of the (true or false!) questions they guessed on wrong. I had people tell me that although yes, in on the average they would do better, there was always the possibility that by chance alone they would get all thirty of the questions they guessed on wrong and end up at a huge disadvantage<sup>1</sup>.<br /><br />I didn't change a single person's mind. The next test, my friends answered just as many \"don't know\"s as the last one.<br /><br />This floored me, because it's not one of those problems about politics or religion where people have little incentive to act rationally. These tests were the main component of the yearly grade in a very high-pressure course. My friend who put down thirty \"don't know\"s could easily have increased his grade in the class 5% by listening to me, maybe even moved up a whole letter grade. Nope. Didn't happen. So here's my theory.<br /><br />The basic mistake seems to be <a href=\"http://en.wikipedia.org/wiki/Loss_aversion\">loss aversion</a><sup>2</sup>, the tendency to regret losses more than one values gains. This could be compounded by students' tendency to discuss answers after the test: I remember each time I heard that one of my guesses had been wrong and I'd lost points, it was a deep psychic blow. No doubt my classmates tended to remember the guesses they'd gotten wrong more than the ones they'd gotten right, leading to the otherwise inexplicable statement that they expect to get more than half of their guesses wrong. But this mistake should disappear once the correct math is explained. Why doesn't it?<br /><br />In <a href=\"/lw/10f/the_terrible_horrible_no_good_very_bad_truth/\">The Terrible...Truth About Morality</a>, Roko gives a good example of the way our emotional and rational minds interact. A person starts with an emotion - in that case, a feeling of disgust about incest, and only later come up with some reason why that emotion is the objectively correct emotion to have and why their action of condemning the relationship is rationally justified.<br /><br />My final exam, thanks to loss aversion, created an emotional inclination against guessing, which most of the students taking it followed. When confronted with an argument against it, my friends tried to come up with reasons why the course they took was logical - reasons which I found very unconvincing.<br /><br />It's really this last part which was so perfect I couldn't resist posting about it. One of my close friends (let's call him Larry) finally admitted, after much pestering on my part, that guessing would increase his score. But, he said, he still wasn't going to guess, because he had a moral objection to doing so. Tests were supposed to measure how much we knew, not how lucky we were, and if he really didn't know the answer, he wanted that ignorance to be reflected in his final score.<br /><br />A few years ago, I would have respected that strong committment to principle. Today, jaded as I am, I waited until the last day of exams, when our test was a slightly different format. Instead of being true-false, it was multiple-choice: choose one of eight. And there was no penalty for guessing; indeed, there wasn't even a \"don't know\" on the answer sheet, although you could still leave it blank if you really wanted.<br /><br />\"So,\" I asked Larry afterwards, \"did you guess on any of the questions?\"<br /><br />\"Yeah, there were quite a few I didn't know,\" he answered.<br /><br />When I reminded him about his moral commitment, he said something about how this was different because there were more answers available so it wasn't really the same as guessing on a fifty-fifty question. At the risk of impugning my friend's subconscious motives, I think he no longer had to use moral ideals to rationalize away his fear of losing points, so he did the smart thing and guessed.<br /><br /><strong>Footnotes</strong></p>\n<p><strong>1: </strong>If I understand the math right, then if you guess on thirty questions using my test's scoring rule, the probability of ending up with a net penalty from guessing is <del>less than one percent</del> <em>[EDIT: Actually just over two percent, thank you ArthurB]</em>. If, after finishing all the questions of which they were \"certain\", a person felt confident that they were right over the cusp of a passing grade, assigned very high importance to passing, and assigned almost no importance to any increase in grade past the passing point, then it might be rational not to guess, to avoid the less than one percent chance of failure. In reality, no one could calculate their grade out this precisely.</p>\n<p><strong>2:</strong> Looking to see if anyone else had been thinking along the same lines<sup>3</sup>, I found <a href=\"http://www.google.com/url?sa=t&amp;source=web&amp;ct=res&amp;cd=1&amp;url=http%3A%2F%2Fwww.accessecon.com%2Fpubs%2FEB%2F2004%2FVolume4%2FEB-04D80001A.pdf&amp;ei=RitlSqu-HpLuswOauuHsDg&amp;usg=AFQjCNE8gwktdt1RncdJpjRl99rmFZV7Tw&amp;sig2=qjck0-ouuI6N62nqdogPwQ\">a very interesting paper</a> describing some work of Kahneman and Tversky on this issue, and proposing a scoring rule that takes loss aversion into account. Although I didn't go through all of the math, the most interesting number in there seems to be that on a true/false test that penalizes wrong answers at the same rate it rewards correct answers (unlike my test, which rewarded guessing), a person with the empirically determined level of human loss aversion will (if I understand the stats right) need to be ~79% sure before choosing to answer (as opposed to the utility maximizing level of &gt;50%). This also linked me to <a href=\"http://en.wikipedia.org/wiki/Prospect_theory\">prospect theory</a>, which is interesting.</p>\n<p><strong>3:</strong> I'm surprised that test-preparation companies haven't picked up on this. Training people to understand calibration and loss aversion could be very helpful on standardized tests like the SATs. I've never taken a Kaplan or Princeton Review course, but those who have tell me this topic isn't covered. I'd be surprised if the people involved didn't know the science, so maybe they just don't know of a reliable way to teach such things?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1, "YTCrHWYHAsAD74EHo": 1, "4R8JYu4QF2FqzJxE5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "znBJwbuT3f5eWgM4E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 95, "baseScore": 119, "extendedScore": null, "score": 0.000186, "legacy": true, "legacyId": "1422", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 119, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 109, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LgavAYtzFQZKg95WC", "t2NN6JwMFaqANuLqH", "B5K3hg8FgrMDHuXjH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-21T07:22:43.020Z", "modifiedAt": null, "url": null, "title": "Of Exclusionary Speech and Gender Politics", "slug": "of-exclusionary-speech-and-gender-politics", "viewCount": null, "lastCommentedAt": "2017-09-24T12:37:40.589Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MyqGb24pM54rJhDpb/of-exclusionary-speech-and-gender-politics", "pageUrlRelative": "/posts/MyqGb24pM54rJhDpb/of-exclusionary-speech-and-gender-politics", "linkUrl": "https://www.lesswrong.com/posts/MyqGb24pM54rJhDpb/of-exclusionary-speech-and-gender-politics", "postedAtFormatted": "Tuesday, July 21st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Of%20Exclusionary%20Speech%20and%20Gender%20Politics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOf%20Exclusionary%20Speech%20and%20Gender%20Politics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMyqGb24pM54rJhDpb%2Fof-exclusionary-speech-and-gender-politics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Of%20Exclusionary%20Speech%20and%20Gender%20Politics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMyqGb24pM54rJhDpb%2Fof-exclusionary-speech-and-gender-politics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMyqGb24pM54rJhDpb%2Fof-exclusionary-speech-and-gender-politics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1524, "htmlBody": "<p>I suspect that the ick reaction being labeled \"objectification\" actually has more to do with the sense that the speaker is addressing a closed group that doesn't include you.</p>\n<p>Suppose I wrote a story about a man named Frank, whose twin brother (Frank has learned) is in the process of being framed for murder this very night.&nbsp; Frank is in the middle of a complicated plot to give his brother an alibi.&nbsp; He's already found the cabdriver and tricked him into waiting outside a certain apartment for an hour.&nbsp; Now all he needs is the last ingredient of his plan - a woman to go home with him (as he poses as his brother).&nbsp; Frank is, with increasing desperation, propositioning ladies at the bar - any girl will do for his plan, it doesn't matter who she is or what she's about...</p>\n<p>I'd bet I could write that story without triggering the ick reaction, because Frank is an equal-opportunity manipulator - he manipulated the cabdriver, too.&nbsp; The story isn't about Frank regarding <em>women </em>as things on the way to implementing his plan, it's about Frank regarding various people, <em>men and women alike</em>, as means to the end of saving his brother.</p>\n<p>If a woman reads that story, I think, she won't get a sense of being <em>excluded from the intended audience</em>.<a id=\"more\"></a></p>\n<p>I <em>suspect </em>that's what the ick factor being called \"objectification\" is really about - the sense that someone who says \"...but you'll still find women alluring\" is talking to an audience that doesn't include <em>you</em>, a woman.&nbsp; It doesn't matter if you happen to be a bi woman.&nbsp; You still get the sense that it never crossed the writer's mind that there might be any women in the audience, and so <em>you</em> are excluded.</p>\n<p><em>In general,</em> starting from a <em>perceptual</em> reaction, it is a <em>difficult </em>cognitive task to say in words <em>exactly </em>why that reaction occurred - to accurately state the necessary and sufficient conditions for its triggering.&nbsp; If the reaction is <em>affective,</em> a good or bad reaction, there is an additional danger:&nbsp; You'll be tempted to zoom in on any bad (good) aspect of the situation, and say, \"Ah, that must be the reason it's bad (good)!\"&nbsp; It's wrong to treat people as means rather than ends, right?&nbsp; People have their own feelings and inner life, and it's wrong to forget that?&nbsp; Clearly, that's <em>a </em>problem with saying, \"And this is how you get girls...\"&nbsp; But<em> </em>is that <em>exactly </em>what went wrong <em>originally</em> - what triggered the original ick reaction?</p>\n<p>And this (I say again) is a tricky cognitive problem in general - the introspective jump from the perceptual to the abstract.&nbsp; It is tricky far beyond the realms of gender...</p>\n<p>But I do suspect that the real problem is <em>speech that makes a particular gender feel excluded.</em>&nbsp; And if that's so, then for the purposes of Less Wrong, I think, it may make sense to zoom in on that <em>speech property.</em>&nbsp; Politics of all sorts have always been a dangerous bit of attractive flypaper, and I think we've had a sense, on Less Wrong, that we ought to steer clear of it - that <a href=\"/lw/gw/politics_is_the_mindkiller/\">politics is the mindkiller</a>.&nbsp; And so I hope that no one will feel that their gender politics are being <em>particularly</em> targeted, if I suggest that, like some other political issues, we might want to steer sort of clear of that.</p>\n<p>I've previously expressed that to build a rationalist community sustainable over time, the sort of gender imbalance that appears among e.g. computer programmers, is not a good thing to have.&nbsp; And so it may make sense, as rationalists <em>qua</em> rationalists, to target gender-exclusionary speech.&nbsp; To say, \"Less Wrong does not want to make any particular gender feel unwelcome.\"</p>\n<p>But I also think that you can <em>just</em> have a policy like that, without opening the floor to discussion of all gender politics <em>qua</em> gender politics.&nbsp; Without having a position on whether, say, \"privilege\" is a useful way to think about certain problems, or a harmful one.</p>\n<p>And the coin does have two sides.&nbsp; It <em>is</em> possible to make men, and not just women, feel unwelcome as a gender.&nbsp; It is harder, because men have fewer painful memories of exclusion to trigger.&nbsp; A single comment by a woman saying \"All men are idiots\" won't do it.&nbsp; But if you've got a conversational thread going between <em>many </em>female posters all agreeing that men are privileged idiots, then a man can start to pick up a perceptual impression of \"This is not a place where I'm welcome; this is a women's locker room.\"&nbsp; And LW shouldn't send <em>that </em>message, either.</p>\n<p>So if we're going to do this, then let's have a policy which says that we don't want to make <em>either</em> gender feel unwelcome.&nbsp; And that aside from this, we're not saying anything official about gender politics <em>qua</em> gender politics.&nbsp; And indeed we might even want to discourage gender-political discussion, because it's probably not going to contribute to our understanding of <a href=\"/lw/31/what_do_we_mean_by_rationality/\">systematic and general methods of epistemic and instrumental rationality</a>, which is our actual alleged topic around here.</p>\n<p>But even if we say we're <em>just </em>going to have a non-declarative <em>procedural </em>rule to avoid language or behavior that makes a gender <em>feel excluded...</em> it still takes us into thorny waters.</p>\n<p>After all, jumping on <em>every tiny hint</em> - say, objecting to the Brennan stories because Brennan is male - will make <em>men</em> feel unwelcome; that this is a blog only for people who agree with feminist politics; that men have to tiptoe while women are allowed to tapdance...</p>\n<p>Now <em>with </em>that said: the point is to avoid language that makes someone <em>feel</em> unwelcome.&nbsp; So if someone says that they felt excluded as a gender, <em>pay attention.</em>&nbsp; The issue is not how to prove they're \"wrong\".&nbsp; Just <em>listen</em> to the one who heard you, when they tell you what they heard.&nbsp; We want to avoid any or either gender, <em>feeling</em> excluded and leaving.&nbsp; So it is the <em>impression</em> that is the key thing.&nbsp; You can argue, perhaps, that the one's threshold for offense was set unforgivably low, that they were <em>listening so hard</em> that no one could whisper softly enough.<em>&nbsp; </em>But not argue that they <em>misunderstood</em> you.&nbsp; For that is still a fact about your speech and its consequences.&nbsp; We shall just try to <em>avoid </em>certain types of misunderstanding, not blame the misunderstander.</p>\n<p>And what if someone decides she's offended by <em>all </em>discussion of evolutionary psychology because that's a patriarchal plot...?</p>\n<p>Well... I think there's something to be said here, about her having impugned the honor of female rationalists everywhere.&nbsp; But let a female rationalist be <a href=\"/lw/13j/of_exclusionary_speech_and_gender_politics/#yp7\">the one to say it</a>.&nbsp; And then we can all downvote the comment into oblivion.</p>\n<p>And if someone decides that <em>all </em>discussion of the PUA (pickup artist) community, makes her feel excluded...?</p>\n<p>Er... I have to say... I sort of get that one.&nbsp; I too can feel the locker-room ambiance rising off it.&nbsp; Now, yes, we have a lot of men here who are operating in gender-imbalanced communities, and we have men here who are nerds; and if you're the sort of person who reads <em>Less Wrong,</em> there is a certain conditional probability that you will be the sort of person who tries to find a detailed manual that solves your problems...</p>\n<p>...while not being quite sane <em>enough</em> to actually <em>notice</em> you're driving away <em>the very gender you're trying to seduce </em>from our nascent rationalist community, and consequentially <em>shut up about PUA</em>...</p>\n<p>...oh, never mind.&nbsp; Gender relations much resembles the rest of human existence, in that it largely consists of people walking around with shotguns shooting off their own feet.&nbsp; In the end, PUA is not something we need to be talking about <em>here,</em> and if it's giving one entire gender the <em>wrong vibes</em> on this website, I say the hell with it.</p>\n<p>And if someone decides that it's not enough that a comment has been downvoted to -5; it needs to be banned, or the user needs to be banned, in order to signify that this website is <em>sufficiently </em>friendly...?</p>\n<p>Sorry - downvoting to -5 should be enough to show that the community disapproves of this lone commenter.</p>\n<p>If someone demands explicit agreement with their-favorite-gender-politics...?</p>\n<p>Then they're probably making the <em>other</em> gender feel unwelcome - the coin does have two sides.</p>\n<p>If someone argues against gay marriage...?</p>\n<p>Respond not to trolls; downvote to oblivion without a word.&nbsp; That's not gender politics, it's kindergarten.</p>\n<p>If you just can't seem to figure out what's wrong with your speech...?</p>\n<p>Then just keep on accepting suggested edits.&nbsp; If you literally don't understand what you're doing wrong, then realize that you have a blind spot and need to steer around it.&nbsp; And if you do keep making the suggested edits, I think that's as much as someone could reasonably ask of you.&nbsp; We need a bit more <em>empathy</em> in all directions here, and that includes empathy for the hapless plight of people who <em>just don't get it</em>, and who aren't <em>going </em>to get it, but who are still doing what they can.</p>\n<p>If you just can't get someone to agree with your stance on explicit gender politics...?</p>\n<p>Take it elsewhere, both of you, please.</p>\n<p>&nbsp;</p>\n<p>Is it clear from this what sort of general policy I'm driving at?&nbsp; What say you?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FkzScn5byCs9PxGsA": 1, "x6evH6MyPK3nxsoff": 1, "DdgSyQoZXjj3KnF4N": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MyqGb24pM54rJhDpb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 96, "baseScore": 88, "extendedScore": null, "score": 0.000141, "legacy": true, "legacyId": "1423", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 88, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 669, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9weLK2AJ9JEt2Tt8f", "RcZCwxFiZzE6X7nsv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2009-07-21T07:22:43.020Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-22T03:23:33.171Z", "modifiedAt": null, "url": null, "title": "Missing the Trees for the Forest", "slug": "missing-the-trees-for-the-forest", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:58.646Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MtNnFg4uN32YPoKNa/missing-the-trees-for-the-forest", "pageUrlRelative": "/posts/MtNnFg4uN32YPoKNa/missing-the-trees-for-the-forest", "linkUrl": "https://www.lesswrong.com/posts/MtNnFg4uN32YPoKNa/missing-the-trees-for-the-forest", "postedAtFormatted": "Wednesday, July 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Missing%20the%20Trees%20for%20the%20Forest&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMissing%20the%20Trees%20for%20the%20Forest%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMtNnFg4uN32YPoKNa%2Fmissing-the-trees-for-the-forest%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Missing%20the%20Trees%20for%20the%20Forest%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMtNnFg4uN32YPoKNa%2Fmissing-the-trees-for-the-forest", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMtNnFg4uN32YPoKNa%2Fmissing-the-trees-for-the-forest", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2097, "htmlBody": "<p><a href=\"/lw/gw/politics_is_the_mindkiller/\">Politics is the mind-killer</a>. A while back, <a href=\"/lw/fp/cheerios_an_untested_new_drug/cfl\">I gave an example: the government's request that <del>Kelloggs&nbsp;</del> [EDIT: General Mills, thanks CronoDAS] top making false claims about Cheerios</a>. By the time the right-wing and left-wing blogospheres had finished with it, this became everything from part of the deliberate strangulation of the American entrepreneurial spirit by a conspiracy of bureaucrats, to a symbol of the radicalization of the political right into a fringe group obsessed with Communism, to a prelude to Obama's plan to commit genocide against all citizens who disagree with him. All because of Cheerios!</p>\n<p>Why? What drives someone to hear about a reasonable change in cereal advertising policy and immediately think of a second Holocaust?</p>\n<p>This reminds me of something I used to notice when reading about politics. Sometimes there would be a seemingly good idea to deregulate something that clearly needed deregulation. The idea's proponents would go on TV and say that, hey, this was obviously a good idea. Whoever by the vagary of politics had to oppose the idea would go on TV and talk about industry's plot to emasculate government safeguards. Predatory corporations! Class solidarity! Consumer safety!</p>\n<p>Then the next day, there would be seemingly good idea to regulate something that clearly needed regulating. The idea's proponents would go on TV and say that, hey, this was obviously a good idea. Its opponents would go on TV and say that all government regulation was inherently bad. Small government! Freedom! Capitalism!</p>\n<p>I have found a pattern: when people consider an idea in isolation, they tend to make good decisions. When they consider an idea a symbol of a vast overarching narrative, they tend to make very bad decisions.</p>\n<p><a id=\"more\"></a></p>\n<p>Let me offer another example.</p>\n<p>A white man is accused of a violent attack on a black woman. In isolation, well, either he did it or he didn't, and without any more facts there's no use discussing it.</p>\n<p>But what if this accusation is viewed as a symbol? What if you have been saying for years that racism and sexism are endemic in this country, and that whites and males are constantly abusing blacks and females, and they're always getting away with it because the police are part of a good ole' boys network who protect their fellow privileged whites?</p>\n<p>Well, right now, you'll probably still ask for the evidence. But if I gave you some evidence, and it was complicated, you'd probably interpret it in favor of the white man's guilt. The heart has its reasons that reasons know not of, and most of them suck. We make unconsciously make decisions based on our own self-interest and what makes us angry or happy, and then later we find reasons why the evidence supports them. If I have a strong interest in a narrative of racism, then I will interpret the evidence to support accusations of racism.</p>\n<p>Lest I sound like I'm picking on the politically correct, I've seen scores of people with the opposite narrative. You know, political correctness has grown rampant in our society, women and minorities have been elevated to a status where they can do no wrong, the liberal intelligentsia always tries to pin everything on the white male. When the person with this narrative hears the evidence in this case, they may be more likely to believe the white man - especially if they'd just listened to their aforementioned counterpart give their speech about how this proves the racist and sexist tendencies of white men.</p>\n<p>Yes, I'm thinking of <a href=\"http://en.wikipedia.org/wiki/Duke_lacrosse_rape_case\">the Duke</a> <a href=\"http://en.wikipedia.org/wiki/Responses_to_the_2006_Duke_University_lacrosse_case\">lacrosse case</a>.</p>\n<p>The problem here is that there are two different questions here: whether this particular white male attacked this particular black woman, and whether our society is racist or \"reverse racist\". The first question definitely has one correct answer which while difficult to ascertain is philosophically simple, whereas the second question is <a href=\"/lw/48/the_power_of_positivist_thinking/\">meaningless, in the same technical sense that \"Islam is a religion of peace\" is meaningless</a>. People are conflating these two questions, and acting as if the answer to the second determines the answer to the first.</p>\n<p><span style=\"font-size: 10px; white-space: pre; \"><span style=\"font-size: 13px; white-space: normal; \">W</span><span style=\"font-size: 13px; white-space: normal;\">hich is all nice and well unless you're one of the people involved in the case, in which case you really don't care about which races are or are not privileged in our society as much as you care about not being thrown in jail for a crime you didn't commit, or about having your attacker brought to justice.</span></span></p>\n<p>I think this is the driving force behind a lot of politics. Let's say we are considering a law mandating businesses to lower their pollution levels. So far as I understand economics, the best decision-making strategy is to estimate how much pollution is costing the population, how much cutting pollution would cost business, and if there's a net profit, pass the law. Of course it's more complicated, but this seems like a reasonable start.</p>\n<p>What actually happens? One side hears the word \"pollution\" and starts thinking of hundreds of times when beautiful pristine forests were cut down in the name of corporate greed. This links into other narratives about corporate greed, like how corporations are oppressing their workers in sweatshops in third world countries, and since corporate executives are usually white and third world workers usually not, let's add racism into the mix. So this turns into one particular battle in the war between All That Is Right And Good and Corporate Greed That Destroys Rainforests And Oppresses Workers And Is Probably Racist.</p>\n<p>The other side hears the words \"law mandating businesses\" and starts thinking of a long history of governments choking off profitable industry to satisfy the needs of the moment and their re-election campaign. The demonization of private industry and subsequent attempt to turn to the government for relief is a hallmark of communism, which despite the liberal intelligentsia's love of it killed sixty million people. Now this is a battle in the war between All That Is Right And Good and an unholy combination of Naive Populism and Soviet Russia. This, I think, is part of what happened to the poor Cheerios.</p>\n<p>Now, if the economists do their calculations and report that actually the law would cause more harm than good, do you think the warriors against Corporate Greed That Destroys Rainforests And Oppresses Workers And Is Probably Racist are going to say \"Oh, okay then\" and stand down? In the face of&nbsp;Corporate Greed That Destroys Rainforests And Oppresses Workers And Is Probably Racist?!?<span style=\"vertical-align: super;\">1</span></p>\n<p>One more completely hypothetical example. Let's say someone uses language that objectifies women on a blog. Not out of malice or anything, it was just a post on evolutionary psychology, it's easy to write evolutionary psychology in a way that sounds like it's objectifying women, and since obviously no one would objectify women on purpose to insult them it will be clear to everyone that it was just a harmless turn of phrase. Right?</p>\n<p>And let's say some feminist comes along and reads this completely innocent phrase about women. Let's say the context is the entire history of gender relations for the past ten thousand years, in which men have usually oppressed women and usually been pretty okay with doing so. And a society that's moving towards not oppressing women and towards treating them as full and equal human beings, but it's still not entirely clear that everyone's on board with this.</p>\n<p>This poorly-worded phrase is now a symbol of All Those Chauvinists Who Think Of Women As Ornaments Or Toys Only Good For Sex And Making Babies<span style=\"vertical-align: super;\">2</span>. The feminist is unhappy. He or she asks for the phrase to be removed.</p>\n<p>Let's say some person who is emphatically not a feminist notices this request for removal. Let's say the context is a society where men are generally portrayed in popular culture as violent bumbling apes who cause all world problems. A culture where women can go on for hours about what boors men are, but any man who says a word about women is immediately branded a sexist pig. A culture where <del>a popular feminist once said that all sex was rape</del> [EDIT: <a href=\"/lw/13k/missing_the_trees_for_the_forest/yo5\">Or not</a>. Apologies for misquote], and many people believed her, one with affirmative action laws mandating that women be hired over equally qualified men, one where you can't say \"chairman of the board\" without someone calling you sexist and accusing you of taking advantage of your male privilege to ignore male privilege if you disagree.</p>\n<p>This request to remove a potentially offensive phrase is now a symbol of All Those Feminists Who Hate Men And Want Them To Feel Guilty All The Time For Vague Reasons. He or she gets angry, and certainly won't remove the offending phrase.</p>\n<p>I'm not <span style=\"font-style: italic;\">sure</span> that's what's happening in this case, but I don't think a few poorly worded phrases followed by a polite request to change those poorly worded phrases would have reached five hundred fifty comments divided over four top-level posts if people were just taking it as a request to use slightly different language. In our completely hypothetical example, of course.</p>\n<p>I call this mistake \"missing the trees for the forest\". If you have a specific case you need to judge, judge it separately on its own merits, not the merits of what agendas it promotes or how it fits with emotionally charged narratives<span style=\"vertical-align: super;\">3</span>.</p>\n<p>&nbsp;</p>\n<p><span style=\"font-weight: bold;\">Footnotes</span></p>\n<p><span style=\"font-weight: bold;\"><span style=\"font-weight: bold;\">1: </span></span>This gets worse once it gets formally organized into political parties. You get people saying something like \"How can you, as an atheist, support the war in Iraq?\" and thinking it makes perfect sense, because, after all, the war in Iraq is a Republican initiative, and the Republicans are the party of religious conservatives, therefore... Oh, yes, people think like this.</p>\n<p><span style=\"font-weight: bold;\"><span style=\"font-weight: bold;\">2:</span></span><span style=\"font-weight: bold;\"> </span>Oh, and this answers a question I sometimes hear asked half-seriously on message boards: how come derogatory jokes are okay in some settings but not in others? For example, how come Polish jokes are generally considered okay, but black jokes definitely aren't? Or how come it's considered okay for a black person to make a racist-sounding joke about black people or use the n-word, whereas it's not okay for a white person?</p>\n<p>I think the answer is that if I were to make a Polish joke, it would be interpreted as what it is - a joke that needed somebody to play the part of a stupid person to be funny, and Polish people have traditionally served that role. There is no active well-known ongoing context of persecution of Polish people for the joke to symbolize, so it symbolizes nothing but itself and is inert. If I were to tell a joke about black people, even if it was clear that I wasn't actually racist and just thought the joke was funny, then since most people have a very active concept of persecution of black people, my joke would be a symbol of that persecution, and all right-thinking people who oppose that persecution would also probably oppose my joke.&nbsp;</p>\n<p>This leads to the odd conclusion that in a society known to be without racism, no one would mind racist jokes or slurs. In fact, this is confirmed by evidence. Black people are, society generally assumes, above suspicion when it comes to anti-black racism, and therefore black people can use the \"n-word\" without most people objecting.</p>\n<p>This is what led to me developing some of these thoughts. I told a joke which I considered to be making fun of racism. Someone who heard it misinterpreted it and thought it was racist, accused me of racism, spread rumors that I was racist, and generally started a large and complicated campaign to discredit me. After that, I noticed that I was always coming to the defense of people who were accused of racism, and was willing to dismiss practically the entire concept of racism in society as a self-serving attempt at personal gain by minorities, a one hundred eighty degree turn from my previous attitude. Eventually I realized that I was just re-fighting the battle I had to fight after this one joke, and fitting everything to my \"sometimes false accusations of racism unfairly harm majority group members and we need to protect against this\" narrative. So I stopped. I think.</p>\n<p>This also could explain why, contrary to Robin Hanson's hopes, people will never stop using disclaimers. They're ways of saying \"I did this action for reasons that do not relate to your narrative; please exclude me from it\", and this is not people's default position.</p>\n<p><span style=\"font-weight: bold;\"><span style=\"font-weight: bold;\">3: </span></span>One objection could be that the specific case could start a slippery slope, or create a climate in which other things become viewed as more acceptable. In my experience, neither of these matter nearly as much as they would have to to justify the number of times people invoke them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FkzScn5byCs9PxGsA": 1, "x6evH6MyPK3nxsoff": 1, "DdgSyQoZXjj3KnF4N": 1, "LDTSbmXtokYAsEq8e": 1, "5f5c37ee1b5cdee568cfb1b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MtNnFg4uN32YPoKNa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 77, "baseScore": 83, "extendedScore": null, "score": 0.000141, "legacy": true, "legacyId": "1424", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 83, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 159, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9weLK2AJ9JEt2Tt8f", "azoP7WeKYYfgCozoh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-22T06:27:56.562Z", "modifiedAt": null, "url": null, "title": "Deciding on our rationality focus", "slug": "deciding-on-our-rationality-focus", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:13.782Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZAoBjz5DqzEtig376/deciding-on-our-rationality-focus", "pageUrlRelative": "/posts/ZAoBjz5DqzEtig376/deciding-on-our-rationality-focus", "linkUrl": "https://www.lesswrong.com/posts/ZAoBjz5DqzEtig376/deciding-on-our-rationality-focus", "postedAtFormatted": "Wednesday, July 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Deciding%20on%20our%20rationality%20focus&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADeciding%20on%20our%20rationality%20focus%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZAoBjz5DqzEtig376%2Fdeciding-on-our-rationality-focus%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Deciding%20on%20our%20rationality%20focus%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZAoBjz5DqzEtig376%2Fdeciding-on-our-rationality-focus", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZAoBjz5DqzEtig376%2Fdeciding-on-our-rationality-focus", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 620, "htmlBody": "<p>I have a problem: I'm not sure what this community is about.</p>\r\n<p>To illustrate, recently I've been <a href=\"http://xuenay.livejournal.com/323308.html\">experimenting with a number of tricks</a> to overcome my akrasia. This morning, a succession of thoughts struck me:</p>\r\n<ol>\r\n<li>The readers of Less Wrong have been interested in the subject of akrasia, maybe I should make a top-level post of my experiences once I see what works and what doesn't.</li>\r\n<li>But wait, that would be straying into the territory of traditional self-help, and I'm sure there are already plenty of blogs and communities for&nbsp;<em>that</em>. It isn't about rationality anymore.</li>\r\n<li>But then, we <em>have</em> already discussed akrasia several times, isn't this then also on-topical?</li>\r\n<li>(Even if this was topical, wouldn't a simple recount of \"what worked for me\" be <a href=\"http://wiki.lesswrong.com/wiki/Other-optimizing\">too Kaj-optimized to work for very many others</a>?)</li>\r\n</ol>\r\n<p>Part of the problem seems to stem from the fact that <a href=\"/lw/31/what_do_we_mean_by_rationality/\">we have a two-fold definition of rationality</a>:</p>\r\n<ol>\r\n<li><strong>Epistemic rationality:</strong> believing, and updating on evidence, so as to systematically improve the correspondence between your map and the territory.  The art of obtaining beliefs that correspond to reality as closely as possible.  This correspondence is commonly termed \"truth\" or \"accuracy\", and we're happy to call it that.</li>\r\n<li><strong>Instrumental rationality:</strong> achieving your values.  Not necessarily \"your values\" in the sense of being selfish values or unshared values: \"your values\" means anything you care about.  The art of choosing actions that steer the future toward outcomes ranked higher in your preferences.  On LW we sometimes refer to this as \"winning\".</li>\r\n</ol>\r\n<p>If this community was only about epistemic rationality, there would be no problem. Akrasia isn't related to epistemic rationality, and neither are most self-help tricks. Case closed.</p>\r\n<p>However, by including instrumental rationality, we have expanded the sphere of potential topics to cover practically&nbsp;<em>anything</em>. Productivity tips, seduction techniques, the best ways for grooming your physical appearance, the most effective ways to relax (and by extension, listing the best movies / books / video games of all time), how you can most effectively combine different rebate coupons and where you can get them from... all of those can be useful in achieving your values.<a id=\"more\"></a></p>\r\n<p>Expanding our focus isn't necessarily a bad thing, by itself. It will allow us to attract a wider audience, and some of the people who then get drawn here might afterwards also become interested in e-rationality. And many of us would probably find the new kinds of discussions useful in their personal lives. The problem, of course, is that epistemic rationality is a relatively narrow subset of instrumental rationality - if we allow all instrumental rationality topics, we'll be drowned in them, and might soon lose our original focus entirely.</p>\r\n<p>There are several different approaches as far as I can see (as well as others I can't see):</p>\r\n<ul>\r\n<li>Treat discussions of both as being fully on the same footing - if i-rationality discussions overwhelm e-rationality ones, that's just how it goes.</li>\r\n<li>Concentrate purely on e-rationality, and ban i-rationality discussions entirely.</li>\r\n<li>Allow i-rationality discussions, but don't promote top-level posts on the topic.</li>\r\n<li>Allow i-rationality discussions, but require a stricter criteria for promoting top-level posts on the topic.</li>\r\n<li>Allow i-rationality discussions, but only in the comments of dedicated monthly posts, resembling the \"open topic\" and \"rationality quotes\" series we have now.</li>\r\n<li>Allow i-rationality discussions, but try to somehow define the term so that silly things like listing the best video games of all time get excluded.</li>\r\n<li>Screw trying to make an official policy on this, let's just see what top-level posts people make and what gets upvoted.</li>\r\n<li>Some combination of the above.</li>\r\n</ul>\r\n<p>I honestly don't know which approach would be the best. Do any of you?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZAoBjz5DqzEtig376", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 39, "extendedScore": null, "score": 6.4e-05, "legacy": true, "legacyId": "1427", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RcZCwxFiZzE6X7nsv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-22T10:44:39.752Z", "modifiedAt": null, "url": null, "title": "Fairness and Geometry", "slug": "fairness-and-geometry", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:02.065Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oZ33pz2FWzFbWrgHT/fairness-and-geometry", "pageUrlRelative": "/posts/oZ33pz2FWzFbWrgHT/fairness-and-geometry", "linkUrl": "https://www.lesswrong.com/posts/oZ33pz2FWzFbWrgHT/fairness-and-geometry", "postedAtFormatted": "Wednesday, July 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fairness%20and%20Geometry&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFairness%20and%20Geometry%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoZ33pz2FWzFbWrgHT%2Ffairness-and-geometry%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fairness%20and%20Geometry%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoZ33pz2FWzFbWrgHT%2Ffairness-and-geometry", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoZ33pz2FWzFbWrgHT%2Ffairness-and-geometry", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 738, "htmlBody": "<p>This post was prompted by Vladimir Nesov's <a href=\"/lw/do/reformalizing_pd/aje\">comments</a>, Wei Dai's <a href=\"/lw/12v/fair_division_of_blackhole_negentropy_an/\">intro to cooperative games</a> and Eliezer's&nbsp;<a href=\"/lw/135/timeless_decision_theory_problems_i_cant_solve/\">decision theory problems</a>. Prerequisite:&nbsp;<a href=\"/lw/do/reformalizing_pd/\">Re-formalizing PD</a>.</p>\n<p>Some people here have expressed interest in how AIs that know each other's source code should play asymmetrical games, e.g. slightly asymmetrized PD. The problem is twofold: somehow assign everyone a strategy so that the overall outcome is \"good and fair\", then somehow force everyone to play the assigned strategies.</p>\n<p>For now let's handwave around the second problem thus: AIs that have access to each other's code and common random bits can enforce any <a href=\"http://en.wikipedia.org/wiki/Correlated_equilibrium\">correlated play</a> by using the quining trick from Re-formalizing PD. If they all agree beforehand that a certain outcome is \"good and fair\", the trick allows them to \"mutually precommit\" to this outcome without at all constraining their ability to aggressively play against those who didn't precommit. This leaves us with the problem of fairness.</p>\n<p>(Get ready, math ahead. It sounds massive, but is actually pretty obvious.)<a id=\"more\"></a></p>\n<p>Pure strategy plays of an N-player game are points in the N-dimensional space of utilities. Correlated plays form the convex hull of this point set, an N-polytope. <a href=\"http://en.wikipedia.org/wiki/Pareto_efficiency\">Pareto-optimal</a> outcomes are points on the polytope's surface where the outward normal vector has all positive components. I want to somehow assign each player a \"bargaining power\" (by analogy with <a href=\"http://www.cse.iitd.ernet.in/~rahul/cs905/lecture15/index.html\">Nash bargaining solutions</a>); collectively they will determine the slope of a hyperplane that touches the Pareto-optimal surface at a single point which we will dub \"fair\". Utilities of different players are classically treated as incomparable, like metres to kilograms, i.e. having different <a href=\"http://en.wikipedia.org/wiki/Dimensional_analysis\">dimensionality</a>;&nbsp;thus we'd like the \"fair point\" to be invariant under affine recalibrations of utility scales. Coefficients of tangent hyperplanes transform as <a href=\"http://en.wikipedia.org/wiki/Linear_functional\">covectors</a> under such recalibrations; components of a covector should have dimensionality inverse to components of a vector for the application operation to make sense; thus&nbsp;the bargaining power of each player must have dimensionality 1/utility of that player.</p>\n<p>(Whew! It'll get easier from now.)</p>\n<p>A little mental visualization involving a sphere and a plane confirms that when a player stretches their utility scale 2x, stretching the sphere along one of the coordinate axes, the player's power (the coefficient of that coordinate in the tangent hyperplane equation) must indeed go down 2x to keep the fair point from moving. Incidentally, this means that we&nbsp;<em>cannot</em>&nbsp;somehow assign each player \"equal power\" in a way that's consistent under recalibration.</p>\n<p>Now, there are many ways to process an N-polytope and obtain N values, dimensioned as 1/coordinate each. A natural way would be to take the inverse measure of the polytope's projection onto each coordinate axis, but this approach fails because <a href=\"http://en.wikipedia.org/wiki/Independence_of_irrelevant_alternatives\">irrelevant alternatives</a> can skew the result wildly. A better idea would be taking the inverse measures of projections of <em>just&nbsp;</em><em>the Pa</em><em>reto-optimal surface region</em> onto the coordinate axes; this decision passes the smoke test of bargaining games, so it might be reasonable.</p>\n<p>To reiterate the hypothesis: <strong>assign each player the amount of bargaining power inversely proportional to the range of their gains possible under Pareto-optimal outcomes</strong>. Then pick the point on the polytope's surface that touches a hyperplane with those bargaining powers for coefficients, and call this point \"fair\".</p>\n<p>(NB: this idea doesn't solve cases where the hyperplane touches the polytope at more than one point, e.g. risk-neutral division of the dollar. Some more refined fairness concept is required for those.)</p>\n<p>At this point I must admit that I don't possess a neat little list of \"fairness properties\" that would make my solution unique and inevitable, <a href=\"http://en.wikipedia.org/wiki/Shapley_value\">Shapley value</a> style. It just... sounds natural. It's an equilibrium, it's symmetric, it's invariant under recalibrations, it often gives a unique answer, it solves asymmetrized PD just fine, and the True PD, and other little games I've tried it on, and something like it might someday solve the general problem outlined at the start of the post; but then again, we've tossed out quite a lot of information along the way. For example, we didn't use the row/column structure of strategies at all.</p>\n<p>What should be the next step in this direction?</p>\n<p>Can we solve fairness?</p>\n<p><strong>EDIT:</strong> thanks to Wei Dai for the next step! Now I know that any \"purely geometric\" construction that looks only at the Pareto set will fail to incentivize players to adopt it. The reason: we can, without changing the Pareto set, give any player an additional non-Pareto-optimal strategy that always assigns them higher utility than my proposed solution, thus making them want to defect. Pretty conclusive! So much for this line of inquiry, I guess.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5A5ZGTQovxbay6fpr": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oZ33pz2FWzFbWrgHT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 13, "extendedScore": null, "score": 5.095658291140817e-07, "legacy": true, "legacyId": "1428", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["z3W8PRHJM9ZanTDcx", "c3wWnvgzdbRhNnNbQ", "5iK6rsa3MSrMhHQyf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-22T19:41:03.478Z", "modifiedAt": null, "url": null, "title": "It's all in your head-land", "slug": "it-s-all-in-your-head-land", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:19.454Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "colinmarshall", "createdAt": "2009-07-08T22:54:59.882Z", "isAdmin": false, "displayName": "colinmarshall"}, "userId": "AioHEjiBZshwBwALh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CbJ87ArBPncWoj4uf/it-s-all-in-your-head-land", "pageUrlRelative": "/posts/CbJ87ArBPncWoj4uf/it-s-all-in-your-head-land", "linkUrl": "https://www.lesswrong.com/posts/CbJ87ArBPncWoj4uf/it-s-all-in-your-head-land", "postedAtFormatted": "Wednesday, July 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20It's%20all%20in%20your%20head-land&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIt's%20all%20in%20your%20head-land%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCbJ87ArBPncWoj4uf%2Fit-s-all-in-your-head-land%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=It's%20all%20in%20your%20head-land%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCbJ87ArBPncWoj4uf%2Fit-s-all-in-your-head-land", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCbJ87ArBPncWoj4uf%2Fit-s-all-in-your-head-land", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3681, "htmlBody": "<p>From David Foster Wallace's <em>Infinite Jest</em>:</p>\n<blockquote>He could do the dextral pain the same way: Abiding. No one single instant of it was unendurable. Here was a second right here: he endured it. What was undealable-with was the thought of all the instants all lined up and stretching ahead, glittering. And the projected future fear... It's too much to think about. To Abide there. But none of it's as of now real... He could just hunker down in the space between each heartbeat and make each heartbeat a wall and live in there. Not let his head look over. What's unendurable is what his own head could make of it all. What his head could report to him, looking over and ahead and reporting. But he could choose not to listen... He hadn't quite gotten this before now, how it wasn't just the matter of riding out cravings for a Substance: everything unendurable was in the head, was the head not Abiding in the Present but hopping the wall and doing a recon and then returning with unendurable news you then somehow believed.</blockquote>\n<p>I've come to draw, or at to emphasize, a distinction separating two realms between which I divide my time: real-land and head-land. Real-land is the physical world, occupied by myself and billions of equally real others, in which my fingers strike a series of keys and a monitor displays strings of text corresponding to these keystrokes. Head-land is the world in which I construct an image of what this sentence will look like when complete, what this paragraph will look like when complete and what this entire post will look like when complete. And it doesn't stop there: in head-land, the finished post is already being read, readers are reacting, readers are (or aren't) responding and the resulting conversations are, for better or for worse, playing themselves out. In head-land, the thoughts I've translated into words and thus defined and developed in this post are already shaping the thoughts to be explored in future posts, the composition of which is going on there <em>even now</em>.<a id=\"more\"></a><br /><br />Head-land is the setting of our predictions. When deciding what actions to take in real-land, we don't base our choices on the possible actions' end results in real-land &mdash; by definition, we don't know what will happen in real-land until it already has &mdash; but on their end results in head-land. The obvious problem: while real-land is built out of all the information that exists everywhere, head-land is built not even out of all the information in a single brain &mdash; and that isn't much &mdash; but of whichever subset of that brain's information <em>happens to be currently employed</em>. Hence the brutally rough, sometimes barely perceptible correspondence between head-land and real-land in the short term and the nearly nonexistent correspondence between them in the long.<br /><br />One might grant that while responding that running models in head-land is nevertheless the best predictor of real-land events that any individual has. And that's true, but it doesn't change our apparent tendency to place far more trust in our head-land models than their dismal accuracy could ever warrant. To take but one example: we <em>really</em> seem to believe our own failures in head-land, head-land being the place we do the vast majority &mdash; and in some cases, all &mdash; of our failing. How many times has someone entertained the dream of, say, painting, but then failed in head-land &mdash; couldn't get a head-land show, say, or couldn't even mix head-land's colors right &mdash; and abandoned the enterprise before beginning it? How many times has someone <em>started</em> painting in real-land, gotten less-than-perfect results, and then extrapolated that scrap of real-land data into a similarly crushing head-land failure? Even established creators are vulnerable to this; could the novelist suffering from a bout of \"writer's block\" simply be the unwitting mark of a head-land vision of himself unable to write? The danger of head-land catastrophes that poison real-land endeavors looms over every step of the path. The possibility of being metaphorically laughed out of the classroom, though probably only illusory to begin with, never quite leaves one's mind. The same, to a lesser extent, goes for experiencing rather than creating; someone who refuses to listen to a new album, sample a new cuisine, watch a new film or visit a new art exhibition on the excuse that they already \"know what [they] like\" appear to have seen, and believed, their head-land selves listening, eating or viewing with irritation, repulsion or boredom.<br /><br />That most of what we get worked up about exists in our imaginations and our imaginations only is less a fresh observation than the stuff of a thousand tired aphorisms. No battle plan survives the first shot fired. You die a thousand deaths awaiting the guillotine. Nothing's as good or as bad as you anticipate. You never know until you try. We have nothing to fear but fear itself. Fear is the mind-killer. It's all in your mind. Don't look down. Quoth John Milton, \"The mind is its own place, and in it self, can make a Heaven of Hell, a Hell of Heaven.\" The more time I spend in head-land, the less time I feel like I <em>should</em> spend in head-land, because its awful, discouraging predictions are practically never borne out in real-land. (Even when one comes close, head-land seems to fail to capture the subjective experience of failure, which I find is either never failure <em>qua </em>failure or always somehow psychologically palliated or attenuated.) Experiencing a disaster in real-land is one thing, and its negative effects are unavoidable, but experiencing hypothetical head-land disasters <em>as negative mental effects in real-land</em> &mdash; which I suspect, we all do, and often &mdash; would seem to be optional.<br /><br />Consider global economic woes. While you more than likely know a few who've had to take cuts in their incomes or find new ones, it's even likelier that you're experiencing all the degradations of destitution in head-land even as your real-land income <em>has not and will not</em> substantially shrink. You're living out the agonies of fumbling with food stamps in a long, angry grocery line despite the fact that you'll never come close. When one is starved for real-land information, one's head-land self gets hit with the worst possible fate. I hear of someone dying in a gruesome real-land freak accident, and I die a dozen times over in more gruesome, freakier head-land accidents. I visit a remote, unpopulated real-land location and my head-land map contracts, desolately and suffocatingly, to encompass only my lonely immediate surroundings. (Then my glasses fall off and break. But there was time! <em>There was time!</em>) I dream up an idea for implementation in real-land, but even before I've fully articulated it my head-land self is already busy enduring various ruinous executions of it. In head-land, worst-case scenarios tend to become <em>the</em> scenarios, presenting huge, faultily-calculated sums of net present real-land misery.<br /><br />Fear of the ordeals that play out in head-land is a hindrance, but the paralysis induced by the sheer weight of countless accumulated hypothetical propositions is crippling. Even riding high on the hog in real-land is no bulwark against the infinite (and infinitely bad) vicissitudes of head-land. Say you're earning a pretty sweet living playing the guitar in real-land. But what if you'd been born without arms? Or in a time before the invention of the guitar? Or in a time when you would've died of an infection before reaching age twelve? <em>Then</em> you sure wouldn't be enjoying yourself as much. And what if you lose your arms in a horrific fishing accident ten years down the line? Or if you suddenly forget what a guitar is? Or if you die of an infection anyway? Despite the fact that none of these dire possibilities have occurred &mdash; or are even likely to occur &mdash; they're nonetheless inflicting real-land pain from across the border.<br /><br />I call this phenomenon a blooming <em>what if</em> plant, beginning as the innocuous seed of a question &mdash; \"What if I hadn't done or encountered such and such earlier thing that proved to be a necessary condition to something from which I enjoy and profit now?\" &mdash; and sprouting rapidly into a staggeringly complex organism, its branches splitting into countless smaller branches which split into yet more branches themselves. More perniciously, this also happens in a situation-specific manner; namely, in situations whose sub-events are particularly unpredictable. The classic example would be approaching the girl one likes in middle school; the possible outcomes are so many and varied, at least in the approacher's mind, that the what-ifs multiply dizzyingly and collectively become unmanageable, especially if his strategy is to prepare responses to all of them. It's no accident that those never-get-the-girl mopes in movies spend so much time vainly rehearsing conversations in advance, and that doing the same in life never, ever works. There's a line to be drawn between the guys in junior high who could talk the girls up effortlessly and the ones who seized up merely contemplating it. I suspect the difference has to do with the ratio of one's relative presence in head-land versus that in real-land.<br /><br />I would submit that, whatever their results, the dudes who could walk right up to those girls and try their luck habitually spent a lot more time in real-land than in head-land. They probably weren't sitting around, eyes fixed on their own navels, building elaborate fictions of inadequacy, embarrassment and ridicule; if they were, wouldn't they have been just as paralyzed? They appeared to operate on a mental model that either didn't conjure such dire possibilities or, if it did, didn't allow them any decisionmaking weight. \"So the chick could turn me down? So what? What if space aliens invade and destroy the Earth? I don't know what'll happen until I try.\"<br /><br />This brings up something else Wallace wrote and thought about &mdash; equivalent verbs for him, I think &mdash; though not, as I dimly recall, in <em>Infinite Jest</em>. In his sports journalism, of which he wrote some truly stunning pieces, he kept looping back to the issue of the correlation and possible causal connection between great athletes' brilliant physical performance and their astonishing unreflectiveness in conversation and prose. I'm thinking of Wallace's profile of Michael Joyce, a not-quite-star tennis player who has no knowledge or interests outside the game and couldn't even grasp the thundering sexual innuendo on a billboard ad. I'm thinking of his review of Tracy Austin's autobiography, a cardboard accretion of blithe assertions, unreached-yet-strongly-stated conclusions and poster-grade sports clich&eacute;s. What must it be like, Wallace asked, to speak or hear prases like \"step it up\" or \"gotta concentrate now\" and have them actually <em>mean something</em>? Is the sports star's nonexistent inner life not the price they pay for their astonishing athletic gift, but rather its very <em>essence</em>?</p>\n<p>One can say many things about bigtime athletes, but that they live in their heads is not one of them. I'd wager that you can't find a group that spends <em>less</em> time in head-land than dedicated athletes; they are, near-purely, creatures of real-land. The dudes who could go right up to the ladies in seventh grade seemed to be, in kind if not in magnitude, equally real-land's inhabitants. It comes as no surprise that so many of them played sports and weren't often seen with books. And not only were they undaunted by the danger (possibly because unperceived) of crushing humiliation, I'd imagine they were inherently less vulnerable to crushing humiliation in the first place, because crushing humiliation, like theoretical arm loss and imagined endeavor failure, is a head-land phenomenon. Humiliation is what makes a million other-people-are-thinking-horrible-thoughts-about-me flowers bloom &mdash; but only in <em>head-land</em>. The impact can only hit so hard if one doesn't spend much time there, because in real-land, direct access to another's thoughts is impossible. In head-land, one can't, as their creator, help <em>but</em> have direct access to everyone else's thoughts, and thus if a head-land resident believes everyone's disparaging him, everyone <em>is</em> disparaging him. \"So what if they're thinking ill of me?\" a full-time real-land occupant might ask. \"I can't know that for sure, and besides, they're probably not; how often do <em>you</em> think, in a way that actually affects them, about someone who's been recently embarrassed?\"</p>\n<p>But there's a problem: saying someone \"lives in their head\" is more or less synonymous with calling them intelligent. \"Hey, look at that brainy scientist go by, lost in thought; the fellow <em>lives</em> in his head!\" As for professional athletes, well... let's just acknowledge the obvious, that professional athleticism is <em>not</em> a byword for advanced intellectual capacity. (Wallace once lamented the archetypal \"basketball genius who cannot read\".) So there's clearly a return to time spent in head-land, and arguing for the benefits of head-land occupancy even to nonintellectuals is a trivial task. How, for instance, would we motivate ourselves without reference to head-land? How could we envision possibilities and thus choose which ones we'd like to realize without seeing them in head-land? Surely even the most narrowly-focused, football-obsessed football player has watched himself polish a Super Bowl ring in head-land. Why else would he strive for that outcome? Head-land is where our fantasies happen, where our goals are formulated, and is that a function we can do without?</p>\n<p>Hokey as it sounds, I do consider myself a somewhat \"goal-oriented\" person, in that I burn a lot of time and mental bandwidth attempting to realize certain head-land states. But, as the above paragraphs reveal, I often experience head-land backfire in the form of discouraging negative imaginings rather than encouraging positive ones. Here I could simply pronounce that I will henceforth only use head-land for envisioning the positive, but it's not quite that easy; I can think of quite a few badly-ending head-land scenarios that I'm happy to experience there &mdash; and only there &mdash; and take into account when making real-land decisions. The head-land prediction that I'll get splattered if I walk blindly into traffic comes to mind.<br /><br />And I'm one of the <em>less</em> head-land-bound people I know! I wouldn't be writing this post if I didn't struggle with the damned place, but traits like my near-inability to write fiction suggest that I don't gravitate toward it as strongly as some. Still, I feel the need to minimize the problems that spring forth from head-land without converting myself into an impulsive dumb beast. The best compromise I have at the moment is not necessarily to stem the flow of predictions out of head-land, but simply to&nbsp;<em>ignore</em> the bulk of their content, to crank down their resolution by 90% or so. Since the accuracy of our predictions drops so precipitously as they extend forward in time and grow dense with specifics, they'd mostly lose noise. Noise simply misleads, and attenuating what misleads is the point of this exercise.<br /><br />There are countless practical ways to implement this. One quick-and-dirty hack to dial down head-land's effect on your real-land calculations is to only pay attention to head-land's shadow plays to the extent that they're near your position in time. If they have to do with the distant future, only consider their broadest outlines: the general nature of the position you envision yourself occupying in twenty years, for instance, rather than the specific event of your buxom assistant bringing you just the right roast of coffee. If they have to do with the past, near or distant, just chuck 'em; head-land models tend to run wild with totally irrelevant oh-if-only-things-had-been-different retrodictions, which are supremely tempting but ultimately counterproductive. (As one incisive BEK cartoon had a therapist say, \"Woulda, shoulda, coulda &mdash; next!\") If they have to do with the near future, they're more valuable, and the nearer the future they deal with, the better you would seem to do to pay attention to them.<br /><br />The concept behind this is one to which I've been devoting thought and practice lately: small units of focus. Alas, this brings us to another set of bromides, athletic and otherwise. One step at a time. Just you and the goal. Break it down. Don't bite off more than you can chew. The disturbing thing is how well operating on such a short horizon seems to work, at least in certain contexts. I find I actually <em>do</em> run better when I think only of the next yard, write better when I think only of the next sentence and talk better when I think only of the subject at hand. When my mind tries instead to load the entire run, the entire essay or the entire conversation, head-land crashes. (This applies to stuff traditionally thought of as more passive as well: I read more effectively when I focus on the sentence, watch films more effectively when I focus on the shot, listen to music more effectively when I focus on the measure.) When Wallace writes about \"the head not Abiding in the Present but hopping the wall and doing a recon and then returning with unendurable news\" and \"[hunkering] down in the space between each heartbeat and [making] each heartbeat a wall and [living] in there\", I think this is what he means.<br /><br />Ignoring all head-land details past a certain threshold, de-weighting head-land predictions with their distance in the future and focusing primarily on small, discrete-seeming, temporally proximate units aren't just techniques to evade internal discouragement, either; they also guard against the perhaps even more sinister (and certainly sneakier) forces of complacency. While failure in head-land can cause one to pack it in in real-land, <em>success</em> in head-land, which is merely a daydream away, can prevent one from even <em>trying</em> in real-land. I can't put it better than <a href=\"http://www.paulgraham.com/love.html\">Paul Graham</a> does: \"If you have a day job you don't take seriously because you plan to be a novelist, are you producing? Are you writing pages of fiction, however bad? As long as you're producing, you'll know you're not merely using the hazy vision of the grand novel you plan to write one day as an opiate.\"<br /><br />This is why I'm starting to believe that coming up with great ideas in head-land and then executing them in real-land may be a misconceived process, or at least suboptimally conceived one. How many projects have been forever delayed because the creator decided to wait until the \"idea\" was just a little bit better, or, in other words, until the head-land simulation came out a little more favorably? It's plausible that this type of stalling lies at the heart of procrastination: one puts the job off until tomorrow because the head-land model doesn't show it as turning out perfect today, never mind the facts that <strong>(a)</strong> it'll never be perfect, no matter when it's started and <strong>(b)</strong> it's unlikely to turn out better with less time available for the work, especially given the unforeseen troubles and opportunities. I provisionally believe that this <em>a priori</em>, head-land idea stuff can be profitably be replaced with small-scale real-land exploratory actions that demand little in the way of time or resource investment. Rather than executing steps one through one hundred in head-land, execute step one in real-land; if nothing else, the data you get in response will be infinitely more reliable and more useful in determining what step two should involve. Those dudes in middle school knew this on some basic level: you just gotta go up to the girl and say something. It's the only gauge you have of whether you should say something more, and of what that something should be. It's all about hammering in the thin end of the wedge.<br /><br />For what it's worth, I've found this borne out in what little creation I've done thus far. I've reached the point of accepting that I don't know &mdash; <em>can't</em> know &mdash; how a project's going to turn out, since each step depends on the accumulated effects of the steps that preceded it. All I can do is get clear on my vague, broad goal and put my best foot forward, keeping my mind open to accept all relevant information as it develops. When I started my first radio show, I had a bunch of head-land projections about how the show would be, but in practice it evolved away from them in real-land rather sharply &mdash; and, I think, for the better. When I started another one a year later, I knew to factor in this unforeseeable real-land evolution from the get-go and thus kept my ideas about what it was supposed to me broad, flexible and small in number, letting the events of real-land fill in the details as they might. With a TV project only just started, I've tried my hardest to stay out of head-land as much as possible; the bajillion variables involved would send whatever old, buggy software my head-land modeler uses straight to the Blue Screen of Death. (Yes, our brains are Windows-based.) Even if it didn't crash, it's not as if I'd be getting sterling predictions out of it. I have, more grandly speaking, come to accept much more of the future's unknowability than once I did; that goes double for the future of my own works. Modeling a successful work in head-land now seems a badly flawed strategy, to be replaced by taking small steps in real-land and working with its response.<br /><br />I could frame this as another rung in thet climb from a thought-heavier life to an action-heavier life. of approaching and affecting the world <em>as it exists in real-land</em> rather than as it is imagined in head-land. I've nevery been what one would call an idealist and I suppose I'm drawing no closer to that label. Some regard flight from idealism as flight toward cynicism, but it's cynicism I'm been fleeing as well, perhaps even primarily; what is cynicism, after all, but a mistaken reliance on pessimistic head-land conclusions?</p>\n<table style=\"height: 32px;\" border=\"0\" cellspacing=\"0\" cellpadding=\"5\" width=\"57\">\n<tbody>\n<tr align=\"left\">\n<td colspan=\"2\" bgcolor=\"#ffffff\">&nbsp;</td>\n</tr>\n</tbody>\n</table>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CbJ87ArBPncWoj4uf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 50, "baseScore": 34, "extendedScore": null, "score": 0.000104, "legacy": true, "legacyId": "1380", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-22T20:41:41.912Z", "modifiedAt": null, "url": null, "title": "An observation on cryocrastination", "slug": "an-observation-on-cryocrastination", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:13.601Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AndrewH", "createdAt": "2009-03-02T15:31:43.543Z", "isAdmin": false, "displayName": "AndrewH"}, "userId": "FgKt6dBPfyiku5Qh8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xLcW4HRubQvKy9cpb/an-observation-on-cryocrastination", "pageUrlRelative": "/posts/xLcW4HRubQvKy9cpb/an-observation-on-cryocrastination", "linkUrl": "https://www.lesswrong.com/posts/xLcW4HRubQvKy9cpb/an-observation-on-cryocrastination", "postedAtFormatted": "Wednesday, July 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20observation%20on%20cryocrastination&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20observation%20on%20cryocrastination%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxLcW4HRubQvKy9cpb%2Fan-observation-on-cryocrastination%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20observation%20on%20cryocrastination%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxLcW4HRubQvKy9cpb%2Fan-observation-on-cryocrastination", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxLcW4HRubQvKy9cpb%2Fan-observation-on-cryocrastination", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 825, "htmlBody": "<p>Why do people cryocrastinate? The most common explanation I&rsquo;ve heard from intelligent people for not getting cryonics is that the money is better spent on some altruistic cause. By itself there is nothing wrong with this belief, but irrationality lies near.<br /> <br />Before I continue, I am not here to argue that cryonics works or not. That has been done <a href=\"http://www.overcomingbias.com/2009/03/break-cryonics-down.html\">before</a>. From this point on, I will assume cryonics derives expected utility from it giving a reasonable chance of continuing life past many currently terminal events, with life being a <a href=\"/lw/xy/the_fun_theory_sequence/\">valuable thing</a>.<br /> <br />We begin with a quick overview of the cost of cryonics. Let us break our cost analysis into two parts: acquisition of the cryonics and life insurance contracts and maintenance of these contracts.<a id=\"more\"></a><br /><br />First, acquisition. The main costs here is the time that could be invested in other activities. I would estimate a reasonably organized person could get it done with 20 hours of continuous work and $200 (all costs USD) upfront. Let us say this is $600 worth of costs. For people in the US, see <a href=\"http://www.rudihoffman.com/\">rudi hoffman</a>, he will do pretty much everything. <br /> <br />Second, maintenance. For myself who lives in a non-silly country like New Zealand, life insurance is $10 per month, and membership fees for e.g. Cryonics Institute is $10 a month, totaling 70 cents a day. Let us say it is $1 a day for places such as the US, as insurance costs an arm and a leg there.<br /> <br />So cryonics costs one dollar a day. Put this way, it doesn't seem much. This comes out to be two starbucks coffees a week. Let me repeat that: in the long term, cryonics costs TWO STARBUCKS COFFEES A WEEK. Very few people can say they cannot optimize themselves so as to have $1 a day more disposable income.<br /> <br />As <a href=\"/lw/wq/you_only_live_twice/\">someone</a> once put it to Eliezer &ldquo;No, really, that's ridiculous.&nbsp; If that's true then my decision isn't just determined, it's overdetermined.&rdquo;. I agree, cryonics is cheap.<br /> <br />Let us now consider the following two beliefs from someone who has a favorite cause, in which donating money gives lots of positive utility. The amount we are considering to donate is exactly the amount we would think about using to get cryonics:<br /> <br />1: Compared to cryonics, which is almost entirely selfish spending, my cause can benefit far more from the money than spending it on cryonics. <br /><br />2:&nbsp; Below a certain amount, the contribution I could make to my cause is not that helpful. It becomes merely a drop in the bucket, of negligible utility.<br /> <br />The first point implies that if people are to optimize themselves (and most people can optimize themselves to get $1 more a day), they should optimize themselves to donate more money to their favorite cause. The second point implies they don&rsquo;t see the small donations as useful to their cause, so they shouldn&rsquo;t bother to optimize themselves. The net result is that you decide it is better to keep on drinking that cup of coffee.<br /> <br />So cryonics is sidestepped in a conversation by point one, at most activating peoples thoughts to donate more. But since the change in donation is so small, the effort is too much that ultimately they do nothing. <br /> <br />There is an assumption in the second point above that if the amount of money you donate is too small, you wont bother donating. However, you <em>do</em> know logically, at a System 2 level, that even a small amount of money counts; $30 a month <em>is</em> useful to any cause. But you usually don&rsquo;t get the pressing feeling that you must optimize your life to donate just a bit more money from logical arguments. <br /> <br />System 1 thought processes see your dollar coin being lost in a sea of dollar coins, so why bother? Of course, it does not help that you might have to give up things that make you personally happy. The outcome is the same: you do not optimize yourself and you go on drinking that cup of coffee.<br /> <br />It is harder to let go of money than it <a href=\"/lw/65/money_the_unit_of_caring/\">should be</a>, but in this case you should simply shut up and multiply. If you believe you can contribute more money to a cause, contribute, it will help, but don&rsquo;t use your cause as an excuse for cryocrastinating. If you believe you have no more money to give to your cause, look to see at what you spend on yourself personally, can it really be worth more than the small cost of cryonics in the long run? Remember, without cryonics death is truly game over.<br /> <br />So for the <a href=\"/lw/fk/survey_results/\">majority of you</a> still not signed up, consider carefully: how more optimized can you be?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xLcW4HRubQvKy9cpb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 12, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "1429", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K4aGvLnHvYgX9pZHS", "yKXKcyoBzWtECzXrE", "ZpDnRCeef2CLEFeKM", "ZWC3n9c6v4s35rrZ3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-23T04:30:48.788Z", "modifiedAt": null, "url": null, "title": "The Price of Integrity", "slug": "the-price-of-integrity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:25.398Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Aurini", "createdAt": "2009-03-19T04:39:40.233Z", "isAdmin": false, "displayName": "Aurini"}, "userId": "5fNCGeJcDQCjxEjnD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MWGetevxNdzjRcHuM/the-price-of-integrity", "pageUrlRelative": "/posts/MWGetevxNdzjRcHuM/the-price-of-integrity", "linkUrl": "https://www.lesswrong.com/posts/MWGetevxNdzjRcHuM/the-price-of-integrity", "postedAtFormatted": "Thursday, July 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Price%20of%20Integrity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Price%20of%20Integrity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMWGetevxNdzjRcHuM%2Fthe-price-of-integrity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Price%20of%20Integrity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMWGetevxNdzjRcHuM%2Fthe-price-of-integrity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMWGetevxNdzjRcHuM%2Fthe-price-of-integrity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1800, "htmlBody": "<p class=\"MsoNormal\"><strong>Related Posts:</strong> <a href=\"/lw/v2/prices_or_bindings/\">Prices or Bindings?</a></p>\n<p class=\"MsoNormal\">On the evening of August 14<sup>th</sup>, 2006 a pair of Fox News journalists, Steve Centanni and Olaf Wiig were seized by Islamic militants while on assignment in Gaza City. <span>&nbsp;</span>Nothing was heard of them for nine days until a group calling themselves the Holy Jihad Brigades took credit for the kidnappings.<span>&nbsp; </span>They issued an ultimatum, demanding the release of Muslims prisoners from American jails within a 72 hour time frame.<span>&nbsp; </span>Their demands were not met.</p>\n<p class=\"MsoNormal\">But then a few days later the journalists were allowed to go free... but not before they&rsquo;d been forced into converting to Islam at gunpoint, and had each videotaped a statement denouncing U.S. and Israeli foreign policy.</p>\n<p class=\"MsoNormal\">The war raged on.</p>\n<p class=\"MsoNormal\">A couple of kidnapped journalists is nothing new (certainly not three years after the fact) and aside from the happy ending this particular case wouldn&rsquo;t worth mentioning if not for a unique twist that occurred after they returned home.<span>&nbsp; </span>A fellow Fox News contributor, Sandy Rios, openly criticized the two men; she said that no true Christian would convert &ndash; falsely or otherwise &ndash; merely because they were threatened with death.<span>&nbsp; </span>As she later explained to <a href=\"http://www.youtube.com/watch?v=309MCU8TonE\">Bill Maher</a>:*<a id=\"more\"></a></p>\n<blockquote>\n<p class=\"MsoNormal\">My point was that Christians &ndash; I don&rsquo;t know what their faith is &ndash; but I&rsquo;m talking about Christians who responded to the story and said that they would have done the same thing...</p>\n<p class=\"MsoNormal\">Christ followers can&rsquo;t do that.<span>&nbsp; </span>We don&rsquo;t have that freedom.<span>&nbsp; </span>We have to profess Christ no matter what... Christianity is, by its very nature, radical.<span>&nbsp; </span>It is not normal or natural to lay down your life for a friend.<span>&nbsp; </span>It is not natural or normal to say &lsquo;I will not deny my faith even if you do cut my head off.'</p>\n</blockquote>\n<p class=\"MsoNormal\">I agree with her, and admire her courage for sticking with her convictions.<span>&nbsp; </span>If you buy into Christianity&rsquo;s metaphysical claims, then bearing false witness to your faith ought be considered a serious crime; not only does it show a pathological attachment to life (when eternal bliss lies just around the corner) furthermore, it completely ignores the core premises of Christianity, as well as the death of its founder.<span>&nbsp; </span>This very question split the early Church: whether or not those who'd become apostates [had renounced Christ] due to persecution at the hands of the Romans could ever be forgiven.<span>&nbsp; </span>There were some who said it should be <em>forgivable</em> (after the proper penance, of course), but no one argued that it ought be condoned.<span>&nbsp; </span></p>\n<p class=\"MsoNormal\">I&rsquo;d wager that the guys in Guantanamo take the question just as seriously.<span>&nbsp; </span>Not every religion has seen it that way, however.</p>\n<p class=\"MsoNormal\">*<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>*<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>*</p>\n<p class=\"MsoNormal\">In 1492 the joint Spanish monarchs, Isabella I and Ferdinand II, issued the Alhambra Decree upon completing the <em>reconquista </em>of Spanish land from the Moors.<span>&nbsp; </span>They gave the local Jewish population three options: leave the kingdom, convert to Christianity, or face death.<span>&nbsp; </span>While the majority left for Portugal, a significant number stayed behind and paid lip service to Christianity while continuing to practice Judaism in secret.</p>\n<p class=\"MsoNormal\">This was hardly the first time something like this had happened in Europe, and given Judaism&rsquo;s tendencies towards isolationism, as well as their lack of evangelical tradition, it should come as no surprise that they didn&rsquo;t give two figs about lying to Christians.<span>&nbsp; </span>The Rabbinical body even has separate terms for meshumadim (those who&rsquo;d convert voluntarily) and the anusim (those who&rsquo;d converted under duress).<span>&nbsp; </span>The latter wasn&rsquo;t encouraged &ndash; at least, not by most Rabbis &ndash; but nonetheless it was accepted.</p>\n<p class=\"MsoNormal\">Now for the question that all of this was leading up to: what ought an Atheist to do in this situation?</p>\n<p class=\"MsoNormal\">*<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>*<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>*</p>\n<p class=\"MsoNormal\">In March of 2007 Iran took 15 British servicemen hostage, alleging that their ship crossed into Iranian waters.<span>&nbsp; </span>They were eventually returned safely to Britain, but for some time they were paraded around on Iranian television, denouncing the country they&rsquo;d sworn to protect.<span>&nbsp; </span>Quite frankly, this was cowardice.</p>\n<p class=\"MsoNormal\">These were sailors and soldiers in Her Majesty&rsquo;s Armed Forces.<span>&nbsp; </span>That they had they sworn an Oath of fealty to the Queen is the least of the reasons they should have kept silent.<span>&nbsp; </span>Far beyond that, they trusted sufficiently in the rightness of British foreign policy that they were <em>willing to take another human&rsquo;s life</em>.<span>&nbsp; Let me repeat that: they so strongly believed int he rightness (or at least the 'Less Wrongness') of British foreign policy that they were ready to kill for it.&nbsp; </span>I am not suggesting that there is something inherently immoral about serving in the military; I spent six years there myself, and my paperwork&rsquo;s still up to date for when the Chinese land on the West Coast.<span>&nbsp; </span>What I am saying is that if you&rsquo;re willing to kill for a cause you&rsquo;d better be willing to die for it, too.<span>&nbsp; </span>Otherwise...</p>\n<p class=\"MsoNormal\">Now admittedly I don&rsquo;t know the whole situation.<span>&nbsp; </span>Maybe the Iranians were threatening to murder a dozen children if these servicemen didn&rsquo;t read the scripts they were given.<span>&nbsp; </span>There may have been some other extenuating circumstances.<span>&nbsp; </span>But having a good reason to act like a coward &ndash; even a <em>really </em>good reason &ndash; does not transform cowardice into heroism.<span>&nbsp; </span>It only transforms cowardice into adequacy.<span>&nbsp; It might be necessary to violate your morals at times, but it is not something to be proud of.&nbsp; And yet, despite that</span>...</p>\n<p class=\"MsoNormal\">You know what?&nbsp; <a href=\"http://thelastpsychiatrist.com/2007/05/the_wrong_lessons_of_iraq.html\">The Last Psychiatrist</a> said it much better than I ever will; these are his words on the topic:</p>\n<blockquote>\n<p class=\"MsoNormal\">I'm sure those soldiers were thinking, \"look, I know who I am, I know I'm not a coward, I'm not helping the Iranians, but I have to do whatever is necessary to get out of this mess.\"&nbsp; What they are saying is that they can declare who they are, and what they do has no impact on it.&nbsp; \"I am a hero, regardless of how I act.\"&nbsp; That's the narcissist fallacy...</p>\n<p class=\"MsoNormal\">But here's the thing: when they returned home to Britain, they were heralded as heroes <em>by other people</em>.&nbsp; Including the British government. &nbsp; Based on what?&nbsp; They didn't actually do anything; heroism isn't simply living through a bad experience.&nbsp; Well, of course: based on the fact that they <em>are</em> heroes who had to pretend to be something else.</p>\n<p class=\"MsoNormal\">That's the narcissist's tautology: you are what you say you are because you said you are.&nbsp; What makes it an example of our collective narcissism is that we agree--&nbsp; we want it to be true that they, and we, can declare an identity.</p>\n</blockquote>\n<p class=\"MsoNormal\">Screw narcissism.<span>&nbsp; </span>How you act is who you are.</p>\n<p class=\"MsoNormal\">Maybe these men traded their integrity for a bloody good reason; let&rsquo;s grant that for sake of argument, because the quality of their moral fibre is irrelevant.<span>&nbsp; </span>Regardless of what they traded it for, they still traded it; whatever they got came at a price.</p>\n<p class=\"MsoNormal\">How many Utilons is worth to lie to a bunch of savages?<span>&nbsp; </span>The Spanish Jews decided &lsquo;not very many,&rsquo; and they prospered for a time.<span>&nbsp; </span>But they only had a short-term advantage; the idea of lying during a baptism was inconceivable to the Christian populace back then.<span>&nbsp; </span>It didn&rsquo;t take long for them to catch on, however, and by that time the Spanish Inquisition was hitting its stride...</p>\n<p class=\"MsoNormal\">Modern day Muslims fundamentalists, on the other hand &ndash; whether or not they know how to use toilet paper &ndash; are not stupid.<span>&nbsp; </span>They know perfectly well that these confessions are forced; I&rsquo;d even say that there&rsquo;s a good chance they&rsquo;re familiar with the Koran&rsquo;s prohibition against forced conversions, and the fact that these aren&rsquo;t *really* conversions is their legalistic loophole (they&rsquo;re generally not that concerned about converting us, anyways; they just want non-Muslims to be second class citizens under a Caliphate, is all - I've even heard anecdotes that Egyptians are more offended by evolution than Atheism).<span>&nbsp; </span>Quite frankly, they&rsquo;re winning more than enough converts in our prisons and our ghettos; they don&rsquo;t need a couple more journalists.</p>\n<p class=\"MsoNormal\">So what is the point of it?<span>&nbsp; </span>Quite simply, it&rsquo;s the point of all terrorism (and all war, for that matter): they&rsquo;re framing the conversation, creating a perception which becomes reality, winning the war before entering the battlefield.<span>&nbsp; </span>It&rsquo;s theatre.<span>&nbsp; </span>The point is to demoralize; to expose the West as hypocritical and cowardly; to drive us into panics, sway our elections, to make us fearful.<span>&nbsp; </span>They&rsquo;re doing it to show us that terrorism works.<span>&nbsp; </span>And so far it's working pretty well.<span>&nbsp; </span></p>\n<p class=\"MsoNormal\">The British servicemen surrendered peacefully to the Iranians out of fear of causing a national incident &ndash; as if capturing and detaining another country&rsquo;s military forces isn&rsquo;t already a national incident.</p>\n<p class=\"MsoNormal\"><em>That </em>is the long-term price of selling your integrity.</p>\n<p class=\"MsoNormal\">*<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>*<span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>*</p>\n<p class=\"MsoNormal\">Few, if any, of the members of Less Wrong are here for the sake of expediency.<span>&nbsp; </span>We say that Rationalists should Win &ndash; and of course they should! &ndash; but if all we cared about was winning (in the short term, proximate sense) then we&rsquo;d be reading Pick-Up-Artist books [exclusively - this article was written before the recent debate], schmoozing the corporate latter, or pumping opiates into our veins.<span>&nbsp; </span>The reason we dedicate so much time to this site is because we hold up the values of Truth, Knowledge, and Humanity as part of a higher purpose.<span>&nbsp; </span>The only way for humanity to Win in the long term is if everybody is trained in the mental martial arts.<span>&nbsp; </span>We train our minds, not to save ourselves, but to save the world.<span>&nbsp; </span>We all assert these values implicitly in our writing, but writing isn&rsquo;t enough; when the rubber hits the road, if we can&rsquo;t walk the talk, then we might as well have plugged in to the heroin drip.<span>&nbsp; </span>The ideals we spoke of will be nothing but empty words from empty men.</p>\n<p class=\"MsoNormal\">Penguins will crowd together at the edge of the iceberg, pushing and shoving, until one of them falls in.<span>&nbsp; </span>After a few moments, if a killer whale hasn&rsquo;t eaten the unlucky test subject, then the rest will jump in, knowing it&rsquo;s safe.<span>&nbsp; </span>Our species doesn&rsquo;t work like that; our species needs Heroes.<span>&nbsp; </span>Everyone here has already taken on the mantle of heroism, dedicating a significant portion of our time to trying to improve the world situation.<span>&nbsp; </span>It's easy to be noble when the sun is shining and the weather is warm.<span>&nbsp; </span>When winter comes that&rsquo;s when we&rsquo;ll really see what we&rsquo;re really made of.</p>\n<p class=\"MsoNormal\">A single choice, a rationalization born out of cowardice, can undermine all that we are, and all that we stand for</p>\n<p class=\"MsoNormal\">So what should an Atheist - more than just a nihilist - do when the terrorist has a gun to his head?</p>\n<p class=\"MsoNormal\">This one would tell them to pull the trigger.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\"><em>No.<span>&nbsp; </span>Not even in the face of Armageddon.<span>&nbsp; </span>Never compromise. </em>~Rorchach<em></em></p>\n<p class=\"MsoNormal\"><em>There... are... four... lights! </em>~Capt. Jean-Luc Picard<em></em></p>\n<p class=\"MsoNormal\"><em>&nbsp;</em></p>\n<p class=\"MsoNormal\">*Minor syntactical edits; spoken and written English are different mediums.</p>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">[If anyone can suggest appropriate tags for this article I'd be much obliged]</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MWGetevxNdzjRcHuM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": -3, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "555", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K2c3dkKErsqFd28Dh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-23T05:15:45.853Z", "modifiedAt": null, "url": null, "title": "Are calibration and rational decisions mutually exclusive? (Part one)", "slug": "are-calibration-and-rational-decisions-mutually-exclusive-0", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:58.400Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cyan", "createdAt": "2009-02-27T22:31:08.528Z", "isAdmin": false, "displayName": "Cyan"}, "userId": "eGtDNuhj58ehX9Wgf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qDucvMYty5gdumHDB/are-calibration-and-rational-decisions-mutually-exclusive-0", "pageUrlRelative": "/posts/qDucvMYty5gdumHDB/are-calibration-and-rational-decisions-mutually-exclusive-0", "linkUrl": "https://www.lesswrong.com/posts/qDucvMYty5gdumHDB/are-calibration-and-rational-decisions-mutually-exclusive-0", "postedAtFormatted": "Thursday, July 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20calibration%20and%20rational%20decisions%20mutually%20exclusive%3F%20(Part%20one)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20calibration%20and%20rational%20decisions%20mutually%20exclusive%3F%20(Part%20one)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqDucvMYty5gdumHDB%2Fare-calibration-and-rational-decisions-mutually-exclusive-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20calibration%20and%20rational%20decisions%20mutually%20exclusive%3F%20(Part%20one)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqDucvMYty5gdumHDB%2Fare-calibration-and-rational-decisions-mutually-exclusive-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqDucvMYty5gdumHDB%2Fare-calibration-and-rational-decisions-mutually-exclusive-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 454, "htmlBody": "<p>I'm planning a two-part sequence with the aim of throwing open the question in the title to the LW commentariat. In this part I&rsquo;ll briefly go over the concept of calibration of probability distributions and point out a discrepancy between calibration and Bayesian updating.</p>\r\n<p>It's a tenet of rationality that we should seek to be well-calibrated. That is, suppose that we are called on to give interval estimates for a large number of quantities; we give each interval an associated <a href=\"http://wiki.lesswrong.com/wiki/Bayesian_probability\">epistemic probability</a>.&nbsp;We declare ourselves well-calibrated if the relative frequency with which the quantities fall within our specified intervals matches our claimed probability.&nbsp;(The <a href=\"http://yudkowsky.net/rational/technical\">Technical Explanation of Technical Explanations</a> discusses calibration in more detail, although it mostly discusses discrete estimands, while here I'm thinking about continuous estimands.)</p>\r\n<p><a href=\"http://en.wikipedia.org/wiki/Frequency_probability\">Frequentists</a>&nbsp;also produce interval estimates, at least when \"random\" data is available. A frequentist \"<a href=\"http://en.wikipedia.org/wiki/Confidence_interval\">confidence interval</a>\" is really a function from the data and a user-specified confidence level (a number from 0 to 1) to an interval. The confidence interval procedure is \"valid\" if in a hypothetical infinite sequence of replications of the experiment, the&nbsp;relative frequency with which the realized intervals contain the estimand is equal to the confidence level. (Less strictly, we may require \"greater than or equal\" rather than \"equal\".) The similarity between&nbsp;valid confidence coverage and well-calibrated epistemic probability intervals is evident.</p>\r\n<p>This similarity suggests an approach for specifying non-informative prior distributions, i.e., we require that such priors yield posterior intervals that are also valid confidence intervals in a frequentist sense. This \"matching prior\" program does not succeed in full generality. There are a few special cases of data distributions where a matching prior exists, but by and large, posterior intervals can at best produce only asymptotically&nbsp;valid confidence coverage. Furthurmore, according to my understanding of the material, if your model of the data-generating process contains more than one scalar parameter, you have&nbsp;to pick one \"interest parameter\" and be satisfied with good confidence coverage for the marginal posterior intervals for that parameter alone. For approximate matching priors with the highest order of accuracy, a different choice of interest parameter usually implies a different prior.</p>\r\n<p>The upshot is that we have good reason to think that Bayesian posterior intervals will not be perfectly calibrated in general. I have good justifications, I think, for&nbsp;using the Bayesian updating procedure, even if it means the resulting posterior intervals are not as well-calibrated as frequentist confidence intervals. (And I mean <em>good </em>confidence intervals, not the obviously pathological ones.) But my justifications are grounded in an epistemic view of probability, and no committed frequentist would find them as compelling as I do. However, there is an argument for Bayesian posteriors over confidence intervals than even a frequentist would have to credit.&nbsp;That will be the focus of the second part.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qDucvMYty5gdumHDB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 5.097428671862449e-07, "legacy": true, "legacyId": "1431", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-23T11:15:54.647Z", "modifiedAt": null, "url": null, "title": "The Nature of Offense", "slug": "the-nature-of-offense", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:56.426Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QPqm5aj2meRmE7kR8/the-nature-of-offense", "pageUrlRelative": "/posts/QPqm5aj2meRmE7kR8/the-nature-of-offense", "linkUrl": "https://www.lesswrong.com/posts/QPqm5aj2meRmE7kR8/the-nature-of-offense", "postedAtFormatted": "Thursday, July 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Nature%20of%20Offense&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Nature%20of%20Offense%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQPqm5aj2meRmE7kR8%2Fthe-nature-of-offense%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Nature%20of%20Offense%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQPqm5aj2meRmE7kR8%2Fthe-nature-of-offense", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQPqm5aj2meRmE7kR8%2Fthe-nature-of-offense", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 815, "htmlBody": "<p><!-- /* Font Definitions */ @font-face {font-family:Wingdings; panose-1:5 0 0 0 0 0 0 0 0 0; mso-font-charset:2; mso-generic-font-family:auto; mso-font-pitch:variable; mso-font-signature:0 268435456 0 0 -2147483648 0;} @font-face {font-family:\u5b8b\u4f53; panose-1:2 1 6 0 3 1 1 1 1 1; mso-font-alt:SimSun; mso-font-charset:134; mso-generic-font-family:auto; mso-font-pitch:variable; mso-font-signature:3 680460288 22 0 262145 0;} @font-face {font-family:\"Cambria Math\"; panose-1:2 4 5 3 5 4 6 3 2 4; mso-font-charset:0; mso-generic-font-family:roman; mso-font-pitch:variable; mso-font-signature:-1610611985 1107304683 0 0 159 0;} @font-face {font-family:Calibri; panose-1:2 15 5 2 2 2 4 3 2 4; mso-font-charset:0; mso-generic-font-family:swiss; mso-font-pitch:variable; mso-font-signature:-1610611985 1073750139 0 0 159 0;} @font-face {font-family:\"\\@\u5b8b\u4f53\"; panose-1:2 1 6 0 3 1 1 1 1 1; mso-font-charset:134; mso-generic-font-family:auto; mso-font-pitch:variable; mso-font-signature:3 680460288 22 0 262145 0;} /* Style Definitions */ p.MsoNormal, li.MsoNormal, div.MsoNormal {mso-style-unhide:no; mso-style-qformat:yes; mso-style-parent:\"\"; margin-top:0in; margin-right:0in; margin-bottom:10.0pt; margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\u5b8b\u4f53; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} a:link, span.MsoHyperlink {mso-style-priority:99; color:blue; mso-themecolor:hyperlink; text-decoration:underline; text-underline:single;} a:visited, span.MsoHyperlinkFollowed {mso-style-noshow:yes; mso-style-priority:99; color:purple; mso-themecolor:followedhyperlink; text-decoration:underline; text-underline:single;} .MsoChpDefault {mso-style-type:export-only; mso-default-props:yes; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\u5b8b\u4f53; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} .MsoPapDefault {mso-style-type:export-only; margin-bottom:10.0pt; line-height:115%;} @page Section1 {size:8.5in 11.0in; margin:1.0in 1.0in 1.0in 1.0in; mso-header-margin:.5in; mso-footer-margin:.5in; mso-paper-source:0;} div.Section1 {page:Section1;} /* List Definitions */ @list l0 {mso-list-id:365721464; mso-list-template-ids:905893616;} @list l0:level1 {mso-level-number-format:bullet; mso-level-text:\uf0b7; mso-level-tab-stop:.5in; mso-level-number-position:left; text-indent:-.25in; mso-ansi-font-size:10.0pt; font-family:Symbol;} ol {margin-bottom:0in;} ul {margin-bottom:0in;} --></p>\n<!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin;} --> <!--[endif]-->\n<p class=\"MsoNormal\">Recently, an extended discussion has taken place over the fact that a portion of comments here were found to be offensive by some members of this community, while others denied their offensive nature or professed to be puzzled by why they are considered offensive. Several possible explanations for why the comments are offensive have been advanced, and solutions offered based on them:</p>\n<ul>\n<li><a href=\"/lw/134/sayeth_the_girl/\">to be thought of, talked about as, or treated like a non-person</a> (Alicorn)</li>\n<li><a href=\"/lw/13g/outside_analysis_and_blind_spots/\">analysis of behavior that puts the reader in the group being analyzed, and the speaker outside it</a> (orthonormal)</li>\n<li><a href=\"/lw/13j/of_exclusionary_speech_and_gender_politics/\">exclusion from the intended audience</a> (Eliezer)</li>\n</ul>\n<p class=\"MsoNormal\">Each of these explanations seems to have an element of truth, and each solution seems to have a chance of ameliorating the problem. But even though the discussion has mostly died down, we appear far from reaching an agreement, and I think one reason may be the lack of a general theory of the phenomenon of \"offense\", in the sense of giving and taking offense, that we can use to explain what has happened, so all of the proposed explanations and solutions feel somewhat arbitrary and unfair.</p>\n<p class=\"MsoNormal\"><a id=\"more\"></a>(I think <a href=\"http://www.slate.com/id/2202303\">this article</a> has it mostly right, but I&rsquo;ll give a much shorter account since I can skip the background evo psych info, and I&rsquo;m not being paid by the word. :)</p>\n<p class=\"MsoNormal\">Let&rsquo;s consider what other behavior are often considered offensive and see if we can find a pattern:</p>\n<ul>\n<li>use of vulgar language (where it's not customarily used)</li>\n<li>failing to address someone by their honorary titles</li>\n<li>not affording someone their customary privileges</li>\n<li>to impugn someone&rsquo;s beauty, intelligence, talent, morality, honor, ancestry, etc.</li>\n<li>making a joke at someone&rsquo;s expense</li>\n</ul>\n<p class=\"MsoNormal\">What do all these have in common? Hint: the answer is quite ironic, given the comment that first triggered this whole fracas.</p>\n<blockquote>\n<p class=\"MsoNormal\">most people here don't value social status enough and (especially the men) don't value having sex with extremely attractive women that money and status would get them</p>\n</blockquote>\n<p class=\"MsoNormal\">As you may have guessed by now, I think the answer is <a href=\"http://www.britannica.com/EBchecked/topic/551450/social-status\"><em>status</em></a>. Specifically, to give offense is to imply that a person or group has or should have low status. Taking offense then becomes easy to explain: it&rsquo;s to defend someone&rsquo;s status from such an implication, out of a sense of either fairness or self-interest. Let&rsquo;s go back to the three hypotheses I collected and see if this theory can cover them as special cases.</p>\n<p class=\"MsoNormal\">&ldquo;<em>to be thought of, talked about as, or treated like a non-person</em>&rdquo; Well, to be like a non-person is clearly to have low status.</p>\n<p class=\"MsoNormal\">&ldquo;<em>analysis of behavior that puts the reader in the group being analyzed, and the speaker outside it</em>&rdquo; A typical situation in which one group analyzes the behavior of another is a scientific study. In such a study, the researchers usually have higher status than the subjects being studied. But even to offer a casual analysis of someone else&rsquo;s behavior is to presume more intelligence, insight, or wisdom than that person.</p>\n<p class=\"MsoNormal\">&ldquo;<em>exclusion from the intended audience</em>&rdquo; To be excluded from the intended audience is to be labeled an outsider by implication, and outsiders typically have lower status than insiders.</p>\n<p class=\"MsoNormal\">But to fully understand why this particular comment is especially offensive, I think we have to consider that it (as well as many PUA discussions) specifically advocates (or appears to advocate) treating women as sex objects instead of potential romantic partners. Now think of the status difference between a sex object and a romantic partner...</p>\n<h4 class=\"MsoNormal\">Ethical Implications</h4>\n<p class=\"MsoNormal\">Usually, one avoids giving offense by minding one&rsquo;s audience and taking care not to use any language that might cause offense to any audience member. This is very easy to do one-on-one, pretty easy in a small group, hard in front of a large audience (case in point: <span><a href=\"http://en.wikipedia.org/wiki/Lawrence_Summers#Differences_between_the_sexes\">Larry Summers&rsquo;s infamous speech</a>), and almost impossible on an Internet forum with a large, diverse, and invisible audience, unless one simply avoids talking about everything that might possibly have anything to do with anyone&rsquo;s status. </span></p>\n<p class=\"MsoNormal\"><span>Still, that doesn&rsquo;t mean that we shouldn&rsquo;t try to avoid giving offense when we can do so without affecting the point that we&rsquo;re making, or consider skipping a minor point if it necessarily gives offense. <span>&nbsp;</span>After all, to lower someone&rsquo;s social status is to cause a real harm. On the other side of this interaction, we should consider the possibility that our offensiveness sense may be tuned too sensitively, perhaps for an ancestral environment where mass media didn&rsquo;t exist and any offense might reasonably be considered both personal and intentional. So perhaps we should also try to be less sensitive and avoid taking offense when discussing ideas that are both important and inextricably linked with status.</span></p>\n<p class=\"MsoNormal\"><span>P.S. It's curious that there hasn't been more research into the evolutionary psychology and ethics of offense. If such research does exist and I simply failed to find them, please let me know.<br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"gHCNhqxuJq2bZ2akb": 2, "MfpEPj6kJneT9gWT6": 2, "izp6eeJJEg9v5zcur": 2, "2EFq8dJbxKNzforjM": 3, "5f5c37ee1b5cdee568cfb1b4": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QPqm5aj2meRmE7kR8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 106, "baseScore": 127, "extendedScore": null, "score": 0.000209, "legacy": true, "legacyId": "1432", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 127, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 179, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gsL6CLqjujPNSLL2o", "oZb6AqZKBG4gmLdAZ", "MyqGb24pM54rJhDpb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-23T11:36:34.233Z", "modifiedAt": null, "url": null, "title": "AndrewH's observation and opportunity costs", "slug": "andrewh-s-observation-and-opportunity-costs", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:22.948Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XpXQ4KNzLa9ZHYw8p/andrewh-s-observation-and-opportunity-costs", "pageUrlRelative": "/posts/XpXQ4KNzLa9ZHYw8p/andrewh-s-observation-and-opportunity-costs", "linkUrl": "https://www.lesswrong.com/posts/XpXQ4KNzLa9ZHYw8p/andrewh-s-observation-and-opportunity-costs", "postedAtFormatted": "Thursday, July 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AndrewH's%20observation%20and%20opportunity%20costs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAndrewH's%20observation%20and%20opportunity%20costs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXpXQ4KNzLa9ZHYw8p%2Fandrewh-s-observation-and-opportunity-costs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AndrewH's%20observation%20and%20opportunity%20costs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXpXQ4KNzLa9ZHYw8p%2Fandrewh-s-observation-and-opportunity-costs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXpXQ4KNzLa9ZHYw8p%2Fandrewh-s-observation-and-opportunity-costs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 499, "htmlBody": "<p>In <a href=\"/lw/13p/an_observation_on_cryocrastination/\">his discussion of \"cryocrastination\"</a>, AndrewH makes a pretty good point. There may be some better things you can do with the money you'd spend on cryonics insurance. The sort of people who are into cryonics would probably accept that donating it to the Singularity Institute is probably, all in all, a higher utility use of however many dollars. Andrew's conclusion is that you should figure out what maximizes utility and do it, regardless of how small a contribution is involved. He's right, but I want to use the same example to push a point that is very slightly different, or maybe a little more general, or maybe the exact same one but phrased differently.<br /><br />Consider an argument frequently made when politicians are discussing the budget. I frequently hear people say it would cost between ten and twenty billion dollars a year to feed all the hungry people in the world. I don't know if that's true or not, and considering the recent skepticism about aid it probably isn't, but let's say the politicians believe it. So when they look at (for example) NASA's budget of fifteen billion dollars, they say something like \"It's criminal to be spending all this money on space probes and radio telescopes when it could eliminate world hunger, so let's cut NASA's budget.\"<br /><br />You see the problem? When we cut NASA's budget, it doesn't immediately go into the \"solve world hunger\" fund. It goes into the rest of the budget, and probably gets divided among the Congressman Johnson Memorial Fisheries Museum and purchasing twelve-thousand-dollar staplers.<a id=\"more\"></a><br /><br />The same is true of cryocrastination. Unless you actually take that money you would have spent on cryonics and donate it to the Singularity Institute, it's going into the rest of your budget, and you'll probably spend it on coffee and plasma TVs and famous statistician trading cards and whatever else.<br /><br />I find myself frequently making this error in the following way: a beggar asks me for money, and I want to give it to them on the grounds that they have activated my urge to help people. Then think to myself \"I can't justify giving the money to this beggar when it would help many more people if I gave it to a responsible charity.\" So I say no, and forget all about it, and never give the money to anyone. Even though (from a charity point of view) I know of a superior alternative to giving the money to the beggar, I would still be better off just giving the beggar the money!<br /><br />All this means that for any entity that does not use its resources with maximum efficiency, the opportunity cost of spending a certain amount of resources should not be calculated as what you'd get earn from the best possible use of those resources, but what you'll earn from the use of those resources which you expect to actually occur.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PDJ6KqJBRzvKPfuS3": 1, "ZnHkaTkxukegSrZqE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XpXQ4KNzLa9ZHYw8p", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 29, "extendedScore": null, "score": 5.098035149633239e-07, "legacy": true, "legacyId": "1433", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 61, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xLcW4HRubQvKy9cpb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-24T00:49:14.505Z", "modifiedAt": null, "url": null, "title": "Are calibration and rational decisions mutually exclusive? (Part two)", "slug": "are-calibration-and-rational-decisions-mutually-exclusive", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:52.380Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cyan", "createdAt": "2009-02-27T22:31:08.528Z", "isAdmin": false, "displayName": "Cyan"}, "userId": "eGtDNuhj58ehX9Wgf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BaffPrQtKYSABigNb/are-calibration-and-rational-decisions-mutually-exclusive", "pageUrlRelative": "/posts/BaffPrQtKYSABigNb/are-calibration-and-rational-decisions-mutually-exclusive", "linkUrl": "https://www.lesswrong.com/posts/BaffPrQtKYSABigNb/are-calibration-and-rational-decisions-mutually-exclusive", "postedAtFormatted": "Friday, July 24th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20calibration%20and%20rational%20decisions%20mutually%20exclusive%3F%20(Part%20two)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20calibration%20and%20rational%20decisions%20mutually%20exclusive%3F%20(Part%20two)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBaffPrQtKYSABigNb%2Fare-calibration-and-rational-decisions-mutually-exclusive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20calibration%20and%20rational%20decisions%20mutually%20exclusive%3F%20(Part%20two)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBaffPrQtKYSABigNb%2Fare-calibration-and-rational-decisions-mutually-exclusive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBaffPrQtKYSABigNb%2Fare-calibration-and-rational-decisions-mutually-exclusive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 349, "htmlBody": "<p>In <a href=\"/lw/13r/are_calibration_and_rational_decisions_mutually/\">my previous post</a>, I alluded to a result that could potentially convince a frequentist to favor Bayesian posterior distributions over confidence intervals. It&rsquo;s called the complete class theorem, due to a statistician named Abraham Wald. Wald developed the structure of frequentist decision theory and characterized the class of decision rules that have a certain optimality property.</p>\n<p>Frequentist decision theory reduces the decision process to its basic constituents, i.e., data, actions, true states, and incurred losses. It connects them using mathematical functions that characterize their dependencies, i.e., the true state determines the probability distribution of the data, the decision rule maps data to a particular action, and the chosen action and true states together determine the incurred loss. To evaluate potential decision rules, frequentist decision theory uses the <a href=\"http://en.wikipedia.org/wiki/Risk_function\">risk function</a>, which is defined as the expected loss of a decision rule with respect to the data distribution. The risk function therefore maps (decision rule, true state)-pairs to the average loss under a hypothetical infinite replication of the decision problem.</p>\n<p>Since the true state is not known, decision rules must be evaluated over all possible true states. A decision rule is said to be &ldquo;dominated&rdquo; if there is another decision rule whose risk is never worse for any possible true state and is better for at least one true state. A decision rule which is not dominated is deemed &ldquo;<a href=\"http://en.wikipedia.org/wiki/Admissible_decision_rule\">admissible</a>&rdquo;. (This is the optimality property alluded to above.) The punch line is that under some weak conditions, the complete class of admissible decision rules is precisely the class of&nbsp;rules which minimize a Bayesian posterior expected loss.</p>\n<p>(This result sparked interest in the Bayesian approach among statisticians in the 1950s. This interest eventually led to the axiomatic decision theory that characterizes rational agents as obeying certain fundamental constraints and proves that they act as if they had a prior distribution and a loss function.)</p>\n<p>Taken together, the calibration results of the&nbsp; previous post and the complete class theorem suggest (to me, anyway) that irrespective of one's philosophical views on frequentism versus Bayesianism, perfect calibration is not possible in full generality for a rational decision-making agent.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BaffPrQtKYSABigNb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 8, "extendedScore": null, "score": 5.09929966439868e-07, "legacy": true, "legacyId": "1435", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qDucvMYty5gdumHDB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-24T22:36:05.049Z", "modifiedAt": null, "url": null, "title": "Celebrate Trivial Impetuses", "slug": "celebrate-trivial-impetuses", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:15.550Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6x9S4LBKcFmqvMJfi/celebrate-trivial-impetuses", "pageUrlRelative": "/posts/6x9S4LBKcFmqvMJfi/celebrate-trivial-impetuses", "linkUrl": "https://www.lesswrong.com/posts/6x9S4LBKcFmqvMJfi/celebrate-trivial-impetuses", "postedAtFormatted": "Friday, July 24th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Celebrate%20Trivial%20Impetuses&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACelebrate%20Trivial%20Impetuses%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6x9S4LBKcFmqvMJfi%2Fcelebrate-trivial-impetuses%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Celebrate%20Trivial%20Impetuses%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6x9S4LBKcFmqvMJfi%2Fcelebrate-trivial-impetuses", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6x9S4LBKcFmqvMJfi%2Fcelebrate-trivial-impetuses", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 422, "htmlBody": "<p>There is a flipside to the <a href=\"/lw/f1/beware_trivial_inconveniences/\">trivial inconvenience</a>: the trivial impetus.&nbsp; This is the objectively inconsequential factor that gets you off your rear and doing something you probably would have left undone.&nbsp; It doesn't have to be a major, crippling akrasia issue.&nbsp; I'm not talking so much about finishing your dissertation or remodeling your house, although a trivial impetus could probably get you to make some progress on either.&nbsp; I'm talking about little things that make your life a little better, like trying a new food or permitting a friend to drag you along to a gathering of people and pizza.</p>\n<p>An illustrative anecdote: the first time I tried guacamole, I was out with my family at a restaurant and my parents decided to order some.&nbsp; The waiter came out with a little cart with decorative little bowls full of ingredients and a couple of avocados, and proceeded to make guacamole right there with all the finesse of one of those chefs at a hibachi restaurant.&nbsp; He then presented us with the dish of guacamole and a basket of chips.</p>\n<p>If my prior reasons for avoiding guacamole had been related to concerns about its freshness or possible arsenic content, this would have been a non-trivial reason to try the new food, but they weren't - I was just twelve, and it was green goop.&nbsp; But on that day, it was green goop that someone had made right in front of me like performance art!&nbsp; I simply had to have some!&nbsp; It was delicious.&nbsp; I have enjoyed guacamole ever since.&nbsp; I would almost certainly have taken years longer to try it, if ever I did, had it not been for that restaurant's habit of making each batch of guacamole fresh in front of the customer.</p>\n<p>Not all trivial impetuses have to be so random and fortuitous.&nbsp; Just as you can arrange trivial inconveniences to stand between you and things you should not be doing, you can often arrange trivial impetuses to push you towards things you should be doing.&nbsp; For instance, I often get my friends to instruct me to do things when I'm having trouble getting moving: sometimes all it takes to get me to stop dithering and start making the pasta salad I agreed to bring to a party is someone agreeing when I say, \"I should make pasta salad now\".&nbsp; Or \"I should go to bed now\", or \"I should probably pay that bill now\".</p>\n<p>Does anyone have any other ideas for trivial impetuses that could be helpful in fighting small-scale akrasia (or large-scale)?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"r7qAjcbfhj2256EHH": 1, "5f5c37ee1b5cdee568cfb23e": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6x9S4LBKcFmqvMJfi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 44, "extendedScore": null, "score": 7.3e-05, "legacy": true, "legacyId": "1439", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["reitXJgJXFzKpdKyd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-25T05:09:28.208Z", "modifiedAt": null, "url": null, "title": "Many Reasons", "slug": "many-reasons", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:13.376Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "steven0461", "createdAt": "2009-02-27T16:16:38.980Z", "isAdmin": false, "displayName": "steven0461"}, "userId": "cn4SiEmqWbu7K9em5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GsG4oSndoHMfGaHQW/many-reasons", "pageUrlRelative": "/posts/GsG4oSndoHMfGaHQW/many-reasons", "linkUrl": "https://www.lesswrong.com/posts/GsG4oSndoHMfGaHQW/many-reasons", "postedAtFormatted": "Saturday, July 25th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Many%20Reasons&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMany%20Reasons%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGsG4oSndoHMfGaHQW%2Fmany-reasons%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Many%20Reasons%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGsG4oSndoHMfGaHQW%2Fmany-reasons", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGsG4oSndoHMfGaHQW%2Fmany-reasons", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 460, "htmlBody": "<p>I'm here to teach you a new phobia. The phobia concerns phrases such as \"for many reasons\".</p>\n<p>Rational belief updating is a random walk without drift. If you expect your belief to go up (down) in response to evidence, you should instead make it go up (down) right now. If you're not convinced, read about <a href=\"/lw/ii/conservation_of_expected_evidence/\">conservation of expected evidence</a>, or the <a href=\"http://www.columbia.edu/~gjw10/lie.pdf\">law of iterated expectations</a>.</p>\n<p>If evidence comes in similar-sized chunks, the number of chunks in the \"for\" direction follows a <a href=\"http://en.wikipedia.org/wiki/Binomial_distribution\">binomial distribution</a>&nbsp;with p=.5. Such a distribution can output most of the pieces being in the same direction, but if the number of pieces is high, this will happen quite rarely.</p>\n<p>So if you can find, say, ten reasons to do or believe something and no reasons not to, <em>something is going on</em>.</p>\n<p>One possibility is it's a one in a thousand coincidence. But let's not dwell on that.</p>\n<p>Another possibility is that the process generating your reasons, while unbiased, is skewed. That is to say, it produces many weak reasons in one direction and a few strong reasons in the other, and it just happened not to produce such a strong reason in your case. And so we have many empirical reasons to think the Sun will rise tomorrow (e.g., it rose on June 3rd 1978 and February 16th 1260), and none that it won't. But this does not seem to describe cases like \"what university should I choose\", \"should I believe in a hard takeoff singularity\", or \"is global warming harmful on net\".</p>\n<p>Another possibility (probably a special case of the previous one, but worth stating on its own) is that what you're describing as \"many reasons\" is really a set of different manifestations of the same underlying reason. Maybe you have a hundred legitimate reasons for not hiring someone, including that he smashes furniture, howls at the moon, and strangles kittens. If so, the reason underlying all these may just be that he's nuts.</p>\n<p>Then there's the last, scariest, most important possibility. You may be biased toward finding reasons in one direction, so that you will predictably trend toward your favorite belief. This means you're doing something wrong! Luckily, thinking about why you thought the phrase \"for many reasons\" caused you to find out.</p>\n<p>In sum, when your brain speaks of \"many reasons\" all going the same way, grab, shake, and strangle it. It may just barf up a better, more compressed way of seeing the world, or confess to confirmation bias.&nbsp;</p>\n<p>(Incidentally, this also applies to the phrase \"in many ways\". If you judge someone to be in many ways a weird person, that suggests he has some underlying property that causes many kinds of weirdness, or that you have some underlying property that causes you to judge his traits as weird. Both are noteworthy.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GsG4oSndoHMfGaHQW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "1441", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jiBFC7DcCrZjGmZnJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-25T08:14:32.185Z", "modifiedAt": null, "url": null, "title": "Freaky Fairness", "slug": "freaky-fairness", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:14.560Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mFrx6YbQ6Dsup2Jvp/freaky-fairness", "pageUrlRelative": "/posts/mFrx6YbQ6Dsup2Jvp/freaky-fairness", "linkUrl": "https://www.lesswrong.com/posts/mFrx6YbQ6Dsup2Jvp/freaky-fairness", "postedAtFormatted": "Saturday, July 25th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Freaky%20Fairness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFreaky%20Fairness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmFrx6YbQ6Dsup2Jvp%2Ffreaky-fairness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Freaky%20Fairness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmFrx6YbQ6Dsup2Jvp%2Ffreaky-fairness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmFrx6YbQ6Dsup2Jvp%2Ffreaky-fairness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 897, "htmlBody": "<p>Consider this game:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\begin{bmatrix}4,1&amp;1,3\\\\2,4&amp;3+\\epsilon_1,2+\\epsilon_2\\end{bmatrix}\" alt=\"\" /></p>\n<p>where the last payoff pair is very close to (3,2). I choose a row and you choose a column simultaneously, then I receive the first payoff in a pair and you receive the second. The game has no Nash equilibria in pure strategies, but that's beside the point right now because we drop the competitive setting and go all cooperative: all payoffs are in dollars and <a href=\"http://en.wikipedia.org/wiki/Transferable_utility\">transferable</a>, and we're allowed beforehand to sign a mutually binding contract about the play and the division of revenue. The question is, how much shall we win and how should we split it?</p>\n<p>Game theory suggests we should convert the competitive game to a <a href=\"http://en.wikipedia.org/wiki/Cooperative_game\">coalitional game</a> and compute the <a href=\"http://en.wikipedia.org/wiki/Shapley_value\">Shapley value</a> to divide the spoils. (Or some other solution concept, like the \"nucleolus\", but let's not go there. Assume for now that the Shapley value is \"fair\".) The first step is to assign a payoff to each of the 2<sup>N</sup> = 4 possible coalitions.&nbsp;Clearly, empty coalitions should receive 0, and the grand coalition (me and you) gets the maximum possible sum: 6 dollars. But what payoffs should we assign to the coalition of me and the coalition of you?</p>\n<p>Now, there are at least two conflicting approaches to doing this: <a href=\"http://209.85.129.132/search?q=cache:p7YzNuiEO4IJ:www.coll.mpg.de/ostmann/papers/ch3-4.pdf+&quot;alpha+game&quot;+shapley+value\">alpha and beta</a>. The alpha approach says that \"the value a coalition can get by itself\" is its security value, i.e. the highest value it can win guaranteed if it chooses the strategy first. My alpha value is 2, and yours is 2+<span style=\"font-family: -webkit-sans-serif; line-height: 19px;\">\u03f5<sub>2</sub></span>. The beta approach says that \"the value a coalition can get by itself\" is the highest value that it cannot be <em>prevented</em> from winning if it chooses its strategy <em>second</em>. My beta value is 3+<span style=\"font-family: -webkit-sans-serif; line-height: 19px;\">\u03f5<sub>1</sub></span>, and yours is 3.</p>\n<p>Astute readers already see the kicker: the Shapley value computed from alphas assigns 3-<span style=\"font-family: -webkit-sans-serif; line-height: 19px;\">\u03f5<sub>2</sub></span>/2 dollars to me and 3+<span style=\"font-family: -webkit-sans-serif; line-height: 19px;\">\u03f5<sub>2</sub></span>/2 dollars to you. The Shapley value of betas does the opposite for&nbsp;<span style=\"font-family: -webkit-sans-serif; line-height: 19px;\">\u03f5<sub>1</sub></span>. So who owes whom a penny?</p>\n<p>That's disturbing.<a id=\"more\"></a></p>\n<p>Aha, you say. We should have considered mixed strategies when computing alpha and beta values! In fact, if we do so, we'll find that my alpha value equals my beta value and your alpha equals your beta, because that's true for games with mixed strategies in general (a result equivalent to the <a href=\"http://en.wikipedia.org/wiki/Minimax\">minimax theorem</a>). My security value is (10+4<span style=\"font-family: -webkit-sans-serif; line-height: 19px; \">\u03f5<sub>1</sub></span>)/(4+<span style=\"font-family: -webkit-sans-serif; line-height: 19px; \">\u03f5<sub>1</sub></span>), and yours is (10-<span style=\"font-family: -webkit-sans-serif; line-height: 19px; \">\u03f5<sub>2</sub></span>)/(4-<span style=\"font-family: -webkit-sans-serif; line-height: 19px; \">\u03f5<sub>2</sub></span>).</p>\n<p>This still means the signs of the epsilons<span style=\"font-family: -webkit-sans-serif; line-height: 19px; \">&nbsp;</span>determine who owes whom a penny. That's funny because, if you plot the game's payoffs, you will see that the game isn't a quadrilateral like the PD; it's a triangle. And the point (3+<span style=\"font-family: -webkit-sans-serif; line-height: 19px;\">\u03f5<sub>1</sub></span>,2+<span style=\"font-family: -webkit-sans-serif; line-height: 19px;\">\u03f5<sub>2</sub></span>) that determines the outcome, the point that we can ever-so-slightly wiggle to change who of us gets more money... <em>lies inside that triangle</em>. It can be reached by a weighted combination of the other three outcomes.</p>\n<p>That's disturbing too.</p>\n<p>...</p>\n<p>Now, this whole rambling series of posts was spurred by Eliezer's offhand remark about \"AIs with knowledge of each other's source code\". I <a href=\"/lw/do/reformalizing_pd/\">formalize</a>&nbsp;the problem thus: all players simultaneously submit programs that will receive everyone else's source code as input and print strategy choices for the game as output. The challenge is to write a good program without running into the <a href=\"http://en.wikipedia.org/wiki/Halting_problem\">halting problem</a>, <a href=\"http://en.wikipedia.org/wiki/Rice's_theorem\">Rice's theorem</a> or other obstacles.</p>\n<p>Without further ado I generalize the procedure described above and present to you an algorithm Freaky Fairness &mdash; implementable in an ordinary programming language like Python &mdash; that achieves a <a href=\"http://en.wikipedia.org/wiki/Nash_equilibrium\">Nash equilibrium</a> in algorithms and a <a href=\"http://en.wikipedia.org/wiki/Pareto_efficiency\">Pareto optimum</a>&nbsp;<em>simultaneously</em>&nbsp;in <em>any</em>&nbsp;N-player game with transferable utility:</p>\n<ol>\n<li>Calculate the security values in mixed strategies for all subsets of players.</li>\n<li>Divide all other players into two groups: those whose source code is an exact copy of Freaky Fairness (friends), and everyone else (enemies).</li>\n<li>If there are no enemies: build a Shapley value from the computed security values of coalitions; play my part in the outcome that yields the highest total sum in the game; give up some of the result to others so that the resulting allocation agrees with the Shapley value.</li>\n<li>If there are enemies: play my part in the outcome that brings the total payoff of the coalition of all enemies down to their security value.</li>\n</ol>\n<p>Proof that all players using this algorithm is a Nash equilibrium: any coalition of players that decides to deviate (collectively or individually) cannot win total payoff greater than their group security value, by point 4. If they cooperate, they collectively get no less than their group security value, by superadditivity and construction of Shapley value.</p>\n<p><span style=\"font-size: 11px;\">(NB: we have tacitly assumed that all payoffs in the game are positive, so the Shapley value makes sense. If some payoffs are negative, give everyone a million dollars before the game and take them away afterward; both the Shapley value and the minimax survive such manipulations.)</span></p>\n<p>In retrospect the result seems both obvious and startling. Obvious because it closely follows the historically original derivation of the Shapley value. Startling because we're dealing with a class of <em>one-shot&nbsp;competitive games</em>: players enter their programs blindly, striving to maximize only their own payoff. Yet all such games turn out to have Nash equilibria that are Pareto-optimal, and in pure strategies to boot. Pretty neat, huh?</p>\n<p>I've seriously doubted whether to post this or not. But there might be mistakes, and many eyes will be more likely to spot them. Critique is welcome!</p>\n<p><strong>UPDATE 12.01.2011:</strong> benelliott <a href=\"/r/discussion/lw/3pv/freaky_unfairness\">found</a> a stupid mistake in my result, so it's way less applicable than I'd thought. Ouch.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 1, "5A5ZGTQovxbay6fpr": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mFrx6YbQ6Dsup2Jvp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "1438", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5iK6rsa3MSrMhHQyf", "gi5xpNeY6wC7Pdwin"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-25T13:36:52.175Z", "modifiedAt": null, "url": null, "title": "Link: Interview with Vladimir Vapnik", "slug": "link-interview-with-vladimir-vapnik", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:16.243Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Daniel_Burfoot", "createdAt": "2009-03-12T02:28:50.970Z", "isAdmin": false, "displayName": "Daniel_Burfoot"}, "userId": "XhcXE3Qk5adX6v2Cg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tMeGhbmbdsMSfRGJp/link-interview-with-vladimir-vapnik", "pageUrlRelative": "/posts/tMeGhbmbdsMSfRGJp/link-interview-with-vladimir-vapnik", "linkUrl": "https://www.lesswrong.com/posts/tMeGhbmbdsMSfRGJp/link-interview-with-vladimir-vapnik", "postedAtFormatted": "Saturday, July 25th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20Interview%20with%20Vladimir%20Vapnik&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20Interview%20with%20Vladimir%20Vapnik%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtMeGhbmbdsMSfRGJp%2Flink-interview-with-vladimir-vapnik%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20Interview%20with%20Vladimir%20Vapnik%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtMeGhbmbdsMSfRGJp%2Flink-interview-with-vladimir-vapnik", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtMeGhbmbdsMSfRGJp%2Flink-interview-with-vladimir-vapnik", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 484, "htmlBody": "<p>I recently stumbled across <a href=\"http://www.learningtheory.org/index.php?option=com_content&amp;view=article&amp;id=9%3Aqlearning-has-just-startedq-an-interview-with-prof-vladimir-vapnik&amp;catid=12%3Ainterviews&amp;Itemid=8&amp;showall=1\">this remarkable interview</a> with <a href=\"http://en.wikipedia.org/wiki/Vladimir_Vapnik\">Vladimir Vapnik</a>, a leading light in statistical learning theory, one of the creators of the <a href=\"http://en.wikipedia.org/wiki/Support_vector_machine\">Support Vector Machine</a> algorithm, and generally a <a href=\"http://lecun.org/gallery/libpro/20011121-allyourbayes/dsc01228-02-h.jpg\">cool guy</a>. The interviewer obviously knows his stuff and asks probing questions. Vapnik describes his current research and also makes some interesting philosophical comments:</p>\n<blockquote>\n<p><strong>V-V</strong>: I believe that something drastic has happened in computer science and machine learning. Until recently, philosophy was based on the very simple idea that the world is simple. In machine learning, for the first time, we have examples where the world is not simple. For example, when we solve the \"<a href=\"http://kdd.ics.uci.edu/databases/covertype/covertype.html\">forest</a>\" problem (which is a low-dimensional problem) and use data of size 15,000 we get 85%-87% accuracy. However, when we use 500,000 training examples we achieve 98% of correct answers. This means that a good decision rule is not a simple one, it cannot be described by a very few parameters. This is actually a crucial point in approach to empirical inference.<a id=\"more\"></a></p>\n<p>&nbsp;&nbsp;&nbsp; This point was very well described by Einstein who said \"when the solution is simple, God is answering\". That is, if a law is simple we can find it. He also said \"when the number of factors coming into play is too large, scientific methods in most cases fail\". In machine learning we dealing with a large number of factors. So the question is what is the real world? Is it simple or complex? Machine learning shows that there are examples of complex worlds. We should approach complex worlds from a completely different position than simple worlds. For example, in a complex world one should give up explain-ability (the main goal in classical science) to gain a better predict-ability.</p>\n<p><strong>R-GB</strong>: Do you claim that the assumption of mathematics and other sciences that there are very few and simple rules that govern the world is wrong?</p>\n<p><strong>V-V</strong>: I believe that it is wrong. As I mentioned before, the (low-dimensional) problem \"<a href=\"http://kdd.ics.uci.edu/databases/covertype/covertype.html\">forest</a>\" has a perfect solution, but it is not simple and you cannot obtain this solution using 15,000 examples.</p>\n</blockquote>\n<p>Later:</p>\n<blockquote>\n<p><strong>R-GB</strong>: What do you think about the bounds on uniform convergence? Are they as good as we can expect them to be?</p>\n<p><strong>V-V</strong>: They are O.K. However the main problem is not the bound. There are conceptual questions and technical questions. From a conceptual point of view, you cannot avoid uniform convergence arguments; it is a necessity. One can try to improve the bounds, but it is a technical problem. My concern is that machine learning is not only about technical things, it is also about philosophy: What is the complex world science about? The improvement of the bound is an extremely interesting problem from mathematical point of view. But even if you'll get a better bound it will not be able help to attack the main problem: what to do in complex worlds?</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"9DNZfxFvY5iKoZQbz": 1, "fpEBgFE7fgpxTm9BF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tMeGhbmbdsMSfRGJp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 22, "extendedScore": null, "score": 5.102825944991752e-07, "legacy": true, "legacyId": "1443", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-25T18:16:54.737Z", "modifiedAt": null, "url": null, "title": "Five Stages of Idolatry", "slug": "five-stages-of-idolatry", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:14.908Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "pwZ6qMgzoKr3JPqx4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vHAyYY48fawQFuAfd/five-stages-of-idolatry", "pageUrlRelative": "/posts/vHAyYY48fawQFuAfd/five-stages-of-idolatry", "linkUrl": "https://www.lesswrong.com/posts/vHAyYY48fawQFuAfd/five-stages-of-idolatry", "postedAtFormatted": "Saturday, July 25th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Five%20Stages%20of%20Idolatry&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFive%20Stages%20of%20Idolatry%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvHAyYY48fawQFuAfd%2Ffive-stages-of-idolatry%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Five%20Stages%20of%20Idolatry%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvHAyYY48fawQFuAfd%2Ffive-stages-of-idolatry", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvHAyYY48fawQFuAfd%2Ffive-stages-of-idolatry", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 770, "htmlBody": "<p>We all have heroes or idols, people we look up to and turn to, in one form or another, for guidance or wisdom. Over the years, I've noticed that my feelings towards those I've idolized tend to follow a predictable pattern. The following is the extraction of this pattern.</p>\n<div>Stage 1: Exposure - you're exposed to the idol through some channel. Maybe something you read, or someone you know, or simply by chance. You begin to learn about them, and you become intrigued. If it's an author, maybe you pick up one of his books. If it's a group, maybe you check out their website. You begin to gradually absorb what the idol is offering. They don't actually become an idol though, until...</div>\n<div>Stage 2: Resonance - after enough exposure, what the idol offers begins to strike a chord with you. You go on to ravenously consume everything related to it. You track down every one of the authors publications, or spend hours staring at all of an artists' paintings. As far as I can tell what's important here isn't actually the content of what the idol offers, but that feeling of resonance it engenders.<a id=\"more\"></a></div>\n<div>Step 3: Incorporation - the idol has become one of the lenses through which you view the world. Everyone and everything is compared to the idol, and everyone invariable comes up short (raise your hand if you've ever thought about someone \"Well, they're pretty smart, but not as smart as Eliezer\"). You change your lifestyle to be more like them, to think more like them. It's as if they have all aspects of life figured out, and you follow along in the hopes that you will reach their same understanding. The most fervent support of the idol lies here.</div>\n<div>Step 4: Backlash - you start to realize that the idol does not, in fact, provide the answers to all of life's questions. That they might be wrong about some things, or that their specific offering does not apply to all life's situations. You've changed yourself to emulate the idol, and you realize that perhaps not all those changes were for the better. Paradoxically, the blame for this gets placed on the idol instead of you. Feelings toward it shift from worship to antipathy.</div>\n<div>Step 5: Re-incorporation - after enough time has been spent hating the idol, you come to realize, if only subconsciously, that the fault lies not in the idol, but in your worship. The cycle ends with a more reserved incorporation of what the idol does offer, along with the realization of what it doesn't. The idol ceases to be an idol, and becomes another earthly entity, complete with faults.</div>\n<div>Of course, the sample size I'm working with is one - this may be a general feature of humans, or simply nuances of my individual brain. If I had to bet though, my money would lie with the general feature - it's happened to me many different times of the years, with my brain in many different stages of development. And I suspect I've seen others in various stages of this (though I have no way of knowing, really.)</div>\n<div>Looking at the above stages, worship seems to mirror the progression of infectious disease. Exposure leads to an infection, which then spreads throughout the body. At this point, the body mounts a counterattack, and produces masses of white blood cells to fight off the infection. The infection is expunged, the white blood cells return to normal levels, and you're left with antibodies which contribute to a more complete immune system.</div>\n<div>The problem with this isn't so much that it isn't rational per se - taking new evidence and updating our beliefs until they converge on the 'right' answer seems to be exactly the sort of thing we should be doing. The problem is how long it can take to get through them. In the past it's taken me <em>years </em>to get through step five after encountering something new, and true believers in something seem to reach step three and then just stay in it.<em> </em>But we should be updating our beliefs as quickly as possible, not languishing with the wrong answer for huge chunks of our lives.</div>\n<div>So my question to the community is twofold:</div>\n<div>1) Is this something that happens to you?</div>\n<div>and</div>\n<div>2) Assuming this is a basic mental process that can't be just turned off, how can we cycle through it faster, so we can more quickly reach accurate beliefs?</div>\n<div>For my part, simply recognizing that this cycle exists seems like it's reduced both it's duration, and the extremes I swing to in each direction. But I'm curious if I can do better.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vHAyYY48fawQFuAfd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "1444", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-26T16:49:51.120Z", "modifiedAt": null, "url": null, "title": "Bayesian Flame", "slug": "bayesian-flame", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:39.292Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WhWTwQJaiEFxvXB96/bayesian-flame", "pageUrlRelative": "/posts/WhWTwQJaiEFxvXB96/bayesian-flame", "linkUrl": "https://www.lesswrong.com/posts/WhWTwQJaiEFxvXB96/bayesian-flame", "postedAtFormatted": "Sunday, July 26th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bayesian%20Flame&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABayesian%20Flame%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWhWTwQJaiEFxvXB96%2Fbayesian-flame%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bayesian%20Flame%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWhWTwQJaiEFxvXB96%2Fbayesian-flame", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWhWTwQJaiEFxvXB96%2Fbayesian-flame", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1062, "htmlBody": "<p>There once lived a great man named <a href=\"http://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes\">E.T. Jaynes</a>. He knew that Bayesian inference is <a href=\"http://www-biba.inrialpes.fr/Jaynes/prob.html\">the only way</a> to do statistics logically and consistently, standing on the shoulders of misunderstood giants Laplace and Gibbs. On numerous occasions he <a href=\"http://bayes.wustl.edu/etj/articles/confidence.pdf\">vanquished</a> traditional \"frequentist\" statisticians with his superior math, demonstrating to anyone with half a brain how the Bayesian way gives faster and more correct results in each example. The weight of evidence falls so heavily on one side that it makes no sense to argue anymore. The fight is over. Bayes wins. <a href=\"/lw/o7/searching_for_bayesstructure/\">The universe runs on Bayes-structure.</a></p>\n<p>Or at least that's what you believe if you learned this stuff from Overcoming Bias.</p>\n<p>Like I was until two days ago, when Cyan <a href=\"/lw/13v/are_calibration_and_rational_decisions_mutually/\">hit me over the head</a> with something utterly incomprehensible. I suddenly had to go out and understand this stuff, not just believe it. (The original intention, if I remember it correctly, was to impress you all by pulling a Jaynes.) Now I've come back and intend to provoke a full-on flame war on the topic. Because if we can have thoughtful flame wars about gender but not math, we're a bad community. Bad, bad community.</p>\n<p>If you're like me two days ago, you kinda \"understand\" what Bayesians do: assume a prior probability distribution over hypotheses, use evidence to morph it into a posterior distribution over same, and bless the resulting numbers as your \"degrees of belief\". But chances are that you have a very vague idea of what frequentists do, apart from <a href=\"/lw/mt/beautiful_probability/\">deriving half-assed results with their ad hoc tools</a>.<a id=\"more\"></a></p>\n<p>Well, here's the ultra-short version: frequentist statistics is <em>the art of drawing true conclusions about the real world</em> instead of assuming prior degrees of belief and coherently adjusting them to avoid Dutch books.</p>\n<p>And here's an ultra-short example of what frequentists can do: estimate 100 independent unknown parameters from 100 different sample data sets and have 90 of the estimates turn out to be <em>true to fact</em> afterward. Like, fo'real. Always 90% in the long run, truly, irrevocably and forever. No Bayesian method known today can reliably do the same: the outcome will depend on the priors you assume for each parameter. I don't believe you're going to get lucky with all 100. And even if I believed you a priori (ahem) that don't make it true.</p>\n<p>(That's what Jaynes did to achieve his awesome victories: use trained intuition to pick good priors by hand on a per-sample basis. Maybe you can learn this skill somewhere, but not from the <a href=\"http://yudkowsky.net/rational/bayes\">Intuitive Explanation</a>.)</p>\n<p>How in the world do you do inference without a prior? Well, the characterization of frequentist statistics as \"trickery\" is totally justified: it has no single coherent approach and the tricks often give conflicting results. Most everybody agrees that you can't do better than Bayes if you have a clear-cut prior; but if you don't, no one is going to kick you out. We sympathize with your predicament and will gladly sell you some twisted technology!</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Confidence_interval\">Confidence intervals</a>:&nbsp;imagine you somehow process some sample data to get an interval. Further imagine that&nbsp;hypothetically, <em>for any given hidden parameter value</em>, this calculation algorithm applied to data sampled under that parameter value yields an interval that covers it with probability 90%. Believe it or not, this perverse trick works 90% of the time without requiring any prior distribution on parameter values.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Bias_of_an_estimator\">Unbiased estimators</a>: you process the sample data to get a number whose expectation magically coincides with the true parameter value.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Statistical_hypothesis_testing\">Hypothesis testing</a>: I give you a black-box random distribution and claim it obeys a specified formula. You sample some data from the box and inspect it. Frequentism allows you to&nbsp;<span style=\"text-decoration: line-through;\">call me a liar and be wrong no more than 10% of the time</span>&nbsp;reject truthful claims no more than 10% of the time, guaranteed, no prior in sight.&nbsp;(Thanks Eliezer for calling out the mistake, and conchis for the correction!)</p>\n<p>But this is getting too academic. I ought to throw you dry wood, good flame material. This <a href=\"http://ba.stat.cmu.edu/journal/2008/vol03/issue03/gelman.pdf\">hilarious PDF</a> from Andrew Gelman should do the trick. Choice quote:</p>\n<blockquote>\n<p>Well, let me tell you something. The 50 states aren't exchangeable. I've lived in a few of them and visited nearly all the others, and calling them exchangeable is just silly. Calling it a hierarchical or multilevel model doesn't change things - it's an additional level of modeling that I'd rather not do. Call me old-fashioned, but I'd rather let the data speak without applying a probability distribution to something like the 50 states which are neither random nor a sample.</p>\n</blockquote>\n<p>As a bonus, the bibliography to that article contains such marvelous titles as \"Why Isn't Everyone a Bayesian?\" And Larry Wasserman's <a href=\"http://ba.stat.cmu.edu/journal/2008/vol03/issue03/wasserman.pdf\">followup</a> is also quite disturbing.</p>\n<p>Another stick for the fire is provided by&nbsp;<a href=\"http://cscs.umich.edu/~crshalizi/weblog/612.html\">Shalizi</a>, who (among other things) makes the correct point that a good Bayesian must never be uncertain about the probability of any future event. That's why he calls Bayesians \"Often Wrong, Never In Doubt\":</p>\n<blockquote>\n<p>The Bayesian, by definition, believes in a joint distribution of the random sequence&nbsp;X&nbsp;and of the hypothesis&nbsp;M. (Otherwise, Bayes's rule makes no sense.) This means that by integrating over&nbsp;M, we get an unconditional, marginal probability for&nbsp;f.</p>\n</blockquote>\n<p>For my final quote it seems only fair to add one more polemical summary of Cyan's point that made me sit up and look around in a bewildered manner. Credit to Wasserman <a href=\"http://ba.stat.cmu.edu/journal/2006/vol01/issue03/wasserman.pdf\">again</a>:</p>\n<blockquote>\n<p><em>Pennypacker:</em> You see, physics has really advanced. All those quantities I estimated&nbsp;have now been measured to great precision. Of those thousands of 95 percent intervals,&nbsp;only 3 percent contained the true values! They concluded I was a fraud.</p>\n<p><em>van Nostrand</em><em>:</em>&nbsp;Pennypacker&nbsp;you fool. I never said those intervals would contain the&nbsp;truth 95 percent of the time. I guaranteed&nbsp;coherence not coverage!</p>\n<p><em>Pennypacker:</em> A lot of good that did me. I should have gone to that objective Bayesian statistician. At least he cares about the frequentist properties of his procedures.</p>\n<p><em>van Nostrand:</em> Well I'm sorry you feel that way Pennypacker. But I can't be responsible for your incoherent colleagues. I've had enough now. Be on your way.</p>\n</blockquote>\n<p>There's often good reason to advocate a correct theory over a wrong one. But all this evidence (ahem) shows that switching to&nbsp;<a href=\"/lw/lz/guardians_of_the_truth/\">Guardian of Truth</a> mode was, at the very least, premature for me. Bayes isn't the correct theory to make conclusions about the world. <em>As of today, we have no coherent theory for making conclusions about the world.</em> Both perspectives have serious problems.&nbsp;So do yourself a favor and switch to truth-seeker mode.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1, "xgpBASEThXPuKRhbS": 1, "LhX3F2SvGDarZCuh6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WhWTwQJaiEFxvXB96", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 48, "baseScore": 41, "extendedScore": null, "score": 0.000103, "legacy": true, "legacyId": "1447", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 163, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QrhAeKBkm2WsdRYao", "BaffPrQtKYSABigNb", "bkSkRwo9SRYxJMiSY", "etBrzxdfNop3DqJvA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-26T22:58:42.349Z", "modifiedAt": "2020-09-23T19:29:11.042Z", "url": null, "title": "The Second Best", "slug": "the-second-best", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:18.600Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Wei_Dai", "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DiBoWCBKTtzhQMwpu/the-second-best", "pageUrlRelative": "/posts/DiBoWCBKTtzhQMwpu/the-second-best", "linkUrl": "https://www.lesswrong.com/posts/DiBoWCBKTtzhQMwpu/the-second-best", "postedAtFormatted": "Sunday, July 26th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Second%20Best&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Second%20Best%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDiBoWCBKTtzhQMwpu%2Fthe-second-best%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Second%20Best%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDiBoWCBKTtzhQMwpu%2Fthe-second-best", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDiBoWCBKTtzhQMwpu%2Fthe-second-best", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 645, "htmlBody": "<p>In economics, the ideal, or first best, outcome for an economy is a Pareto-efficient one, meaning one in which no market participant can be made better off without someone else made worse off. But it can only occur under the conditions of \u201c<a href=\"http://en.wikipedia.org/wiki/Perfect_competition\">Perfect Competition</a>\u201d in all markets, which never occurs in reality. And when it is impossible to achieve Perfect Competition due to some unavoidable market failures, to obtain the <a href=\"http://en.wikipedia.org/wiki/Theory_of_the_Second_Best\">second best</a> (i.e., best given the constraints) outcome may involve further distorting markets away from Perfect Competition.</p><p>To me, perhaps because it was the first such result that I learned, \u201csecond best\u201d has come to stand generally for the yawning gap between individual rationality and group rationality. But similar results abound. For example, in <a href=\"http://en.wikipedia.org/wiki/Social_choice_theory\">Social Choice Theory</a>, <a href=\"http://en.wikipedia.org/wiki/Arrow%27s_Impossibility_Theorem\">Arrow's Impossibility Theorem</a> states that there is no voting method that satisfies a certain set of axioms, which are usually called fairness axioms, but can perhaps be better viewed as group rationality axioms. In <a href=\"http://en.wikipedia.org/wiki/Industrial_Organization\">Industrial Organization</a>, a duopoly can best maximize profits by colluding to raise prices. In <a href=\"http://en.wikipedia.org/wiki/Contract_theory\">Contract Theory</a>, rational individuals use up resources to send <a href=\"http://en.wikipedia.org/wiki/Signalling_%28economics%29\">signals</a> that do not contribute to social welfare. In <a href=\"http://en.wikipedia.org/wiki/Public_Choice_Theory\">Public Choice Theory</a>, special interest groups successfully lobby the government to implement inefficient policies that benefit them at the expense of the general public (and each other).</p><p>On an individual level, the fact that individual and group rationality rarely coincide means that often, to pursue one is to give up the other. For example, if you\u2019ve never cheated on your taxes, or slacked off at work, or lost a mutually beneficial deal because you bargained too hard, or failed to inform yourself about a political candidate before you voted, or tried to monopolize a market, or annoyed your spouse, or annoyed your neighbor, or gossiped maliciously about a rival, or sounded more confident about an argument than you were, or took <a href=\"/lw/13s/the_nature_of_offense/\">offense</a> to a truth, or [insert your own here], then you probably haven't been individually rational.</p><p>\"But, I'm an altruist,\" you might claim, \"my only goal is societal well-being.\" Well, unless everyone you deal with is also an altruist, and with the exact same utility function, the above still applies, although perhaps to a lesser extent. You should still cheat on your taxes because the government won't spend your money as effectively as you can. You should still bargain hard enough to risk losing deals occasionally because the money you save will do more good for society (by your values) if left in your own hands.</p><p>What is the point of all this? It's that group rationality is damn hard, and we should have realistic expectations about what's possible. (Maybe then we won't be so easily <a href=\"/lw/b9/welcome_to_less_wrong/z8n\">disappointed</a>.) I don't know if you noticed, but Pareto efficiency, that so called optimality criterion, is actually incredibly weak. It says nothing about how conflicts between individual values must be adjudicated, just that if there is a way to get a better result for some with others no worse off, we'll do that. In individual rationality, its analog would be something like, \"given two choices where the first better satisfies every value you have, you won't choose the second,\" which is so trivial that we never bother to state it explicitly. But we don't know how to achieve even this weak form of group rationality in most settings.</p><p>In a way, the difficulty of group rationality makes sense. After all, rationality (or the potential for it) is almost a defining characteristic of individuality. If individuals from a certain group always acted for the good of the group, then what makes them individuals, rather than interchangeable parts of a single entity? For example, don't we see a <a href=\"http://en.wikipedia.org/wiki/Borg_%28Star_Trek%29\">Borg</a> cube as one individual precisely because it is too rational as a group? Since achieving perfect Borg-like group rationality presumably isn't what we want anyway, maybe settling for second best isn't so bad.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zv7v2ziqexSn5iS9v": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DiBoWCBKTtzhQMwpu", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 18, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "1446", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["QPqm5aj2meRmE7kR8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-27T14:28:55.021Z", "modifiedAt": null, "url": null, "title": "Bayesian Utility: Representing Preference by Probability Measures", "slug": "bayesian-utility-representing-preference-by-probability", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:13.461Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kYgWmKJnqq8QkbjFj/bayesian-utility-representing-preference-by-probability", "pageUrlRelative": "/posts/kYgWmKJnqq8QkbjFj/bayesian-utility-representing-preference-by-probability", "linkUrl": "https://www.lesswrong.com/posts/kYgWmKJnqq8QkbjFj/bayesian-utility-representing-preference-by-probability", "postedAtFormatted": "Monday, July 27th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bayesian%20Utility%3A%20Representing%20Preference%20by%20Probability%20Measures&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABayesian%20Utility%3A%20Representing%20Preference%20by%20Probability%20Measures%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkYgWmKJnqq8QkbjFj%2Fbayesian-utility-representing-preference-by-probability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bayesian%20Utility%3A%20Representing%20Preference%20by%20Probability%20Measures%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkYgWmKJnqq8QkbjFj%2Fbayesian-utility-representing-preference-by-probability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkYgWmKJnqq8QkbjFj%2Fbayesian-utility-representing-preference-by-probability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 663, "htmlBody": "<p>This is a simple transformation of standard expected utility formula that I found conceptually interesting.</p>\n<p>For simplicity, let's consider a finite discrete <a href=\"http://en.wikipedia.org/wiki/Probability_space\">probability space</a> with non-zero probability at each point p(x), and a utility function u(x) defined on its sample space. Expected utility of an event A (set of the points of the sample space) is the average value of utility function weighted by probability over the event, and is written as</p>\n<p><img src=\"http://www.codecogs.com/png.latex?EU(A)=\\frac{\\sum_{x\\in A}{p(x)\\cdot u(x)}}{\\sum_{x\\in A}{p(x)}}\" alt=\"EU(A)=\\frac{\\sum_{x\\in A}{p(x)\\cdot u(x)}}{\\sum_{x\\in A}{p(x)}}\" /></p>\n<p><a id=\"more\"></a> Expected utility is a way of comparing events (sets of possible outcomes) that correspond to, for example, available actions. Event A is said to be preferable to event B when EU(A)&gt;EU(B). Preference relation doesn't change when utility function is transformed by positive affine transformations. Since the sample space is assumed finite, we can assume without loss of generality that for all x, u(x)&gt;0. Such utility function can be additionally rescaled so that for all sample space</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\sum_{x}{p(x)\\cdot u(x)}=1\" alt=\"\\sum_{x}{p(x)\\cdot u(x)}=1\" /></p>\n<p>Now, if we define</p>\n<p><img src=\"http://www.codecogs.com/png.latex?q(x)=p(x)\\cdot u(x)\" alt=\"q(x)=p(x)\\cdot u(x)\" /></p>\n<p>the expected utility can be rewritten as</p>\n<p><img src=\"http://www.codecogs.com/png.latex?EU(A)=\\frac{\\sum_{x\\in A}{q(x)}}{\\sum_{x\\in A}{p(x)}}\" alt=\"EU(A)=\\frac{\\sum_{x\\in A}{q(x)}}{\\sum_{x\\in A}{p(x)}}\" /></p>\n<p>or</p>\n<p><img src=\"http://www.codecogs.com/png.latex?EU(A)=\\frac{Q(A)}{P(A)}\" alt=\"EU(A)=\\frac{Q(A)}{P(A)}\" /></p>\n<p>Here, P and Q are two probability measures. It's easy to see that this form of expected utility formula has the same expressive power, so preference relation can be defined directly by a pair of probability measures on the same sample space, instead of using a utility function.</p>\n<p>Expected utility written in this form only uses probability of the whole event in both measures, without looking at the individual points. I tentatively call measure Q \"shouldness\", together with P being \"probability\". Conceptual advantage of this form is that probability and utility are now on equal footing, and it's possible to work with both of them using the familiar Bayesian updating, in exactly the same way. To compute expected utility of an event given additional information, just use the posterior shouldness and probability:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?EU(A|B)=\\frac{Q(A|B)}{P(A|B)}\" alt=\"EU(A|B)=\\frac{Q(A|B)}{P(A|B)}\" /></p>\n<p>If events are drawn as points (vectors) in (P,Q) coordinates, expected utility is monotone on the polar angle of the vectors. Since coordinates show measures of events, a vector depicting a union of nonintersecting events is equal to the sum of vectors depicting these events:</p>\n<p><img src=\"http://www.codecogs.com/png.latex?(P(A\\cup B),Q(A\\cup B)) = (P(A),Q(A))+(P(B),Q(B)),\\ A\\cap B=\\emptyset\" alt=\"(P(A\\cup B),Q(A\\cup B)) = (P(A),Q(A))+(P(B),Q(B)),\\ A\\cap B=\\emptyset\" /></p>\n<p>This allows to graphically see some of the structure of simple sigma-algebras of the sample space together with a preference relation defined by a pair of measures. See also <a href=\"/lw/148/bayesian_utility_representing_preference_by/11hn\">this comment</a> on some examples of applying this geometric representation of preference.</p>\n<p>Preference relation defined by expected utility this way also doesn't depend on constant factors in the measures, so it's unnecessary to require the measures to sum up to 1.</p>\n<p>Since P and Q are just devices representing the preference relation, there is nothing inherently \"epistemic\" about P. Indeed, it's possible to mix P and Q together without changing the preference relation. A pair (p',q') defined by</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\begin{matrix} \\left\\{\\begin{matrix} p' &amp;=&amp; \\alpha\\cdot p + (1-\\beta)\\cdot q\\\\ q' &amp;=&amp; \\beta\\cdot q + (1-\\alpha)\\cdot p \\end{matrix}\\right.\\\\ \\alpha&gt;\\beta \\end{matrix}\" alt=\"\\begin{matrix} \\left\\{\\begin{matrix} p' &amp;=&amp; \\alpha\\cdot p + (1-\\beta)\\cdot q\\\\ q' &amp;=&amp; \\beta\\cdot q + (1-\\alpha)\\cdot p \\end{matrix}\\right.\\\\ \\alpha&gt;\\beta \\end{matrix}\" /></p>\n<p>gives the same preference relation,</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\frac{Q(A)}{P(A)}&gt;\\frac{Q(B)}{P(B)} \\Leftrightarrow \\frac{Q'(A)}{P'(A)}&gt;\\frac{Q'(B)}{P'(B)}\" alt=\"\\frac{Q(A)}{P(A)}&gt;\\frac{Q(B)}{P(B)} \\Leftrightarrow \\frac{Q'(A)}{P'(A)}&gt;\\frac{Q'(B)}{P'(B)}\" /></p>\n<p>(Coefficients can be negative or more than 1, but values of p and q must remain positive.)</p>\n<p>Conversely, given a fixed measure P, it isn't possible to define an arbitrary preference relation by only varying Q (or utility function). For example, for a sample space of three elements, a, b and c, if p(a)=p(b)=p(c), then EU(a)&gt;EU(b)&gt;EU(c) means that EU(a+c)&gt;EU(b+c), so it isn't possible to choose q such that EU(a+c)&lt;EU(b+c). If we are free to choose p, however, an example that has these properties (allowing zero values for simplicity) is a=(0,1/4), b=(1/2,3/4), c=(1/2,0), with a+c=(1/2,1/4), b+c=(1,3/4), so EU(a+c)&lt;EU(b+c).</p>\n<p>Prior is an integral part of preference, and it works exactly the same way as shouldness. Manipulations with probabilities, or Bayesian \"levels of certainty\", are manipulations with \"half of preference\". The problem of choosing Bayesian priors is in general the problem of formalizing preference, it can't be solved completely without considering utility, without formalizing values, and values are <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">very complicated</a>. No simple morality, no simple probability.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6nS8oYmSMuFMaiowF": 1, "sYm3HiWcfZvrGu3ui": 1, "LhX3F2SvGDarZCuh6": 1, "HAFdXkW4YW4KRe2Gx": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kYgWmKJnqq8QkbjFj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 44, "extendedScore": null, "score": 7e-05, "legacy": true, "legacyId": "1448", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": true, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-27T17:05:33.099Z", "modifiedAt": null, "url": null, "title": "Religion and conservation of evidence", "slug": "religion-and-conservation-of-evidence", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:14.338Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NuRqybAgstvKk9E45/religion-and-conservation-of-evidence", "pageUrlRelative": "/posts/NuRqybAgstvKk9E45/religion-and-conservation-of-evidence", "linkUrl": "https://www.lesswrong.com/posts/NuRqybAgstvKk9E45/religion-and-conservation-of-evidence", "postedAtFormatted": "Monday, July 27th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Religion%20and%20conservation%20of%20evidence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReligion%20and%20conservation%20of%20evidence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNuRqybAgstvKk9E45%2Freligion-and-conservation-of-evidence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Religion%20and%20conservation%20of%20evidence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNuRqybAgstvKk9E45%2Freligion-and-conservation-of-evidence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNuRqybAgstvKk9E45%2Freligion-and-conservation-of-evidence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 100, "htmlBody": "<p>Stereotypically, people say that religion is the \"opiate of the masses\", and expect poor people to be religious, because it gives them solace for their problems.</p>\n<p>But most of the religious people I've known are relatively wealthy, Mercedes-driving people, to whom Christianity gives the comfort of believing that their wealth is the result of God's plan, and of their own virtue, rather than accident.</p>\n<p>And the people in-between rich and poor tend to deviate less from accepted social behavior, and thus are more likely to be religious.</p>\n<p>So every possible economic status increases the priors of being religious.&nbsp; Wait, that can't be right.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NuRqybAgstvKk9E45", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -7, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "1449", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-27T22:46:57.377Z", "modifiedAt": null, "url": null, "title": "The Trolley Problem in popular culture: Torchwood Series 3", "slug": "the-trolley-problem-in-popular-culture-torchwood-series-3", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:28.180Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "botogol", "createdAt": "2009-02-27T09:28:08.710Z", "isAdmin": false, "displayName": "botogol"}, "userId": "PbDqfJ6jRqwXhqSy4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/REpzLJaQjJ2hJb6sR/the-trolley-problem-in-popular-culture-torchwood-series-3", "pageUrlRelative": "/posts/REpzLJaQjJ2hJb6sR/the-trolley-problem-in-popular-culture-torchwood-series-3", "linkUrl": "https://www.lesswrong.com/posts/REpzLJaQjJ2hJb6sR/the-trolley-problem-in-popular-culture-torchwood-series-3", "postedAtFormatted": "Monday, July 27th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Trolley%20Problem%20in%20popular%20culture%3A%20Torchwood%20Series%203&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Trolley%20Problem%20in%20popular%20culture%3A%20Torchwood%20Series%203%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FREpzLJaQjJ2hJb6sR%2Fthe-trolley-problem-in-popular-culture-torchwood-series-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Trolley%20Problem%20in%20popular%20culture%3A%20Torchwood%20Series%203%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FREpzLJaQjJ2hJb6sR%2Fthe-trolley-problem-in-popular-culture-torchwood-series-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FREpzLJaQjJ2hJb6sR%2Fthe-trolley-problem-in-popular-culture-torchwood-series-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 886, "htmlBody": "<p>It's just possible that some lesswrong readers may be unfamiliar with <a href=\"http://en.wikipedia.org/wiki/Torchwood\">Torchwood</a>: It's a British sci-fi TV series, a spin-off from the more famous, and very long-running cult show <a href=\"http://en.wikipedia.org/wiki/Dr_who\">Dr Who</a>.</p>\n<p>Two weeks ago Torchwood <a href=\"http://www.bbc.co.uk/torchwood/\">Series 3</a> aired. It took the form of a single story arc, over five days, shown in five parts on consecutive nights. What hopefully makes it interesting to rationalist lesswrong readers who are not (yet) Whovians was not only the space monsters <sup>(1)</sup> but also the show's determined and methodical exploration of an iterated <a href=\"http://en.wikipedia.org/wiki/Trolley_problem\">Trolley Problem</a>:&nbsp; in a process familiar to seasoned thought-experimenters the characters were tested with a dilemma followed by a succession of variations of increasing complexity, with their choices ascertained and the implications discussed and reckoned with.</p>\n<p>An hypothetical, iterated rationalist dilemma... with space monsters... and monsters a great deal more scary - and messier - than Omega -&nbsp; what's not to like?</p>\n<p>So, on the off chance that you missed it, and as a summer diversion from more academic lesswrong fare, I thought a brief description of how a familiar dilemma was handled on popular British TV this month, might be of passing interest (warning: spoilers follow)<a id=\"more\"></a></p>\n<p>The details of the scenario need not concern us too much here (and readers are warned not too expend too much mental energy exploring the various implausibilities, for want of distraction) but suffice to say that the <em>456</em>, a race of evil aliens, landed on Earth and demanded that a certain number of children be turned over to them to suffer a horrible fate-worse-than-death or else we face the familiar prospects of all out attack and the likely destruction of mankind.</p>\n<p>Resistance, it almost goes without saying, was futile.</p>\n<p>The problems faced by the team could be roughly sorted into some themes</p>\n<p><strong>The Numbers dilemma</strong> - is it worth sacrificing <em>any </em>amount of children to save the rest?</p>\n<ul>\n<li>first <sup>(2)</sup> the 456 demand just 35 children</li>\n<li>but later they demand 10% of the child population.</li>\n</ul>\n<p><strong>The Quality dilemma:</strong> does it make any difference <em>which </em>children?</p>\n<ul>\n<li>what if, say, the sacrificed children are solely those <em>whom nobody will miss</em>? </li>\n</ul>\n<p><strong>The choice dilemma</strong>: how should the sacrifical cohort be chosen?</p>\n<ul>\n<li>are there any criteria that are fairer, or more rational, than the obvious lottery?</li>\n<li>other than 'fairness' what other criteria can you think of for selecting the victims?</li>\n</ul>\n<p><strong>The limits of human rationality</strong>: are there certain 'rational' decisions that are simply too much to expect a human being to be able to make?</p>\n<ul>\n<li>what if it is your own children? Grand-children? Nephews? Neighbours?&nbsp;</li>\n</ul>\n<p>Actually despite my jocular tone in the first paragraph I don't want to make too light of this series, as it was disturbing viewing.</p>\n<p>Anyway: that being said: rationalist lesswrong community members may want to think dispassionately about the their answers before I reveal the conclusions that Russell T Davies (the writer) came to:</p>\n<p><strong>Numbers</strong></p>\n<ul>\n<li>Lesswrongers will be encouraged to learn that the Torchwood characters were rationalists to a man and woman - there was little hesitation in agreeing to the 456's demands. </li>\n<li>A rational attempt to negotiate (600 children were offered) was faced down by the 465's intemperate release of swine flu into the negotiating chamber.</li>\n</ul>\n<p><strong>Quality</strong></p>\n<ul>\n<li>In the Torchwood universe it made a great deal of difference which children - or units as they were plausibly referred to - were sacrificed, with governments paying attention to round up the orphans, the refugees and the unloved - for the unexpectedly rational reason of minimising the suffering of the survivors.</li>\n</ul>\n<p><strong>Choice</strong></p>\n<p style=\"padding-left: 60px;\">This was handled by the politicians who considered two dimensions in the selection:</p>\n<ul>\n<li>&nbsp; \n<ul>\n<li>the <em>Darwinian dimension</em>: could they improve the human gene pool by sacrificing the least desirable 10%?&nbsp; If this seemed like a good rationalist plan (it did) then how could they quickly identify the losers? Answer: in a UK-centric political twist they chose those attending the schools with the poorest exam results.</li>\n<li>the question of <em>practicality and enforcabilty</em>. This was the consideration that caused the children of decision-makers and enforcers to be spared. In a cute nested trolley problem the children of the soldiers were <em>saved in order to</em> ensure the cooperation of their parents in rounding up the unlucky and underserving, <em>in order to </em>ensure the survival of the rest.</li>\n<li>at no stage was it necessary to push a fat man off a bridge</li>\n</ul>\n</li>\n</ul>\n<p><strong>Rationality at the limit</strong></p>\n<p style=\"padding-left: 60px;\">On the question of 'how close' a straightforward evolutionary approach was used. Children of the decision-makers were safe, and grandchildren.</p>\n<p style=\"padding-left: 60px;\">\"And our nephews?\" \"Don't push it\".</p>\n<p style=\"padding-left: 60px;\">But the limits of rationality, it seems, are dependent upon gender: While it was recognised that no woman could be expected to agree to the rational sacrifice of her child, it was expected by some that men might have to, and in the end the main character - male - sacrificed a grandchild.</p>\n<p>And that's it. Perhaps not a complete disposal of the trolley problem, but nevertheless an interesting excursion into the realms of philosphical dilemmas for a popular drama.&nbsp;&nbsp; Rationalism is a meme - pass it on.</p>\n<hr />\n<address>(1) like many TV aliens: surprisingly able to construct spaceships without the benefit of an opposable thumb</address> <address>(2) Yes, that was actually the 1965 back-story.<br /></address>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LMFBzsJaCRADQqw3F": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "REpzLJaQjJ2hJb6sR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 16, "extendedScore": null, "score": 3.7e-05, "legacy": true, "legacyId": "1392", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>It's just possible that some lesswrong readers may be unfamiliar with <a href=\"http://en.wikipedia.org/wiki/Torchwood\">Torchwood</a>: It's a British sci-fi TV series, a spin-off from the more famous, and very long-running cult show <a href=\"http://en.wikipedia.org/wiki/Dr_who\">Dr Who</a>.</p>\n<p>Two weeks ago Torchwood <a href=\"http://www.bbc.co.uk/torchwood/\">Series 3</a> aired. It took the form of a single story arc, over five days, shown in five parts on consecutive nights. What hopefully makes it interesting to rationalist lesswrong readers who are not (yet) Whovians was not only the space monsters <sup>(1)</sup> but also the show's determined and methodical exploration of an iterated <a href=\"http://en.wikipedia.org/wiki/Trolley_problem\">Trolley Problem</a>:&nbsp; in a process familiar to seasoned thought-experimenters the characters were tested with a dilemma followed by a succession of variations of increasing complexity, with their choices ascertained and the implications discussed and reckoned with.</p>\n<p>An hypothetical, iterated rationalist dilemma... with space monsters... and monsters a great deal more scary - and messier - than Omega -&nbsp; what's not to like?</p>\n<p>So, on the off chance that you missed it, and as a summer diversion from more academic lesswrong fare, I thought a brief description of how a familiar dilemma was handled on popular British TV this month, might be of passing interest (warning: spoilers follow)<a id=\"more\"></a></p>\n<p>The details of the scenario need not concern us too much here (and readers are warned not too expend too much mental energy exploring the various implausibilities, for want of distraction) but suffice to say that the <em>456</em>, a race of evil aliens, landed on Earth and demanded that a certain number of children be turned over to them to suffer a horrible fate-worse-than-death or else we face the familiar prospects of all out attack and the likely destruction of mankind.</p>\n<p>Resistance, it almost goes without saying, was futile.</p>\n<p>The problems faced by the team could be roughly sorted into some themes</p>\n<p><strong>The Numbers dilemma</strong> - is it worth sacrificing <em>any </em>amount of children to save the rest?</p>\n<ul>\n<li>first <sup>(2)</sup> the 456 demand just 35 children</li>\n<li>but later they demand 10% of the child population.</li>\n</ul>\n<p><strong>The Quality dilemma:</strong> does it make any difference <em>which </em>children?</p>\n<ul>\n<li>what if, say, the sacrificed children are solely those <em>whom nobody will miss</em>? </li>\n</ul>\n<p><strong>The choice dilemma</strong>: how should the sacrifical cohort be chosen?</p>\n<ul>\n<li>are there any criteria that are fairer, or more rational, than the obvious lottery?</li>\n<li>other than 'fairness' what other criteria can you think of for selecting the victims?</li>\n</ul>\n<p><strong>The limits of human rationality</strong>: are there certain 'rational' decisions that are simply too much to expect a human being to be able to make?</p>\n<ul>\n<li>what if it is your own children? Grand-children? Nephews? Neighbours?&nbsp;</li>\n</ul>\n<p>Actually despite my jocular tone in the first paragraph I don't want to make too light of this series, as it was disturbing viewing.</p>\n<p>Anyway: that being said: rationalist lesswrong community members may want to think dispassionately about the their answers before I reveal the conclusions that Russell T Davies (the writer) came to:</p>\n<p><strong id=\"Numbers\">Numbers</strong></p>\n<ul>\n<li>Lesswrongers will be encouraged to learn that the Torchwood characters were rationalists to a man and woman - there was little hesitation in agreeing to the 456's demands. </li>\n<li>A rational attempt to negotiate (600 children were offered) was faced down by the 465's intemperate release of swine flu into the negotiating chamber.</li>\n</ul>\n<p><strong id=\"Quality\">Quality</strong></p>\n<ul>\n<li>In the Torchwood universe it made a great deal of difference which children - or units as they were plausibly referred to - were sacrificed, with governments paying attention to round up the orphans, the refugees and the unloved - for the unexpectedly rational reason of minimising the suffering of the survivors.</li>\n</ul>\n<p><strong id=\"Choice\">Choice</strong></p>\n<p style=\"padding-left: 60px;\">This was handled by the politicians who considered two dimensions in the selection:</p>\n<ul>\n<li>&nbsp; \n<ul>\n<li>the <em>Darwinian dimension</em>: could they improve the human gene pool by sacrificing the least desirable 10%?&nbsp; If this seemed like a good rationalist plan (it did) then how could they quickly identify the losers? Answer: in a UK-centric political twist they chose those attending the schools with the poorest exam results.</li>\n<li>the question of <em>practicality and enforcabilty</em>. This was the consideration that caused the children of decision-makers and enforcers to be spared. In a cute nested trolley problem the children of the soldiers were <em>saved in order to</em> ensure the cooperation of their parents in rounding up the unlucky and underserving, <em>in order to </em>ensure the survival of the rest.</li>\n<li>at no stage was it necessary to push a fat man off a bridge</li>\n</ul>\n</li>\n</ul>\n<p><strong id=\"Rationality_at_the_limit\">Rationality at the limit</strong></p>\n<p style=\"padding-left: 60px;\">On the question of 'how close' a straightforward evolutionary approach was used. Children of the decision-makers were safe, and grandchildren.</p>\n<p style=\"padding-left: 60px;\">\"And our nephews?\" \"Don't push it\".</p>\n<p style=\"padding-left: 60px;\">But the limits of rationality, it seems, are dependent upon gender: While it was recognised that no woman could be expected to agree to the rational sacrifice of her child, it was expected by some that men might have to, and in the end the main character - male - sacrificed a grandchild.</p>\n<p>And that's it. Perhaps not a complete disposal of the trolley problem, but nevertheless an interesting excursion into the realms of philosphical dilemmas for a popular drama.&nbsp;&nbsp; Rationalism is a meme - pass it on.</p>\n<hr>\n<address>(1) like many TV aliens: surprisingly able to construct spaceships without the benefit of an opposable thumb</address> <address>(2) Yes, that was actually the 1965 back-story.<br></address>", "sections": [{"title": "Numbers", "anchor": "Numbers", "level": 1}, {"title": "Quality", "anchor": "Quality", "level": 1}, {"title": "Choice", "anchor": "Choice", "level": 1}, {"title": "Rationality at the limit", "anchor": "Rationality_at_the_limit", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "87 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 87, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-28T16:08:16.244Z", "modifiedAt": null, "url": null, "title": "Thomas C. Schelling's \"Strategy of Conflict\"", "slug": "thomas-c-schelling-s-strategy-of-conflict", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:16.474Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tJQsxD34maYw2g5E4/thomas-c-schelling-s-strategy-of-conflict", "pageUrlRelative": "/posts/tJQsxD34maYw2g5E4/thomas-c-schelling-s-strategy-of-conflict", "linkUrl": "https://www.lesswrong.com/posts/tJQsxD34maYw2g5E4/thomas-c-schelling-s-strategy-of-conflict", "postedAtFormatted": "Tuesday, July 28th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thomas%20C.%20Schelling's%20%22Strategy%20of%20Conflict%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThomas%20C.%20Schelling's%20%22Strategy%20of%20Conflict%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtJQsxD34maYw2g5E4%2Fthomas-c-schelling-s-strategy-of-conflict%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thomas%20C.%20Schelling's%20%22Strategy%20of%20Conflict%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtJQsxD34maYw2g5E4%2Fthomas-c-schelling-s-strategy-of-conflict", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtJQsxD34maYw2g5E4%2Fthomas-c-schelling-s-strategy-of-conflict", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 719, "htmlBody": "<p>It's an <a href=\"http://www.amazon.com/Strategy-Conflict-Thomas-C-Schelling/dp/0674840313\">old book</a>, I know, and one that many of us have already read.&nbsp;But if you haven't, you should.</p>\n<p>If there's anything in the world that deserves to be called a&nbsp;<a href=\"/lw/gn/the_martial_art_of_rationality/\">martial art of rationality</a>, this book is the closest approximation yet. Forget rationalist Judo: this is rationalist eye-gouging, rationalist gang warfare, rationalist nuclear deterrence.&nbsp;Techniques that let you win, but you don't want to look in the mirror afterward.</p>\n<p>Imagine you and I have been separately parachuted into an unknown mountainous area. We both have maps and radios, and we know our own positions, but don't know each other's positions. The task is to rendezvous. Normally we'd coordinate by radio and pick a suitable meeting point, but this time you got lucky. So lucky in fact that I want to strangle you: upon landing you discovered that your radio is broken. It can transmit but not receive.</p>\n<p>Two days of rock-climbing and stream-crossing later, tired and dirty, I arrive at the hill where you've been sitting all this time smugly enjoying your lack of information.</p>\n<p>And after we split the prize and cash our checks I learn that you broke the radio on purpose.<a id=\"more\"></a></p>\n<p>Schelling's book walks you through numerous conflict situations where an unintuitive and often self-limiting move helps you win, slowly building up to the topic of nuclear deterrence between the US and the Soviets. And it's not idle speculation either: the author <a href=\"http://en.wikipedia.org/wiki/Thomas_Schelling\">worked</a> at the White House at the dawn of the Cold War and his theories eventually found wide military application in deterrence and arms control. Here's a selection of quotes to give you a flavor: the <em>whole book</em> is like this, except interspersed with game theory math.</p>\n<blockquote>\n<p>The use of a professional collecting agency by a business firm for the collection of debts is a means of achieving unilateral rather than bilateral communication with its debtors and of being therefore unavailable to hear pleas or threats from the debtors.</p>\n</blockquote>\n<blockquote>\n<p>A sufficiently severe and certain penalty on the <em>payment</em> of blackmail can protect a potential victim.</p>\n</blockquote>\n<blockquote>\n<p>One may have to pay the bribed voter if the election is won, not on how he voted.</p>\n</blockquote>\n<div>\n<blockquote>\n<p>I can block your car in the road by placing my car in your way; my deterrent threat is passive, the decision to collide is up to you. If you, however, find me in your way and threaten to collide unless I move, you enjoy no such advantage: the decision to collide is still yours, and I enjoy deterrence. You have to arrange to&nbsp;<em>have</em>&nbsp;to collide unless I move, and that is a degree more complicated.</p>\n</blockquote>\n</div>\n<blockquote>\n<p>We have learned that the threat of massive destruction may deter an enemy only if there is a corresponding implicit promise of nondestruction in the event he complies, so that we must consider whether too great a capacity to strike him by surprise may induce him to strike first to avoid being disarmed by a first strike from us.</p>\n</blockquote>\n<blockquote>\n<p>Leo Szilard has even pointed to the paradox that one might wish to confer immunity on foreign spies rather than subject them to prosecution, since they may be the only means by which the enemy can obtain persuasive evidence of the important truth that we are making no preparations for embarking on a surprise attack.</p>\n</blockquote>\n<p>I sometimes think of game theory as being roughly divided in three parts, like Gaul. There's competitive zero-sum game theory, there's &nbsp;<a href=\"/lw/12v/fair_division_of_blackhole_negentropy_an/\">cooperative game theory</a>,&nbsp;and there are games where players compete but also have some shared interest. Except this third part isn't a middle ground. It's actually better thought of as&nbsp;<em style=\"font-style: italic;\">ultra-competitive</em>&nbsp;game theory. Zero-sum settings are relatively harmless: you minimax and that's it. It's the variable-sum games that make you&nbsp;<em style=\"font-style: italic;\">nuke&nbsp;</em>your neighbour.</p>\n<p>Sometime ago in my wild and reckless youth that hopefully isn't over yet, a certain ex-girlfriend took to harassing me with suicide threats. (So making her stay alive was presumably our common interest in this variable-sum game.) As soon as I got around to looking at the situation through Schelling goggles, it became clear that ignoring the threats just leads to escalation. The correct solution was making myself&nbsp;<em style=\"font-style: italic;\">unavailable</em>&nbsp;for threats. Blacklist the phone number,&nbsp;block\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nthe email, spend a lot of time out of home. If any messages get through, pretend I didn't receive them anyway. It worked. It felt kinda bad, but it worked.</p>\n<div>Hopefully you can also find something that works.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 6, "4Kcm4etxAJjmeDkHP": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tJQsxD34maYw2g5E4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 113, "baseScore": 134, "extendedScore": null, "score": 0.000211, "legacy": true, "legacyId": "1450", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 134, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 154, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["teaxCFgtmCQ3E9fy8", "z3W8PRHJM9ZanTDcx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-29T12:08:31.135Z", "modifiedAt": null, "url": null, "title": "Information cascades in scientific practice", "slug": "information-cascades-in-scientific-practice", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:17.451Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QtG2iDnYGZEumXzsb/information-cascades-in-scientific-practice", "pageUrlRelative": "/posts/QtG2iDnYGZEumXzsb/information-cascades-in-scientific-practice", "linkUrl": "https://www.lesswrong.com/posts/QtG2iDnYGZEumXzsb/information-cascades-in-scientific-practice", "postedAtFormatted": "Wednesday, July 29th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Information%20cascades%20in%20scientific%20practice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInformation%20cascades%20in%20scientific%20practice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQtG2iDnYGZEumXzsb%2Finformation-cascades-in-scientific-practice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Information%20cascades%20in%20scientific%20practice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQtG2iDnYGZEumXzsb%2Finformation-cascades-in-scientific-practice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQtG2iDnYGZEumXzsb%2Finformation-cascades-in-scientific-practice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 136, "htmlBody": "<p>Here's an interesting recent paper in the British Medical Journal: <a href=\"http://www.bmj.com/cgi/content/full/339/jul20_3/b2680\">\"How citation distortions create unfounded authority: analysis of a citation network\"</a>. (I don't know if this is freely accessible, but the abstract should be.)</p>\n<p>From the paper:</p>\n<p>\"<strong>Objective</strong> To understand belief in a specific scientific claim by studying the pattern of citations among papers stating it.\"</p>\n<p>\"<strong>Conclusion</strong> Citation is both an impartial scholarly method and a powerful form of social communication. Through distortions in its social use that include bias, amplification, and invention, citation can be used to generate information cascades resulting in unfounded authority of claims. Construction and analysis of a claim specific citation network may clarify the nature of a published belief system and expose distorted methods of social citation.\"</p>\n<p>It also includes a list of specific ways in which citations were found to amplify or invent evidence.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sHbKQDqrSinRPcnBv": 2, "3uE2pXvbcnS9nnZRE": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QtG2iDnYGZEumXzsb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 5.111901000713212e-07, "legacy": true, "legacyId": "1451", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-30T00:12:06.160Z", "modifiedAt": null, "url": null, "title": "The Obesity Myth", "slug": "the-obesity-myth", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:16.304Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Matt_Simpson", "createdAt": "2009-03-05T21:06:45.432Z", "isAdmin": false, "displayName": "Matt_Simpson"}, "userId": "v4krJe8Qa4jnhPTmd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WfHiyRxMj6aL7PN7i/the-obesity-myth", "pageUrlRelative": "/posts/WfHiyRxMj6aL7PN7i/the-obesity-myth", "linkUrl": "https://www.lesswrong.com/posts/WfHiyRxMj6aL7PN7i/the-obesity-myth", "postedAtFormatted": "Thursday, July 30th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Obesity%20Myth&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Obesity%20Myth%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWfHiyRxMj6aL7PN7i%2Fthe-obesity-myth%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Obesity%20Myth%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWfHiyRxMj6aL7PN7i%2Fthe-obesity-myth", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWfHiyRxMj6aL7PN7i%2Fthe-obesity-myth", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 370, "htmlBody": "<p><strong>Related To: &nbsp;</strong><a href=\"/lw/a6/the_unfinished_mystery_of_the_shangrila_diet/\">The Unfinished Mystery of the Shangri-La Diet</a> and <a href=\"/lw/af/missed_distinctions/\">Missed Distinctions</a>&nbsp;</p>\n<p>Megan McArdles <a href=\"http://meganmcardle.theatlantic.com/archives/2009/07/americas_moral_panic_over_obes.php\">blogs</a> an interview with Paul Campos, author of <a href=\"https://www.amazon.com/dp/1592400663?tag=livefromthewt-20&amp;camp=0&amp;creative=0&amp;linkCode=as1&amp;creativeASIN=1592400663&amp;adid=0BDJFXPQP5S6SN79B7P8&amp;\">The Obesity Myth</a>. &nbsp;I'll let anyone who is interest read the whole thing, but here's some interesting excerpts:<a id=\"more\"></a></p>\n<blockquote>\n<p><span style=\"font-family: Georgia; color: #303030; line-height: 19px;\">I mean, there's no better established empirical proposition in medical science that we don't know how to make people thinner. But apparently this proposition is too disturbing to consider, even though it's about as well established as that cigarettes cause lung cancer. So all these proposals about improving public health by making people thinner are completely crazy. They are as non-sensical as anything being proposed by public officials in our culture right now, which is saying something.&nbsp;</span></p>\n<p><span style=\"font-family: Georgia; color: #303030; line-height: 19px;\">It's conceivable that through some massive policy interventions you might be able to reduce the population's average BMI from 27 to 25 or something like that. But what would be the point? There aren't any health differences to speak of for people between BMIs of about 20 and 35, so undertaking the public health equivalent of the Apollo program to reduce the populace's average BMI by a unit or two (and again I will emphasize that we don't actually know if we could do even&nbsp;<em>that</em>) is an incredible waste of public health resources</span></p>\n</blockquote>\n<p><span style=\"font-family: Georgia; color: #303030; line-height: 19px;\">and</span></p>\n<blockquote>\n<p><span style=\"font-family: Georgia; color: #303030; line-height: 19px;\"><strong>Megan</strong>:&nbsp;<em>An economist recently pointed out that we don't encourage people to move to the country, even though rural people live more than three years longer than urban people, and the diffefence in their healthy life expectancy is even more outsized. Nor do we encourage people to find Jesus or get married. We target \"unhealthy\" behaviors that are already stigmatized</em>.<br /><br /><strong>Paul</strong>: Right, as Mary Douglas the anthropologist has pointed out, we focus on risks not on the basis of \"rational\" cost-benefit analysis, but because of the symbolic work focusing on those risks does -- most particularly signalling disapproval of certain groups and behaviors. In this culture fatness is a metaphor for poverty, lack of self-control, and other stuff that freaks out the new Puritans all across the ideological spectrum, which is why the war on fat is so ferocious -- it appeals very strongly to both the right and the left, for related if different reasons.</span></p>\n</blockquote>\n<p><span style=\"font-family: Georgia; color: #303030;\"><span style=\"line-height: 19px;\"><br /></span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WfHiyRxMj6aL7PN7i", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 14, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "1452", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 61, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BD4oExxQguTgpESdm", "Afvk6GGfoo8mea5cb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-31T04:25:28.630Z", "modifiedAt": "2021-05-18T19:13:06.169Z", "url": null, "title": "The Hero With A Thousand Chances", "slug": "the-hero-with-a-thousand-chances", "viewCount": null, "lastCommentedAt": "2022-03-26T03:11:22.913Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EKu66pFKDHFYPaZ6q/the-hero-with-a-thousand-chances", "pageUrlRelative": "/posts/EKu66pFKDHFYPaZ6q/the-hero-with-a-thousand-chances", "linkUrl": "https://www.lesswrong.com/posts/EKu66pFKDHFYPaZ6q/the-hero-with-a-thousand-chances", "postedAtFormatted": "Friday, July 31st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Hero%20With%20A%20Thousand%20Chances&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Hero%20With%20A%20Thousand%20Chances%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEKu66pFKDHFYPaZ6q%2Fthe-hero-with-a-thousand-chances%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Hero%20With%20A%20Thousand%20Chances%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEKu66pFKDHFYPaZ6q%2Fthe-hero-with-a-thousand-chances", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEKu66pFKDHFYPaZ6q%2Fthe-hero-with-a-thousand-chances", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2605, "htmlBody": "<p>\"Allow me to make sure I have this straight,\" the hero said.&nbsp; \"I've been untimely ripped from my home world to fight unspeakable horrors, and you say I'm here because I'm <em>lucky?</em>\"</p>\n<p>Aerhien dipped her eyelashes in elegant acknowledgment; and quietly to herself, she thought:&nbsp; <em>Thirty-seven.</em>&nbsp; Thirty-seven heroes who'd said just that, more or less, on arrival.</p>\n<p>Not a sign of the thought showed on her outward face, where the hero could see, or the other council members of the Eerionnath take note.&nbsp; Over the centuries since her accidental immortality she'd built a reputation for serenity, more or less because it seemed to be expected.</p>\n<p>\"There are kinds and kinds of luck,\" Aerhien said serenely.&nbsp; \"Not every person desires their personal happiness above all else.&nbsp; Those who are lucky in aiding others, those whose luck is great in succor and in rescue, these ones are not always happy themselves.&nbsp; You are here, hero, because you have a <em>hero's </em>luck.&nbsp; The boy whose dusty heirloom sword proves to be magical. The peasant girl who finds herself the heir to a great kingdom.&nbsp; Those who discover, in time of sudden stress, an untrained wild magic within themselves.&nbsp; Success born not of learning, not of skill, not of determination, but unplanned coincidence and fortunes of birth:&nbsp; That is a hero's luck.\"<a id=\"more\"></a></p>\n<p>\"Gosh,\" said the hero after a long, awkward pause, \"thanks for the compliment.\"</p>\n<p>\"It is not a compliment,\" Aerhien said, \"but this is: that you have taken good advantage of your luck.&nbsp; Our enemy does not speak, we do not know if there is any aliveness in it to <em>think</em>; but it learns, or seems to learn.&nbsp; We have never won against it using the same trick twice.&nbsp; It is rare now that a hero succeeds in conceiving a genuinely new trick, for we have fought this shadow long under our sun.&nbsp; For this reason we have taken to summoning heroes from distant dimensions with other modes of thought; sometimes one such knows a truly new technique, and at least they fight differently.&nbsp; But far more often, hero, the hero wins by luck.\"</p>\n<p>\"Huh,\" said the hero.&nbsp; He frowned; more in thought, it seemed, than in displeasure.&nbsp; \"How... very odd.&nbsp; I wonder why that is.&nbsp; What kind of enemy can be defeated only by luck?\"</p>\n<p>\"A nameless enemy and null,\" said Aerhien.&nbsp; \"Structureless and empty, horrible and dark, the most terrifying thing imaginable:&nbsp; We call it Dust.&nbsp; That seems to be its only desire, to tear down every bit of structure in the world, grind it into specks of perfect chaos.&nbsp; Always the Dust is defeated, always it takes a new shape immune to its last defeat.\"</p>\n<p>\"I wonder,\" murmured the hero, \"if it will run out of shapes, and then end; or if it will finally become invincible.\"</p>\n<p>(One of the other Eerionnath shuddered.)</p>\n<p>\"I do not know,\" Aerhien said simply.&nbsp; \"I do not know the nature of the Dust, nor the nature of the Counter-Force that opposes it.&nbsp; The Dust is terrible and our world should long since have ended.&nbsp; We are not fools enough to believe we could be lucky so many times by chance alone.&nbsp; But the Counter-Force has never acted openly; it never reveals itself except in - a hero's luck.&nbsp; And so we, the council Eerionnath to prevent the world from destruction, are at your disposal to command; and all the power and resource that this world holds, for your battle.\"</p>\n<p>And she, Aerhien, and the council Eerionnath, bowed low.</p>\n<p>Then they waited to see if the hero would demand dominions or slaves as payment, before condescending to rescue a people in distress.</p>\n<p>If so they would dispose of him, and summon another.</p>\n<p>This one, though, seemed to have at least some qualities of a true hero; his face showed no avarice, only an abstracted puzzlement.&nbsp; \"A hidden Counter-Force...\" he murmured.&nbsp; \"Excuse me, but this is all very vague.&nbsp; Can you give me a <em>specific example </em>of a hero's luck?\"</p>\n<p>Aerhien opened her mouth, and then the breath caught in her throat; suddenly and involuntarily, her memory went back to that huge spell gone out of control which had blasted the then-form of the Dust, killed the hero her lover, ruined their home and country, and rendered her accidentally immortal, all those centuries ago -</p>\n<p>Ghandhol, the second-oldest of the council, must have guessed her silent distress; for he spoke up to cover the gap:&nbsp; \"There was a certain time,\" he said gravely, \"when the hero of that age, sent off the entire army of the world in a diversionary attack against the strongest fortification of the enemy.&nbsp; While he, with but a single friend, walked directly into enemy territory, carrying undefended the single most valuable magic the Dust could possibly gain.&nbsp; Then the Dust captured and corrupted the hero's mind.&nbsp; And when all seemed absolutely lost, they only won because - in an event that was no part at all of their original plan - a hungry creature bit off the hero's finger and then accidentally fell into an open lava flow, which in turn caused -\"</p>\n<p>\"<em>That </em>was an <em>extreme </em>case,\" said one of the younger councilors; that one looked a bit nervous, lest <em>this </em>hero get the wrong idea.&nbsp; \"None since have tried to imitate the Volcano Suicide Hero -\"</p>\n<p>\"Ah!\" said the hero in a tone of sudden enlightenment.</p>\n<p>Then the hero frowned.&nbsp; \"Oh, <em>dear</em>...\" he said under his breath.</p>\n<p>The councilors looked at one another in mute puzzlement.&nbsp; The hairs pricked on Aerhien's neck; she had lived long enough to have seen almost everything at least once before.&nbsp; And her lover had frowned, just like that, an instant before his spell went wild.</p>\n<p>The hero's brow was furrowed like a father whose child has just asked a question which has an answer, but whose answer no child can understand.&nbsp; \"Do you...\" he said at last.&nbsp; \"Do you have knowledge... about the <em>khanfhighur...</em> that's not even translating, is it.&nbsp; Do you know about... the things that things are made of?&nbsp; And are the things constantly splitting all the time?&nbsp; Not singly, but in - in groups -\"</p>\n<p>The other councilors Eerionnath were staring at him in mute incomprehension.&nbsp; But Aerhien, who had been through it all before, gravely shook her head.&nbsp; \"We do not possess that knowledge; nor do we know why our sun burns, or why the sky is red, or what makes a word a spell; nor has any summoned hero succeeded in raveling them.\"&nbsp; Aerhien held up her hand.&nbsp; \"Hand, made of fingers; beneath the finger, skin and muscle and vein, beneath the muscle, <em>sharrak</em> and <em>flom.</em>&nbsp; That is the limit of our knowledge.&nbsp; Some worlds, it seems, are harder to ravel than others.\"</p>\n<p>The hero waved it off.&nbsp; \"No, it doesn't matter - well, it matters a great deal, but not for now.&nbsp; I only asked to see if I could get confirmation... it doesn't matter.\"</p>\n<p>Aerhien waited patiently; they were rare, this sort of hero, but the more distant and alien sort did sometimes treat her world as a puzzle to be solved.&nbsp; She usually sought those similar enough in body and mind to feel empathy for her people's plight; but sometimes she thought of the great victory won by the Icky Blob Hero, and wondered if she should look further afield.</p>\n<p>\"What would happen if the Dust won?\" asked the hero.&nbsp; \"Would the whole world be destroyed in a single breath?\"</p>\n<p>Aerhien's brow quirked ever so slightly.&nbsp; \"No,\" she said serenely.&nbsp; Then, because the question was strange enough to demand a longer answer:&nbsp; \"The Dust expands slowly, using territory before destroying it; it enslaves people to its service, before slaying them.&nbsp; The Dust is patient in its will to destruction.\"</p>\n<p>The hero flinched, then bowed his head.&nbsp; \"I suppose <em>that </em>was too much to hope for; there wasn't really any reason to hope, except hope... it's not required by the logic of the situation, alas...\"</p>\n<p>Suddenly the hero looked up sharply; there was a piercing element, now, in his gaze.&nbsp; \"There's a great deal you're neglecting to tell me about this heroing business.&nbsp; Were you planning to mention that the 'hero' which your council chooses and anoints, often turns out not to be the real hero at all?&nbsp; That the Counter-Force often ends up working through someone else entirely?\"</p>\n<p>The members of the council traded glances.&nbsp; \"You didn't exactly <em>ask </em>about that,\" said Ghandhol mildly.</p>\n<p>The hero nodded.&nbsp; \"I suppose not.&nbsp; And the Volcano Suicide Hero - what exactly <em>happened </em>to him, that caused no hero to ever dare tempt fate so much again, in the history you remember?\"</p>\n<p>\"His home country was ruined,\" Aerhien said softly, \"while the army marched elsewhere on his diversion.&nbsp; It threw him into a misery from which he never recovered, until one day he set sail in a ship and did not return.\"</p>\n<p>The hero nodded.&nbsp; \"Poor payment, one would think, for saving the world.\"&nbsp; The hero's face grew grim, and his voice became solemn and formal, mimicking Aerhien's cadences.&nbsp; \"But the Counter-Force is <em>not </em>the pure power of Good.&nbsp; It seems to care only and absolutely about stopping the Dust.&nbsp; It cares <em>nothing </em>for heroes, or countries, or innocent lives and victims.&nbsp; If it could save a thousand children from death, only by nudging the fall of a pebble, it would not bother; it has had such opportunities, and not acted.\"</p>\n<p>Ghufhus, the youngest member of the council, grimaced, looking offended.&nbsp; \"How is it our right to ask for <em>more?</em>\" he demanded.&nbsp; \"That we are saved from the Dust is miracle enough -\"</p>\n<p>Ghufhus stopped, noticing then that the other Eerionnath were sitting frozen.&nbsp; Even Aerhien's mask of dispassion had cracked.</p>\n<p>\"Ah...\" Ghufhus said, puzzled.&nbsp; \"How do you... know all this?&nbsp; Is there a Counter-Force in your own world?\"</p>\n<p><em>Fool,</em> Aerhien thought to herself.&nbsp; The hero had seemed puzzled by the idea, at first, and had needed to ask for examples.&nbsp; She decided then and there that Ghufhus would meet with an accident before the next council meeting; their world had no room for stupid Eerionnath.</p>\n<p>And the hero himself shook his head.&nbsp; \"No,\" the hero said.&nbsp; \"You have never summoned a hero who remembers a Counter-Force like yours.\"</p>\n<p>This was also true.</p>\n<p>\"Nor will you ever,\" the hero added, \"unless you try some way of seeking that specifically, in your summoning.&nbsp; It would never happen by accident.\"</p>\n<p>Aerhien willed her stiff lips to move.&nbsp; It should have been wonderful news, but the hero himself seemed anything but happy.&nbsp; \"You... have fathomed the nature of the Counter-Force?\"</p>\n<p>The hero nodded.</p>\n<p>\"And?\" Aerhien said.&nbsp; \"What is the rest of it?&nbsp; The part you are still considering whether to tell us?\"</p>\n<p>Ghandhol's eyebrows went up a tiny fraction, and his head tilted ever so slightly toward her, signaling his surprise and appreciation.</p>\n<p>The hero hesitated.&nbsp; Then he sighed.</p>\n<p>\"The Counter-Force isn't going to help you this time.&nbsp; No hero's luck.&nbsp; Nothing but creativity and any scraps of <em>real</em> luck - and true random chance is as liable to hurt <em>you </em>as the Dust.&nbsp; Even if you do survive this time, the Counter-Force won't help you next time either.&nbsp; Or the time after that.&nbsp; What you remember happening before - will not happen for you ever again.\"</p>\n<p>Aerhien felt the nausea; like a blow to the pit of her stomach it felt, the end of the world.&nbsp; The rest of the council Eerionnath seemed torn between fear and skepticism; but her own instincts, honed over long centuries, left little room for doubt.&nbsp; The distant heroes sometimes knew things... and sometimes guessed wrong.&nbsp; But after a hero had been right a few times, you learned to listen to that one, even if you couldn't understand the reasons or the logic...</p>\n<p>\"Why?\" Ghufhus said, sounding skeptical.&nbsp; \"Why would the Counter-Force work all this time, and then suddenly -\"</p>\n<p>Ghandhol interrupted with the far more urgent question.&nbsp; \"How can we restore the Counter-Force?\"</p>\n<p>\"You can't,\" said the hero.</p>\n<p>There was a remote sadness in his eyes, the only sign that he knew exactly what he was saying.</p>\n<p>\"Then you have pronounced the absolute doom of this world,\" Ghandhol said heavily.</p>\n<p>And <em>then</em> the hero smiled, and it was twisted and grim and defiant, all at the same time.&nbsp; \"Oh... not quite <em>absolute</em> doom.&nbsp; In my own world, we have our own notions about heroes, which are not about heroic luck.&nbsp; One of us said: a hero is someone who can stand there at the moment when all hope is dead, and look upon the abyss without flinching.&nbsp; Another said: a superhero is someone who can save people who could not be saved by any ordinary means; whether it is few people or many people, a superhero is someone who can save people who cannot be saved.&nbsp; We shall try a little of my own world's style of heroism, then.&nbsp; Your world cannot be saved by any ordinary means; it is doomed.&nbsp; Like <a href=\"http://www.forbes.com/forbes/2002/0107/138_print.html\">a child born with a fatal disease</a>; it contained the seed of its own death from the beginning.&nbsp; Your annihilation is not an unlucky chance to be prevented, or an unpleasant possibility to avert.&nbsp; It is your destiny that has already been written from the beginning.&nbsp; You are the walking dead, and this is a dead world spinning, and many other worlds like this one are already destroyed.\"</p>\n<p>\"But <em>this</em> world is going to live anyway.&nbsp; I have decided it.\"</p>\n<p>\"<em>That </em>is my own world's heroism.\"</p>\n<p>\"How?\" Aerhien said simply.&nbsp; \"How can our world live, if what you say is true?\"</p>\n<p>The hero's eyes had gone unfocused, his face somewhat slack.&nbsp; \"You will deliver to me the record of every single hero that your history remembers.&nbsp; You will bring historians here for my consultation.&nbsp; Your world <em>cannot </em>survive if it must fight this battle over and over again, with the Dust growing stronger each time.&nbsp; It is my thought that on <em>this</em> attempt, we must neutralize the Dust once and for all -\"</p>\n<p>\"Do you think that hasn't been <em>tried?</em>\" Ghufhus demanded incredulously.</p>\n<p>The hero smiled that twisted smile again.&nbsp; \"Ah, but if you had <em>succeeded,</em> you would not have needed to summon me, now would you?&nbsp; Though I am not quite sure that is valid logic, in a case like this...&nbsp; But it does seem that none of the other heroes fathomed your Counter-Force, which puts an upper limit on their perception.\"&nbsp; The hero nodded to himself.&nbsp; \"All things have a pattern.&nbsp; Bring me the records, and I will see if I can fathom this Dust, and the limit of its learning ability - there <em>must </em>be a limit, or <em>no </em>amount of luck could ever save you.&nbsp; All things have a cause:&nbsp; If something like the Dust came into existence once, perhaps a true Counter-Force can be created to oppose it.&nbsp; Those are the ideas that occur to me in the first thirty seconds, at any rate.&nbsp; I must study.&nbsp; Bring me your keepers of knowledge.&nbsp; They will be my army.\"</p>\n<p>Aerhien bowed, in truth this time, and very low, and the Eerionnath bowed with her.&nbsp; \"Command and we shall obey, hero,\" she said simply.</p>\n<p>The hero turned from her, and looked out the window at the red sky, and the small dots on the land that were the homes of the innocents to be protected.</p>\n<p>\"Don't call me that,\" he said, and it was a command.&nbsp; \"You can call me that after we've won.\"</p>\n<p>\"But -\"</p>\n<p>It was Ghufhus who said it, and Aerhien promised herself that if it was a stupid question, his accident would be a painful one.</p>\n<p>\"But what <em>is</em> - what <em>was</em> the Counter-Force?\"</p>\n<p>Aerhien wavered, then decided against it.</p>\n<p>It might not matter now, but she also wanted to know.</p>\n<p>The hero sighed.&nbsp; \"It's a long story,\" he said.&nbsp; \"And to be frank, if you're to understand this <em>properly</em>, there's a lot of other things I have to explain first before I get to the <em>ahntharhapik</em> principle.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 2, "PbShukhzpLsWpGXkM": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EKu66pFKDHFYPaZ6q", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 98, "baseScore": 129, "extendedScore": null, "score": 0.0002, "legacy": true, "legacyId": "1457", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 129, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 170, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2009-07-31T04:25:28.630Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-31T09:13:47.353Z", "modifiedAt": null, "url": null, "title": "Pract: A Guessing and Testing Game", "slug": "pract-a-guessing-and-testing-game", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:15.843Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "sFZXfM5397tStmsrK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JAyz8mEir5mZMTDK7/pract-a-guessing-and-testing-game", "pageUrlRelative": "/posts/JAyz8mEir5mZMTDK7/pract-a-guessing-and-testing-game", "linkUrl": "https://www.lesswrong.com/posts/JAyz8mEir5mZMTDK7/pract-a-guessing-and-testing-game", "postedAtFormatted": "Friday, July 31st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pract%3A%20A%20Guessing%20and%20Testing%20Game&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APract%3A%20A%20Guessing%20and%20Testing%20Game%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJAyz8mEir5mZMTDK7%2Fpract-a-guessing-and-testing-game%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pract%3A%20A%20Guessing%20and%20Testing%20Game%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJAyz8mEir5mZMTDK7%2Fpract-a-guessing-and-testing-game", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJAyz8mEir5mZMTDK7%2Fpract-a-guessing-and-testing-game", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1026, "htmlBody": "<p>Here&rsquo;s a game that you can play for real against a human opponent. If the administrators don&rsquo;t mind, you can play it right here in the comments.</p>\n<p>The game is called Pract.<a id=\"more\"></a></p>\n<p><a name=\"rules\"></a></p>\n<h2 id=\"rules\">Rules</h2>\n<p>Pract is played using finite sequences of integers, called &ldquo;sequences.&rdquo;</p>\n<p>To start, each player chooses a well-defined infinite set of sequences, such that every sequence is either demonstrably in or demonstrably out of the set. The game ends when one player guesses the other&rsquo;s set.<a name=\"fnref1\"></a><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\"><sup>1</sup></a></p>\n<p>Once they have picked their sets, the players take turns.</p>\n<p><a name=\"definitions\"></a></p>\n<h3 id=\"definitions\">Definitions</h3>\n<ol class=\"decimal\">\n<li>A player <em>specifies</em> a sequence by writing each integer, in order, in decimal.</li>\n<li>A sequence&rsquo;s <em>classification</em> relative to a set is a statement of whether the sequence is in or out of the set.</li>\n<li>A player <em>classifies</em> a sequence by writing the sequence&rsquo;s classification relative to that player&rsquo;s chosen set.</li>\n<li>The <em>current player</em> is the player whose turn it currently is.</li>\n<li>A player&rsquo;s <em>final score</em> is the number of guesses made by that player plus the length of that player&rsquo;s statement (at the end of the game) of the definition of his or her own set.</li>\n</ol>\n<p><a name=\"gameplay\"></a></p>\n<h3 id=\"gameplay\">Gameplay</h3>\n<p>On each turn, the current player may either:</p>\n<ol class=\"decimal\">\n<li>Try to guess the other player&rsquo;s set, in which case the other player indicates whether the guess was correct or not. Or,</li>\n<li>Specify a sequence, in which case both players classify that sequence.<ol class=\"decimal\">\n<li>If, during the previous turn, the other player made an incorrect guess of the current player&rsquo;s set, the specified sequence must have an opposite classification relative to the actual set than it does relative to the guessed set.</li>\n<li>If the other player instead specified a sequence, the two sequences (the one specified by the current player this turn and the one specified the other player on the previous turn) must have opposite classifications relative to the current player&rsquo;s set.</li>\n</ol></li>\n</ol>\n<p>When a correct guess is made, each player states the definition of his or her own set. The player who guessed correctly makes this statement second. Then the game ends.</p>\n<p><a name=\"winning-and-losing\"></a></p>\n<h3 id=\"winning-and-losing\">Winning and Losing</h3>\n<p>If the player who guessed correctly has a lower final score than the other player, the player who guessed correctly wins and the other player loses. Otherwise, both players lose.</p>\n<p>If a player withdraws from the game during his or her own turn, both players lose.<a name=\"fnref2\"></a><a id=\"fnref2\" class=\"footnoteRef\" href=\"#fn2\"><sup>2</sup></a></p>\n<p>Statement lengths affect final scores, so language, notation, and encoding are relevant. In most cases, common sense, context, and the medium itself should suffice. (Pract is meant to be played over the Internet in a forum, chat room, mailing list, or other similar medium.)</p>\n<p>Pract is meant as way to practice both reasoning and being reasonable. Those who would rather argue well than play well should spectate.</p>\n<p><a name=\"examples\"></a></p>\n<h2 id=\"examples\">Examples</h2>\n<p>Here&rsquo;s an example of the beginning of a game between Alice and Bob:</p>\n<pre><code>alice&gt; 2, 4, 6\nalice&gt; in\nbob&gt; out\nbob&gt; 7, 2, 4\nbob&gt; in\nalice&gt; out\n</code></pre>\n<p>The above example shows two turns: the first is Alice&rsquo;s and the second is Bob&rsquo;s. Both players have used their turn to specify a sequence rather than make a guess, and each player classifies each sequence as required by the rules.</p>\n<p>Bob is being redundant when he classifies the sequence he specified. The rules prevent him from selecting a sequence that is out of his own set because the sequence specified by Alice is out of his set.</p>\n<p>Here&rsquo;s an example showing the end of a game:</p>\n<pre><code>cathy&gt; 5, 7, 8\ncathy&gt; out\ndave&gt; out\ndave&gt; increasing\ncathy&gt; no\ncathy&gt; 3, 2\ncathy&gt; in\ndave&gt; out\ndave&gt; 4, 4, 2, 2\ndave&gt; in\ncathy&gt; out\ncathy&gt; even numbers\ndave&gt; yes\ndave&gt; all even\ncathy&gt; odd sum\ndave&gt; length 8 + 5 guesses = 13\ncathy&gt; length 7 + 4 guesses = 11\n</code></pre>\n<p>The first turn shown is Cathy&rsquo;s, and she uses it to specify a sequence. On the next turn, Dave makes an incorrect guess. Then there are two more turns on which a sequence is specified. Finally, Cathy ends the game by making a correct guess. The players state their sets and calculate their scores. From the calculations, we can infer that there were several previous guesses by each player.</p>\n<p>Cathy is being redundant when she classifies <code>3, 2</code>. The rules prohibit her from selecting a sequence that is both non-increasing and out of her set because Dave incorrectly guessed on the previous turn that her set was the set of all increasing sequences. (In a typical game, nearly half the classifications are redundant. The redundancy makes it much easier to verify that players are following the rules.)</p>\n<p>The players are not using full sentences or punctuation to describe the sets, and Dave&rsquo;s description of his own set is shorter than Cathy&rsquo;s equivalent description when she guesses it. This is reasonable, as long as it does not introduce ambiguity or involve silly things like languages invented on the fly.</p>\n<p>There may be situations where it is not clear how the rules apply. For example, the correctness of a guess may depend on some open question in mathematics. So long as players try to avoid such situations, rather than cause them, they should be rare and resolvable.</p>\n<div class=\"footnotes\">\n<hr />\n<ol><a name=\"fn1\"></a>\n<li id=\"fn1\">\n<p>ETA: Pract is inspired in part by <a href=\"http://en.wikipedia.org/wiki/Zendo_%28game%29\">Zendo</a>, as well as various ideas I&rsquo;ve seen around LessWrong like the <a href=\"/lw/iw/positive_bias_look_into_the_dark/\">2 4 6 experiment</a>, and <a href=\"/lw/14a/thomas_c_schellings_strategy_of_conflict/\">variable-sum games</a>. <a class=\"footnoteBackLink\" title=\"Jump back to footnote 1\" href=\"#fnref1\">\u21a9</a></p>\n</li>\n<a name=\"fn2\"></a>\n<li id=\"fn2\">\n<p>ETA: New rule for withdrawing. My main concern from the beginning was that both players would be stumped by their own biases and unable to finish the game. The discussion on how to play in bad faith gave me an idea for how to deal with that. <a class=\"footnoteBackLink\" title=\"Jump back to footnote 2\" href=\"#fnref2\">\u21a9</a></p>\n</li>\n</ol></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JAyz8mEir5mZMTDK7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 5.116239695939822e-07, "legacy": true, "legacyId": "1458", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Here\u2019s a game that you can play for real against a human opponent. If the administrators don\u2019t mind, you can play it right here in the comments.</p>\n<p>The game is called Pract.<a id=\"more\"></a></p>\n<p><a name=\"rules\"></a></p>\n<h2 id=\"Rules\">Rules</h2>\n<p>Pract is played using finite sequences of integers, called \u201csequences.\u201d</p>\n<p>To start, each player chooses a well-defined infinite set of sequences, such that every sequence is either demonstrably in or demonstrably out of the set. The game ends when one player guesses the other\u2019s set.<a name=\"fnref1\"></a><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\"><sup>1</sup></a></p>\n<p>Once they have picked their sets, the players take turns.</p>\n<p><a name=\"definitions\"></a></p>\n<h3 id=\"Definitions\">Definitions</h3>\n<ol class=\"decimal\">\n<li>A player <em>specifies</em> a sequence by writing each integer, in order, in decimal.</li>\n<li>A sequence\u2019s <em>classification</em> relative to a set is a statement of whether the sequence is in or out of the set.</li>\n<li>A player <em>classifies</em> a sequence by writing the sequence\u2019s classification relative to that player\u2019s chosen set.</li>\n<li>The <em>current player</em> is the player whose turn it currently is.</li>\n<li>A player\u2019s <em>final score</em> is the number of guesses made by that player plus the length of that player\u2019s statement (at the end of the game) of the definition of his or her own set.</li>\n</ol>\n<p><a name=\"gameplay\"></a></p>\n<h3 id=\"Gameplay\">Gameplay</h3>\n<p>On each turn, the current player may either:</p>\n<ol class=\"decimal\">\n<li>Try to guess the other player\u2019s set, in which case the other player indicates whether the guess was correct or not. Or,</li>\n<li>Specify a sequence, in which case both players classify that sequence.<ol class=\"decimal\">\n<li>If, during the previous turn, the other player made an incorrect guess of the current player\u2019s set, the specified sequence must have an opposite classification relative to the actual set than it does relative to the guessed set.</li>\n<li>If the other player instead specified a sequence, the two sequences (the one specified by the current player this turn and the one specified the other player on the previous turn) must have opposite classifications relative to the current player\u2019s set.</li>\n</ol></li>\n</ol>\n<p>When a correct guess is made, each player states the definition of his or her own set. The player who guessed correctly makes this statement second. Then the game ends.</p>\n<p><a name=\"winning-and-losing\"></a></p>\n<h3 id=\"Winning_and_Losing\">Winning and Losing</h3>\n<p>If the player who guessed correctly has a lower final score than the other player, the player who guessed correctly wins and the other player loses. Otherwise, both players lose.</p>\n<p>If a player withdraws from the game during his or her own turn, both players lose.<a name=\"fnref2\"></a><a id=\"fnref2\" class=\"footnoteRef\" href=\"#fn2\"><sup>2</sup></a></p>\n<p>Statement lengths affect final scores, so language, notation, and encoding are relevant. In most cases, common sense, context, and the medium itself should suffice. (Pract is meant to be played over the Internet in a forum, chat room, mailing list, or other similar medium.)</p>\n<p>Pract is meant as way to practice both reasoning and being reasonable. Those who would rather argue well than play well should spectate.</p>\n<p><a name=\"examples\"></a></p>\n<h2 id=\"Examples\">Examples</h2>\n<p>Here\u2019s an example of the beginning of a game between Alice and Bob:</p>\n<pre><code>alice&gt; 2, 4, 6\nalice&gt; in\nbob&gt; out\nbob&gt; 7, 2, 4\nbob&gt; in\nalice&gt; out\n</code></pre>\n<p>The above example shows two turns: the first is Alice\u2019s and the second is Bob\u2019s. Both players have used their turn to specify a sequence rather than make a guess, and each player classifies each sequence as required by the rules.</p>\n<p>Bob is being redundant when he classifies the sequence he specified. The rules prevent him from selecting a sequence that is out of his own set because the sequence specified by Alice is out of his set.</p>\n<p>Here\u2019s an example showing the end of a game:</p>\n<pre><code>cathy&gt; 5, 7, 8\ncathy&gt; out\ndave&gt; out\ndave&gt; increasing\ncathy&gt; no\ncathy&gt; 3, 2\ncathy&gt; in\ndave&gt; out\ndave&gt; 4, 4, 2, 2\ndave&gt; in\ncathy&gt; out\ncathy&gt; even numbers\ndave&gt; yes\ndave&gt; all even\ncathy&gt; odd sum\ndave&gt; length 8 + 5 guesses = 13\ncathy&gt; length 7 + 4 guesses = 11\n</code></pre>\n<p>The first turn shown is Cathy\u2019s, and she uses it to specify a sequence. On the next turn, Dave makes an incorrect guess. Then there are two more turns on which a sequence is specified. Finally, Cathy ends the game by making a correct guess. The players state their sets and calculate their scores. From the calculations, we can infer that there were several previous guesses by each player.</p>\n<p>Cathy is being redundant when she classifies <code>3, 2</code>. The rules prohibit her from selecting a sequence that is both non-increasing and out of her set because Dave incorrectly guessed on the previous turn that her set was the set of all increasing sequences. (In a typical game, nearly half the classifications are redundant. The redundancy makes it much easier to verify that players are following the rules.)</p>\n<p>The players are not using full sentences or punctuation to describe the sets, and Dave\u2019s description of his own set is shorter than Cathy\u2019s equivalent description when she guesses it. This is reasonable, as long as it does not introduce ambiguity or involve silly things like languages invented on the fly.</p>\n<p>There may be situations where it is not clear how the rules apply. For example, the correctness of a guess may depend on some open question in mathematics. So long as players try to avoid such situations, rather than cause them, they should be rare and resolvable.</p>\n<div class=\"footnotes\">\n<hr>\n<ol><a name=\"fn1\"></a>\n<li id=\"fn1\">\n<p>ETA: Pract is inspired in part by <a href=\"http://en.wikipedia.org/wiki/Zendo_%28game%29\">Zendo</a>, as well as various ideas I\u2019ve seen around LessWrong like the <a href=\"/lw/iw/positive_bias_look_into_the_dark/\">2 4 6 experiment</a>, and <a href=\"/lw/14a/thomas_c_schellings_strategy_of_conflict/\">variable-sum games</a>. <a class=\"footnoteBackLink\" title=\"Jump back to footnote 1\" href=\"#fnref1\">\u21a9</a></p>\n</li>\n<a name=\"fn2\"></a>\n<li id=\"fn2\">\n<p>ETA: New rule for withdrawing. My main concern from the beginning was that both players would be stumped by their own biases and unable to finish the game. The discussion on how to play in bad faith gave me an idea for how to deal with that. <a class=\"footnoteBackLink\" title=\"Jump back to footnote 2\" href=\"#fnref2\">\u21a9</a></p>\n</li>\n</ol></div>", "sections": [{"title": "Rules", "anchor": "Rules", "level": 1}, {"title": "Definitions", "anchor": "Definitions", "level": 2}, {"title": "Gameplay", "anchor": "Gameplay", "level": 2}, {"title": "Winning and Losing", "anchor": "Winning_and_Losing", "level": 2}, {"title": "Examples", "anchor": "Examples", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "42 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rmAbiEKQDpDnZzcRf", "tJQsxD34maYw2g5E4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-07-31T12:14:48.507Z", "modifiedAt": null, "url": null, "title": "An Alternative Approach to AI Cooperation", "slug": "an-alternative-approach-to-ai-cooperation", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:15.737Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/S4Jg3EAdMq57y587y/an-alternative-approach-to-ai-cooperation", "pageUrlRelative": "/posts/S4Jg3EAdMq57y587y/an-alternative-approach-to-ai-cooperation", "linkUrl": "https://www.lesswrong.com/posts/S4Jg3EAdMq57y587y/an-alternative-approach-to-ai-cooperation", "postedAtFormatted": "Friday, July 31st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20Alternative%20Approach%20to%20AI%20Cooperation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20Alternative%20Approach%20to%20AI%20Cooperation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS4Jg3EAdMq57y587y%2Fan-alternative-approach-to-ai-cooperation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20Alternative%20Approach%20to%20AI%20Cooperation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS4Jg3EAdMq57y587y%2Fan-alternative-approach-to-ai-cooperation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS4Jg3EAdMq57y587y%2Fan-alternative-approach-to-ai-cooperation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 631, "htmlBody": "<p>[This post summarizes my side of a <a href=\"/lw/13y/freaky_fairness/zo0?context=6#comments\">conversation</a> between me and cousin_it, and continues it.]</p>\n<p>Several people here have shown interest in an approach to modeling AI interactions that was <a href=\"http://lists.extropy.org/pipermail/extropy-chat/2008-May/043381.html\">suggested</a> by Eliezer Yudkowsky: assume that AIs can gain common knowledge of each other's source code, and <a href=\"/lw/13o/fairness_and_geometry/\">explore</a> the decision/game theory that results from this assumption.</p>\n<p>In this post, I'd like to describe an <a href=\"http://www.nabble.com/-sl4--trade-or-merge--td18500141.html#a18503316\">alternative approach</a><sup>*</sup>, based on the idea that two or more AIs may be able to securely merge themselves into a joint machine, and allow this joint machine to make and carry out subsequent decisions. I argue that this assumption is as plausible as that of common knowledge of source code, since it can be built upon the same technological foundation that has been <a href=\"http://www.nabble.com/Proof-by-construction%2C-again-%28was-Re%3A--sl4--prove-your-source-code%29-p18545070.html\">proposed</a> to implement common knowledge of source code. That proposal, by Tim Freeman, was this:</p>\n<blockquote>\n<p>Entity A could prove to entity B that it has source code S by <br />consenting to be replaced by a new entity A' that was constructed by a <br />manufacturing process jointly monitored by A and B. &nbsp;During this <br />process, both A and B observe that A' is constructed to run source <br />code S. &nbsp;After A' is constructed, A shuts down and gives all of its <br />resources to A'.</p>\n</blockquote>\n<p>Notice that the same technology can be used for two AIs to merge into a single machine running source code S (which they both agreed upon). All that needs to be changed from the above process is for B to also shut down and give all of its resources to A' after A' is constructed. Not knowing if there is a standard name for this kind of technology, I've given it the moniker \"secure joint construction.\"<a id=\"more\"></a></p>\n<p>I conjecture that the two approaches are equal in power, in the sense that any cooperation made possible by the common knowledge of source code is also possible given the secure merger ability, and vice versa. This is because under the assumption of common knowledge of source code, the <a href=\"/lw/13y/freaky_fairness/\">likely outcome</a> is for all AIs to modify themselves into using the same decision algorithm, with that algorithm making and carrying out subsequent decisions. The collection of these cooperative machines running the same algorithm can be viewed as one distributed machine, thus suggesting the equivalence of the two approaches.</p>\n<p>It is conceptually simpler to assume that AIs will merge into a centralized joint machine. This causes no loss of generality, since if for some reason the AIs find it more advantageous to merge into a distributed joint machine, they will surely come up with solutions to distributed computing problems like <a href=\"/lw/do/reformalizing_pd/\">friend-or-foe recognition</a> and <a href=\"/lw/14a/thomas_c_schellings_strategy_of_conflict/zt4\">consensus</a> by themselves. The merger approach allows such issues to be abstracted away as implementation details of the joint machine.</p>\n<p>Another way to view these two approaches is that they each offers a way for AIs to enforce agreements, comparable with the enforcement of contracts by a court, except that the assumed technologies allow the AIs to enforce agreements without a trusted third party, and with potentially higher assurance of compliance. This allows most AI-AI interactions to be modeled using cooperative game theory, which assumes such enforcement of agreements.</p>\n<p>* My original proposal, posted on SL4, was that AIs would use <a href=\"/lw/12y/the_popularization_bias/zuc\">Bayesian aggregation</a> to determine the decision algorithm of their joint machine. I later realized that cooperative game theory is a better fit, because only a cooperative game solution ensures that each AI has sufficient incentives to merge.</p>\n<p>[It appears to me that cousin_it and I share many understandings, while Vladimir Nesov and Eliezer seem to have ideas closer to each other and to share certain insights that I am not able to access. I hope this post encourages them to clarify their ideas relative to those of cousin_it and I.]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "S4Jg3EAdMq57y587y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 8, "extendedScore": null, "score": 5.116530245947796e-07, "legacy": true, "legacyId": "1453", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["oZ33pz2FWzFbWrgHT", "mFrx6YbQ6Dsup2Jvp", "5iK6rsa3MSrMhHQyf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-01T15:06:40.211Z", "modifiedAt": null, "url": null, "title": "Open Thread: August 2009", "slug": "open-thread-august-2009", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:54.103Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zaDHi9rdZvF4ctskB/open-thread-august-2009", "pageUrlRelative": "/posts/zaDHi9rdZvF4ctskB/open-thread-august-2009", "linkUrl": "https://www.lesswrong.com/posts/zaDHi9rdZvF4ctskB/open-thread-august-2009", "postedAtFormatted": "Saturday, August 1st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20August%202009&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20August%202009%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzaDHi9rdZvF4ctskB%2Fopen-thread-august-2009%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20August%202009%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzaDHi9rdZvF4ctskB%2Fopen-thread-august-2009", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzaDHi9rdZvF4ctskB%2Fopen-thread-august-2009", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 31, "htmlBody": "<p>Here's our place to discuss Less Wrong topics that have not appeared in recent posts. If something gets a lot of discussion feel free to convert it into an independent post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zaDHi9rdZvF4ctskB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "1462", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 193, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-02T19:12:38.493Z", "modifiedAt": null, "url": null, "title": "Pain", "slug": "pain", "viewCount": null, "lastCommentedAt": "2022-01-25T20:42:31.797Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TcJKD2E4uE9XLNxBP/pain", "pageUrlRelative": "/posts/TcJKD2E4uE9XLNxBP/pain", "linkUrl": "https://www.lesswrong.com/posts/TcJKD2E4uE9XLNxBP/pain", "postedAtFormatted": "Sunday, August 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pain&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APain%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTcJKD2E4uE9XLNxBP%2Fpain%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pain%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTcJKD2E4uE9XLNxBP%2Fpain", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTcJKD2E4uE9XLNxBP%2Fpain", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 617, "htmlBody": "<p>Some time ago, I came across the All Souls College <a href=\"http://elcenia.com/itemz/allsouls.pdf\">philosophy fellowship exam</a>.&nbsp; It's interesting reading throughout, but one question in particular brought me up short when I read it.</p>\n<p><em>What, if anything, is bad about pain?</em></p>\n<p>The fact that I couldn't answer this immediately was fairly disturbing.&nbsp; Approaching it from the opposite angle was much simpler.&nbsp; It is in fact trivially easy to say what is <em>good</em> about pain.&nbsp; To do so, all you need to do is look at the people who are born without the ability to feel it: <a href=\"http://en.wikipedia.org/wiki/Congenital_insensitivity_to_pain_with_anhidrosis\">CIPA patients</a>.&nbsp; You wouldn't want your kid saddled with this condition, unless for some reason you'd find it welcome for the child to die (painlessly) before the age of three, and if that fate were escaped, to spend a lifetime massively inconvenienced, disabled, and endangered by undetected and untreated injuries and illnesses great and small.</p>\n<p>But... <em>what, if anything, is bad about pain?</em></p>\n<p>I don't <em>enjoy</em> it, to be sure, but I also don't enjoy soda or warm weather or chess or the sound of vacuum cleaners, and it seems that it would be a different thing entirely to claim that these things are <em>bad</em>.&nbsp; <em>Most</em> people don't enjoy pain, but <em>most</em> people also don't enjoy lutefisk or rock climbing or musical theater or having sex with a member of the same sex, and it seems like a different claim to hold that lutefisk and rock climbing and musical theater and gay sex are <em>bad</em>.&nbsp; And it's just not the case that <em>all</em> people don't <a href=\"http://en.wikipedia.org/wiki/Masochism\">enjoy pain</a>, so that's an immediate dead end.</p>\n<p>So... <em>what, if anything, is bad about pain?</em></p>\n<p><a id=\"more\"></a>Let's go back to the CIPA patients.&nbsp; I suggested that they indicate what's good about pain by showing us what happens to people without any: failure to detect and respond to injury and illness leads to exacerbation of their effects, up to and including untimely death.&nbsp; What's bad about those things?&nbsp; If we're doubting the badness of pain, we may as well doubt the badness of other stuff we don't like and try to avoid, like death.&nbsp; With death, there are some readier answers: you could call it a tragic loss of a just-plain-inherently-valuable individual, but if you don't like that answer (and many people don't seem to), you can point to the grief of the loved ones (conveniently ignoring that not everybody has loved ones) which is... um... pain.&nbsp; Whoops.&nbsp; Well, you could try making it about the end of the productive contribution to society, on the assumption that the dead person did something useful (and conveniently ignore why we tend not to be huge fans of death even when it happens to unproductive persons).&nbsp; Maybe we've just lost an anesthesiologist, who, um.... relieves pain.</p>\n<p>And... <em>what, if anything, is bad about pain?</em></p>\n<p>Your <a href=\"http://plato.stanford.edu/entries/consequentialism/#ClaUti\">standard-issue utilitarianism</a> is, among other things, \"hedonic\".&nbsp; That means it includes among its tenets hedonism, which is the idea that pleasure is good and pain is bad, end of story.&nbsp; Lots of pleasure is better than a little and lots of pain is worse than a little and you can give these things units and do arithmetic to them to figure out how good or bad something is and then wag your finger or supply accolades to whoever is responsible for that thing.&nbsp; Since hedonists are just as entitled as anyone to their primitive notions, that's fine, but it's not much help to our question.&nbsp; \"It is a primitive notion of my theory\" is the adult equivalent of \"it just is, that's all, your question is stupid!\"&nbsp; (I don't claim that this is never an appropriate answer.&nbsp; Some questions are pretty stupid.&nbsp; But I don't think that one of them is...)</p>\n<p>...<em>what, if anything, is bad about pain?</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LaDu5bKDpe8LxaR7C": 1, "ZTRNmvQGgoYiymYnq": 1, "8e9e8fzXuW5gGBS3F": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TcJKD2E4uE9XLNxBP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 52, "baseScore": 46, "extendedScore": null, "score": 7.4e-05, "legacy": true, "legacyId": "1463", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 46, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 202, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-03T16:02:38.270Z", "modifiedAt": null, "url": null, "title": "Suffering", "slug": "suffering", "viewCount": null, "lastCommentedAt": "2021-01-27T00:45:00.633Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tiiba", "createdAt": "2009-02-27T06:55:57.544Z", "isAdmin": false, "displayName": "Tiiba"}, "userId": "FngsS7fwH2r3ikxTm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Dvc7zrqsdCYy6dCFR/suffering", "pageUrlRelative": "/posts/Dvc7zrqsdCYy6dCFR/suffering", "linkUrl": "https://www.lesswrong.com/posts/Dvc7zrqsdCYy6dCFR/suffering", "postedAtFormatted": "Monday, August 3rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Suffering&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuffering%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDvc7zrqsdCYy6dCFR%2Fsuffering%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Suffering%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDvc7zrqsdCYy6dCFR%2Fsuffering", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDvc7zrqsdCYy6dCFR%2Fsuffering", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 303, "htmlBody": "<p>For a long time, I wanted to ask something. I was just thinking about it again when I saw that Alicorn has a post on a similar topic. So I decided to go ahead.</p>\n<p>The question is: what is the difference between morally neutral stimulus responces and agony? What features must an animal, machine, program, alien, human fetus, molecule, or anime character have before you will say that if their utility meter is low, it needs to be raised. For example, if you wanted to know if lobsters suffer when they're cooked alive, what exactly are you asking?<a id=\"more\"></a></p>\n<p>On reflection, I'm actually asking two questions: what is a morally significant agent (MSA; is there an established term for this?) whose goals you would want to further; and having determined that, under what conditions would you consider it to be suffering, so that you would?</p>\n<p>I think that an MSA would not be defined by one feature. So try to list several features, possibly assigning relative weights to each.</p>\n<p>IIRC, I read a study that tried to determine if fish suffer by injecting them with toxins and observing whether their reactions are planned or entirely instinctive. (They found that there's a bit of planning among bony fish, but none among the cartilaginous.) I don't know why they had to actually hurt the fish, especially in a way that didn't leave much room for planning, if all they wanted to know was if the fish can plan. But that was their definition. You might also name introspection, remembering the pain after it's over...</p>\n<p>This is the ultimate subjective question, so the only wrong answer is one that is never given. Speak, or be wrong. I will downvote any post you don't make.</p>\n<p>BTW, I think the most important defining feature of an MSA is ability to kick people's asses. Very humanizing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LaDu5bKDpe8LxaR7C": 2, "nSHiKwWyMZFdZg5qt": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Dvc7zrqsdCYy6dCFR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 9, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "1464", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 96, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-04T00:31:41.782Z", "modifiedAt": null, "url": null, "title": "Why You're Stuck in a Narrative", "slug": "why-you-re-stuck-in-a-narrative", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:09.034Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "pwZ6qMgzoKr3JPqx4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FSPKLFfMNbRGPFjmY/why-you-re-stuck-in-a-narrative", "pageUrlRelative": "/posts/FSPKLFfMNbRGPFjmY/why-you-re-stuck-in-a-narrative", "linkUrl": "https://www.lesswrong.com/posts/FSPKLFfMNbRGPFjmY/why-you-re-stuck-in-a-narrative", "postedAtFormatted": "Tuesday, August 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20You're%20Stuck%20in%20a%20Narrative&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20You're%20Stuck%20in%20a%20Narrative%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFSPKLFfMNbRGPFjmY%2Fwhy-you-re-stuck-in-a-narrative%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20You're%20Stuck%20in%20a%20Narrative%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFSPKLFfMNbRGPFjmY%2Fwhy-you-re-stuck-in-a-narrative", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFSPKLFfMNbRGPFjmY%2Fwhy-you-re-stuck-in-a-narrative", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1396, "htmlBody": "<p><!-- BODY { FONT-FAMILY:Tahoma; FONT-SIZE:10pt } P { FONT-FAMILY:Tahoma; FONT-SIZE:10pt } DIV { FONT-FAMILY:Tahoma; FONT-SIZE:10pt } TD { FONT-FAMILY:Tahoma; FONT-SIZE:10pt } --></p>\n<p><!-- BODY { FONT-FAMILY:Tahoma; FONT-SIZE:10pt } P { FONT-FAMILY:Tahoma; FONT-SIZE:10pt } DIV { FONT-FAMILY:Tahoma; FONT-SIZE:10pt } TD { FONT-FAMILY:Tahoma; FONT-SIZE:10pt }&nbsp; --></p>\n<div>For some reason the narrative fallacy does not seem to get as much play as the other major cognitive fallacies. Apart from discussions of \"The Black Swan\", I never see it mentioned anywhere. Perhaps this is because it's not considered a \"real\" bias, or because it's an amalgamation of several lower-level biases, or because it's difficult to do controlled studies for. Regardless, I feel it's one of the more pernicious and damaging fallacies, and as such deserves an internet-indexable discussion.</div>\n<div>From Taleb's \"The Black Swan\"</div>\n<blockquote>\n<div>The narrative fallacy addresses our limited ability to look at sequences of facts without weaving an explanation into them, or, equivalently, forcing a logical link, an arrow of relationship upon them. Explanations bind facts together. They make them all the more easily remembered; they help them make more sense. Where this propensity can go wrong is when it increases our impression of understanding.</div>\n</blockquote>\n<div>Essentially, the narrative fallacy is our tendency to turn everything we see into a story - a linear chain of cause and effect, with a beginning and an end. Obviously the real world isn't like this - events are complex and interrelated, direct causation is extremely rare, and outcomes are probabilistic. Verbally, we know this - the hard part, as always, is convincing our brain of the fact.<a id=\"more\"></a></div>\n<div>Our brains are engines designed to analyze the environment, pick out the important parts, and use those to extrapolate into the future. To trot out some theoretical evolutionary support, only extremely basic extrapolation would be required in the ancestral evolutionary environment. Things like [Gather Food -&gt; Eat Food -&gt; Sate Hunger] or [See Tiger -&gt; Run -&gt; Don't Die]. Being able to produce simple chains of cause and effect would confer a significant survival advantage, but you wouldn't need anything more than that. The world was simple enough that we didn't have to deal with complex interactions - linear extrapolation was \"good enough\". The world is much different and much more complex today, but unforunately, we're still stuck with the same linear extrapolation hardware.</div>\n<div>You can see the results of this 'good enough' solution in the design and function of our brain. Cognitively, it's much cheaper to interpret a group of things as a story - a pattern - than to remember each one of them seperately. Simplifying, summarizing, clustering, and chaining ideas together - reducing complex data to a few key factors, lets us get away with, say having an extremely small working memory, or a relatively slow neuron firing speed. Compression of some sort is needed for our brains to function - it'd be impossible to analyze the terabytes of data we receive every second from our senses otherwise. As such, we naturally reduce everything to the simplest pattern possible, and then process the pattern. So we're much better at remembering things as part of a pattern than as a random assortment. The alphabet is first learned as a song to help it stick. Mnemonic devices improve memory by establishing easy to remember relationships. By default our natural tendency, for any information, is to establish links and patterns in it to aid in processing. This by itself isn't a problem - the essence of of knowledge is drawing connections and making inferences. The problem is that because our hardware is designed to do it, it insists on finding links and patterns <a href=\"http://scienceblogs.com/cortex/2008/11/its_one_of_the_more.php\" target=\"_blank\">whether they actually exist or not</a>.&nbsp; We're biologically inclined to reduce complex events to a simpler, more palatable, more easily understood pattern - a story.</div>\n<div>This tendency can be seen in a variety of lower level biases. For instance, the availability heuristic causes us to make predictions and inferences based on what most quickly comes to mind - what's most easily remembered. Hindsight bias causes us to interpret past events as obviously and inevitably causing future ones. Consistency bias causes us to reinterpret past events and behaviors to be consistent with new information. Confirmation bias causes us to only look for data to support the conclusions we've already arrived at. There's also our tendency to engage in rationalization, and create post-hoc explanations for our behavior. They all have the effect of of molding, shaping, and simplifying events into a kind of linear narrative, ignoring any contradiction, complexity, and general messiness.</div>\n<div>Additionally, there's evidence that forming narratives out of the amalgamated behavior of semi-independent mental modules is one of the primary functions of consciousness. Dennet makes this argument in his paper \"<a href=\"http://ase.tufts.edu/cogstud/papers/selfctr.htm\" target=\"_blank\">The Self as a Narrative Center of Gravity</a>\":</div>\n<blockquote>\n<div>That is, it does seem that we are all virtuoso novelists, who find ourselves engaged in all sorts of behavior, more or less unified, but sometimes disunified, and we always put the best \"faces\" on it we can. We try to make all of our material cohere into a single good story. And that story is our autobiography.</div>\n</blockquote>\n<div>Because the brain is a hodge podge of dirty hacks and disconnected units, smoothing over and reinterpreting their behaviors to be part of a consistent whole is necessary to have a unified 'self'. Drescher makes a somewhat related conjecture in \"Good and Real\", introducing the idea of consciousness as a 'Cartesian Camcorder', a mental module which records and plays back perceptions and outputs from other parts of the brain, in a continuous stream. It's the idea of \"I am not the one who thinks my thoughts, I am the one who hears my thoughts\", the source of which escapes me. Empirical support of this comes from the experiments of Benjamin Libet, which show that a subconscious electrical processes precede conscious actions - implying that consciousness doesn't engage until after an action has already been decided. If this is in fact how we handle internal information - smoothing out the rough edges to provide some appearance of coherence, it shouldn't be suprising that we tend to handle external information in the same matter.</div>\n<div>It seems then, that creating narratives isn't so much a choice as it is a basic feature of the architecture of our minds. From the paper \"<a href=\"http://www.jstor.org/pss/3685505\" target=\"_blank\">The Neurology of Narrative</a>\" (JSTOR), discussing people with damage to the area of the frontal lobe which processes higher order input:</div>\n<blockquote>\n<div>They are unable to provide (and likely fail to generate internally) a narrative account of their experiences, wishes, and actions, although they are fully cognizant of their visual, auditory, and tactile surroundings. These individuals lead \"denarrated\" lives, aware but failing to organize experience in an action generating temporal frame. In the extreme, they do not speak unless spoken to and do not move unless very hungry. These patients illustrate the inseparable connection between narrativity and personhood. Brain injured individuals may lose their linguistic, mathematic, syllogistic, visuospatial, mnestic, or kinesthetic competencies and still be recognizably the same persons. Individuals who have lost the ability to construct narrative, however,have lost their selves.</div>\n</blockquote>\n<div>You can see the extremes our tendency toward narrative can go with people who see themselves as the star or hero in a \"<a href=\"http://thelastpsychiatrist.com/2008/12/heidis_real_problem_on_the_hil_1.html\" target=\"_blank\">movie about their life</a>\". These people tend to be severe narcissists (though I've heard some self help \"experts\" espouse this as a healthy outlook to adopt), but it's not hard to see why such a view is so appealing. As the star of a movie, the events in your life are all extremely important, and are building to something that will inevitably occur later. You'll face difficulties, but you will ultimately overcome them, and your triumph will be all the greater for it (we seldom imagine our lives as a tragedy). You'll fight and conquer your enemies. You'll win over the love interest. It's all immensely appealing to our most basic desires, whether we're narcissists or not.</div>\n<div>A good story, then, is a superstimulus. The very structure of our minds is tilted to be vulnerable to it. It appeals to our primitive brains so strongly that it doesn't matter if it resembles the real world or not - we prefer the engaging story. We're designed to produce narratives, whether we like it or not. Fortunately, our minds also come with the ability to build new processes that can overrule the older ones. So how do we beat it? From \"The Black Swan\":</div>\n<blockquote>\n<div>There are ways to escape the narrative fallacy...by making conjectures and running experiments, by making testable predictions.</div>\n</blockquote>\n<div>In other words, concentrate your probability mass. Force your beliefs to be falsifiable. Make them pay rent in anticipated experience. All the the things a good rationalist should be doing already.</div>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4R8JYu4QF2FqzJxE5": 1, "pszEEb3ctztv3rozd": 1, "5f5c37ee1b5cdee568cfb1b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FSPKLFfMNbRGPFjmY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 44, "extendedScore": null, "score": 7.1e-05, "legacy": true, "legacyId": "1466", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-04T05:57:39.009Z", "modifiedAt": null, "url": null, "title": "Unspeakable Morality", "slug": "unspeakable-morality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:37.132Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QjPevgYZrqWKbJCkL/unspeakable-morality", "pageUrlRelative": "/posts/QjPevgYZrqWKbJCkL/unspeakable-morality", "linkUrl": "https://www.lesswrong.com/posts/QjPevgYZrqWKbJCkL/unspeakable-morality", "postedAtFormatted": "Tuesday, August 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Unspeakable%20Morality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUnspeakable%20Morality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQjPevgYZrqWKbJCkL%2Funspeakable-morality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Unspeakable%20Morality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQjPevgYZrqWKbJCkL%2Funspeakable-morality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQjPevgYZrqWKbJCkL%2Funspeakable-morality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1889, "htmlBody": "<p>It is a general and primary principle of rationality, that we should not believe that which there is insufficient reason to believe; likewise, a principle of social morality that we should not enforce upon our fellows a law which there is insufficient justification to enforce.</p>\n<p>Nonetheless, I've always felt a bit nervous about demanding that people be able to <em>explain things in words</em>, because, while I happen to be pretty good at that, most people <em>aren't.</em></p>\n<blockquote>\n<p>\"I remember this paper I wrote on existentialism. My teacher gave it back with an F. She&rsquo;d underlined true and truth wherever it appeared in the essay, probably about twenty times, with a question mark beside each. She wanted to know what I meant by truth.\" &mdash;Danielle Egan (journalist)</p>\n</blockquote>\n<p>This experience permanently traumatized Ms. Egan, by the way.&nbsp; Because years later, at a WTA conference, one of the speakers said that something was true, and Ms. Egan said \"What do you mean, 'true'?\", and the speaker gave some incorrect answer or other; and afterward I quickly walked over to Ms. Egan and explained the correspondence theory of truth:&nbsp; \"The sentence 'snow is white' is true if and only if snow is white\"; if you're using a bucket of pebbles to count sheep then <a href=\"http://yudkowsky.net/rational/the-simple-truth\">an empty bucket is true if and only if the pastures are empty</a>.&nbsp; I don't know if this cured her; I suspect that it didn't.&nbsp; But up until that point, at any rate, it seems Ms. Egan had been so traumatized by this childhood experience that she believed there was no such thing as truth - that because her teacher had demanded a definition in words, and she hadn't been able to give a good definition in words, that no good definition existed.</p>\n<p>Of which I usually say:&nbsp; \"There was a time when no one could define gravity in exquisitely rigorous detail, but if you walked off a cliff, you would fall.\"</p>\n<p>On the other hand - it is a general and primary principle of rationality that when you have <em>no</em> justification, it is very important that there be some way of <a href=\"/lw/i9/the_importance_of_saying_oops/\">saying \"Oops\"</a>, <a href=\"/lw/gx/just_lose_hope_already/\">losing hope</a>, and <a href=\"/lw/mz/zut_allais/\">just giving up already</a>.&nbsp; (I really should post, at some point, on how the ability to <em>just give up already</em> is one of the primary distinguishing abilities of a rationalist.)&nbsp; So, really, if you find yourself totally unable to justify something in words, one possibility is that there <em>is</em> no justification.&nbsp; To ignore this and just casually stroll along, would not be a good thing.</p>\n<p>And with moral questions, this problem is doubled and squared.&nbsp; <a id=\"more\"></a>For any given person, <a href=\"/lw/sm/the_meaning_of_right/\">the meaning of \"right\"</a> is a huge <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">complicated</a> function, not explicitly believed so much as implicitly embodied.&nbsp; And if we keep asking \"Why?\", at some point we end up replying \"Because that is just what the term 'right', <em>means;</em> <a href=\"/lw/sx/inseparably_right_or_joy_in_the_merely_good/\">there is no pure essence of rightness that you can abstract away from the specific content of your values</a>.\"</p>\n<p>But if you were allowed to answer this in response to <em>any</em> demand for justification, and have the other bow and walk away - well, you would no longer be computing what <em>we</em> know as morality, where 'right' does mean some things and not others.</p>\n<p>Not to mention that in questions of public policy, it ought to require some <em>overlap</em> in values to make a law.&nbsp; I do think that human values often overlap enough that different people can legitimately use the same word 'right' to refer to that-which-they-compute.&nbsp; But if someone wants a legal ban on pepperoni pizza because it's <em>inherently wrong</em>, then I may feel impelled to ask, \"Why do you think this is part of the <em>overlap</em> in our values?\"</p>\n<p>Demands for moral justification have their Charybdis and their Scylla:</p>\n<p>The traditionally given Charybdis is letting someone say that interracial marriage should be legally banned because it \"feels icky\" to them.&nbsp; We could call this \"the unwisdom of repugnance\" - if you can just say \"That feels repugnant\" and win a case for public intervention, then you lose all the cases of what we now regard as tremendous moral progress, which made someone feel vaguely icky at the time; women's suffrage, divorces, atheists not being burned at the stake.&nbsp; Moral progress - which I currently see as an iterative process of learning new facts, processing new arguments, and becoming more the sort of person you wished you were - demands that people go on <em>thinking</em> about morality, for which purpose it is very useful to have people go on <em>arguing</em> about morality.&nbsp; If <em>saying</em> the word \"intuition\" is a moral trump card, then people, who, by their natures, are lazy, will just say \"intuition!\" all the time, believing that no one is allowed to question that or argue with it; and that will be the end of their moral <em>thinking.</em></p>\n<p>And the Scylla, I think, was <a href=\"/lw/10f/the_terrible_horrible_no_good_very_bad_truth/#sx7\">excellently presented by Silas Barta</a> when... actually this whole comment is just worth quoting directly:</p>\n<blockquote>\n<p>Let's say we're in an alternate world with strong, codified rules about social status and authority, but weak, vague, unspoken norms against harm that nevertheless keep harm at a low level.</p>\n<p>Then let's say you present the people of this world with this \"dilemma\" to make Greene's point:</p>\n<blockquote>\n<p>Say your country is at war with another country that is particularly aggressive and willing to totally demolish your social order and enslave your countrymen. In planning how to best fight off this threat, your President is under a lot of stress. To help him relieve his stress, he orders a citizen, Bob, to be brought before him and tortured and murdered, while the President laughs his head off at the violence.</p>\n<p>He feels much more relieved and so is able to craft and motivate a war plan that leads to the unconditional surrender of the enemy. The President promises that this was just a one-time thing he had to do to handle the tremendous pressure he was under to win the war and protect his people. Bob's family, in turn, says that they are honored by the sacrifice Bob has made for his country. Everyone agrees that the President is the legitimate ruler of the country and the Constitution and tradition give him authority to do what he did to Bob.</p>\n<p>Was it okay for the President to torture and kill Bob for his personal enjoyment?</p>\n</blockquote>\n<p>Then, because of the deficiency in the vocabulary of \"harms\", you would get responses like:</p>\n<p>\"Look, I can't explain why, but obviously, it's wrong to torture and kill someone for enjoyment. No disrespect to the President, of course.\"</p>\n<p>\"What? I don't get it. Why would the President order a citizen killed? There would be outrage. He'd feel so much guilt that it wouldn't even relieve the stress you claim it does.\"</p>\n<p>\"Yeah, I agree the President has authority to do that, but God, it just burns me up to think about someone getting tortured like that for someone else's enjoyment, even if it is our great President.\"</p>\n<p>Would you draw the same conclusion Greene does about these responses?</p>\n</blockquote>\n<p>Unfortunately, it does happen to be a fact that most people are <em>not </em>good at explaining themselves in words, unless they've already heard the explanation from someone else.&nbsp; Even if you challenge a professional philosopher who holds a position, to justify it, and they can't... well, frankly, you can't conclude much even from that, in terms of inferring that no good explanation exists.&nbsp; Philosophers, I've observed, are not much good at this sort of job either.&nbsp; It's Bayesian evidence, by the law of conservation of evidence; if a good explanation would be a sign that justification exists, then the absence of such explanation must be evidence that justification does not exist.&nbsp; It's just not very <em>strong </em>evidence, because we don't strongly anticipate that even professional philosophers will be able to put a justification into words, correctly and convincingly, when justification does in fact exist.</p>\n<p>Even <em>conditioning</em> on the proposition that there <em>is </em>overlap in what you and others mean by 'right' - the huge function that is what-we-try-to-do - and that the judgment in question is stable when taken to the limits of knowledge, thought, and reflective coherence - well, it's still not sure that you'd be able to put it into <em>words</em>.&nbsp; You <em>might </em>be able to.&nbsp; But you might not.</p>\n<p>And we also have to allow a certain probability of convincing-sounding complicated verbal justification, in cases where no justification exists.&nbsp; But then if you use that as an excuse to flush all disliked arguments down the toilet, you shall be left rotting forever in a pit of <a href=\"http://wiki.lesswrong.com/wiki/Motivated_Skepticism\">convenient skepticism</a>, saying, \"All that intellekshual stuff could be wrong, after all.\"</p>\n<p>So here are my proposed rules of conduct for arguing morality in words:</p>\n<ul>\n<li>\"Intuition\" is <em>not</em> a trump card.&nbsp; If you had to spell out what your intuition was, and where it came from (evolution? culture?), and whether it has consequences beyond itself, it's possible that we would find it unconvincing in the stark light of reflection; that we would wish to intuit some other intuition than this.&nbsp; We can't hold up the intuition for reflective judgment unless we know what it is.&nbsp; So spelling it out, is important; and if you can win arguments by saying \"Intuition!\" then no one will bother to spell things out any more.&nbsp; Please try to say what sort of intuition it is.</li>\n<li>\"I can't put it into words\" <em>is </em>believable to some extent, but constitutes <em>weak </em>evidence against the existence of valid justification.&nbsp; If this is a popular debate and <em>no</em> one on your side, politician or philosopher or interested scientist or eloquent blogger, is able to give a convincing justification in words, then that is <em>stronger</em> evidence that no good justification exists.&nbsp; The longer the failure continues, the stronger the evidence.</li>\n<li>Still, at the end of the day, we don't really <em>expect </em>people to be very good at verbalizing moral intuitions, especially since most of them have incoherent explicit metaethics.&nbsp; So if you can give a justification for your political policy that stutters off into incoherence <em>only</em> at the point of explaining why pain is a bad thing - if you can give reasonable arguments for everything else up <em>until</em> that point - that's probably about as much as we can demand of anyone short of a full-fledged master reductionist.</li>\n<li>But we also expect that people may pass judgments that they would revoke in the light of better information or new arguments; and, especially <em>before </em>passing to that limit, it may be that sociopaths do not overlap with the values shared by most in a society.&nbsp; So if A says that event B is inherently wrong and awful, and C disagrees on the grounds that it just doesn't seem all that awful to them, then the burden of argument needs to lie on A before any <em>social, legal, public</em> action is brought into play.&nbsp; We should bear in mind that people of the past would have a lot of icky feelings about things that we, today, think are not only permitted but virtuous or even mandatory - the challenging of these icky feelings for good and sufficient public justification, was a key element of their relinquishment, which we regard as moral progress.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QjPevgYZrqWKbJCkL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 32, "extendedScore": null, "score": 5.12518267361007e-07, "legacy": true, "legacyId": "1467", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 119, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wCqfCLs8z5Qw4GbKS", "waqC6FihC2ryAZuAq", "zNcLnqHF5rvrTsQJx", "fG3g3764tSubr6xvs", "JynJ6xfnpq9oN3zpb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-04T06:14:48.531Z", "modifiedAt": null, "url": null, "title": "The Difficulties of Potential People and Decision Making", "slug": "the-difficulties-of-potential-people-and-decision-making", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:17.837Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qBEtr2iaGEtLFyLvu/the-difficulties-of-potential-people-and-decision-making", "pageUrlRelative": "/posts/qBEtr2iaGEtLFyLvu/the-difficulties-of-potential-people-and-decision-making", "linkUrl": "https://www.lesswrong.com/posts/qBEtr2iaGEtLFyLvu/the-difficulties-of-potential-people-and-decision-making", "postedAtFormatted": "Tuesday, August 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Difficulties%20of%20Potential%20People%20and%20Decision%20Making&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Difficulties%20of%20Potential%20People%20and%20Decision%20Making%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqBEtr2iaGEtLFyLvu%2Fthe-difficulties-of-potential-people-and-decision-making%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Difficulties%20of%20Potential%20People%20and%20Decision%20Making%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqBEtr2iaGEtLFyLvu%2Fthe-difficulties-of-potential-people-and-decision-making", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqBEtr2iaGEtLFyLvu%2Fthe-difficulties-of-potential-people-and-decision-making", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1522, "htmlBody": "<p>In connection to existential risk and the utility of bringing future people into being as compared with the utility of protecting those currently alive, I&rsquo;ve been looking into the issues and paradoxes present in the ethics of potential persons. This has led to an observation that I can find no record of anyone else making, which may help explain why those issues and paradoxes arise. For some time all I had was the observation, but a few days ago an actual prescriptive rule came together. This got long however so for the sake of readers I&rsquo;ll make a post about the normative rule later.&nbsp;</p>\r\n<p>A dichotomy in utilitarianism exists between total utilitarianism and average utilitarianism, one suggesting that the greatest good comes from the highest total sum of utility and the other suggesting the greatest good comes from the highest utility per capita. These can come to heads when discussing potential persons as the total view holds we are obligated to bring new people into existence if they will have worthwhile lives and won&rsquo;t detract from others&rsquo; wellbeing, and the average view suggests that it is perfectly acceptable not to.</p>\r\n<p>Both the total and average utilitarian views have surprising implications. Default total utilitarianism leads to what Derek Parfit and others call &ldquo;The Repugnant Conclusion&rdquo;: For any population in which people enjoy very high welfare there is an outcome in which [a much larger group of] people enjoy very low welfare which is preferable, all other things being equal. On the other hand average utilitarianism suggests that in a population of individuals possessed of very high utility it would be unethical to bring another person into being if they enjoyed positive but less than average utility.<a id=\"more\"></a> There are some attempts to resolve these oddities which are not explained here. From my reading I came across few professional philosophers or ethicists fully satisfied with [any such attempt]( <a href=\"http://plato.stanford.edu/entries/repugnant-conclusion/#EigWayDeaRepCon\">http://plato.stanford.edu/entries/repugnant-conclusion/#EigWayDeaRepCon</a>) (without rejecting one of the views of utilitarianism).</p>\r\n<p>To explain my observation I will make the assumptions that an ethical decision should be measured with reference to the people or beings it affects, and that actions do not affect nonexistent entities (assumptions which seem relatively widespread and I hope are considered reasonable). Assuming a negligible discount rate, if a decision affects our neighbors now or our descendants a thousand years hence we should include its effect upon them when deciding whether to take that action. It is when we consider actions that bring people into existence that the difficulty presents itself. If we choose to bring into existence a population possessed of positive welfare, we should consider our effect upon that then-existing population (a positive experience). If we choose not to bring into existence that population, we should judge this action only with regards to how it affects the people existing in that world, which does not include the unrealized people (assuming that we can even refer to an unrealized person).&nbsp; Under these assumptions we can observe that the metric by which our decision is measured changes with relation to the decision we make!</p>\r\n<p>By analogy assume you are considering organizing a local swim meet in which you also plan to compete, and at which there will be a panel of judges to score diving. Will you receive a higher score from the panel of judges if you call together the swim meet than if you do not? (To work as an analogy this requires that one considers &ldquo;the panel&rdquo; to only exist when serving as the panel, and not being merely the group of judges.)</p>\r\n<p>Without making this observation that the decision changes the metric by which the decision is measured, one will try to apply a single metric to both outcomes and find themselves in surprising implications and confusing statements.&nbsp; In his paper &ldquo;The Person Affecting Restriction, Comparativism, and the Moral Status of Potential People&rdquo;, (<a href=\"http://people.su.se/~guarr/\">http://people.su.se/~guarr/</a>) Gustaf Arrhenius quotes John Broome as saying:</p>\r\n<p>&ldquo;&hellip;[I]t cannot ever be true that it is better for a person that she lives than that she should never have lived at all. If it were better for a person that she lives than that she should never have lived at all, then if she had never lived at all, that would have been worse for her than if she had lived. But if she had never lived at all, there would have been no her for it to be worse for, so it could not have been worse for her.&rdquo; (My apologies for not yet having time to read Broome&rsquo;s work itself, I spend all my time attempting to prevent existential disaster and other activities seemed more pressing. Not reading Broome&rsquo;s work may well be a fault I should correct, but it wasn&rsquo;t sacrificed in order to watch another episode of Weeds.)</p>\r\n<p>The error here is that Broome passes over to another metric without seeming to notice. From the situation where she lives and enjoys life, it would be worse for her to have never lived. That is, now that she can consider anything, she can consider a world in which she does not exist as less preferable. In the situation in which she never lived and can consider nothing, she cannot consider it worse that she never lived. When we change from considering one situation to the other, our metric changes along with the situation.</p>\r\n<p>Likewise Arrhenius fails to make this observation, and approaches the situation with the strategy of comparing uniquely realizable people (who would be brought into existence by our actions) and non-uniquely realizable people. In two different populations with subpopulations that only exist in one population or the other, he correctly points out the difficulty of comparing the wellbeing of those subpopulations between the two situations. However he then goes on to say that we cannot make any comparison in their wellbeing between the situations. A subtle point, but the difficulty lies not in there being no comparison of their wellbeing, but in there being too many comparisons of their wellbeing, the 2 conflicting comparisons depending on whether they do or do not come to exist.</p>\r\n<p>As long as the populations are a fixed, unchangeable size and our metric constant, both the total utilitarian view and the average utilitarian view are in agreement: maximizing the average and maximizing the total become one and the same. In this situation we may not even find reason to distinguish the two views. However in regards to the difficulty of potential persons and changing metrics, both views strive to apply a constant metric to both situations; total utilitarianism uses the metric of the situation in which new people are realized, and average utilitarianism is perhaps interpretable as using the metric in which&nbsp; the new people are not realized.</p>\r\n<p>The seeming popularity of the total utilitarian view in regards to potential persons might be due to the fact that an application of that view increases utility by the its own metric (happy realized people are happy they were realized), while an application of the metric of the situation in which people are unrealized creates no change in utility (unrealized people are neither happy nor unhappy [nor even neutral!] about not being realized). This gives the appearance of suggesting we espouse total utilitarianism as in a comparison between increased utility and effectively unchanged utility, an increased utility seems preferable, but I am not convinced such a meta-comparison actually avoids applying one metric to both situations. Again, if we bring people of positive welfare into the world it is a preferable thing to have done so, but if we do not bring them into the world it causes no harm whatsoever to not have done so. My personal beliefs do not support the idea of unrealized people being unhappy about being unrealized, though we might note in the unrealized people situation a decreased utility experienced by total utilitarians unhappy with the outcome.</p>\r\n<p>I suggest that we apply the metric of whichever situation comes to be. One oddity of this is the seeming implication that once you&rsquo;ve killed someone they no longer exist or care, and thus your action is not unethical. If we take a preference utilitarian view and also assume that you are alive at the time you are murdered, we can resolve this by pointing out that the act of murder frustrates your preferences and can be considered unethical, and that it is impossible to kill someone when they are already dead and have no preferences. In contrast if we choose to not realize a potential person, at no point did they develop preferences that we frustrated.</p>\r\n<p>Regardless, merely valuing the situation from the metric of the situation that comes to be tells us nothing about which situation we ought to bring about. As I mentioned previously I now have an idea for a potential rule, but that will follow in a separate post.</p>\r\n<p>(A second though distinct argument for the difficulty or impossibility of making a fully sensible prescription in the case of future persons is present in Narveson, J. &ldquo;Utilitarianism and New Generations.&rdquo; Mind 78 (1967):62-72, if you can manage to track it down. I had to get it from my campus library.)</p>\r\n<p>(<strong>ETA:</strong> I've now posted my&nbsp;suggestion for a <a href=\"/lw/14z/a_normative_rule_for_decisionchanging_metrics/\" target=\"_blank\">normative rule</a>.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qBEtr2iaGEtLFyLvu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 7, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "1468", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4vQ2zrB8KjJTi7w6p"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-04T16:39:29.310Z", "modifiedAt": null, "url": null, "title": "Wits and Wagers", "slug": "wits-and-wagers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:35.718Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Johnicholas", "createdAt": "2009-02-27T15:01:52.708Z", "isAdmin": false, "displayName": "Johnicholas"}, "userId": "kBvTXutfPytNtzPyD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/C7KXC2eRZxmsNf2Mr/wits-and-wagers", "pageUrlRelative": "/posts/C7KXC2eRZxmsNf2Mr/wits-and-wagers", "linkUrl": "https://www.lesswrong.com/posts/C7KXC2eRZxmsNf2Mr/wits-and-wagers", "postedAtFormatted": "Tuesday, August 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Wits%20and%20Wagers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWits%20and%20Wagers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC7KXC2eRZxmsNf2Mr%2Fwits-and-wagers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Wits%20and%20Wagers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC7KXC2eRZxmsNf2Mr%2Fwits-and-wagers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC7KXC2eRZxmsNf2Mr%2Fwits-and-wagers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 41, "htmlBody": "<p>Wits and Wagers is apparently a board game, where players compete to be well-calibrated with respect to their trivia knowledge. I haven't played it.</p>\n<p>Has someone else here played it? If so, what was your experience? Would it be good rationalist/bayesian training?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "C7KXC2eRZxmsNf2Mr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 5.126217009811122e-07, "legacy": true, "legacyId": "1470", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-04T19:00:44.114Z", "modifiedAt": "2021-08-02T11:14:57.804Z", "url": null, "title": "The usefulness of correlations", "slug": "the-usefulness-of-correlations", "viewCount": null, "lastCommentedAt": "2017-03-23T13:52:54.534Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZrjiQWBGS45iexNZo/the-usefulness-of-correlations", "pageUrlRelative": "/posts/ZrjiQWBGS45iexNZo/the-usefulness-of-correlations", "linkUrl": "https://www.lesswrong.com/posts/ZrjiQWBGS45iexNZo/the-usefulness-of-correlations", "postedAtFormatted": "Tuesday, August 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20usefulness%20of%20correlations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20usefulness%20of%20correlations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZrjiQWBGS45iexNZo%2Fthe-usefulness-of-correlations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20usefulness%20of%20correlations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZrjiQWBGS45iexNZo%2Fthe-usefulness-of-correlations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZrjiQWBGS45iexNZo%2Fthe-usefulness-of-correlations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1094, "htmlBody": "<p>I sometimes wonder just how useful probability and statistics are. There is the theoretical argument that Bayesian probability is the fundamental method of correct reasoning, and that logical reasoning is just the limit as p=0 or 1 (although that never seems to be applied at the meta-level: what is the probability that Bayes' Theorem is true?), but today I want to consider the practice.</p>\n<p>Casinos, lotteries, and quantum mechanics: no problem. The information required for deterministic measurement is simply not available, by adversarial design in the first two cases, and by <a title=\"The mystery of the Born probabilities\" href=\"/lw/py/the_born_probabilities/\">we know not what</a> in the third.&nbsp;Insurance: by definition, this only works when it's impossible to predict the catastrophes insured against. No-one will offer insurance against a risk that will happen, and no-one will buy it for a risk that won't.&nbsp;Randomised controlled trials are the gold standard of medical testing; but over on OB Robin Hanson points out from time to time that the marginal dollar of medical spending has little effectiveness. And we don't actually know how a lot of treatments work. Quality control: test a random sample from your production run and judge the whole batch from the results. Fine -- it may be too expensive to test every widget, or impossible if the test is destructive. But wherever someone is doing statistical quality control of how accurately you're filling jam jars with the weight of jam it says on the label, someone else will be thinking about how to weigh every single one, and how to make the filling process more accurate. (And someone else will be trying to get the labelling regulations amended to let you sell the occasional 15-ounce pound of jam.)</p>\n<p>But when you can make real measurements, that's the way to go.&nbsp;Here is a technical illustration.</p>\n<p><a id=\"more\"></a>Prof. Sagredo has assigned a problem to his two students Simplicio and Salviati: \"X is difficult to measure accurately. Predict it in some other way.\"</p>\n<p>Simplicio collects some experimental data consisting of a great many pairs (X,Y) and with high confidence finds a correlation of 0.6 between X and Y. So given the value y of Y, his best prediction for the value of X is 0.6y. [Edit: that formula is mistaken. The regression line for Y against X is Y = bcX/a, assuming the means have been normalised to zero, where a and b are the standard deviations of X and Y respectively. For the Y=X+D<sub>1</sub> model below, bc/a is equal to 1.]</p>\n<p>Salviati instead tries to measure X, and finds a variable Z which is experimentally found to have a good chance of lying close to X. Let us suppose that the standard deviation of Z-X is 10% that of X.</p>\n<p>How do these two approaches compare?</p>\n<p>A correlation of 0.6 is generally considered pretty high in psychology and social science, especially if it's established with p=0.001 to be above, say, 0.5. So Simplicio is quite pleased with himself.</p>\n<p>A measurement whose range of error is 10% of the range of the thing measured is about as bad as it could be and still be called a measurement. (One might argue that any sort of entanglement whatever is a measurement, but one would be wrong.) It's a rubber tape measure. By that standard, Salviati is doing rather badly.</p>\n<p>In effect, Simplicio is trying to predict someone's weight from their height, while Salviati is putting them on a (rather poor) weighing machine (and both, presumably, are putting their subjects on a very expensive and accurate weighing machine to obtain their true weights).</p>\n<p>So we are comparing a good correlation with a bad measurement. How do they stack up? Let us suppose that the underlying reality is that Y = X +&nbsp;D<sub>1</sub>&nbsp;and Z = X + D<sub>2</sub>, where X, D<sub>1</sub>, and&nbsp;D<sub>2</sub>&nbsp;are normally distributed and uncorrelated (and causally unrelated, which is a <a title=\"Causality does not imply correlation\" href=\"/lw/12a/causality_does_not_imply_correlation/\">stronger condition</a>). I'm choosing the normal distribution because it's easy to calculate exact numbers, but I don't believe the conclusions would be substantially different for other distributions.</p>\n<p>For convenience, assume the variables are normalised to all have mean zero, and let X,&nbsp;D<sub>1</sub>, and&nbsp;D<sub>2</sub>&nbsp;have standard deviations 1, d<sub>1</sub>, and d<sub>2</sub> respectively.</p>\n<p>Z-X is D<sub>2</sub>, so d<sub>2</sub>&nbsp;= 0.1. The correlation between Z and X is c(X,Z) = cov(X,Z)/(sd(X)sd(Z)) = 1/sqrt(1+d<sub>2&nbsp;</sub><sup>2</sup>) = 0.995.</p>\n<p>The correlation between X and Y is c(X,Y) = 1/sqrt(1+d<sub>1&nbsp;</sub><sup>2</sup>) = 0.6, so d<sub>1</sub> = 1.333.</p>\n<p>We immediately see something suspicious here. Even a terrible measurement yields a sky-high correlation. Or put the other way round, if you're bothering to measure correlations, your data are rubbish. Even this \"good\" correlation gives a signal to noise ratio of less than 1. But let us proceed to calculate the mutual informations. How much do Y and Z tell you about X, separately or together?</p>\n<p>For the bivariate normal distribution, the mutual information between variables A and B with correlation c is lg(I), where lg is the binary logarithm and I = sd(A)/sd(A|B). (The denominator here -- the standard deviation of A conditional on the value of B -- happens to be independent of the particular value of B for this distribution.) This works out to 1/sqrt(1-c<sup>2</sup>). So the mutual information is -lg(sqrt(1-c<span style=\"font-size: 11px;\"><sup>2</sup></span>)).</p>\n<table border=\"0\" align=\"center\">\n<tbody>\n<tr>\n<th>&nbsp;</th> <th>&nbsp;&nbsp;&nbsp;</th><th>corr.</th> <th>&nbsp;&nbsp;&nbsp;</th> <th>mut. inf.</th>\n</tr>\n<tr>\n<td>Simplicio</td>\n<td>&nbsp;</td>\n<td>0.6</td>\n<td>&nbsp;</td>\n<td>0.3219</td>\n</tr>\n<tr>\n<td>Salviati</td>\n<td>&nbsp;</td>\n<td>0.995</td>\n<td>&nbsp;</td>\n<td>3.3291</td>\n</tr>\n</tbody>\n</table>\n<p>What can you do with one third of a bit? If Simplicio tries to predict just the sign of X from the sign of Y, he will be right only 70% of the time (i.e. cos<sup>-1</sup>(-c(X,Y))/&pi;). Salviati will be right 96.8% of the time. Salviati's estimate will even be in the right decile 89% of the time, while on that task Simplicio can hardly do better than chance. So even a good correlation is useless as a measurement.</p>\n<p>Simplicio and Salviati show their results to Prof. Sagredo. Simplicio can't figure out how Salviati did so much better without taking measurements on thousands of samples. Salviati seemed to just think about the problem and come up with a contraption out of nowhere that did the job, without doing a single statistical test. \"But at least,\" says Simplicio, \"you can't throw away my 0.3219, it all adds up!\" Sagredo points out that it literally does not add up. The information gained about X from Y and Z together is not&nbsp;0.3219+3.3291 = 3.6510 bits. The correct result is found from the standard deviation of X conditional on both Y and Z, which is sqrt(1/(1 + 1/d<sub>1&nbsp;</sub><sup>2</sup>&nbsp;+ 1/d<sub>2&nbsp;</sub><sup>2</sup>)). The information gained is then lg(sqrt(1 + 1/d<sub>1&nbsp;</sub><sup>2</sup>&nbsp;+ 1/d<sub>2&nbsp;</sub><sup>2</sup>)) =&nbsp;0.5*lg(101.5625) = 3.3331. The extra information over knowing just Z is only 0.0040 = 1/250 of a bit, because nearly all of Simplicio's information is already included in Salviati's.</p>\n<p>Sagredo tells Simplicio to go away and come up with some real data.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZrjiQWBGS45iexNZo", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 20, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "1471", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3ZKvf9u2XEWddGZmS", "CXcYoRMyMcs7zyknh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2009-08-04T19:00:44.114Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-04T19:10:49.712Z", "modifiedAt": null, "url": null, "title": "She Blinded Me With Science", "slug": "she-blinded-me-with-science", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:17.463Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jonathan_Graehl", "createdAt": "2009-02-27T23:21:15.671Z", "isAdmin": false, "displayName": "Jonathan_Graehl"}, "userId": "eKsWtKKceoRYwcc7s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YYoK5H4QFTdt4bM5C/she-blinded-me-with-science", "pageUrlRelative": "/posts/YYoK5H4QFTdt4bM5C/she-blinded-me-with-science", "linkUrl": "https://www.lesswrong.com/posts/YYoK5H4QFTdt4bM5C/she-blinded-me-with-science", "postedAtFormatted": "Tuesday, August 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20She%20Blinded%20Me%20With%20Science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShe%20Blinded%20Me%20With%20Science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYYoK5H4QFTdt4bM5C%2Fshe-blinded-me-with-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=She%20Blinded%20Me%20With%20Science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYYoK5H4QFTdt4bM5C%2Fshe-blinded-me-with-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYYoK5H4QFTdt4bM5C%2Fshe-blinded-me-with-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 947, "htmlBody": "<p>Scrutinize claims of scientific fact in support of opinion journalism.</p>\n<p>Even with honest intent, it's difficult to apply science correctly, and it's rare that dishonest uses are punished. Citing a scientific result gives an easy patina of authority, which is rarely scratched by a casual reader. Without actually lying, the arguer may select from dozens of studies only the few with the strongest effect in their favor, when the overall body of evidence may point at no effect or even in the opposite direction. The reader only sees \"statistically significant evidence for X\". In some fields, the majority of published studies claim unjustified significance in order to gain publication, inciting these abuses.</p>\n<p>Here are two recent examples:</p>\n<blockquote>\n<p>Women are often better communicators because their brains are more networked for language. The majority of women are better at \"mind-reading,\" than most men; they can read the emotions written on people's faces more quickly and easily, a talent jump-started by the vast swaths of neural real estate dedicated to processing emotions in the female brain.</p>\n</blockquote>\n<p>- Susan Pinker, a psychologist, in <a href=\"http://roomfordebate.blogs.nytimes.com/2009/08/02/do-women-make-better-bosses/\">NYT's \"DO Women Make Better Bosses\"</a></p>\n<blockquote>Twin studies and adoptive studies show that the overwhelming determinant of your weight is not your willpower; it's your genes. The heritability of weight is between .75 and .85. The heritability of height is between .9 and .95. And the older you are, the more heritable weight is.</blockquote>\n<p>- Megan McArdle, linked from the LW article <a href=\"/lw/14c/the_obesity_myth/\">The Obesity Myth</a></p>\n<p><a id=\"more\"></a></p>\n<hr />\n<p>Mike, a biologist, gives an exasperated explanation of what heritability <a href=\"http://scienceblogs.com/mikethemadbiologist/2009/08/obesity_makes_people_stupidabo.php\">actually means</a>:</p>\n<blockquote>Quantitative geneticists use [heritability] to calculate the changes to be expected from artificial or natural selection in a statistically steady environment. It says nothing about how much the over-all level of the trait is under genetic control, and it says nothing about how much the trait can change under environmental interventions.</blockquote>\n<hr />\n<p>Susan Pinker's female-boss-brain cheerleading is refuted by <a href=\"http://www.gabrielarana.com/2009/08/reinforcing-stereotypes-about-men-and.html\">Gabriel Arana</a>. A specific scientific claim Pinker makes (\"the thicker corpus callosum connecting women's two hemispheres provides a swifter superhighway for processing social messages\") is contradicted by a meta-analysis (<a href=\"http://www.sciencedirect.com/science?_ob=GatewayURL&amp;_method=citationSearch&amp;_uoikey=B6T0J-3SFMR9T-4&amp;_origin=SDEMFRHTML&amp;_version=1&amp;md5=f478cb3b5eae6d08a0b39424faa7bd61\">Sex Differences in the Human Corpus Callosum: Myth or Reality?</a>), and without that, you have only just-so evolutionary psychology argument.</p>\n<p>The Bishop and Wahlsten meta-analysis claims that the only consistent finding is for slightly larger average whole brain size and a very slightly larger corpus callosum in adult males. Here are some highlights:</p>\n<blockquote>Given that the CC interconnects so many functionally different regions of cerebral cortex, there is no reason to believe that a small difference in overall CC size will pertain to any specific psychological construct. Total absence of the corpus callosum tends to be associated with a ten-point or greater reduction in full-scale IQ, but more specific functional differences from IQ-matched controls are difficult to identify.</blockquote>\n<blockquote>In one recent study, a modest correlation between cerebrum size and IQ within a sex was detected. At the same time, males and females differ substantially in brain size but not IQ. There could easily be some third factor or array of processes that acts to increase both brain size and IQ score for people of the same sex, even though brain size per se does not mediate the effect of the other factor on IQ.</blockquote>\n<blockquote>The journal Science has refused to publish failures to replicate the 1982 claims of de Lacoste-Utamsing and Holloway (Byne, personal communication).</blockquote>\n<p>Obviously, if journals won't publish negative results, then this weakens the effective statistical significance of the positive results we do read. The authors don't find this to be significant for the topic (the above complaint isn't typical).</p>\n<blockquote>When many small-scale studies of small effects are published, the chances are good that a few will report a statistically significant sex difference. ... One of our local newspapers has indeed printed claims promulgated over wire services about new studies finding a sex difference in the corpus callosum but has yet to print a word about contrary findings which, as we have shown, far outnumber the statistically significant differences.</blockquote>\n<p>This effect is especially notable in media coverage of health and diet research.</p>\n<blockquote>The gold-standard in the medical literature is a cumulative meta-analysis conducted using the raw data. We urge investigators to make their raw data or, better yet, the actual tracings available for cumulative meta-analysis. We attempted to collect the raw data from studies of sex differences in the CC cited in an earlier version of this paper by writing to the authors. The level of response was astoundingly poor. In several studies that used MRI, the authors even stated that the original observations were no longer available.</blockquote>\n<p>This is disturbing. I suspect that many authors are hesitant to subject themselves to the sort of scrutiny they ought to welcome.</p>\n<blockquote>By convention, we are taught that the null hypothesis of no sex difference should be rejected if the probability of erroneously rejecting the null on the basis of a set of data is 5% or less. If 10 independent measures are analysed in one study, each with the &alpha; = 0.05 criterion, the probability of finding at least one &lsquo;significant&rsquo; sex difference by chance alone is 1 &minus; (1 &minus; 0.05)10 = 0.40 or 40%. Consequently, when J tests involving the same object, e.g. the corpus callosum, are done in one study, the criterion for significance of each test might better be adjusted to &alpha;/J, the Dunn or Bonferroni criterion that is described in many textbooks. All but two of 49 studies of the CC adopted &alpha; = 0.05 or even 0.10, and for 45 of these studies, an average of 10.2 measures were assessed with independent tests.</blockquote>\n<p>This is either rank incompetence, or even worse, the temptation to get <em>some</em> positive result out of the costly data collection.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YYoK5H4QFTdt4bM5C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 16, "extendedScore": null, "score": 3e-05, "legacy": true, "legacyId": "1472", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WfHiyRxMj6aL7PN7i"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-04T23:36:37.364Z", "modifiedAt": null, "url": null, "title": "The Machine Learning Personality Test", "slug": "the-machine-learning-personality-test", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:31.251Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/avyjAqcifkfaJcuSe/the-machine-learning-personality-test", "pageUrlRelative": "/posts/avyjAqcifkfaJcuSe/the-machine-learning-personality-test", "linkUrl": "https://www.lesswrong.com/posts/avyjAqcifkfaJcuSe/the-machine-learning-personality-test", "postedAtFormatted": "Tuesday, August 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Machine%20Learning%20Personality%20Test&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Machine%20Learning%20Personality%20Test%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FavyjAqcifkfaJcuSe%2Fthe-machine-learning-personality-test%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Machine%20Learning%20Personality%20Test%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FavyjAqcifkfaJcuSe%2Fthe-machine-learning-personality-test", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FavyjAqcifkfaJcuSe%2Fthe-machine-learning-personality-test", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1677, "htmlBody": "<p>You've probably heard of the Briggs-Myers personality test, which is a classification system of 16 different personality types based on the writings of Carl Jung, a man who believed that his library books sometimes spontaneously exploded.&nbsp; Its main advantage is that it manages to classify people without insulting them.&nbsp; (This is accomplished by confounding dimensions:&nbsp; Instead of measuring one property of personality along one dimension, which leads to some scores being considered better than others, you subtract a measurement along one desirable property of personality from a measurement along another desirable property of personality, and call the result one dimension.)</p>\n<p>You've probably also heard of the MMPI, a test designed by giving lots of questions to mental patients and seeing which ones were answered differently by people with particular diagnoses.&nbsp; This is more like personality clustering for fault diagnosis than a personality test.&nbsp; You may find it useful if you're crazy.&nbsp; (One of the criticisms of this test is that religious people often test as psychotic: \"Do you sometimes think someone else is directing your actions?&nbsp; Is someone else trying to plan events in your life?\"&nbsp; Is that a bug, or a feature?)</p>\n<p>You may have heard of the Personality Assessment Inventory, a test devised by listing things that psychotherapists thought were important, and trying to come up with questions to test them.</p>\n<p>The Big 5 personality test is constructed in a well-motivated way, using factor analysis to try to discover from the data what the true dimensions of personality are.</p>\n<p>But these all work from the top down, looking at human behavior (answers), and trying to uncover latent factors farther down.&nbsp; I'm instead going to propose a personality system that, instead, starts from the very bottom of your hardware and leaves it to you to work your way up to the variables of interest:&nbsp; the Machine Learning Personality Test (\"MLPT\").<a id=\"more\"></a></p>\n<p>Other personality tests try to measure things that people want to measure, but that might not be psychologically real.&nbsp; The MLPT is just the opposite:&nbsp; It tries to measure things that are probably psychologically real, but are at such a low level that people aren't interested in them.&nbsp; Your mission, should you choose to accept it, is to figure out the connection between the dimensions of the MLPT, and personality traits that you understand and care about.</p>\n<p>LW readers are familiar with thinking of people as optimizers.&nbsp; Take that idea, and make 3 assumptions:</p>\n<ol>\n<li>People optimize using something like existing algorithms for machine learning.</li>\n<li>A person learns parameters for their learning algorithms according to the data they are exposed to.</li>\n<li>These parameters generalize across tasks.</li>\n</ol>\n<p>Assumption 1 is critical for the MLPT to make any sense.&nbsp; What it does is to classify people according to the parameter settings they use when learning and optimizing.&nbsp; I mostly use parameters from classification / categorization algorithms.</p>\n<p>Assumption 2 is important if you wish to change yourself.&nbsp; This is the great advantage of the MLPT:&nbsp; It not only tells you your personality type, but also how to change your personality type.&nbsp; Simply expose yourself to data such that the MLPT type you desire is more effective than yours at learning that data.</p>\n<p>Assumption 3 is something I have no evidence for at all, and may be wholly false.</p>\n<p>Here are the dimensions I've thought of.&nbsp; Can you name others worth adding?</p>\n<p><strong>Learning rate</strong>:&nbsp; This is a parameter used to say how much you change your weights in response to new information.&nbsp; I was going to say, \"how much you change your beliefs\", but that would be misleading; because we're talking about a much finer level of detail.&nbsp; In a neural network model, the learning parameter determines how much you change the weight on a connection between 2 neurons each time you want to change the degree to which one of those neuron's output affects the other neuron's input.</p>\n<p>People with a high learning rate learn fast and easily, and may be great at memorizing facts.&nbsp; But when it comes to problems where you have a lot of data and are trying to get extremely high performance, they are not able to get as good an optimum.&nbsp; This suggests that expert violinists or baseball players tend to have poor memory and be categorized as slow learners.&nbsp; (Although I'm skeptical that learning rate on motor tasks would generalize to learning rate on history exams.)<strong></strong></p>\n<p><strong>Regularization rate</strong>:&nbsp; This parameter says how strongly to bias your outcome back towards your priors.&nbsp; If your regularization rate isn't high enough, the parameters you learn may drift to absurdly large values.&nbsp; In some cases, this will cause the entire network to become unstable, at which point learning ceases and you need to be rebooted.</p>\n<p>In most ways, regularization is opposed to learning.&nbsp; Increasing the regularization rate without changing the learning rate effectively decreases the learning rate.</p>\n<p>People with a high regularization rate might be less prone to mental illness, but not very creative.&nbsp; People with a low regularization rate will get some of the advantages of a high learning rate, without the disadvantages.<strong></strong></p>\n<p><strong>Exploration/Exploitation setting</strong>:&nbsp; High exploration means you try out new solutions and new things often.&nbsp; High exploitation means you don't.&nbsp; High exploitation is conceptually a lot like high regularization.<strong></strong></p>\n<p><strong>Number of dimensions to classify on</strong>:&nbsp; When you're learning how to categorize something, how many dimensions do you use?&nbsp; An astonishing percentage of what we do is based on single-dimension discriminations.&nbsp; Some people use only a single dimension even for important and highly complex discrimination tasks, such as choosing a new president, or deciding on the morality of an action.</p>\n<p>Using a small number of dimensions results in a high error rate (where \"error\", since I'm not assuming category labels exist out in the world, is going to mean your error in predicting outcomes based on your categorizations).&nbsp; Using a large number of dimensions results in slow learning and slow thinking, construction of categories no one else understands, stress when faced with complex situations, and errors from overgeneralizing and from perceiving patterns where there are none, because you don't have enough data to learn whether a distinction in outcome is really due to a difference along one of your dimensions, or just chance.</p>\n<p>People using too few dimensions will be, well, stupid.&nbsp; They will be incapable of learning many things no matter how much data they're exposed to.&nbsp; But they can make decisions under pressure and with confidence.&nbsp; They may make good managers.&nbsp; People using too many dimensions will take too long to make decisions, wanting too much data.&nbsp; This dimension may correspond closely to \"intelligence\", of the kind that scores well on IQ tests.</p>\n<p>People using different dimensions and different numbers of dimensions will have a very hard time understanding each other.</p>\n<p>It may be worth breaking this separately into number of input dimensions and number of output dimensions.&nbsp; But I kinda doubt it.&nbsp; (I guess I'm just a low-dimensional kinda guy.)</p>\n<p><strong>Binary / Discrete / Continuous thinking</strong>:&nbsp; Do you categorize your inputs before thinking about them, or try to juggle all their values and do regression in your head?&nbsp; Are you trying to put things in bins, or place them along a continuum?</p>\n<p>This probably has the same implications as number of input and output dimensions.</p>\n<p><strong>Degree of independence/correlation assumed to exist between dimensions</strong>:&nbsp; If the things you are categorizing have measurements along different dimensions that are independent on different dimensions, categorization becomes much easier, and you can handle many more dimensions.</p>\n<p>People assuming high independence might make good scientists, as science has so far been the art of finding dimensions in the real world that are independent and using them for analysis.&nbsp; People assuming high correlations might be better at art, and at perceiving holistic patterns.&nbsp; They might tend to give credence towards New-Age notions that everything is interconnected.<strong></strong></p>\n<p><strong>Degree of linearity/nonlinearity assumed</strong>:&nbsp; Assuming linearity has similar advantages and disadvantages as assuming independence, and assuming nonlinearity has similar effects to assuming correlations.&nbsp; (They are not the same; sometimes the real world presents linearity with correlations, or independence with nonlinearity.&nbsp; I just can't think of anything different to say about them personality-wise.)<strong></strong></p>\n<p>&nbsp;</p>\n<p>I'm going to merge independence/correlation and linearity/nonlinearity, because I don't have anything useful to say to distinguish them.&nbsp; I'm going to merge regularization rate and exploration/exploitation for similar reasons; those two are a lot like each other anyway.&nbsp; I'm going to ignore binary/discrete/continuous, because I didn't think of it until after writing the personality types below and I'm too lazy to redo them.&nbsp; It's a lot like number of dimensions anyway.</p>\n<p>Now we need to find cute acronyms for our resulting personality types.&nbsp; For this, we will organize our dimensions so that the first and last dimensions are specified with vowels, and the second and third by consonants.&nbsp; (Changing the fourth letter to a vowel and thus providing catchier names is, I think, the main advantage of this test over the Myers-Briggs.)</p>\n<ul>\n<li>Regularization rate: high = (<strong>I</strong>)nertial, low = (<strong>U</strong>)nconventional</li>\n<li>Learning rate: high = (<strong>F</strong>)ast / (<strong>S</strong>)low</li>\n<li>Number of dimensions: (<strong>M</strong>)any / (<strong>F</strong>)ew</li>\n<li>Independence / linearity: (<strong>I</strong>)ndependent and linear / h(<strong>O</strong>)listic and nonlinear</li>\n</ul>\n<p>Now you may be eager to take the MLPT and find your results!</p>\n<p>Sadly, it does not exist.&nbsp; As I said, I'm just proposing it.</p>\n<p>But we can at least write fun, horoscope-like personality summaries!&nbsp; (NOTE: These may not be as accurate as an actual horoscope.)</p>\n<ul>\n<li><strong>IFMI</strong>: You like things that appear complex, but can be mastered with a few fundamental rules.&nbsp; You may become an engineer.</li>\n<li><strong>ISMI</strong>:&nbsp; Like IFMI, but you were on the chess team instead of \"It's Academic\".</li>\n<li><strong>IFMO</strong>:&nbsp; You should go to med school.</li>\n<li><strong>IFFI</strong>:&nbsp; You know what you like, and what others should like.&nbsp; You thought four dimensions was too many.&nbsp; You may vote Republican.</li>\n<li><strong>ISMO</strong>:&nbsp; You may be a go master.</li>\n<li><strong>UFMI, USMI</strong>: You over-analyze everything, often arriving at unconventional answers, and this makes you a pain in the ass to those around you.&nbsp; You probably read Less Wrong.</li>\n<li><strong>USMO</strong>:&nbsp; You are very artistic.&nbsp; You don't believe in personality classification schemes.&nbsp; You may have been to Taos, New Mexico.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dBPou4ihoQNY4cquv": 1, "fpEBgFE7fgpxTm9BF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "avyjAqcifkfaJcuSe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 31, "extendedScore": null, "score": 5.126888083591097e-07, "legacy": true, "legacyId": "1473", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-05T05:07:05.158Z", "modifiedAt": null, "url": null, "title": "A Normative Rule for Decision-Changing Metrics", "slug": "a-normative-rule-for-decision-changing-metrics", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:17.604Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4vQ2zrB8KjJTi7w6p/a-normative-rule-for-decision-changing-metrics", "pageUrlRelative": "/posts/4vQ2zrB8KjJTi7w6p/a-normative-rule-for-decision-changing-metrics", "linkUrl": "https://www.lesswrong.com/posts/4vQ2zrB8KjJTi7w6p/a-normative-rule-for-decision-changing-metrics", "postedAtFormatted": "Wednesday, August 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Normative%20Rule%20for%20Decision-Changing%20Metrics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Normative%20Rule%20for%20Decision-Changing%20Metrics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4vQ2zrB8KjJTi7w6p%2Fa-normative-rule-for-decision-changing-metrics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Normative%20Rule%20for%20Decision-Changing%20Metrics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4vQ2zrB8KjJTi7w6p%2Fa-normative-rule-for-decision-changing-metrics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4vQ2zrB8KjJTi7w6p%2Fa-normative-rule-for-decision-changing-metrics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1260, "htmlBody": "<p><em>Yesterday I wrote about the </em><a href=\"/lw/14s/the_difficulties_of_potential_people_and_decision/\" target=\"_blank\"><em>difficulties of ethics and potential people</em></a><em>. Namely, that whether you bring a person into existence or not changes the moral metric by which your decision is measured. At first all I had was the observation suggesting that the issue was complex, but no answer to the question \"Well then, what should we do?\" I will write now about an answer that came to me.</em></p>\r\n<p>All theories regarding potential people start by comparing outcomes to find which is most desirable, then moving towards it. However I believe I have shown that there are two metrics regarding such questions, and those metrics can disagree. What then do we do?</p>\r\n<p>We are always in a particular population ourselves, and so we can ask not which outcome is preferable, but if we should move from one situation to another. This allows us to consider the alternate metrics in series. For an initial name more attractive than \"my rule\" I will refer to the system as Deontological Consequentialism, or DC. I'm open to other suggestions.<a id=\"more\"></a></p>\r\n<p><strong>Step 1:</strong>&nbsp;Consider your action with the metric of new people not coming to be: that is, only the welfare of the people who will exist regardless of your decision.* I will assume in this discussion there are three possibilities: people receive higher utility, lower utility, or effectively unchanged utility. You might dispense with the third option, the results are similar.</p>\r\n<p>First, if you expect reduced utility for existing people from taking an action: <em>do not take that action</em>. This is regardless of how many new people might otherwise exist or how much utility they might have; if we never bring them into existence, we have wronged no one.</p>\r\n<p>This is the least intuitive aspect of this system, though it is also the most critical for avoiding the paradoxes of which I am aware. I think this unintuitive nature mostly stems from automatically considering future people as if they exist. I'd also note that our intuitions are really not used to dealing with this sort of question, but one more approachable example exists. If a couple strongly expects they will be unhappy having children, will derive little meaning from parenthood and few material benefits from their children and we believe them, if in short they really don't want kids, I suspect few will tell them they really ought to have children anyway as long as the children will be happy. I think few people consider how very happy the kids or grandkids might be either, even if the couple would be great parents; if the couple will be miserable we probably advocate they stay childless. Imagining such unhappy parents we also tend to imagine unhappy children, so some effort might be required to keep from ruining the example. See also Eliezer's discussion of the fallibility of intuition in a <a href=\"/lw/14r/unspeakable_morality/\" target=\"_blank\">recent post</a>.</p>\r\n<p>Second, if we expect effectively unchanged utility for existing people it is again perfectly acceptable to not create new people, as you wrong no one by doing so. But as you don't harm yourself either, it's fine for you if you create them, bringing us to</p>\r\n<p><strong>Step 2a:</strong>&nbsp;Now we consider the future people. If they will have negative utility, i.e. roughly speaking they wish they hadn't been born, or for their sake <em>we </em>wish they hadn't been born, then we ought not to bring them into existence. We don't get anything and they suffer. If instead they experience entirely neutral lives (somehow), if they have no opinion on their creation, then it <em>really</em> doesn't matter if we create them or not.</p>\r\n<p>If they will experience positive lives, then it would be a good thing to have created them, as it was \"no skin off our backs anyway\". However I would theoretically hold it's <em>still</em> perfectly acceptable not to bring them into existence, as if we don't they'll never mind that we didn't. But my neutrality towards bringing them into existence is such that I would even accept a rule I didn't agree with, saying that I ought to create the new people.</p>\r\n<p>Now back into step 1, there is the case were existing people will benefit by creating new people. Here we are forced to consider their wellbeing in</p>\r\n<p><strong>Step 2b:</strong>&nbsp;Now if the new people have positive utility, or zero utility in totally neutral lives, then we should go ahead and bring them into existence, as we benefit and at least they don't suffer. However if they will have overall negative lives, then we should compare how much utility existing people gain and subtract the negative utility of the new people. You might dislike the idea of this inequality (I do as well) but this is a general issue with utilitarianism separate from potential people; here we&rsquo;re considering them the same as existing people (as they then would be). If you've got a solution, such as weighting negative utility more heavily or just forcing in egalitarian considerations, apply it here.</p>\r\n<p>&nbsp;</p>\r\n<p>This concludes Deontological Consequentialism. I won't go over them all here, but this rule seems to avoid unattractive answers to all paradoxes I've seen debated. There is one that threw me for a loop, the largely intractable \"<a href=\"http://plato.stanford.edu/entries/repugnant-conclusion/#EigWayDeaRepCon\" target=\"_blank\">Mere Addition Paradox</a>\". I'll describe it briefly here, mostly to show how DC (at first)&nbsp;seems to fall&nbsp;prey to it as well.</p>\r\n<p>A classic paradox is the Repugant Conclusion, which takes total utilitarianism to suggest we should prefer a vast population with lives barely worth living more than relatively few lives of very high utility. Using DC, if we are in the high utility population we note that we (existing people) would all experience drastically lower utility by bringing about the second situation, and so avoid it.</p>\r\n<p>In the Mere Addition Paradox, you start from the high utility situation. Then you consider, is it moral to increase the utility of existing people while bringing into being huge numbers of people with very low but positive utility? DC seems to suggest we ought to, as we benefit and they have positive lives as well. Now that we've done that, ought we to reduce our own utility if it would allow the low-utility people to have higher utility, such that the total is drastically increased but everyone still experiences utility not far above zero? Deontological Consequentialism is only about potential people, but here both average and total utilitarianism suggest we should do this, increasing both the total and average.</p>\r\n<p>And with this we find that by taking it in these steps we have arrived at the Repugnant Conclusion! For DC this is accomplished by \"slipping in\" the new people so that they become existing people, and our consideration of their wellbeing changes. The solution here is to take account of our own future actions: we see that by adding these new people at first, we will then seek to distribute our own utility to much increase theirs, and in the end we actually do experience reduced utility. That is, we see that by bringing them into existence we are in reality choosing the Repugnant Conclusion. Realizing this, we do not create them, and avoid the paradox. (In most conventional situations it seems more likely we can increase the new people's utility without such a decrease to our own.)</p>\r\n<p><br />*An interesting situation arises when we know new people will come to be regardless of our decision. I suggest here that we average the utility of all new people in each population, treat the situation with fewer total people as our \"existing population\", and apply DC from there. Unless people are interested in talking about this arguably unusual situation however, I won't go into more detail.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4vQ2zrB8KjJTi7w6p", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 5.127421294896626e-07, "legacy": true, "legacyId": "1475", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qBEtr2iaGEtLFyLvu", "QjPevgYZrqWKbJCkL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-05T15:30:46.718Z", "modifiedAt": null, "url": null, "title": "Recommended reading: George Orwell on knowledge from authority", "slug": "recommended-reading-george-orwell-on-knowledge-from", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:31.432Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CronoDAS", "createdAt": "2009-02-27T04:42:19.587Z", "isAdmin": false, "displayName": "CronoDAS"}, "userId": "Q2oaNonArzibx5cQN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/baETQBENSe6dEQoPM/recommended-reading-george-orwell-on-knowledge-from", "pageUrlRelative": "/posts/baETQBENSe6dEQoPM/recommended-reading-george-orwell-on-knowledge-from", "linkUrl": "https://www.lesswrong.com/posts/baETQBENSe6dEQoPM/recommended-reading-george-orwell-on-knowledge-from", "postedAtFormatted": "Wednesday, August 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Recommended%20reading%3A%20George%20Orwell%20on%20knowledge%20from%20authority&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARecommended%20reading%3A%20George%20Orwell%20on%20knowledge%20from%20authority%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbaETQBENSe6dEQoPM%2Frecommended-reading-george-orwell-on-knowledge-from%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Recommended%20reading%3A%20George%20Orwell%20on%20knowledge%20from%20authority%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbaETQBENSe6dEQoPM%2Frecommended-reading-george-orwell-on-knowledge-from", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbaETQBENSe6dEQoPM%2Frecommended-reading-george-orwell-on-knowledge-from", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 802, "htmlBody": "<p>This is an excerpt from an article George Orwell wrote in 1946. I will let the text speak for itself.</p>\n<blockquote>\n<p>&nbsp;&nbsp;&nbsp; Somewhere or other &mdash; I think it is in the preface to saint Joan &mdash; Bernard Shaw remarks that we are more gullible and superstitious today than we were in the Middle Ages, and as an example of modern credulity he cites the widespread belief that the earth is round. The average man, says Shaw, can advance not a single reason for thinking that the earth is round. He merely swallows this theory because there is something about it that appeals to the twentieth-century mentality.<a id=\"more\"></a></p>\n<p>&nbsp;&nbsp;&nbsp; Now, Shaw is exaggerating, but there is something in what he says, and the question is worth following up, for the sake of the light it throws on modern knowledge. Just why do we believe that the earth is round? I am not speaking of the few thousand astronomers, geographers and so forth who could give ocular proof, or have a theoretical knowledge of the proof, but of the ordinary newspaper-reading citizen, such as you or me.</p>\n<p>&nbsp;&nbsp;&nbsp; As for the Flat Earth theory, I believe I could refute it. If you stand by the seashore on a clear day, you can see the masts and funnels of invisible ships passing along the horizon. This phenomenon can only be explained by assuming that the earth's surface is curved. But it does not follow that the earth is spherical. Imagine another theory called the Oval Earth theory, which claims that the earth is shaped like an egg. What can I say against it?</p>\n<p>&nbsp;&nbsp;&nbsp; Against the Oval Earth man, the first card I can play is the analogy of the sun and moon. The Oval Earth man promptly answers that I don't know, by my own observation, that those bodies are spherical. I only know that they are round, and they may perfectly well be flat discs. I have no answer to that one. Besides, he goes on, what reason have I for thinking that the earth must be the same shape as the sun and moon? I can't answer that one either.</p>\n<p>&nbsp;&nbsp;&nbsp; My second card is the earth's shadow: When cast on the moon during eclipses, it appears to be the shadow of a round object. But how do I know, demands the Oval Earth man, that eclipses of the moon are caused by the shadow of the earth? The answer is that I don't know, but have taken this piece of information blindly from newspaper articles and science booklets.</p>\n<p>&nbsp;&nbsp;&nbsp; Defeated in the minor exchanges, I now play my queen of trumps: the opinion of the experts. The Astronomer Royal, who ought to know, tells me that the earth is round. The Oval Earth man covers the queen with his king. Have I tested the Astronomer Royal's statement, and would I even know a way of testing it? Here I bring out my ace. Yes, I do know one test. The astronomers can foretell eclipses, and this suggests that their opinions about the solar system are pretty sound. I am, to my delight, justified in accepting their say-so about the shape of the earth.</p>\n<p>&nbsp;&nbsp;&nbsp; If the Oval Earth man answers &mdash; what I believe is true &mdash; that the ancient Egyptians, who thought the sun goes round the earth, could also predict eclipses, then bang goes my ace. I have only one card left: navigation. People can sail ship round the world, and reach the places they aim at, by calculations which assume that the earth is spherical. I believe that finishes the Oval Earth man, though even then he may possibly have some kind of counter.</p>\n<p>&nbsp;&nbsp;&nbsp; It will be seen that my reasons for thinking that the earth is round are rather precarious ones. Yet this is an exceptionally elementary piece of information. On most other questions I should have to fall back on the expert much earlier, and would be less able to test his pronouncements. And much the greater part of our knowledge is at this level. It does not rest on reasoning or on experiment, but on authority. And how can it be otherwise, when the range of knowledge is so vast that the expert himself is an ignoramus as soon as he strays away from his own specialty? Most people, if asked to prove that the earth is round, would not even bother to produce the rather weak arguments I have outlined above. They would start off by saying that \"everyone knows\" the earth to be round, and if pressed further, would become angry. In a way Shaw is right. This is a credulous age, and the burden of knowledge which we now have to carry is partly responsible.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "baETQBENSe6dEQoPM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 9, "extendedScore": null, "score": 1.4e-05, "legacy": true, "legacyId": "1477", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-06T01:58:49.178Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes - August 2009", "slug": "rationality-quotes-august-2009", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:33.368Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tz6cJZdhbuhMfSXc4/rationality-quotes-august-2009", "pageUrlRelative": "/posts/tz6cJZdhbuhMfSXc4/rationality-quotes-august-2009", "linkUrl": "https://www.lesswrong.com/posts/tz6cJZdhbuhMfSXc4/rationality-quotes-august-2009", "postedAtFormatted": "Thursday, August 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20-%20August%202009&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20-%20August%202009%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftz6cJZdhbuhMfSXc4%2Frationality-quotes-august-2009%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20-%20August%202009%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftz6cJZdhbuhMfSXc4%2Frationality-quotes-august-2009", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftz6cJZdhbuhMfSXc4%2Frationality-quotes-august-2009", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 83, "htmlBody": "<p>A monthly thread for posting any interesting rationality-related quotes you've seen recently on the Internet, or had stored in your quotesfile for ages.</p>\n<ul>\n<li>Please post all quotes separately (so that they can be voted up/down separately) unless they are strongly related/ordered. </li>\n<li>Do not quote yourself. </li>\n<li>Do not quote comments/posts on LW/OB - if we do this, there should be a separate thread for it. </li>\n<li>No more than 5 quotes per person per monthly thread, please. </li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tz6cJZdhbuhMfSXc4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 9, "extendedScore": null, "score": 1.4e-05, "legacy": true, "legacyId": "1478", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 122, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-06T07:39:04.821Z", "modifiedAt": null, "url": null, "title": "Why Real Men Wear Pink", "slug": "why-real-men-wear-pink", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:33.424Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vXCK3kptLLggEfojX/why-real-men-wear-pink", "pageUrlRelative": "/posts/vXCK3kptLLggEfojX/why-real-men-wear-pink", "linkUrl": "https://www.lesswrong.com/posts/vXCK3kptLLggEfojX/why-real-men-wear-pink", "postedAtFormatted": "Thursday, August 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20Real%20Men%20Wear%20Pink&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20Real%20Men%20Wear%20Pink%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvXCK3kptLLggEfojX%2Fwhy-real-men-wear-pink%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20Real%20Men%20Wear%20Pink%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvXCK3kptLLggEfojX%2Fwhy-real-men-wear-pink", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvXCK3kptLLggEfojX%2Fwhy-real-men-wear-pink", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1029, "htmlBody": "<p><em>\"Fashion is a form of ugliness so intolerable we have to alter it every six months.\"</em></p>\n<p>-- Oscar Wilde</p>\n<p>For the past few decades, I and many other men my age have been locked in a battle with the clothing industry. I want simple, good-looking apparel that covers my nakedness and maybe even makes me look attractive. The clothing industry believes someone my age wants either clothing laced with profanity, clothing that objectifies women, clothing that glorifies alcohol or drug use, or clothing that makes them look like a gangster. And judging by <a href=\" http://www.roadkilltshirts.com/\">the clothing I see people wearing</a>, on the whole they are right.<br /><br />I've been working my way through Steven Pinker's <em>How The Mind Works</em>, and reached the part where he quotes approvingly Quentin Bell's theory of fashion. The theory provides a good explanation for why so much clothing seems so deliberately outrageous.</p>\n<p><a id=\"more\"></a></p>\n<p>Bell starts by offering his own explanation of the \"fashion cycle\". He claims that the goal of fashion is to signal status. So far, so obvious. But low-status people would like to subvert the signal. Therefore, the goal of lower class people is to look like upper class people, and the goal of upper class people is to <em>not</em> look like lower class people.<br /><br />One solution is for the upper class to wear clothing so expensive the lower class could not possibly afford it. This worked for medieval lords and ladies, but nowadays after a while mass production will kick in and K-Mart will have a passable rhinestone based imitation available for $49.95. Once the lower class is wearing the once fashionable item, the upper class wouldn't be caught dead in it. They have to choose a new item of clothing to be the status signal, after a short period of grace the lower class copy that too, and the cycle begins again.<br /><br />For example, maybe in early 2009 a few very high-status people start wearing purple. Everyone who is \"in the know\" enough to understand that they are trend-setters switches to purple. Soon it becomes obvious that lots of \"in the know\" people are wearing purple, and anyone who reads fashion magazines starts stocking up on purple clothing. Soon, only the people too out-of-the-loop to know about purple and the people too poor to immediately replace all their clothes are wearing any other color. In mid-2009, some extremely high-status people now go out on a limb and start wearing green; everyone else is too low-status to be comfortable unilaterally breaking the status quo. Soon everyone switches to green. Wearing purple is a way of broadcasting that you're so dumb or so poor you don't have green clothes yet, which is why it's so mortifying to be caught wearing yesterday's fashion (or so I'm told). When the next cycle comes around, no one will immediately go back to wearing purple, because that would signal that they're unfashionable. But by 2015, that stigma will be gone and purple has a chance to come \"back in style\".<br /><br />Bell describes a clever way the rich can avoid immediately being copied by the middle class. What is the greatest fear of the fashionista? To be confused with a person of a lower class. So the rich wear lower class clothes. The theory is that the middle class is terrified of wearing lower class clothes, but the rich are so obviously not lower class that they can get away with it. Bell wrote before the \"ghetto look\" went into style, but his theory explains quite well why wealthy teenagers and young adults would voluntarily copy the styles of the country's poorest underclass.<br /><br />Bell also explained a second way to signal high-status: conspicuous outrage. Wear a shirt with the word \"FUCK\" on it in big letters (or, if you prefer, <a href=\"http://en.wikipedia.org/wiki/Fcuk\">FCUK</a>). This signals \"I am so high status that I think I can wear the word 'FUCK' in big letters on a t-shirt and get away with it.\" It's a pretty good signal. It signals that you don't give a...well...fcuk what anyone else thinks, and the only people who would be able, either economically or psychologically, to get away with that are the high status<sup>1</sup>.<br /><br />The absolute best real world example, which again I think Bell didn't live to see, is the bright pink shirt for men that says \"REAL MEN WEAR PINK\". The signal is that this guy is so confident in his masculinity that he can go around wearing a pink shirt. It's an odd case because it gets away with explaining exactly what signal it's projecting right on the shirt. And it only works because real men do not wear pink without a disclaimer explaining that they are only wearing pink to signal that they are real men.<br /><br />Pinker notes the similarity to evolutionary strategies that signal fitness by handicapping. A peacock's tail is a way of signalling that its owner is so fit it can afford to have a big maladaptive tail on it and still survive, just as a rich guy in a backwards baseball cap is signalling that its owner is so rich he can afford to copy the lower class and still get invited to parties. The same process produces a body part of astounding beauty in the animal kingdom, and ghetto fashion in human society. I wonder if nature is laughing at us.</p>\n<p><strong>Footnotes:</strong></p>\n<p><strong>1:</strong> Bell (or possibly Pinker, it's not clear) has a similar theory about art. Buying a hip \"modern art\" painting that's just a white canvas with a black line through it is supposed to signal \"I am so rich that I can afford to pay lots of money for a painting even if it is unpopular and hard to appreciate,\" or even \"I am so self-confident in my culturedness that I can endorse this art that is low quality by all previous standards, and people will continue to respect me and my judgments.\" Then the middle class starts buying white canvases with black lines through them, and rich people have to buy sculptures made of human dung just to keep up.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q6P8jLn8hH7kbuXRr": 1, "2EFq8dJbxKNzforjM": 4, "gHCNhqxuJq2bZ2akb": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vXCK3kptLLggEfojX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 85, "baseScore": 91, "extendedScore": null, "score": 0.000143, "legacy": true, "legacyId": "1480", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 91, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 155, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-06T10:33:40.153Z", "modifiedAt": null, "url": null, "title": "The Objective Bayesian Programme", "slug": "the-objective-bayesian-programme", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:18.291Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/L8aMhHYnYRaDZpLmt/the-objective-bayesian-programme", "pageUrlRelative": "/posts/L8aMhHYnYRaDZpLmt/the-objective-bayesian-programme", "linkUrl": "https://www.lesswrong.com/posts/L8aMhHYnYRaDZpLmt/the-objective-bayesian-programme", "postedAtFormatted": "Thursday, August 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Objective%20Bayesian%20Programme&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Objective%20Bayesian%20Programme%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL8aMhHYnYRaDZpLmt%2Fthe-objective-bayesian-programme%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Objective%20Bayesian%20Programme%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL8aMhHYnYRaDZpLmt%2Fthe-objective-bayesian-programme", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL8aMhHYnYRaDZpLmt%2Fthe-objective-bayesian-programme", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1411, "htmlBody": "<p><strong>Followup to:</strong> <a href=\"/lw/147/bayesian_flame/\">Bayesian Flame</a>.</p>\n<p>This post is a chronicle of my attempts to understand Cyan's&nbsp;<a href=\"/lw/13v/are_calibration_and_rational_decisions_mutually/\">#2</a>. (Bayesian Flame was an approximate parse of <a href=\"/lw/13r/are_calibration_and_rational_decisions_mutually/\">#1</a>.) Warning: long, some math, lots of links, probably lots of errors. At the very least I want this to serve as a good reference for further reading.</p>\n<p><strong><em>Introduction</em></strong></p>\n<p>To the mathematical eye, many statistical problems share the following minimal structure:</p>\n<ol>\n<li>A space of parameters. (Imagine a freeform blob without assuming any <a href=\"http://en.wikipedia.org/wiki/Metric_(mathematics)\">metric</a> or <a href=\"http://en.wikipedia.org/wiki/Measure_(mathematics)\">measure</a>.)</li>\n<li>A space of possible outcomes. (Imagine another, similarly unstructured blob.)</li>\n<li>Each point in the parameter space determines a <a href=\"http://en.wikipedia.org/wiki/Probability_space\">probability measure</a> on the outcome space.</li>\n</ol>\n<p>By itself, this kind of input is too sparse to yield solutions to statistical problems. What additional structure on the spaces should we introduce?</p>\n<p><em><strong>The answer that we all know and love</strong></em></p>\n<p>Assuming some \"prior\" probability measure on the parameter space yields a solution that's <a href=\"http://en.wikipedia.org/wiki/Cox's_theorem\">unique, consistent and wonderful</a> in <a href=\"http://www-biba.inrialpes.fr/Jaynes/prob.html\">all sorts of ways</a>.&nbsp;This has led some people to adopt the \"subjectivist\" position saying priors are so basic that they ought not be questioned. One of its most prominent defenders was <a href=\"http://en.wikipedia.org/wiki/Leonard_Jimmie_Savage\">Leonard Jimmie Savage</a> who put forward the following argument:<a id=\"more\"></a></p>\n<blockquote>\n<p>Suppose, for example, that the person is offered an even-money bet for five dollars - or, to be ultra-rigorous, for five utiles - that internal combustion engines in American automobiles will be obsolete by 1970. If there is any event to which an objectivist would refuse to attach probability, that corresponding to the obsolescence in question is one... Yet, I think I may say without presumption that you would regard the bet against obsolescence as a very sound investment.</p>\n</blockquote>\n<p>This is a fine argument for using priors when you're betting money, but there's a snag: however much you are willing to bet, this doesn't give you grounds to <em>publish papers</em> about the future that you inferred from your intuitive prior! Any apriori information used in science should be&nbsp;<em>justified</em>&nbsp;for <em>scientific objectivity</em>.</p>\n<p>(At this point Eliezer raises the&nbsp;<a href=\"/lw/147/bayesian_flame/zem\">suggestion</a> that scientists ought to communicate with <a href=\"http://www.overcomingbias.com/2009/02/share-likelihood-ratios-not-posterior-beliefs.html\">likelihood ratios only</a>. That might be a brave new world to live in; too bad we'll have to stop teaching kids that <em>g</em> approximately equals 9.8 m/s<sup>2</sup>&nbsp;and give them likelihood profiles instead.)</p>\n<p>Rather than dive deeper into the fascinating topic of \"<a href=\"http://en.wikipedia.org/wiki/Jeffreys_prior\">uninformative</a> <a href=\"http://en.wikipedia.org/wiki/Principle_of_maximum_entropy\">priors</a>\", let's go back to the surface.&nbsp;Take a closer look at the basic formulation above to see what&nbsp;<em style=\"font-style: italic;\">other</em>&nbsp;structures we can introduce instead of priors to get interesting results.</p>\n<p><strong><em>The minimax approach</em></strong></p>\n<p>In the mid-20th century a statistician named <a href=\"http://en.wikipedia.org/wiki/Abraham_Wald\">Abraham Wald</a>&nbsp;made a valiant effort to step outside the problem. His <a href=\"http://en.wikipedia.org/wiki/Decision_theory\">decision theory</a>&nbsp;idea&nbsp;encompasses both frequentist and Bayesian inference. Roughly, it goes like this: we no longer know our prior probabilities, but we do know our <em>utilities</em>. More concretely, we compute a <em>decision</em> from the observed dataset, and later suffer a <em>loss</em>&nbsp;that depends on our decision and the actual true parameter value. Substituting different \"spaces of decisions\" and \"loss functions\", we get a wide range of situations to study.</p>\n<p>But wait! Doesn't the \"optimal\" decision depend on the prior distribution&nbsp;of parameters as well?</p>\n<p>Wald's crucial insight was that... no, not necessarily.</p>\n<p>If\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nwe don't know the prior and are trying to be \"scientifically objective\", it makes sense to treat the problem\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nof statistical inference\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nas a <em>game</em>. The\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nstatistician chooses a decision rule, Nature chooses a true parameter value, randomness determines the payoff. Since the game is <a href=\"http://en.wikipedia.org/wiki/Zero-sum\">zero-sum</a>,\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nwe can\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nreasonably expect it to have a <a href=\"http://en.wikipedia.org/wiki/Minimax\">minimax value</a>: there's a decision rule that minimizes the maximum loss the statistician can suffer, whatever Nature\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nmay choose.</p>\n<p>Now, as Ken Binmore accurately noted, in real life&nbsp;you don't minimax unless \"your relationship with the universe has reached such a low ebb that you keep your pants up with both belt and suspenders\", so the minimax principle gives off a whiff of the paranoia that we've come to associate with frequentism. Haha, gotcha! Wald's results apply to Bayesianism just as well. His \"complete class theorem\" proves that Bayesian-rational strategies with well-defined priors constitute precisely the class of <em>non-dominated strategies</em> in the game described. (If you squint the right way, this last sentence compresses the whole philosophical justification of Bayesianism.)</p>\n<p>The game-theoretic approach gives our Bayesian friends even more than that. The statistical game's <a href=\"http://en.wikipedia.org/wiki/Minimax_estimator\">minimax decision rules</a>&nbsp;often correspond to Bayes strategies with a certain uninformative prior, called the \"least favorable prior\" for that risk function. This gives you a frequentist-valid procedure that also happens to be Bayesian, which means immunity to <a href=\"http://en.wikipedia.org/wiki/Dutch_book\">Dutch books</a>, <a href=\"/lw/13v/are_calibration_and_rational_decisions_mutually/zbl\">negative masses</a> and similar criticisms. In a particularly fascinating convergence, the well-known \"reference prior\" (<a href=\"http://en.wikipedia.org/wiki/Jeffreys_prior\">the Jeffreys prior</a>&nbsp;properly generalized to N dimensions) turns out to be asymptotically least favorable when optimizing the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Mutual_information\">Shannon mutual information</a> between the parameter and the sample.</p>\n<p>At this point the Bayesians in the audience should be rubbing their hands. I told ya it would be fun! Our frequentist friends on the other hand have dozed off, so let's pull another stunt to wake them up.</p>\n<p><strong><em>Confidence coverage demystified</em></strong></p>\n<p>Informally, we want to say things about the world like \"I'm 90% sure that this physical constant lies within those bounds\" and be actually right 90% of the time when we say such things.</p>\n<p>...Semi-formally, we want to a procedure to calculate from each sample a \"<a href=\"http://en.wikipedia.org/wiki/Confidence_region\">confidence subset</a>\" of the parameter space such that such subsets cover include the true parameter values with probability greater or equal to 90%, while the sets themselves are as small as possible.</p>\n<p style=\"padding-left: 30px;\"><small>(NB: this is <em>not</em> equivalent to deriving a \"correct\" posterior distribution on the parameter space. Not every method of choosing small subsets with given posterior masses will give you <em>uniformly</em> correct confidence coverage, and each such method corresponds to <em>many</em> different posterior distributions in the N-dimensional case.)</small></p>\n<p>...Formally, we introduce a new structure on the parameter space - a \"not-quite-measure\" to determine the size of confidence sets&nbsp;- and then, upon receiving a sample, determine from it a 90% confidence set with the smallest possible \"not-quite-measure\".</p>\n<p style=\"padding-left: 30px;\"><small>(NB: I'm calling it \"not-quite-measure\" because of a subtlety in the N-dimensional case. If we're estimating just one parameter out of several, the \"measure\" corresponds to <em>span</em> in that coordinate and thus is not additive under set union, hence \"not-quite\".)</small></p>\n<p>Except this doesn't work. There might be two procedures to compute confidence sets, the first of which is sometimes better and sometimes worse than the second. We have no comparison function to determine the winner, and in reality the \"uniformly most accurate\" procedure doesn't always exist.</p>\n<p>But if we replace the \"size\" of the confidence set with its <em>expected size </em>under each single parameter value, this gives us just enough information to apply the game-theoretic minimax approach.&nbsp;Solving the game thus gives us&nbsp;\"minimax expected size\" confidence sets, or MES, that people are <a href=\"http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.bj/1126126761\">actually using</a>. Which isn't saying much, but still.</p>\n<p><strong><em>More on subjectivity</em></strong></p>\n<p>The minimax principle sounds nice, but the construction of the least favorable prior distribution for any given experiment and risk function has a problem: it typically depends on the <em>whole</em> sample space and thus on the experiment's <em>stopping rule</em>. When do we stop gathering data? What subsets of observed samples do we thus rule out? In the general case the least favorable prior depends on the number of samples we intend to draw!&nbsp;This blatantly violates the <a href=\"http://en.wikipedia.org/wiki/Likelihood_principle\">likelihood principle</a> that Eliezer so eloquently <a href=\"/lw/mt/beautiful_probability/\">defended</a>.</p>\n<p><em>But,</em> ordinary probability theory tells us unambiguously that 90% of your conclusions will be true whatever stopping rules you choose for each of them, as long you choose before observing any data from the experiments. (Otherwise all bets are off, like if you'd decided to pick your Bayesian prior based on the data.)&nbsp;<em>But,</em> the conclusions themselves will be different from rule to rule. <em>But,</em> you cannot deliberately engineer a situation where the minimax of one stopping rule reliably makes you more wrong than another one...</p>\n<p>Does this look more like an eternal mathematical law or an ad hoc tool? To me it looks like a mystery. Like frequentists are trying to solve a problem that Bayesians don't even attempt to solve. The answer is somewhere out there; we can guess that something like today's Bayesianism will be a big part of it, but not the only part.</p>\n<p><strong><em>Conclusion</em></strong></p>\n<p>When some field is afflicted with deep and persistent philosophical conflicts, this isn't necessarily a sign that one of the sides is right and the other is just being silly. It might be a sign that some crucial unifying insight is waiting several steps ahead. Minimaxing doesn't look to me like the beginning and end of \"objective\" statistics, but the right answer that we don't know yet has got to be&nbsp;<a href=\"http://www.overcomingbias.com/2008/05/the-born-prob-1.html\">at least this normal</a>.</p>\n<p><strong>Further reading</strong>: James Berger,&nbsp;<a href=\"http://ba.stat.cmu.edu/journal/2006/vol01/issue03/berger.pdf\">The Case for Objective Bayesian Analysis</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "L8aMhHYnYRaDZpLmt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 19, "extendedScore": null, "score": 5.130269027731726e-07, "legacy": true, "legacyId": "1476", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Followup to:</strong> <a href=\"/lw/147/bayesian_flame/\">Bayesian Flame</a>.</p>\n<p>This post is a chronicle of my attempts to understand Cyan's&nbsp;<a href=\"/lw/13v/are_calibration_and_rational_decisions_mutually/\">#2</a>. (Bayesian Flame was an approximate parse of <a href=\"/lw/13r/are_calibration_and_rational_decisions_mutually/\">#1</a>.) Warning: long, some math, lots of links, probably lots of errors. At the very least I want this to serve as a good reference for further reading.</p>\n<p><strong id=\"Introduction\"><em>Introduction</em></strong></p>\n<p>To the mathematical eye, many statistical problems share the following minimal structure:</p>\n<ol>\n<li>A space of parameters. (Imagine a freeform blob without assuming any <a href=\"http://en.wikipedia.org/wiki/Metric_(mathematics)\">metric</a> or <a href=\"http://en.wikipedia.org/wiki/Measure_(mathematics)\">measure</a>.)</li>\n<li>A space of possible outcomes. (Imagine another, similarly unstructured blob.)</li>\n<li>Each point in the parameter space determines a <a href=\"http://en.wikipedia.org/wiki/Probability_space\">probability measure</a> on the outcome space.</li>\n</ol>\n<p>By itself, this kind of input is too sparse to yield solutions to statistical problems. What additional structure on the spaces should we introduce?</p>\n<p><em><strong>The answer that we all know and love</strong></em></p>\n<p>Assuming some \"prior\" probability measure on the parameter space yields a solution that's <a href=\"http://en.wikipedia.org/wiki/Cox's_theorem\">unique, consistent and wonderful</a> in <a href=\"http://www-biba.inrialpes.fr/Jaynes/prob.html\">all sorts of ways</a>.&nbsp;This has led some people to adopt the \"subjectivist\" position saying priors are so basic that they ought not be questioned. One of its most prominent defenders was <a href=\"http://en.wikipedia.org/wiki/Leonard_Jimmie_Savage\">Leonard Jimmie Savage</a> who put forward the following argument:<a id=\"more\"></a></p>\n<blockquote>\n<p>Suppose, for example, that the person is offered an even-money bet for five dollars - or, to be ultra-rigorous, for five utiles - that internal combustion engines in American automobiles will be obsolete by 1970. If there is any event to which an objectivist would refuse to attach probability, that corresponding to the obsolescence in question is one... Yet, I think I may say without presumption that you would regard the bet against obsolescence as a very sound investment.</p>\n</blockquote>\n<p>This is a fine argument for using priors when you're betting money, but there's a snag: however much you are willing to bet, this doesn't give you grounds to <em>publish papers</em> about the future that you inferred from your intuitive prior! Any apriori information used in science should be&nbsp;<em>justified</em>&nbsp;for <em>scientific objectivity</em>.</p>\n<p>(At this point Eliezer raises the&nbsp;<a href=\"/lw/147/bayesian_flame/zem\">suggestion</a> that scientists ought to communicate with <a href=\"http://www.overcomingbias.com/2009/02/share-likelihood-ratios-not-posterior-beliefs.html\">likelihood ratios only</a>. That might be a brave new world to live in; too bad we'll have to stop teaching kids that <em>g</em> approximately equals 9.8 m/s<sup>2</sup>&nbsp;and give them likelihood profiles instead.)</p>\n<p>Rather than dive deeper into the fascinating topic of \"<a href=\"http://en.wikipedia.org/wiki/Jeffreys_prior\">uninformative</a> <a href=\"http://en.wikipedia.org/wiki/Principle_of_maximum_entropy\">priors</a>\", let's go back to the surface.&nbsp;Take a closer look at the basic formulation above to see what&nbsp;<em style=\"font-style: italic;\">other</em>&nbsp;structures we can introduce instead of priors to get interesting results.</p>\n<p><strong id=\"The_minimax_approach\"><em>The minimax approach</em></strong></p>\n<p>In the mid-20th century a statistician named <a href=\"http://en.wikipedia.org/wiki/Abraham_Wald\">Abraham Wald</a>&nbsp;made a valiant effort to step outside the problem. His <a href=\"http://en.wikipedia.org/wiki/Decision_theory\">decision theory</a>&nbsp;idea&nbsp;encompasses both frequentist and Bayesian inference. Roughly, it goes like this: we no longer know our prior probabilities, but we do know our <em>utilities</em>. More concretely, we compute a <em>decision</em> from the observed dataset, and later suffer a <em>loss</em>&nbsp;that depends on our decision and the actual true parameter value. Substituting different \"spaces of decisions\" and \"loss functions\", we get a wide range of situations to study.</p>\n<p>But wait! Doesn't the \"optimal\" decision depend on the prior distribution&nbsp;of parameters as well?</p>\n<p>Wald's crucial insight was that... no, not necessarily.</p>\n<p>If\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nwe don't know the prior and are trying to be \"scientifically objective\", it makes sense to treat the problem\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nof statistical inference\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nas a <em>game</em>. The\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nstatistician chooses a decision rule, Nature chooses a true parameter value, randomness determines the payoff. Since the game is <a href=\"http://en.wikipedia.org/wiki/Zero-sum\">zero-sum</a>,\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nwe can\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nreasonably expect it to have a <a href=\"http://en.wikipedia.org/wiki/Minimax\">minimax value</a>: there's a decision rule that minimizes the maximum loss the statistician can suffer, whatever Nature\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nmay choose.</p>\n<p>Now, as Ken Binmore accurately noted, in real life&nbsp;you don't minimax unless \"your relationship with the universe has reached such a low ebb that you keep your pants up with both belt and suspenders\", so the minimax principle gives off a whiff of the paranoia that we've come to associate with frequentism. Haha, gotcha! Wald's results apply to Bayesianism just as well. His \"complete class theorem\" proves that Bayesian-rational strategies with well-defined priors constitute precisely the class of <em>non-dominated strategies</em> in the game described. (If you squint the right way, this last sentence compresses the whole philosophical justification of Bayesianism.)</p>\n<p>The game-theoretic approach gives our Bayesian friends even more than that. The statistical game's <a href=\"http://en.wikipedia.org/wiki/Minimax_estimator\">minimax decision rules</a>&nbsp;often correspond to Bayes strategies with a certain uninformative prior, called the \"least favorable prior\" for that risk function. This gives you a frequentist-valid procedure that also happens to be Bayesian, which means immunity to <a href=\"http://en.wikipedia.org/wiki/Dutch_book\">Dutch books</a>, <a href=\"/lw/13v/are_calibration_and_rational_decisions_mutually/zbl\">negative masses</a> and similar criticisms. In a particularly fascinating convergence, the well-known \"reference prior\" (<a href=\"http://en.wikipedia.org/wiki/Jeffreys_prior\">the Jeffreys prior</a>&nbsp;properly generalized to N dimensions) turns out to be asymptotically least favorable when optimizing the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Mutual_information\">Shannon mutual information</a> between the parameter and the sample.</p>\n<p>At this point the Bayesians in the audience should be rubbing their hands. I told ya it would be fun! Our frequentist friends on the other hand have dozed off, so let's pull another stunt to wake them up.</p>\n<p><strong id=\"Confidence_coverage_demystified\"><em>Confidence coverage demystified</em></strong></p>\n<p>Informally, we want to say things about the world like \"I'm 90% sure that this physical constant lies within those bounds\" and be actually right 90% of the time when we say such things.</p>\n<p>...Semi-formally, we want to a procedure to calculate from each sample a \"<a href=\"http://en.wikipedia.org/wiki/Confidence_region\">confidence subset</a>\" of the parameter space such that such subsets cover include the true parameter values with probability greater or equal to 90%, while the sets themselves are as small as possible.</p>\n<p style=\"padding-left: 30px;\"><small>(NB: this is <em>not</em> equivalent to deriving a \"correct\" posterior distribution on the parameter space. Not every method of choosing small subsets with given posterior masses will give you <em>uniformly</em> correct confidence coverage, and each such method corresponds to <em>many</em> different posterior distributions in the N-dimensional case.)</small></p>\n<p>...Formally, we introduce a new structure on the parameter space - a \"not-quite-measure\" to determine the size of confidence sets&nbsp;- and then, upon receiving a sample, determine from it a 90% confidence set with the smallest possible \"not-quite-measure\".</p>\n<p style=\"padding-left: 30px;\"><small>(NB: I'm calling it \"not-quite-measure\" because of a subtlety in the N-dimensional case. If we're estimating just one parameter out of several, the \"measure\" corresponds to <em>span</em> in that coordinate and thus is not additive under set union, hence \"not-quite\".)</small></p>\n<p>Except this doesn't work. There might be two procedures to compute confidence sets, the first of which is sometimes better and sometimes worse than the second. We have no comparison function to determine the winner, and in reality the \"uniformly most accurate\" procedure doesn't always exist.</p>\n<p>But if we replace the \"size\" of the confidence set with its <em>expected size </em>under each single parameter value, this gives us just enough information to apply the game-theoretic minimax approach.&nbsp;Solving the game thus gives us&nbsp;\"minimax expected size\" confidence sets, or MES, that people are <a href=\"http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.bj/1126126761\">actually using</a>. Which isn't saying much, but still.</p>\n<p><strong id=\"More_on_subjectivity\"><em>More on subjectivity</em></strong></p>\n<p>The minimax principle sounds nice, but the construction of the least favorable prior distribution for any given experiment and risk function has a problem: it typically depends on the <em>whole</em> sample space and thus on the experiment's <em>stopping rule</em>. When do we stop gathering data? What subsets of observed samples do we thus rule out? In the general case the least favorable prior depends on the number of samples we intend to draw!&nbsp;This blatantly violates the <a href=\"http://en.wikipedia.org/wiki/Likelihood_principle\">likelihood principle</a> that Eliezer so eloquently <a href=\"/lw/mt/beautiful_probability/\">defended</a>.</p>\n<p><em>But,</em> ordinary probability theory tells us unambiguously that 90% of your conclusions will be true whatever stopping rules you choose for each of them, as long you choose before observing any data from the experiments. (Otherwise all bets are off, like if you'd decided to pick your Bayesian prior based on the data.)&nbsp;<em>But,</em> the conclusions themselves will be different from rule to rule. <em>But,</em> you cannot deliberately engineer a situation where the minimax of one stopping rule reliably makes you more wrong than another one...</p>\n<p>Does this look more like an eternal mathematical law or an ad hoc tool? To me it looks like a mystery. Like frequentists are trying to solve a problem that Bayesians don't even attempt to solve. The answer is somewhere out there; we can guess that something like today's Bayesianism will be a big part of it, but not the only part.</p>\n<p><strong id=\"Conclusion\"><em>Conclusion</em></strong></p>\n<p>When some field is afflicted with deep and persistent philosophical conflicts, this isn't necessarily a sign that one of the sides is right and the other is just being silly. It might be a sign that some crucial unifying insight is waiting several steps ahead. Minimaxing doesn't look to me like the beginning and end of \"objective\" statistics, but the right answer that we don't know yet has got to be&nbsp;<a href=\"http://www.overcomingbias.com/2008/05/the-born-prob-1.html\">at least this normal</a>.</p>\n<p><strong>Further reading</strong>: James Berger,&nbsp;<a href=\"http://ba.stat.cmu.edu/journal/2006/vol01/issue03/berger.pdf\">The Case for Objective Bayesian Analysis</a>.</p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "The minimax approach", "anchor": "The_minimax_approach", "level": 1}, {"title": "Confidence coverage demystified", "anchor": "Confidence_coverage_demystified", "level": 1}, {"title": "More on subjectivity", "anchor": "More_on_subjectivity", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WhWTwQJaiEFxvXB96", "BaffPrQtKYSABigNb", "qDucvMYty5gdumHDB", "bkSkRwo9SRYxJMiSY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-06T13:35:40.985Z", "modifiedAt": null, "url": null, "title": "LW/OB Rationality Quotes - August 2009", "slug": "lw-ob-rationality-quotes-august-2009", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:20.442Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Rune", "createdAt": "2009-03-21T18:16:37.470Z", "isAdmin": false, "displayName": "Rune"}, "userId": "fvDuey7i36uP3fg5u", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bTdDKX4sK35Q9t4v2/lw-ob-rationality-quotes-august-2009", "pageUrlRelative": "/posts/bTdDKX4sK35Q9t4v2/lw-ob-rationality-quotes-august-2009", "linkUrl": "https://www.lesswrong.com/posts/bTdDKX4sK35Q9t4v2/lw-ob-rationality-quotes-august-2009", "postedAtFormatted": "Thursday, August 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LW%2FOB%20Rationality%20Quotes%20-%20August%202009&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALW%2FOB%20Rationality%20Quotes%20-%20August%202009%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbTdDKX4sK35Q9t4v2%2Flw-ob-rationality-quotes-august-2009%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LW%2FOB%20Rationality%20Quotes%20-%20August%202009%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbTdDKX4sK35Q9t4v2%2Flw-ob-rationality-quotes-august-2009", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbTdDKX4sK35Q9t4v2%2Flw-ob-rationality-quotes-august-2009", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 112, "htmlBody": "<p>I always see that the monthly Rationality Quotes thread has this line: \"Do not quote comments/posts on LW/OB - if we do this, there should be a separate thread for it.\" This is the thread for those quotes.</p>\n<div>\n<p>This is a (possibly) monthly thread for posting any interesting rationality-related quotes you've seen on LW/OB.</p>\n<ul>\n<li>Please post all quotes separately (so that they can be voted up/down separately) unless they are strongly related/ordered. </li>\n<li>Do not quote yourself. </li>\n<li>Do not post quotes that are <strong>NOT</strong> comments/posts on LW/OB - there is a separate thread for this. </li>\n<li>No more than 5 quotes per person per monthly thread, please. </li>\n</ul>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bTdDKX4sK35Q9t4v2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 7, "extendedScore": null, "score": 5.130562593088035e-07, "legacy": true, "legacyId": "1479", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-06T16:17:43.983Z", "modifiedAt": null, "url": null, "title": "Exterminating life is rational", "slug": "exterminating-life-is-rational", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:32.967Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LkCeA4wu8iLmetb28/exterminating-life-is-rational", "pageUrlRelative": "/posts/LkCeA4wu8iLmetb28/exterminating-life-is-rational", "linkUrl": "https://www.lesswrong.com/posts/LkCeA4wu8iLmetb28/exterminating-life-is-rational", "postedAtFormatted": "Thursday, August 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Exterminating%20life%20is%20rational&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExterminating%20life%20is%20rational%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLkCeA4wu8iLmetb28%2Fexterminating-life-is-rational%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Exterminating%20life%20is%20rational%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLkCeA4wu8iLmetb28%2Fexterminating-life-is-rational", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLkCeA4wu8iLmetb28%2Fexterminating-life-is-rational", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2989, "htmlBody": "<p>Followup to <a title=\"This Failing Earth\" href=\"/lw/gg/this_failing_earth/\">This Failing Earth</a>, <a href=\"/lw/12h/our_society_lacks_good_selfpreservation_mechanisms\">Our society lacks good self-preservation mechanisms</a>, <a href=\"/lw/12j/debate_is_short_term_planning_in_humans_due_to_a/\">Is short term planning in humans due to a short life or due to bias?</a></p>\n<p>I don't mean that deciding to exterminate life is rational.&nbsp; But if, as a society of rational agents, we each maximize our expected utility, this may inevitably lead to our exterminating life, or at least intelligent life.</p>\n<p>Ed Regis reports on p 216 of &ldquo;Great Mambo Chicken and the TransHuman Condition,&rdquo; (Penguin Books, London, 1992):</p>\n<blockquote>\n<p>Edward Teller had thought about it, the chance that the atomic explosion would light up the surrounding air and that this conflagration would then propagate itself around the world. Some of the bomb makers had even calculated the numerical odds of this actually happening, coming up with the figure of three chances in a million they&rsquo;d incinerate the Earth. Nevertheless, they went ahead and exploded the bomb.</p>\n</blockquote>\n<p>Was this a bad decision?&nbsp; Well, consider the expected value to the people involved.&nbsp; Without the bomb, there was a much, much greater than 3/1,000,000 chance that either a) they would be killed in the war, or b) they would be ruled by Nazis or the Japanese.&nbsp; The loss to them if they ignited the atmosphere would be another 30 or so years of life.&nbsp; The loss to them if they lost the war and/or were killed by their enemies would also be another 30 or so years of life.&nbsp; The loss in being conquered would also be large.&nbsp; Easy decision, really.</p>\n<p>Suppose that, once a century, some party in a conflict chooses to use some technique to help win the conflict that has a p=3/1,000,000 chance of eliminating life as we know it.&nbsp; Then our expected survival time is 100 times the sum from n=1 to infinity of np(1-p)<sup>n-1</sup>.&nbsp; If I've done my math right, that's &asymp; 33,777,000 years.</p>\n<p><a id=\"more\"></a>This supposition seems reasonable to me.&nbsp; There is a balance between offensive and defensive capability that shifts as technology develops.&nbsp; If technology keeps changing, it is inevitable that, much of the time, a technology will provide the ability to destroy all life before the counter-technology to defend against it has been developed.&nbsp; In the near future, biological weapons will be more able to wipe out life than we are able to defend against them.&nbsp; We may then develop the ability to defend against biological attacks; we may then be safe until the next dangerous technology.</p>\n<p>If you believe in accelerating change, then the number of important events in a given time interval increases exponentially, or, equivalently, the time intervals that should be considered equivalent opportunities for important events shorten exponentially.&nbsp; The 34M years remaining to life is then in subjective time, and must be mapped into realtime.&nbsp; If we suppose the subjective/real time ratio doubles every 100 years, this gives life an expected survival time of 2000 more realtime years.&nbsp; If we instead use Ray Kurzweil's figure of about 2 years, this gives life about 40 remaining realtime years.&nbsp; (I don't recommend Ray's figure.&nbsp; I'm just giving it for those who do.)</p>\n<p>Please understand that I am not yet another \"prophet\" bemoaning the foolishness of humanity.&nbsp; Just the opposite:&nbsp; I'm saying this is not something we will outgrow.&nbsp; If anything, becoming more rational only makes our doom more certain.&nbsp; For the agents who must actually make these decisions, it would be irrational not to take these risks.&nbsp; The fact that this level of risk-tolerance will inevitably lead to the snuffing out of all life does not make the expected utility of these risks negative for the agents involved.</p>\n<p>I can think of only a few ways that rationalilty can not inevitably exterminate all life in the cosmologically (even geologically) near future:</p>\n<ul>\n<li>\n<p>We can outrun the danger:&nbsp; We can spread life to other planets, and to other solar systems, and to other galaxies, faster than we can spread destruction.</p>\n</li>\n<li>\n<p>Technology will not continue to develop, but will stabilize in a state in which all defensive technologies provide absolute, 100%, fail-safe protection against all offensive technologies.</p>\n</li>\n<li>\n<p>People will stop having conflicts.</p>\n</li>\n<li>Rational agents incorporate the benefits to others into their utility functions.</li>\n<li>\n<p>Rational agents with long lifespans will protect the future for themselves.</p>\n</li>\n<li>\n<p>Utility functions will change so that it is no longer rational for decision-makers to take tiny chances of destroying life for any amount of utility gains.</p>\n</li>\n<li>Independent agents will cease to exist, or to be free (the Singleton scenario).</li>\n</ul>\n<p>Let's look at these one by one:</p>\n<h2>We can outrun the danger.<br /></h2>\n<p>We will colonize other planets; but we may also&nbsp; figure out how to make the Sun go nova on demand.&nbsp; We will colonize other star systems; but we may also figure out how to liberate much of the energy in the black hole at the center of our galaxy in a giant explosion that will move outward at near the speed of light.</p>\n<p>One problem with this idea is that apocalypses are correlated; one may trigger another.&nbsp; A disease may spread to another planet.&nbsp; The choice to use a planet-busting bomb on one planet may lead to its retaliatory use on another planet.&nbsp; It's not clear whether spreading out <em>and</em> increasing in population actually makes life more safe.&nbsp; If you think in the other direction, a smaller human population (say ten million) stuck here on Earth would be safer from human-instigated disasters.</p>\n<p>But neither of those are my final objection.&nbsp; More important is that our compression of subjective time can be exponential, while our ability to escape from ever-broader swaths of destruction is limited by lightspeed.</p>\n<h2>Technology will stabilize in a safe state.<br /></h2>\n<p>Maybe technology will stabilize, and we'll run out of things to discover.&nbsp; If that were to happen, I would expect that conflicts would increase, because people would get bored.&nbsp; As I mentioned in another thread, one good explanation for the incessant and counterproductive wars in the middle ages - a reason some of the actors themselves gave in their writings - is that the nobility were bored.&nbsp; They did not have the concept of progress; they were just looking for something to give them purpose while waiting for Jesus to return.</p>\n<p>But that's not my final rejection.&nbsp; The big problem is that by \"safe\", I mean really, really safe.&nbsp; We're talking about bringing existential threats to chances less than 1 in a million per century.&nbsp; I don't know of any defensive technology that can guarantee a less than 1 in a million failure rate.</p>\n<h2>People will stop having conflicts.</h2>\n<p>That's a nice thought.&nbsp; A lot of people - maybe the majority of people - believe that we are inevitably progressing along a path to less violence and greater peace.</p>\n<p>They thought that just before World War I.&nbsp; But that's not my final rejection.&nbsp; Evolutionary arguments are a more powerful reason to believe that people will continue to have conflicts.&nbsp; Those that avoid conflict will be out-competed by those that do not.</p>\n<p>But that's not my final rejection either.&nbsp; The bigger problem is that this isn't something that arises only in conflicts.&nbsp; All we need are desires.&nbsp; We're willing to tolerate risk to increase our utility.&nbsp; For instance, we're willing to take some unknown, but clearly greater than one in a million chance, of the collapse of much of civilization due to climate warming.&nbsp; In return for this risk, we can enjoy a better lifestyle now.</p>\n<p>Also, we haven't burned all physics textbooks along with all physicists.&nbsp; Yet I'm confident there is at least a one in a million chance that, in the next 100 years, some physicist will figure out a way to reduce the earth to powder, if not to crack spacetime itself and undo the entire universe.&nbsp; (In fact, I'd guess the chance is nearer to 1 in 10.)<sup>1</sup>&nbsp; We take this existential risk in return for a continued flow of benefits such as better graphics in Halo 3 and smaller iPods.&nbsp; And it's reasonable for us to do this, because an improvement in utility of 1% over an agent's lifespan is, to that agent, exactly balanced by a 1% chance of destroying the Universe.</p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Safety_of_particle_collisions_at_the_Large_Hadron_Collider\">Wikipedia entry</a> on Large Hadcon Collider risk says, \"In the book <em><a title=\"Our Final Hour\" href=\"http://en.wikipedia.org/wiki/Our_Final_Hour\">Our Final Century: Will the Human Race Survive the Twenty-first Century?</a></em>, English cosmologist and astrophysicist <a class=\"mw-redirect\" title=\"Martin Rees\" href=\"http://en.wikipedia.org/wiki/Martin_Rees\">Martin Rees</a> calculated an upper limit of 1 in 50 million for the probability that the Large Hadron Collider will produce a global catastrophe or black hole.\"&nbsp; The more authoritative \"Review of the Safety of LHC Collisions\" by the LHC Safety Assessment Group concluded that there was at most a 1 in 10<sup>31 </sup>chance of destroying the Earth.</p>\n<p>The LHC conclusions are criminally low.&nbsp; Their evidence was this: \"Nature has already conducted the LHC experimental programme about one billion times via the collisions of cosmic rays with the Sun - and the Sun still exists.\"&nbsp; There followed a couple of sentences of handwaving to the effect that if any other stars had turned to black holes due to collisions with cosmic rays, we would know it - apparently due to our flawless ability to detect black holes and ascertain what caused them - and therefore we can multiply this figure by the number of stars in the universe.</p>\n<p>I believe there is much more than a one-in-a-billion chance that our understanding in one of the steps used in arriving at these figures is incorrect.&nbsp; Based on my experience with peer-reviewed papers, there's at least a one-in-ten chance that there's a basic arithmetic error in their paper that no one has noticed yet.&nbsp; I'm thinking more like one-in-a-million, once you correct for the anthropic principle and for the chance that there is a mistake in the argument.&nbsp; (That's based on a belief that priors for anything likely enough that smart people even thought of the possibility should be larger than one in a billion, unless they were specifically trying to think of examples of low-probability possibilities such as all of the air molecules in the room moving to one side.)</p>\n<p>The Trinity test was done for the sake of winning World War II.&nbsp; But the LHC was turned on for... well, no practical advantage that I've heard of yet.&nbsp; It seems that we are willing to tolerate one-in-a-million chances of destroying the Earth for very little benefit.&nbsp; And this is&nbsp; rational, since the LHC will probably improve our lives by more than one part in a million.</p>\n<h2>Rational agents incorporate the benefits to others into their utility functions.</h2>\n<p>\"But,\" you say, \"I wouldn't risk a 1% chance of destroying the universe for a 1% increase in my utility!\"</p>\n<p>Well... yes, you would, if you're a rational expectation maximizer.&nbsp; It's possible that you would take a much higher risk, if your utility is at risk of going negative; it's not possible that you would not accept a .999% risk, unless you are not maximizing expected value, or you assign the null state after universe-destruction negative utility.&nbsp; (This seems difficult, but is worth exploring.)&nbsp; If you still think that you wouldn't, it's probably because you're thinking a 1% increase in your utility means something like a 1% increase in the pleasure you experience.&nbsp; It doesn't.&nbsp; It's a 1% increase in your <em>utility.</em>&nbsp; If you factor the rest of your universe into your utility function, then it's already in there.</p>\n<p>The US national debt should be enough to convince you that people act in their self-interest.&nbsp; Even the most moral people - in fact, <em>especially </em>the \"most moral\" people - do not incorporate the benefits to others, especially future others, into their utility functions.&nbsp; If we did that, we would engage in massive eugenics programs.&nbsp; But eugenics is considered the greatest immorality.</p>\n<p>But maybe they're just not as rational as you.&nbsp; Maybe you really are a rational saint who considers your own pleasure no more important than the pleasure of everyone else on Earth.&nbsp; Maybe you have never, ever bought anything for yourself that did not bring you as much benefit as the same amount of money would if spent to repair cleft palates or distribute vaccines or mosquito nets or water pumps in Africa.&nbsp; Maybe it's really true that, if you met the girl of your dreams and she loved you, and you won the lottery, put out an album that went platinum, and got published in Science, all in the same week, it would make an imperceptible change in your utility versus if everyone you knew died, Bernie Madoff spent all your money, and you were unfairly convicted of murder and diagnosed with cancer.</p>\n<p>It doesn't matter.&nbsp; Because you would be adding up everyone else's utility, and everyone else is getting that 1% extra utility from the better graphics cards and the smaller iPods.</p>\n<p>But that will stop you from risking atmospheric ignition to defeat the Nazis, right?&nbsp; Because you'll incorporate them into your utility function?&nbsp; Well, that is a subset of the claim \"People will stop having conflicts.\"&nbsp; See above.</p>\n<p>And even if you somehow worked around all these arguments, evolution, again, thwarts you.<sup>2</sup>&nbsp; Even if you don't agree that rational agents are selfish, your unselfish agents will be out-competed by selfish agents.&nbsp; The claim that rational agents are not selfish implies that rational agents are unfit.</p>\n<h2>Rational agents with long lifespans will protect the future for themselves.<br /></h2>\n<p>The most familiar idea here is that, if people expect to live for millions of years, they will be \"wiser\" and take fewer risks with that time.&nbsp; But the flip side is that they also have more time to lose.&nbsp; If they're deciding whether to risk igniting the atmosphere in order to lower the risk of being killed by Nazis, lifespan cancels out of the equation.</p>\n<p>Also, if they live a million times longer than us, they're going to get a million times the benefit of those nicer iPods.&nbsp; They may be less willing to take an existential risk for something that will benefit them only temporarily.&nbsp; But benefits have a way of increasing, not decreasing, over time.&nbsp; The discovery of the law of gravity and of the invisible hand benefit us in the 21st century more than they did the people of the 17th century.</p>\n<p>But that's not my final rejection.&nbsp; More important is time-discounting.&nbsp; Agents will time-discount, probably exponentially, due to uncertainty.&nbsp; If you considered benefits to the future without exponential time-discounting, the benefits to others and to future generations would outweigh any benefits to yourself so much that in many cases you wouldn't even waste time trying to figure out what you wanted.&nbsp; And, since future generations will be able to get more utility out of the same resources, we'd all be obliged to kill ourselves, unless we reasonably think that we are contributing to the development of that capability.</p>\n<p>Time discounting is always (so far) exponential, because non-asymptotic functions don't make sense.&nbsp; I supposed you could use a trigonometric function instead for time discounting, but I don't think it would help.</p>\n<p>Could a continued exponential population explosion outweigh exponential time-discounting?&nbsp; Well, you can't have a continued exponential population explosion, because of the speed of light and the Planck constant.&nbsp; (I leave the details as an exercise to the reader.)</p>\n<p>Also, even if you had <em>no</em> time-discounting, I think that a rational agent must do identity-discounting.&nbsp; You can't stay you forever.&nbsp; If you change, the future you will be less like you, and weigh less strongly in your utility function.&nbsp; Objections to this generally assume that it makes sense to trace your identity by following your physical body.&nbsp; Physical bodies will not have a 1-1 correspondence with personalities for more than another century or two, so just forget that idea.&nbsp; And if you don't change, well, what's the point of living?</p>\n<p>Evolutionary arguments may help us with self-discounting.&nbsp; Evolutionary forces encourage agents to emphasize continuity or ancestry over resemblance in an agent's selfness function.&nbsp; The major variable is reproduction rate over lifespan.&nbsp; This applies to genes or memes.&nbsp; But they can't help us with time-discounting.</p>\n<p>I think there may be a way to make this one work.&nbsp; I just haven't thought of it yet.</p>\n<h2>A benevolent singleton will save us all.<br /></h2>\n<p>This case takes more analysis than I am willing to do right now.&nbsp; My short answer is that I place a very low expected utility on singleton scenarios.&nbsp; I would almost rather have the universe eat, drink, and be merry for 34 million years, and then die.</p>\n<p>I'm not ready to place my faith in a singleton.&nbsp; I want to work out what is wrong with the rest of this argument, and how we can survive without a singleton.</p>\n<p>(Please don't conclude from my arguments that you should go out and create a singleton.&nbsp; Creating a singleton is hard to undo.&nbsp; It should be deferred nearly as long as possible.&nbsp; Maybe we don't have 34 million years, but this essay doesn't give you any reason not to wait a few thousand years at least.)</p>\n<h2>In conclusion</h2>\n<p>I think that the figures I've given here are conservative.&nbsp; I expect existential risk to be much greater than 3/1,000,000 per century.&nbsp; I expect there will continue to be externalities that cause suboptimal behavior, so that the actual risk will be greater even than the already-sufficient risk that rational agents would choose.&nbsp; I expect population and technology to continue to increase, and existential risk to be proportional to population times technology.&nbsp; Existential risk will very possibly increase exponentially, on top of the subjective-time exponential.</p>\n<p>Our greatest chance for survival is that there's some other possibility I haven't thought of yet.&nbsp; Perhaps some of you will.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup> If you argue that the laws of physics may turn out to make this impossible, you don't understand what \"probability\" means.</p>\n<p><sup>2</sup> Evolutionary dynamics, the speed of light, and the Planck constant are the three great enablers and preventers of possible futures, which enable us to make predictions farther into the future and with greater confidence than seem intuitively reasonable.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LkCeA4wu8iLmetb28", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 19, "extendedScore": null, "score": 3.2e-05, "legacy": true, "legacyId": "1304", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Followup to <a title=\"This Failing Earth\" href=\"/lw/gg/this_failing_earth/\">This Failing Earth</a>, <a href=\"/lw/12h/our_society_lacks_good_selfpreservation_mechanisms\">Our society lacks good self-preservation mechanisms</a>, <a href=\"/lw/12j/debate_is_short_term_planning_in_humans_due_to_a/\">Is short term planning in humans due to a short life or due to bias?</a></p>\n<p>I don't mean that deciding to exterminate life is rational.&nbsp; But if, as a society of rational agents, we each maximize our expected utility, this may inevitably lead to our exterminating life, or at least intelligent life.</p>\n<p>Ed Regis reports on p 216 of \u201cGreat Mambo Chicken and the TransHuman Condition,\u201d (Penguin Books, London, 1992):</p>\n<blockquote>\n<p>Edward Teller had thought about it, the chance that the atomic explosion would light up the surrounding air and that this conflagration would then propagate itself around the world. Some of the bomb makers had even calculated the numerical odds of this actually happening, coming up with the figure of three chances in a million they\u2019d incinerate the Earth. Nevertheless, they went ahead and exploded the bomb.</p>\n</blockquote>\n<p>Was this a bad decision?&nbsp; Well, consider the expected value to the people involved.&nbsp; Without the bomb, there was a much, much greater than 3/1,000,000 chance that either a) they would be killed in the war, or b) they would be ruled by Nazis or the Japanese.&nbsp; The loss to them if they ignited the atmosphere would be another 30 or so years of life.&nbsp; The loss to them if they lost the war and/or were killed by their enemies would also be another 30 or so years of life.&nbsp; The loss in being conquered would also be large.&nbsp; Easy decision, really.</p>\n<p>Suppose that, once a century, some party in a conflict chooses to use some technique to help win the conflict that has a p=3/1,000,000 chance of eliminating life as we know it.&nbsp; Then our expected survival time is 100 times the sum from n=1 to infinity of np(1-p)<sup>n-1</sup>.&nbsp; If I've done my math right, that's \u2248 33,777,000 years.</p>\n<p><a id=\"more\"></a>This supposition seems reasonable to me.&nbsp; There is a balance between offensive and defensive capability that shifts as technology develops.&nbsp; If technology keeps changing, it is inevitable that, much of the time, a technology will provide the ability to destroy all life before the counter-technology to defend against it has been developed.&nbsp; In the near future, biological weapons will be more able to wipe out life than we are able to defend against them.&nbsp; We may then develop the ability to defend against biological attacks; we may then be safe until the next dangerous technology.</p>\n<p>If you believe in accelerating change, then the number of important events in a given time interval increases exponentially, or, equivalently, the time intervals that should be considered equivalent opportunities for important events shorten exponentially.&nbsp; The 34M years remaining to life is then in subjective time, and must be mapped into realtime.&nbsp; If we suppose the subjective/real time ratio doubles every 100 years, this gives life an expected survival time of 2000 more realtime years.&nbsp; If we instead use Ray Kurzweil's figure of about 2 years, this gives life about 40 remaining realtime years.&nbsp; (I don't recommend Ray's figure.&nbsp; I'm just giving it for those who do.)</p>\n<p>Please understand that I am not yet another \"prophet\" bemoaning the foolishness of humanity.&nbsp; Just the opposite:&nbsp; I'm saying this is not something we will outgrow.&nbsp; If anything, becoming more rational only makes our doom more certain.&nbsp; For the agents who must actually make these decisions, it would be irrational not to take these risks.&nbsp; The fact that this level of risk-tolerance will inevitably lead to the snuffing out of all life does not make the expected utility of these risks negative for the agents involved.</p>\n<p>I can think of only a few ways that rationalilty can not inevitably exterminate all life in the cosmologically (even geologically) near future:</p>\n<ul>\n<li>\n<p>We can outrun the danger:&nbsp; We can spread life to other planets, and to other solar systems, and to other galaxies, faster than we can spread destruction.</p>\n</li>\n<li>\n<p>Technology will not continue to develop, but will stabilize in a state in which all defensive technologies provide absolute, 100%, fail-safe protection against all offensive technologies.</p>\n</li>\n<li>\n<p>People will stop having conflicts.</p>\n</li>\n<li>Rational agents incorporate the benefits to others into their utility functions.</li>\n<li>\n<p>Rational agents with long lifespans will protect the future for themselves.</p>\n</li>\n<li>\n<p>Utility functions will change so that it is no longer rational for decision-makers to take tiny chances of destroying life for any amount of utility gains.</p>\n</li>\n<li>Independent agents will cease to exist, or to be free (the Singleton scenario).</li>\n</ul>\n<p>Let's look at these one by one:</p>\n<h2 id=\"We_can_outrun_the_danger_\">We can outrun the danger.<br></h2>\n<p>We will colonize other planets; but we may also&nbsp; figure out how to make the Sun go nova on demand.&nbsp; We will colonize other star systems; but we may also figure out how to liberate much of the energy in the black hole at the center of our galaxy in a giant explosion that will move outward at near the speed of light.</p>\n<p>One problem with this idea is that apocalypses are correlated; one may trigger another.&nbsp; A disease may spread to another planet.&nbsp; The choice to use a planet-busting bomb on one planet may lead to its retaliatory use on another planet.&nbsp; It's not clear whether spreading out <em>and</em> increasing in population actually makes life more safe.&nbsp; If you think in the other direction, a smaller human population (say ten million) stuck here on Earth would be safer from human-instigated disasters.</p>\n<p>But neither of those are my final objection.&nbsp; More important is that our compression of subjective time can be exponential, while our ability to escape from ever-broader swaths of destruction is limited by lightspeed.</p>\n<h2 id=\"Technology_will_stabilize_in_a_safe_state_\">Technology will stabilize in a safe state.<br></h2>\n<p>Maybe technology will stabilize, and we'll run out of things to discover.&nbsp; If that were to happen, I would expect that conflicts would increase, because people would get bored.&nbsp; As I mentioned in another thread, one good explanation for the incessant and counterproductive wars in the middle ages - a reason some of the actors themselves gave in their writings - is that the nobility were bored.&nbsp; They did not have the concept of progress; they were just looking for something to give them purpose while waiting for Jesus to return.</p>\n<p>But that's not my final rejection.&nbsp; The big problem is that by \"safe\", I mean really, really safe.&nbsp; We're talking about bringing existential threats to chances less than 1 in a million per century.&nbsp; I don't know of any defensive technology that can guarantee a less than 1 in a million failure rate.</p>\n<h2 id=\"People_will_stop_having_conflicts_\">People will stop having conflicts.</h2>\n<p>That's a nice thought.&nbsp; A lot of people - maybe the majority of people - believe that we are inevitably progressing along a path to less violence and greater peace.</p>\n<p>They thought that just before World War I.&nbsp; But that's not my final rejection.&nbsp; Evolutionary arguments are a more powerful reason to believe that people will continue to have conflicts.&nbsp; Those that avoid conflict will be out-competed by those that do not.</p>\n<p>But that's not my final rejection either.&nbsp; The bigger problem is that this isn't something that arises only in conflicts.&nbsp; All we need are desires.&nbsp; We're willing to tolerate risk to increase our utility.&nbsp; For instance, we're willing to take some unknown, but clearly greater than one in a million chance, of the collapse of much of civilization due to climate warming.&nbsp; In return for this risk, we can enjoy a better lifestyle now.</p>\n<p>Also, we haven't burned all physics textbooks along with all physicists.&nbsp; Yet I'm confident there is at least a one in a million chance that, in the next 100 years, some physicist will figure out a way to reduce the earth to powder, if not to crack spacetime itself and undo the entire universe.&nbsp; (In fact, I'd guess the chance is nearer to 1 in 10.)<sup>1</sup>&nbsp; We take this existential risk in return for a continued flow of benefits such as better graphics in Halo 3 and smaller iPods.&nbsp; And it's reasonable for us to do this, because an improvement in utility of 1% over an agent's lifespan is, to that agent, exactly balanced by a 1% chance of destroying the Universe.</p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Safety_of_particle_collisions_at_the_Large_Hadron_Collider\">Wikipedia entry</a> on Large Hadcon Collider risk says, \"In the book <em><a title=\"Our Final Hour\" href=\"http://en.wikipedia.org/wiki/Our_Final_Hour\">Our Final Century: Will the Human Race Survive the Twenty-first Century?</a></em>, English cosmologist and astrophysicist <a class=\"mw-redirect\" title=\"Martin Rees\" href=\"http://en.wikipedia.org/wiki/Martin_Rees\">Martin Rees</a> calculated an upper limit of 1 in 50 million for the probability that the Large Hadron Collider will produce a global catastrophe or black hole.\"&nbsp; The more authoritative \"Review of the Safety of LHC Collisions\" by the LHC Safety Assessment Group concluded that there was at most a 1 in 10<sup>31 </sup>chance of destroying the Earth.</p>\n<p>The LHC conclusions are criminally low.&nbsp; Their evidence was this: \"Nature has already conducted the LHC experimental programme about one billion times via the collisions of cosmic rays with the Sun - and the Sun still exists.\"&nbsp; There followed a couple of sentences of handwaving to the effect that if any other stars had turned to black holes due to collisions with cosmic rays, we would know it - apparently due to our flawless ability to detect black holes and ascertain what caused them - and therefore we can multiply this figure by the number of stars in the universe.</p>\n<p>I believe there is much more than a one-in-a-billion chance that our understanding in one of the steps used in arriving at these figures is incorrect.&nbsp; Based on my experience with peer-reviewed papers, there's at least a one-in-ten chance that there's a basic arithmetic error in their paper that no one has noticed yet.&nbsp; I'm thinking more like one-in-a-million, once you correct for the anthropic principle and for the chance that there is a mistake in the argument.&nbsp; (That's based on a belief that priors for anything likely enough that smart people even thought of the possibility should be larger than one in a billion, unless they were specifically trying to think of examples of low-probability possibilities such as all of the air molecules in the room moving to one side.)</p>\n<p>The Trinity test was done for the sake of winning World War II.&nbsp; But the LHC was turned on for... well, no practical advantage that I've heard of yet.&nbsp; It seems that we are willing to tolerate one-in-a-million chances of destroying the Earth for very little benefit.&nbsp; And this is&nbsp; rational, since the LHC will probably improve our lives by more than one part in a million.</p>\n<h2 id=\"Rational_agents_incorporate_the_benefits_to_others_into_their_utility_functions_\">Rational agents incorporate the benefits to others into their utility functions.</h2>\n<p>\"But,\" you say, \"I wouldn't risk a 1% chance of destroying the universe for a 1% increase in my utility!\"</p>\n<p>Well... yes, you would, if you're a rational expectation maximizer.&nbsp; It's possible that you would take a much higher risk, if your utility is at risk of going negative; it's not possible that you would not accept a .999% risk, unless you are not maximizing expected value, or you assign the null state after universe-destruction negative utility.&nbsp; (This seems difficult, but is worth exploring.)&nbsp; If you still think that you wouldn't, it's probably because you're thinking a 1% increase in your utility means something like a 1% increase in the pleasure you experience.&nbsp; It doesn't.&nbsp; It's a 1% increase in your <em>utility.</em>&nbsp; If you factor the rest of your universe into your utility function, then it's already in there.</p>\n<p>The US national debt should be enough to convince you that people act in their self-interest.&nbsp; Even the most moral people - in fact, <em>especially </em>the \"most moral\" people - do not incorporate the benefits to others, especially future others, into their utility functions.&nbsp; If we did that, we would engage in massive eugenics programs.&nbsp; But eugenics is considered the greatest immorality.</p>\n<p>But maybe they're just not as rational as you.&nbsp; Maybe you really are a rational saint who considers your own pleasure no more important than the pleasure of everyone else on Earth.&nbsp; Maybe you have never, ever bought anything for yourself that did not bring you as much benefit as the same amount of money would if spent to repair cleft palates or distribute vaccines or mosquito nets or water pumps in Africa.&nbsp; Maybe it's really true that, if you met the girl of your dreams and she loved you, and you won the lottery, put out an album that went platinum, and got published in Science, all in the same week, it would make an imperceptible change in your utility versus if everyone you knew died, Bernie Madoff spent all your money, and you were unfairly convicted of murder and diagnosed with cancer.</p>\n<p>It doesn't matter.&nbsp; Because you would be adding up everyone else's utility, and everyone else is getting that 1% extra utility from the better graphics cards and the smaller iPods.</p>\n<p>But that will stop you from risking atmospheric ignition to defeat the Nazis, right?&nbsp; Because you'll incorporate them into your utility function?&nbsp; Well, that is a subset of the claim \"People will stop having conflicts.\"&nbsp; See above.</p>\n<p>And even if you somehow worked around all these arguments, evolution, again, thwarts you.<sup>2</sup>&nbsp; Even if you don't agree that rational agents are selfish, your unselfish agents will be out-competed by selfish agents.&nbsp; The claim that rational agents are not selfish implies that rational agents are unfit.</p>\n<h2 id=\"Rational_agents_with_long_lifespans_will_protect_the_future_for_themselves_\">Rational agents with long lifespans will protect the future for themselves.<br></h2>\n<p>The most familiar idea here is that, if people expect to live for millions of years, they will be \"wiser\" and take fewer risks with that time.&nbsp; But the flip side is that they also have more time to lose.&nbsp; If they're deciding whether to risk igniting the atmosphere in order to lower the risk of being killed by Nazis, lifespan cancels out of the equation.</p>\n<p>Also, if they live a million times longer than us, they're going to get a million times the benefit of those nicer iPods.&nbsp; They may be less willing to take an existential risk for something that will benefit them only temporarily.&nbsp; But benefits have a way of increasing, not decreasing, over time.&nbsp; The discovery of the law of gravity and of the invisible hand benefit us in the 21st century more than they did the people of the 17th century.</p>\n<p>But that's not my final rejection.&nbsp; More important is time-discounting.&nbsp; Agents will time-discount, probably exponentially, due to uncertainty.&nbsp; If you considered benefits to the future without exponential time-discounting, the benefits to others and to future generations would outweigh any benefits to yourself so much that in many cases you wouldn't even waste time trying to figure out what you wanted.&nbsp; And, since future generations will be able to get more utility out of the same resources, we'd all be obliged to kill ourselves, unless we reasonably think that we are contributing to the development of that capability.</p>\n<p>Time discounting is always (so far) exponential, because non-asymptotic functions don't make sense.&nbsp; I supposed you could use a trigonometric function instead for time discounting, but I don't think it would help.</p>\n<p>Could a continued exponential population explosion outweigh exponential time-discounting?&nbsp; Well, you can't have a continued exponential population explosion, because of the speed of light and the Planck constant.&nbsp; (I leave the details as an exercise to the reader.)</p>\n<p>Also, even if you had <em>no</em> time-discounting, I think that a rational agent must do identity-discounting.&nbsp; You can't stay you forever.&nbsp; If you change, the future you will be less like you, and weigh less strongly in your utility function.&nbsp; Objections to this generally assume that it makes sense to trace your identity by following your physical body.&nbsp; Physical bodies will not have a 1-1 correspondence with personalities for more than another century or two, so just forget that idea.&nbsp; And if you don't change, well, what's the point of living?</p>\n<p>Evolutionary arguments may help us with self-discounting.&nbsp; Evolutionary forces encourage agents to emphasize continuity or ancestry over resemblance in an agent's selfness function.&nbsp; The major variable is reproduction rate over lifespan.&nbsp; This applies to genes or memes.&nbsp; But they can't help us with time-discounting.</p>\n<p>I think there may be a way to make this one work.&nbsp; I just haven't thought of it yet.</p>\n<h2 id=\"A_benevolent_singleton_will_save_us_all_\">A benevolent singleton will save us all.<br></h2>\n<p>This case takes more analysis than I am willing to do right now.&nbsp; My short answer is that I place a very low expected utility on singleton scenarios.&nbsp; I would almost rather have the universe eat, drink, and be merry for 34 million years, and then die.</p>\n<p>I'm not ready to place my faith in a singleton.&nbsp; I want to work out what is wrong with the rest of this argument, and how we can survive without a singleton.</p>\n<p>(Please don't conclude from my arguments that you should go out and create a singleton.&nbsp; Creating a singleton is hard to undo.&nbsp; It should be deferred nearly as long as possible.&nbsp; Maybe we don't have 34 million years, but this essay doesn't give you any reason not to wait a few thousand years at least.)</p>\n<h2 id=\"In_conclusion\">In conclusion</h2>\n<p>I think that the figures I've given here are conservative.&nbsp; I expect existential risk to be much greater than 3/1,000,000 per century.&nbsp; I expect there will continue to be externalities that cause suboptimal behavior, so that the actual risk will be greater even than the already-sufficient risk that rational agents would choose.&nbsp; I expect population and technology to continue to increase, and existential risk to be proportional to population times technology.&nbsp; Existential risk will very possibly increase exponentially, on top of the subjective-time exponential.</p>\n<p>Our greatest chance for survival is that there's some other possibility I haven't thought of yet.&nbsp; Perhaps some of you will.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup> If you argue that the laws of physics may turn out to make this impossible, you don't understand what \"probability\" means.</p>\n<p><sup>2</sup> Evolutionary dynamics, the speed of light, and the Planck constant are the three great enablers and preventers of possible futures, which enable us to make predictions farther into the future and with greater confidence than seem intuitively reasonable.</p>", "sections": [{"title": "We can outrun the danger.", "anchor": "We_can_outrun_the_danger_", "level": 1}, {"title": "Technology will stabilize in a safe state.", "anchor": "Technology_will_stabilize_in_a_safe_state_", "level": 1}, {"title": "People will stop having conflicts.", "anchor": "People_will_stop_having_conflicts_", "level": 1}, {"title": "Rational agents incorporate the benefits to others into their utility functions.", "anchor": "Rational_agents_incorporate_the_benefits_to_others_into_their_utility_functions_", "level": 1}, {"title": "Rational agents with long lifespans will protect the future for themselves.", "anchor": "Rational_agents_with_long_lifespans_will_protect_the_future_for_themselves_", "level": 1}, {"title": "A benevolent singleton will save us all.", "anchor": "A_benevolent_singleton_will_save_us_all_", "level": 1}, {"title": "In conclusion", "anchor": "In_conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "279 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 279, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2KNN9WPcyto7QH9pi", "rGY5i4FGwutouMvWM", "mp5Lhd8EiDwC6kBTW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-06T20:10:39.207Z", "modifiedAt": null, "url": null, "title": "Robin Hanson's lists of Overcoming Bias Posts", "slug": "robin-hanson-s-lists-of-overcoming-bias-posts", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:18.826Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AndrewH", "createdAt": "2009-03-02T15:31:43.543Z", "isAdmin": false, "displayName": "AndrewH"}, "userId": "FgKt6dBPfyiku5Qh8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sj53JvAvbGH54SH4v/robin-hanson-s-lists-of-overcoming-bias-posts", "pageUrlRelative": "/posts/sj53JvAvbGH54SH4v/robin-hanson-s-lists-of-overcoming-bias-posts", "linkUrl": "https://www.lesswrong.com/posts/sj53JvAvbGH54SH4v/robin-hanson-s-lists-of-overcoming-bias-posts", "postedAtFormatted": "Thursday, August 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Robin%20Hanson's%20lists%20of%20Overcoming%20Bias%20Posts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARobin%20Hanson's%20lists%20of%20Overcoming%20Bias%20Posts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsj53JvAvbGH54SH4v%2Frobin-hanson-s-lists-of-overcoming-bias-posts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Robin%20Hanson's%20lists%20of%20Overcoming%20Bias%20Posts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsj53JvAvbGH54SH4v%2Frobin-hanson-s-lists-of-overcoming-bias-posts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsj53JvAvbGH54SH4v%2Frobin-hanson-s-lists-of-overcoming-bias-posts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<p>I have created a list of Overcoming Bias posts for Robin Hanson available <a href=\"http://www.cs.auckland.ac.nz/~andwhay/hpostlist.html\">here</a>. Additionally, using the links inside each posts, I have created a set of graphs (available <a href=\"http://www.cs.auckland.ac.nz/~andwhay/graphlist2.html\">here</a>) such that if post A has a link to post B, then there is an arc from B to A. Enjoy! (There are also ones for Eliezer <a href=\"http://www.cs.auckland.ac.nz/~andwhay/postlist.html\">here</a>).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GQyPQcdEQF4zXhJBq": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sj53JvAvbGH54SH4v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 26, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "1482", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-07T14:49:38.900Z", "modifiedAt": null, "url": null, "title": "Fighting Akrasia: Finding the Source", "slug": "fighting-akrasia-finding-the-source", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:23.455Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gworley", "createdAt": "2009-03-26T17:18:20.404Z", "isAdmin": false, "displayName": "G Gordon Worley III"}, "userId": "gjoi5eBQob27Lww62", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BH8BcP7d2sbmzZig8/fighting-akrasia-finding-the-source", "pageUrlRelative": "/posts/BH8BcP7d2sbmzZig8/fighting-akrasia-finding-the-source", "linkUrl": "https://www.lesswrong.com/posts/BH8BcP7d2sbmzZig8/fighting-akrasia-finding-the-source", "postedAtFormatted": "Friday, August 7th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fighting%20Akrasia%3A%20Finding%20the%20Source&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFighting%20Akrasia%3A%20Finding%20the%20Source%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBH8BcP7d2sbmzZig8%2Ffighting-akrasia-finding-the-source%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fighting%20Akrasia%3A%20Finding%20the%20Source%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBH8BcP7d2sbmzZig8%2Ffighting-akrasia-finding-the-source", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBH8BcP7d2sbmzZig8%2Ffighting-akrasia-finding-the-source", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 881, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/dp/fighting_akrasia_incentivising_action/\">Fighting Akrasia:&nbsp; Incentivising Action</a></p>\n<p><strong>Influenced by</strong>:&nbsp; <a href=\"/lw/dr/generalizing_from_one_example/\">Generalizing From One Example</a></p>\n<p>Previously I looked at how we might fight akrasia by creating incentives for actions.&nbsp; Based on the comments to the previous article and Yvain's now classic post <a href=\"/lw/dr/generalizing_from_one_example/\">Generalizing From One Example</a>, I want to take a deeper look at the source of akrasia and the techniques used to fight it.</p>\n<p>I feel foolish for not looking at this closer first, but let's begin by asking what akrasia is and what causes it.&nbsp; As commonly used, akrasia is the weakness-of-will we feel when we desire to do something but find ourselves doing something else.&nbsp; So why do we experience akrasia?&nbsp; Or, more to the point, why to we feel a desire to take actions contrary the actions we desire most, as indicated by our actions?&nbsp; Or, if it helps, flip that question and ask why are the actions we take not always the ones we feel the greatest desire for?</p>\n<p>First, we don't know the fine details of how the human brain makes decisions.&nbsp; We know what it feels like to come to a decision about an action (or anything else), but how the <a href=\"http://www.overcomingbias.com/2008/02/algorithm-feels.html\">algorithm feels from the inside</a> is not a reliable way to figure out how the decision was actually made.&nbsp; But because most people can relate to a feeling of akrasia, this suggests that there is some disconnect between how the brain decides what actions are most desirable and what actions we believe are most desirable.&nbsp; The hypothesis that I consider most likely is that the ability to form beliefs about desirable actions evolved well after the ability to make decisions about what actions are most desirable, and the decision-making part of the brain only bothers to consult the belief-about-desirability-of-actions part of the brain when there is a reason to do so from evolution's point of view.<sup>1</sup>&nbsp; As a result we end up with a brain that only does what we think we really want when evolutionarily prudent, hence we experience akrasia whenever our brain doesn't consider it appropriate to consult what we experience as desirable.</p>\n<p>This suggests two main ways of overcoming akrasia assuming my hypothesis (or something close to it) is correct:&nbsp; make the actions we believe to be desirable also desirable to the decision-making part of the brain or make the decision-making part of the brain consult the belief-about-desirability-of-actions part of the brain when we want it to.&nbsp; Most techniques fall into the former category since this is by far the easier strategy, but however a technique works, an overriding theme of the akrasia-related articles and comments on Less Wrong is that no technique yet found seems to work for all people.</p>\n<p><a id=\"more\"></a>For convenience, here is a list of some of the techniques discussed here and elsewhere in the productivity literature for fighting akrasia that work for some people but not for everyone.</p>\n<ul>\n<li>Schedule work times</li>\n<li>Set deadlines</li>\n<li>Make to do list</li>\n<li>Create financial consequences for failure</li>\n<li>Create social consequences for failure</li>\n<li>Create physical consequences for failure</li>\n<li>Create existential consequences for failure</li>\n<li>Create additional rewards for success</li>\n<li>Set incremental goals</li>\n<li>Create special environments for working only towards a particular goal</li>\n</ul>\n<p>And there are many more tricks and workarounds people have discovered that work for them and some segment of the population. But so far no one has found a Unifying Theory of Akrasia Fighting; otherwise they would have <a href=\"/lw/9v/beware_of_otheroptimizing/\">other optimized</a> us all and be rich.&nbsp; So all we have so far is a collection of techniques that sometimes work for some people, but because most promoters of these techniques are busy trying to other optimize because they <a href=\"/lw/dr/generalizing_from_one_example/\">generalized from one example</a>, we don't even have a good way to see if a technique will work for any particular individual short of having them try it.</p>\n<p>I don't expect us to find a universal solution to fighting akrasia any time soon, and it may require the medical technology to \"rewire\" or \"reprogram\" the brain (pick your inapt metaphor).&nbsp; But what we can do is make things a little easier for those looking for what they can do that will actually work.&nbsp; In that vein, I've created a survey for the Less Wrong community that will hopefully give us a chance to collect enough data to predict what types of akrasia fighting techniques will work best for which people.&nbsp; It asks a number of questions about your behaviors and thoughts and then focus on what techniques for fighting akrasia you've tried and how well they worked for you.&nbsp; My hope is that I can put all of this data together to make some predictions about how likely a particular technique will work for you, assuming I've asked the right questions.</p>\n<p>Please feel free to share this survey (and post) with anyone who you think might be interested, even if they would otherwise not be interested in Less Wrong.&nbsp; The more responses we can get the more useful the data will be.&nbsp; Thanks!</p>\n<p><a href=\"http://spreadsheets.google.com/viewform?hl=en&amp;formkey=cmpGck9ZSDFqUk0yNUQzRHlyZmlDSFE6MA..\" target=\"_blank\">Take the survey</a></p>\n<p><strong>Footnotes:</strong></p>\n<p><sup>1</sup> That is to say, there were statistically regular occasions in the environment of evolutionary adaptation that lead those of our ancestors who consulted the belief-about-desirability-of-actions part of the brain <em>on those occasions</em> when making decisions to reproduce at a higher rate.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BH8BcP7d2sbmzZig8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 7, "extendedScore": null, "score": 5.133005580238989e-07, "legacy": true, "legacyId": "513", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KW5m4eREWGitPb8Ev", "baTWMegR42PAsH9qJ", "6NvbSwuSAooQxxf7f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-07T18:56:55.392Z", "modifiedAt": null, "url": null, "title": "A note on hypotheticals", "slug": "a-note-on-hypotheticals", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:20.055Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LZwFMXvwTGCbWcaiq/a-note-on-hypotheticals", "pageUrlRelative": "/posts/LZwFMXvwTGCbWcaiq/a-note-on-hypotheticals", "linkUrl": "https://www.lesswrong.com/posts/LZwFMXvwTGCbWcaiq/a-note-on-hypotheticals", "postedAtFormatted": "Friday, August 7th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20note%20on%20hypotheticals&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20note%20on%20hypotheticals%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLZwFMXvwTGCbWcaiq%2Fa-note-on-hypotheticals%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20note%20on%20hypotheticals%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLZwFMXvwTGCbWcaiq%2Fa-note-on-hypotheticals", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLZwFMXvwTGCbWcaiq%2Fa-note-on-hypotheticals", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 863, "htmlBody": "<p>People frequently describe hypothetical situations on LW.&nbsp; Often, other people make responses that suggest they don't understand the purpose of hypotheticals.</p>\n<ul>\n<li>When someone puts forth the hypothetical A, it doesn't mean they believe it is true.&nbsp; They may be trying to show not(A).</li>\n<li>When someone posits A =&gt; B (A implies B), it doesn't mean that they believe A is true.&nbsp; The proposition A =&gt; B is commonly used to prove that B is true, or that A is false.</li>\n<li>A solution to a hypothetical scenario is useful only if, when you map it back into the original domain, it solves the original problem.</li>\n</ul>\n<p>I'll expand on the last point.&nbsp; Sorry for being vague.&nbsp; I'm trying not to name names.</p>\n<p>When a hypothetical is put forward to test a theory, ignore aspects of the hypothetical scenario that don't correspond to parts of the theory.&nbsp; Don't get emotionally involved.&nbsp; Don't think of the hypothetical as a narrative.&nbsp; A hypothetical about Omega sounds a lot like a story about a genie from a lamp, but you should approach it in a completely different way.&nbsp; Don't try to outsmart Omega (unless you're making a point about the impossibility of an Omega who can eg decide undecidable problems).&nbsp; When you find a loophole in the way the hypothetical is posed, that doesn't exist in the original domain, point it out only if you are doing so to improve the phrasing of the hypothetical situation.<a id=\"more\"></a></p>\n<p>John Searle's <a href=\"http://en.wikipedia.org/wiki/Chinese_room\">Chinese Room</a> is an example of a hypothetical in which it is important to not get emotionally involved.&nbsp; Searle's conclusion is that the man in the Chinese room doesn't understand Chinese; therefore, a computer doesn't understand Chinese.&nbsp; His model maps the running software onto the complete system of room plus man plus cards; but when he interprets it, he empathizes with the human on each half of the mapping,&nbsp; and so maps the locus of consciousness from the running software onto just the man.<sup>1</sup></p>\n<p>Sometimes it's difficult to know whether your solution to a hypothetical is exploiting a loophole in the hypothetical, or finding a solution to the original problem.&nbsp; But when the original problem is testing a mathematical model, it's usually obvious.&nbsp; There are a few general situations where it's not obvious.</p>\n<p>Consciousness is often a tricky area that makes it hard to tell whether you are looking at a solution to the original problem, or a loophole in the hypothetical.&nbsp; Sometimes the original problem is a paradox because of consciousness, so you can't map it away.&nbsp; In the <a href=\"http://en.wikipedia.org/wiki/Newcomb's_paradox\">Newcomb paradox</a>, if you replace the person with a computer program, people would be much quicker to say:&nbsp; You should write a computer program that will one-box.&nbsp; But you can phrase it that way only if you're sure that the Newcomb paradox isn't really a question about free will.&nbsp; The \"paradox\" might be regarded as the assertion that there is no such thing as free will.</p>\n<p>Another tricky case involves infinities.&nbsp; A paradox of infinities typically involves taking two different infinities, but treating them as a single infinity, so that they don't cancel out the way they should, or do cancel out when they shouldn't.&nbsp; Zeno's paradox is an example: The hypothetical doesn't notice that the infinity of intervals is cancelled out by their infinitesimal sizes.&nbsp; Eliezer discusses some other cases <a href=\"/lw/na/trust_in_bayes/\">here</a>.</p>\n<p>Another category of tricky cases is when the hypothetical involves impossibilities.&nbsp; It's possible to accidentally construct a hypothetical that makes an assumption that isn't valid in our universe.&nbsp; (I think these paradoxes were unknown before the 20th century, but there may be a math example.)&nbsp; These crop up frequently in modern physics.&nbsp; The ultraviolet catastrophe may be the first such paradox discovered.&nbsp; The hypothetical in which a massive black hole suddenly appears one light-minute away from you, and you want to know how you can be influenced by its gravity before gravity waves have time to reach you, might be an example.&nbsp; The aspect of the Newcomb paradox that allows Omega to predict what you will do without fail may be such a flawed hypothetical.</p>\n<p>If you are giving a solution to a hypothetical scenario that tests a mathematical model, and your response doesn't use math, and doesn't hinge on a consciousness, infinity, or impossibility <em>from the original problem domain</em>, your response is likely irrelevant.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup> He makes other errors as well.&nbsp; It's a curious case in which amassing a large number of errors in a model makes it harder to rebut, because it's harder to figure out what the model is.&nbsp; This is a clever way of exploiting the scientific process.&nbsp; Searle takes on challengers one at a time.&nbsp; Each challenger, being a scientist, singles out one error in Searle's reasoning.&nbsp; Searle uses other errors in his reasoning to construct a workaround; and may immediately move on to another challenger, and use the error that the original challenger focused on to work around the error that the second challenger focused on.&nbsp; This sort of trickery can be detected by looking at all of someone's counterarguments en masse, and checking that they all define the same terms the same way, and agree on which statements are assumptions and which statements are conclusions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"rjEZWSbSffhaWYRvo": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LZwFMXvwTGCbWcaiq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 23, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "1486", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BL9DuE2iTCkrnuYzx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-08T22:31:22.994Z", "modifiedAt": null, "url": null, "title": "Dreams with Damaged Priors", "slug": "dreams-with-damaged-priors", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:06.313Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8z2Fm2yaHpQz8rr5B/dreams-with-damaged-priors", "pageUrlRelative": "/posts/8z2Fm2yaHpQz8rr5B/dreams-with-damaged-priors", "linkUrl": "https://www.lesswrong.com/posts/8z2Fm2yaHpQz8rr5B/dreams-with-damaged-priors", "postedAtFormatted": "Saturday, August 8th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dreams%20with%20Damaged%20Priors&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADreams%20with%20Damaged%20Priors%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8z2Fm2yaHpQz8rr5B%2Fdreams-with-damaged-priors%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dreams%20with%20Damaged%20Priors%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8z2Fm2yaHpQz8rr5B%2Fdreams-with-damaged-priors", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8z2Fm2yaHpQz8rr5B%2Fdreams-with-damaged-priors", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 948, "htmlBody": "<p>Dreaming is the closest I've gotten to <a href=\"/lw/12s/the_strangest_thing_an_ai_could_tell_you/x7c?context=1#x7c\">testing myself against the challenge of maintaining rationality under brain damage</a>.&nbsp; So far, my trials have exhibited mixed results.</p>\n<p>In one memorable dream a few years ago, I dreamed that the Wall Street Journal had published an article about \"Eliezer Yudkowsky\", but it wasn't me, it was a different \"Eliezer Yudkowsky\", and in the dream I wondered if I needed to write a letter to clarify this.&nbsp; Then I realized I was dreaming within the dream... and worried to myself, still dreaming:&nbsp; \"But what if the Wall Street Journal really <em>does</em> have an article about an 'Eliezer Yudkowsky' who isn't me?\"</p>\n<p>But then I thought:&nbsp; \"Well, the probability that I would dream about a WSJ article like that, given that a WSJ article like that actually exists in this morning's paper, is the same as the probability that I would have such a dream, given that no such article is in this morning's paper.&nbsp; So by Bayes's Theorem, the dream isn't evidence one way or the other.&nbsp; Thus there's no point in trying to guess the answer now - I'll find out in the morning whether there's an article like that.\"&nbsp; And, satisfied, my mind went back to ordinary sleep.</p>\n<p>I find it fascinating that I was able to <em>explicitly </em>apply Bayes's Theorem in my sleep to <em>correctly </em>compute the 1:1 <em>likelihood ratio</em>, but my dreaming mind didn't notice the damaged <em>prior</em> - didn't notice that the prior probability of such a WSJ article was too low to <a href=\"/lw/jo/einsteins_arrogance/\">justify raising the hypothesis to my attention</a>.</p>\n<p>At this point even I must concede that there is something to the complaint that, in real-world everyday life, Bayesians dispense too little advice about how to compute priors.&nbsp; With a damaged intuition for the <em>weight of evidence</em>, my dreaming mind was able to explicitly compute a likelihood ratio and correct itself.&nbsp; But with a damaged intuition for the <em>prior probability</em>, my mind didn't successfully check itself, or even notice a problem - didn't get as far as asking \"But what is the prior probability?\"</p>\n<p>On July 20 I had an even more dramatic dream - sparking this essay - when I dreamed that I'd googled my own name and discovered that one of my OBLW articles had been translated into German and published, without permission but with attribution, in a special issue of the <em>Journal of Applied Logic</em> to commemorate the death of Richard Thaler (don't worry, he is in fact still alive)...<a id=\"more\"></a></p>\n<p>Then I half woke-up... and wondered if maybe one of my OBLW articles really <em>had</em> been \"borrowed\" this way.&nbsp; But I reasoned, in my half-awake state, that the dream couldn't be evidence because it didn't <a href=\"/lw/jl/what_is_evidence/\">form part of a causal chain wherein the outside environment impressed itself onto my brain</a>, and that only actual sensory impressions of Google results could form the base of a legitimate chain of inferences.</p>\n<p>So - still half-asleep - I wanted to get out of bed and <em>actually</em> look at Google, to see if a result turned up for the Journal of Applied Logic issue.</p>\n<p>And several times I fell back asleep and <em>dreamed</em> I'd looked at Google and seen the result; but each time on half-awaking I thought:&nbsp; \"No, I still seem to be in bed; that was a dream, not a sense-impression, so it's not valid evidence - I still need to actually look at Google.\"&nbsp; And the cycle continued.</p>\n<p>By the time I woke up entirely, my brain had fully switched on and I realized that the prior probability was tiny; and no, I did not bother to check the actual Google results.&nbsp; Though I did Google to check whether Richard Thaler was alive, since I was legitimately unsure of that when I started writing this post.</p>\n<p>If my dreaming brain had been talking in front of an audience, that audience might have applauded the intelligent-sounding sophisticated reasoning about what constituted evidence - which was even correct, so far as it went.&nbsp; And yet my half-awake brain didn't notice that <a href=\"http://www.partiallyclips.com/index.php?id=1594\">at the base of the whole issue</a> was a big complicated specific hypothesis whose prior probability fell off a cliff and vanished.&nbsp; Eliezer<sub>Dreaming</sub> didn't try to <a href=\"http://wiki.lesswrong.com/wiki/Occam%27s_razor\">measure the length of the message</a>, tot up the weight of <a href=\"/lw/jk/burdensome_details/\">burdensome details</a>, or even explicitly ask, \"What is the prior probability?\"</p>\n<p>I'd mused before that the state of being religious seemed <a href=\"http://imgur.com/a/0e0ZY#114\">similar to the state of being half-asleep</a>.&nbsp; But my recent dream made me wonder if the analogy really is a deep one.&nbsp; Intelligent theists can often be shepherded into admitting that their argument X is not valid evidence.&nbsp; Intelligent theists often confess explicitly that they have <em>no </em>supporting evidence - just like I explicitly realized that my dreams offered no evidence about the actual <em>Wall Street Journal</em> or the <em>Journal of Applied Logic.</em>&nbsp; But then they stay \"half-awake\" and go on wondering whether the dream happens to be true.&nbsp; They don't \"wake up completely\" and realize that, in the absence of evidence, the whole thing has a prior probability too low to deserve specific attention.</p>\n<p>My dreaming brain can, in its sleep, reason explicitly about likelihood ratios, Bayes's Theorem, cognitive chains of causality, permissible inferences, strong arguments and non-arguments.&nbsp; And yet still maintain a dreaming inability to reasonably evaluate <em>priors</em>, to notice burdensome details and sheer ridiculousness.&nbsp; If my dreaming brain's behavior is a true product of dissociation - of brainware modules or software modes that can be independently switched on or off - then the analogy to religion may be more than surface similarity.</p>\n<p>Conversely it could just be a matter of habits playing out in in my dreaming self; that I habitually pay more attention to arguments than priors, or habitually evaluate arguments deliberately but priors intuitively.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8z2Fm2yaHpQz8rr5B", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 52, "extendedScore": null, "score": 8.3e-05, "legacy": true, "legacyId": "1415", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 61, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["MwQRucYo6BZZwjKE7", "6s3xABaXKPdFwA3FS", "Yq6aA4M3JKWaQepPJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-08T22:57:09.266Z", "modifiedAt": null, "url": null, "title": "Would Your Real Preferences Please Stand Up?", "slug": "would-your-real-preferences-please-stand-up", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:04.010Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z3cTkXbA7jgwGWPcv/would-your-real-preferences-please-stand-up", "pageUrlRelative": "/posts/z3cTkXbA7jgwGWPcv/would-your-real-preferences-please-stand-up", "linkUrl": "https://www.lesswrong.com/posts/z3cTkXbA7jgwGWPcv/would-your-real-preferences-please-stand-up", "postedAtFormatted": "Saturday, August 8th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Would%20Your%20Real%20Preferences%20Please%20Stand%20Up%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWould%20Your%20Real%20Preferences%20Please%20Stand%20Up%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz3cTkXbA7jgwGWPcv%2Fwould-your-real-preferences-please-stand-up%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Would%20Your%20Real%20Preferences%20Please%20Stand%20Up%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz3cTkXbA7jgwGWPcv%2Fwould-your-real-preferences-please-stand-up", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz3cTkXbA7jgwGWPcv%2Fwould-your-real-preferences-please-stand-up", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1116, "htmlBody": "<p><strong>Related to: </strong><a href=\"/lw/yh/cynicism_in_evpsych_and_econ/\">Cynicism in Ev Psych and Econ</a></p>\n<p>In <a href=\"/lw/e9/fighting_akrasia_finding_the_source/\">Finding the Source</a>, a commenter <a href=\"/lw/e9/fighting_akrasia_finding_the_source/10w9\">says</a>:</p>\n<blockquote>\n<p>I have begun wondering whether claiming to be victim of 'akrasia' might just be a way of admitting that your real preferences, as revealed in your actions, don't match the preferences you want to signal (believing what you want to signal, even if untrue, makes the signals more effective).</p>\n</blockquote>\n<p>I think I've seen Robin put forth <del>something like this argument</del> [EDIT: Something related, <a href=\"/lw/15c/would_your_real_preferences_please_stand_up/110l\">but very different</a>], and TGGP points out that <a href=\"http://econlog.econlib.org/archives/2007/11/selling_selfcon.html\">Brian Caplan</a> explicitly believes pretty much the same thing<sup>1</sup>:</p>\n<blockquote>\n<p>I've previously argued that much - perhaps most - talk about \"self-control\" problems reflects social desirability bias rather than genuine inner conflict.<br /><br />Part of the reason why people who spend a lot of time and money on socially disapproved behaviors say they \"want to change\" is that that's what they're supposed to say.<br /><br />Think of it this way: A guy loses his wife and kids because he's a drunk. Suppose he sincerely prefers alcohol to his wife and kids. He still probably won't admit it, because people judge a sinner even more harshly if he is unrepentent. The drunk who says \"I was such a fool!\" gets some pity; the drunk who says \"I like Jack Daniels better than my wife and kids\" gets horrified looks. And either way, he can keep drinking.</p>\n</blockquote>\n<p>I'll call this the Cynic's Theory of Akrasia, as opposed to the Naive Theory. I used to think it was plausible. Now that I think about it a little more, I find it meaningless. Here's what changed my mind.<br /><a id=\"more\"></a></p>\n<p>What part of the mind, exactly, prefers a socially unacceptable activity (like drinking whiskey or browsing Reddit) to an acceptable activity (like having a wife and kids, or studying)? The conscious mind? As Bill said in his comment, it doesn't seem like it works this way. I've had akrasia myself, and I never consciously think \"Wow, I really like browsing Reddit...but I'll trick everyone else into thinking I'd rather be studying so I get more respect. Ha ha! The fools will never see it coming!\"<br /><br />No, my conscious mind fully believes that I would rather be studying<sup>2</sup>. And this even gets reflected in my actions. I've tried anti-procrastination techniques, both successfully and unsuccessfully, without ever telling them to another living soul. People trying to diet don't take out the cupcakes as soon as no one else is looking (or, if they do, they feel guilty about it).<br /><br />This is as it should be. It is a classic finding in evolutionary psychology: <a href=\"http://www.overcomingbias.com/2007/07/self-interest-i.html\">the person who wants to fool others begins by fooling themselves</a>. Some people even call the conscious mind the \"public relations officer\" of the brain, and argue that its entire point is to sit around and get fooled by everything we want to signal. As Bill said, \"believing the signals, even if untrue, makes the signals more effective.\" <br /><br />Now we have enough information to see why the Cynic's Theory is equivalent to the Naive Theory.<br /><br />The Naive Theory says that you really want to stop drinking, but some force from your unconscious mind is hijacking your actions. The Cynic's Theory says that you really want to keep drinking, but your conscious mind is hijacking your thoughts and making you think otherwise.<br /><br />In both cases, the conscious mind determines the signal and the unconscious mind determines the action. The only difference is which preference we define as \"real\" and worthy of sympathy. In the Naive Theory, we sympathize with the conscious mind, and the <em>problem </em>is the unconscious mind keeps committing contradictory actions. In the Cynic's Theory, we symapthize with the unconscious mind, and the <em>problem</em> is the conscious mind keeps sending out contradictory signals. The Naive say: find some way to make the unconscious mind stop hijacking actions! The Cynic says: find some way to make the conscious mind stop sending false signals!<br /><br />So why prefer one theory over the other? Well, I'm not surprised that it's mostly economists who support the Cynic's Theory. Economists are understandably interested in revealed preferences<sup>3</sup>, because revealed preferences are revealed by economic transactions and are the ones that determine the economy. It's perfectly reasonable for an economist to care only about those and dimiss any other kind of preference as a red herring that has to be removed before economic calculations can be done. Someone like a philosopher, who is more interested in thought and the mind, might be more susceptible to the identify-with-conscious-thought Naive Theory.</p>\n<p>But notice how the theory you choose also has serious political implications<sup>4</sup>. Consider how each of the two ways of looking at the problem would treat this example:</p>\n<blockquote>\n<p>A wealthy liberal is a member of many environmental organizations, and wants taxes to go up to pay for better conservation programs. However, she can't bring herself to give up her gas-guzzling SUV, and is usually too lazy to sort all her trash for recycling.</p>\n</blockquote>\n<p>I myself throw my support squarely behind the Naive Theory. Conscious minds are potentially rational<sup>5</sup>, informed by morality, and <a href=\"/lw/14o/suffering/109j\">qualia-laden</a>. Unconscious minds aren't, so <a href=\"/lw/14o/suffering/\">who cares what they think</a>?</p>\n<p>&nbsp;</p>\n<p><strong>Footnotes:</strong></p>\n<p><strong>1: </strong>Caplan says that the lack of interest in Stickk offers support for the Cynic's Theory, but I don't see why it should, unless we believe the mental balance of power should be different when deciding whether to use Stickk than when deciding whether to do anything else.</p>\n<p>Caplan also suggests in another article that he has <a href=\"http://econlog.econlib.org/archives/2006/07/selfcontrol_and.html\">never experienced procrastination as akrasia</a>. Although I find this surprising, I don't find it absolutely impossible to believe. His mind may either be exceptionally well-integrated, or it may send signals differently. It seems within the range of normal <a href=\"/lw/dr/generalizing_from_one_example/\">human mental variation</a>.</p>\n<p><strong>2: </strong>Of course, I could be lying here, to signal to you that I have socially acceptable beliefs. I suppose I can only make my point if you often have the same experience, or if you've caught someone else fighting akrasia when they didn't know you were there.</p>\n<p><strong>3: </strong>Even the term \"revealed preferences\" imports this value system, as if the act of buying something is a revelation that drives away the mist of the false consciously believed preferences.</p>\n<p><strong>4: </strong>For a real-world example of a politically-charged conflict surrounding the question of whether we should judge on conscious or unconscious beliefs, see Robin's post <a href=\"http://www.overcomingbias.com/2009/06/redistribution-isnt-about-sympathy.html\">Redistribution Isn't About Sympathy </a>and <a href=\"http://www.overcomingbias.com/2009/06/redistribution-isnt-about-sympathy.html#comment-429428\">my reply</a>.</p>\n<p><strong>5: </strong>Differences between the conscious and unconscious mind should usually correspond to differences between the goals of a person and the \"goals\" of the genome, or else between subgoals important today and subgoals important in the EEA.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"iP2X4jQNHMWHRNPne": 7, "Ng8Gice9KNkncxqcj": 2, "xexCWMyds6QLWognu": 2, "exZi6Bing5AiM4ZQB": 2, "PDJ6KqJBRzvKPfuS3": 2, "yEs5Tdwfw5Zw8yGWC": 2, "r7qAjcbfhj2256EHH": 1, "dqx5k65wjFfaiJ9sQ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z3cTkXbA7jgwGWPcv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 57, "baseScore": 64, "extendedScore": null, "score": 0.0001, "legacy": true, "legacyId": "1488", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 64, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 132, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KC5qGJiWSxt9zpyDy", "BH8BcP7d2sbmzZig8", "Dvc7zrqsdCYy6dCFR", "baTWMegR42PAsH9qJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-09T17:15:34.708Z", "modifiedAt": null, "url": null, "title": "Calibration fail", "slug": "calibration-fail", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:20.818Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/54jkJNpPyMRRGeme8/calibration-fail", "pageUrlRelative": "/posts/54jkJNpPyMRRGeme8/calibration-fail", "linkUrl": "https://www.lesswrong.com/posts/54jkJNpPyMRRGeme8/calibration-fail", "postedAtFormatted": "Sunday, August 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Calibration%20fail&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACalibration%20fail%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F54jkJNpPyMRRGeme8%2Fcalibration-fail%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Calibration%20fail%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F54jkJNpPyMRRGeme8%2Fcalibration-fail", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F54jkJNpPyMRRGeme8%2Fcalibration-fail", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 207, "htmlBody": "<p>I was talking on Friday with two people who've been to Italy recently and saw the leaning tower of Pisa.&nbsp; One of them was surprised at how short it was:&nbsp; \"about 30 feet tall\", she said.&nbsp; Then the other person, who'd also seen it, agreed that it was surprisingly short, and said it was \"only about as tall as the Washington Monument\".</p>\n<p>\"Wait,\" I said. \"You just said it's 30 feet tall; and you just said it's as tall as the Washington monument, which has got to be at least 100 yards tall.&nbsp; And you agreed with each other.\"</p>\n<p>And they both shook their heads, and said, \"No, the Washington Monument isn't anywhere near 100 yards tall.\"&nbsp; We all live near Washington DC, and have seen the Washington Monument many times.</p>\n<p>I said that the Washington Monument must be taller than that.&nbsp; One of them said that it was just as tiring to walk to the top of the Leaning Tower as to walk to the top of the Washington Monument; which was odd, since the WM stairs have been closed since before he was born.</p>\n<p>They finally agreed that both structures were about 30 yards tall.</p>\n<p>The Leaning Tower is 183 feet tall, and the Washington Monument is 555 feet tall.</p>\n<p>WTF?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "54jkJNpPyMRRGeme8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 10, "extendedScore": null, "score": 5.137894498726195e-07, "legacy": true, "legacyId": "1489", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-09T19:11:41.763Z", "modifiedAt": null, "url": null, "title": "Guess Again", "slug": "guess-again", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:53.658Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SRqEbwqErb9v9rNpM/guess-again", "pageUrlRelative": "/posts/SRqEbwqErb9v9rNpM/guess-again", "linkUrl": "https://www.lesswrong.com/posts/SRqEbwqErb9v9rNpM/guess-again", "postedAtFormatted": "Sunday, August 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Guess%20Again&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGuess%20Again%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSRqEbwqErb9v9rNpM%2Fguess-again%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Guess%20Again%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSRqEbwqErb9v9rNpM%2Fguess-again", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSRqEbwqErb9v9rNpM%2Fguess-again", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 500, "htmlBody": "<p>In <a href=\"/lw/em/bead_jar_guesses/\">Bead Jar Guesses</a>, I made a slightly clumsy attempt at carving out a kind of guess based on so little information that even a rationally-supposed, very small probability of some outcome doesn't confer a commensurate level of <em>surprise</em> when that outcome occurs.&nbsp; Here are several categories of probability assignment (including a re-statement of the bead jar thing) that I think might be worth considering separately.&nbsp; (I'm open to changing their names if other people have better ideas.)</p>\n<p><strong>Bewilderment</strong>: You don't even have enough information to understand the question.&nbsp; What is your probability that any given shren is a darkling?&nbsp; What is your probability that Safaitic is sometimes recorded in boustrophedon?&nbsp; What is your probability that \u4f60\u6709\u9f3b\u5b50? &nbsp;&nbsp; (Ignore question 1 if you have read <a href=\"http://elcenia.com\">Elcenia</a>, especially if you've seen more than is currently published; ignore question 2 if you know what either of those funny words mean; ignore question 3 if you can read Chinese.)&nbsp; In this case, you might find yourself in a situation where you have to make a guess, but even if you were then told the answer it wouldn't tell you much in the absence of further context.<sup>1</sup>&nbsp; You would have no reason to be surprised by such an answer, no matter what probability you'd assigned.<a id=\"more\"></a></p>\n<p><strong>Bead Jar</strong>: You understand the question, but have no information about anything that causally interacts with the answer.&nbsp; To guess, you have to grasp at the flimsiest of straws in the wording of the question and the motivations of the asker, or much broader beliefs about the general kind of question.&nbsp; What is your probability that Omega will pull out a red bead?&nbsp; What's your probability that I'm making the peace sign as I type this question with the other hand?&nbsp; What's your probability that the fruit on the tree in my best friend's backyard is delicious?&nbsp; Like Bewilderment questions, Bead Jar guesses come with no significant chance of surprise.&nbsp; Even if you have a tiny probability that the bead is lilac, it should not surprise you.</p>\n<p><strong>Bingo</strong>: You understand the question and you know something about what causes the answer, but the mechanism by which those conditions come about is known to be random (in a practical epistemic sense, not necessarily in the sense of being physically undetermined).&nbsp; You can have an excellent, well-calibrated probability.&nbsp; Here, there are two variants: one where the outcomes have mostly commensurate likelihood (the probability that you'll draw any given card from a deck) or one where the outcomes have a variety of probabilities (like the probability that <a href=\"/lw/108/exterminating_life_is_rational/10qv\">you draw a card with a skull, or one with a star</a>).&nbsp; You shouldn't be surprised no matter what happens in the first case (unless the outcome is somehow special to you - be surprised if a personal friend of yours wins the lottery!), but in the second case, surprise might be warranted if something especially unlikely happens.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup>About 5/6 of shrens are darklings, depending on population fluctuations; Safaitic is indeed sometimes recorded in boustrophedon; and \u6709 (I hope).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SRqEbwqErb9v9rNpM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 17, "extendedScore": null, "score": 5.138082270993069e-07, "legacy": true, "legacyId": "1490", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mXaPPjud9MuuboWgd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-09T20:13:52.895Z", "modifiedAt": null, "url": null, "title": "Misleading the witness", "slug": "misleading-the-witness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:31.246Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Bo102010", "createdAt": "2009-04-21T03:29:33.819Z", "isAdmin": false, "displayName": "Bo102010"}, "userId": "xWXvgcZTCQb5ixvQc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zA3aM4YEENz2GLAfL/misleading-the-witness", "pageUrlRelative": "/posts/zA3aM4YEENz2GLAfL/misleading-the-witness", "linkUrl": "https://www.lesswrong.com/posts/zA3aM4YEENz2GLAfL/misleading-the-witness", "postedAtFormatted": "Sunday, August 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Misleading%20the%20witness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMisleading%20the%20witness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzA3aM4YEENz2GLAfL%2Fmisleading-the-witness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Misleading%20the%20witness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzA3aM4YEENz2GLAfL%2Fmisleading-the-witness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzA3aM4YEENz2GLAfL%2Fmisleading-the-witness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 706, "htmlBody": "<p><strong>Related:</strong> <a href=\"/lw/mu/trust_in_math/\">Trust in Math</a></p>\n<p>I was reading <a href=\"http://en.wikipedia.org/wiki/John_Allen_Paulos\">John Allen Paulos</a>' <a href=\"http://www.amazon.com/Mathematician-Plays-Stock-Market/dp/0465054803\">A Mathematician Plays the Stock Market</a>, in which Paulos relates a version of the well-known \"missing dollar\" riddle. I had heard it once before, but only vaguely remembered it. If you don't remember it, here it is:</p>\n<blockquote>\n<p>Three people stay in a hotel overnight. The innkeeper tells them that the price for three rooms is $30, so each pays $10.</p>\n<p>After the guests go to their rooms, the innkeeper realizes that there is a special discount for groups, and that the guests' total should have only been $25.</p>\n<p>The innkeeper gives a bellhop $5 with the instructions to return it to the guests.</p>\n<p>The bellhop, not wanting to get change, gives each guest $1 and keeps $2.</p>\n<p>Later, the bellhop thinks \"Wait - something isn't right. Each guest paid $10. I gave them each back $1, so they each paid $9. $9 times 3 is $27. I kept $2. $27 + $2 is $29. Where did the missing dollar go?\"</p>\n</blockquote>\n<p><a id=\"more\"></a></p>\n<p>I remembered that the solution involves trickery, but it still took me a minute or two to figure out where it is. At first, I started mentally keeping track of the dollars in the riddle, trying to see where one got dropped so their sum would be 30.</p>\n<p>Then I figured it out. The story should end:</p>\n<blockquote>\n<p>Later, the bellhop thinks \"Wait - something isn't right. Each guest paid $10. I gave them each back $1, so they each paid $9. $9 times 3 is $27. The cost for their rooms was $25. $27 - $25 = $2, so they collectively overpaid by $2, which is the amount I kept. Why am I such a jerk?\"</p>\n</blockquote>\n<p>I told my fiance the riddle, and asked her where the missing dollar went. She went through the same process as I did, looking for a place in the story where $1 could go missing.</p>\n<p>It's remarkable to me how blatantly deceptive the riddle is. The riddler states or implies at the end of the story that the dollars paid by the guests and the dollars kept by the bellhop should be summed, and that that sum should be $30. In fact, there's no reason to sum the dollars paid by the guests and the dollars kept by the bellhop, and no reason for any accounting we do to end up with $30.</p>\n<p>The contrasts somewhat with the various <a href=\"http://en.wikipedia.org/wiki/1%3D2#Proof_that_2_.3D_1\">proofs that 1 = 2</a>, in which the misstep is hidden somewhere within a chain of reasoning, not boldly announced at the end of the narrative.</p>\n<p>Both Paulos and <a href=\"http://en.wikipedia.org/wiki/Missing_dollar_riddle\">Wikipedia</a> give examples with different numbers that make the deception in the missing dollar riddle more obvious (and less effective). In the case of the missing dollar riddle, the fact that $25, $27, and $30 are close to each other makes following the incorrect path very seductive.</p>\n<p>This riddle made me remember reading about how beginning magicians are very nervous in their first public performances, since some of their tricks involve misdirecting the audience by openly lying (e.g., casually pick up a stack of cards shuffled by a volunteer, say \"Hmm, good shuffle\" while adding a known card to the top of the stack, hand the deck back to the volunteer, and then boldly announce \"notice that I have not touched or manipulated the cards!\"<sup>1</sup>). However, they learn to be more comfortable once they find out how easily the audience will pretty much accept whatever false statements they make.</p>\n<p>Thinking about these things makes me wonder about how to think rationally given the tendency for human minds to accept some deceptive statements at face value. Can anyone think of good ways to notice when outright deception is being used? How could a rationalist practice her skills at a magic show?</p>\n<p>How about other examples of flagrant misdirection? I suspect that political debates might be able to make use of such techniques (I think that there might be some examples in the recent debates over health care reform accounting and the costs of obesity to the health care system, but I haven't been able to find any yet.)</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Footnote 1:</strong> I remember reading this example very recently, maybe at this site. Please let me know whom to credit for it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"cHoCqtfE9cF7aSs9d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zA3aM4YEENz2GLAfL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 16, "extendedScore": null, "score": 2.9e-05, "legacy": true, "legacyId": "1491", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 116, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2MqXKvBym3kRxvJMv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-10T19:20:20.968Z", "modifiedAt": null, "url": null, "title": "Utilons vs. Hedons", "slug": "utilons-vs-hedons", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:20.556Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Psychohistorian", "createdAt": "2009-04-05T18:10:22.976Z", "isAdmin": false, "displayName": "Psychohistorian"}, "userId": "mAQgf4N8jv2jTMK5B", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yYTv6J6wFL8XK3Q7o/utilons-vs-hedons", "pageUrlRelative": "/posts/yYTv6J6wFL8XK3Q7o/utilons-vs-hedons", "linkUrl": "https://www.lesswrong.com/posts/yYTv6J6wFL8XK3Q7o/utilons-vs-hedons", "postedAtFormatted": "Monday, August 10th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Utilons%20vs.%20Hedons&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUtilons%20vs.%20Hedons%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyYTv6J6wFL8XK3Q7o%2Futilons-vs-hedons%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Utilons%20vs.%20Hedons%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyYTv6J6wFL8XK3Q7o%2Futilons-vs-hedons", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyYTv6J6wFL8XK3Q7o%2Futilons-vs-hedons", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1773, "htmlBody": "<p>Related to:<a href=\"/lw/15c/would_your_real_preferences_please_stand_up\"> Would Your Real Preferences Please Stand Up?</a></p>\n<p>I have to admit, there are a lot of people I don't care about. Comfortably over six billion, I would bet. It's not that I'm a callous person; I simply don't know that many people, and even if I did I hardly have time to process that much information. Every day hundreds of millions of incredibly wonderful and terrible things happen to people out there, and if they didn't, I wouldn't even know it.</p>\n<p>On the other hand, my professional goals deal with economics, policy, and improving decision making for the purpose of making millions of people I'll never meet happier. Their happiness does not affect my experience of life one bit, but I believe it's a good thing and I plan to work hard to figure out how to create more happiness.</p>\n<p>This underscores an essential distinction in understanding any utilitarian viewpoint: the difference between experience and values. One can value unweighted total utility. One cannot experience unweighted total utility. It will always hurt more if a friend or loved one dies than if someone you never knew in a place you never heard of dies. I would be truly amazed to meet someone who is an exception to this rule and is not an absolute stoic. Your experiential utility function may have coefficients for other people's happiness (or at least your perception of such), but there's no way it has an identical coefficient for everyone everywhere, unless that coefficient is zero. On the other hand, you probably care in an abstract way about whether people you don't know die or are enslaved or imprisoned, and may even contribute some money or effort to prevent such from happening. I'm going to use \"utilons\" to refer to value utility units and \"hedons\" to refer to experiential utility units; I'll demonstrate that this is a meaningful distinction shortly, and that we value utilons over hedons explains much of our moral reasoning appearing to fail.<a id=\"more\"></a></p>\n<p>Let's try a hypothetical to illustrate the difference between experiential and value utility. An employee of Omega, LLC,<sup>1</sup> offers you a deal to absolutely double your hedons but kill five people in, say, rural China, then wipe your memory of the deal.&nbsp; Do you take it? What about five hundred? Five hundred thousand?</p>\n<p>I can't speak for you, so I'll go through my evaluation of this deal and hope it generalizes reasonably well. I don't take it at any of these values. There's no clear hedonistic explanation for this - after all, I forget it happened. It would be absurd to say that the disutility I&nbsp; experience between entering the agreement and having my memory wiped is so tremendous as to outweigh everything I will experience for the rest of my life (particularly since I immediately forget this disutility), and this is the only way I can see my rejection could be explained with hedons. In fact, even if the memory wipe weren't part of the deal, I doubt the act of having a few people killed would really cause me more displeasure than doubling my future hedons would yield; I'd bet more than five people have died in rural China as I've written this post, and it hasn't upset me in the slightest.</p>\n<p>The reason I don't take the deal is my values; I believe it's wrong to kill random people to improve my own happiness. If I knew that they were people who sincerely wanted to be dead or that they were, say, serial killers, my decision would be different, even though my hedonic experience would probably not be that different. If I knew that millions of people in China would be significantly happier as a result, as well, then there's a good chance I'd take the deal even if it didn't help <em>me</em>. I seem to be maximizing utilons and not hedons, and I think most people would do the same.</p>\n<p>Also, as another example so obvious that I feel like it's cheating, if most people read the headline \"100 workers die in Beijing factory fire\" or \"1000 workers die in Beijing factory fire,\" they will not feel ten times the hedonic blow, even if they live in Beijing. That it is ten times worse is measured in our values, not our experiences; these values are correct, since there are roughly ten times as many people who have seriously suffered from the fire, but if we're talking about people's hedons, no individual suffers ten times as much.</p>\n<p>In general, people value utilons much more than hedons. Drugs being illegal are an illustration of this. Arguments for (and against) drug legalization are an even better illustration of this. Such arguments usually involve weakening organized crime, increasing safety, reducing criminal behaviour, reducing expenditures on prisons, improving treatment for addicts, and improving similar values. \"Lots of people who want to will get really, really high\" is only very rarely touted as a major argument, even though the net hedonic value of drug legalization would probably be massive (just as the hedonic cost of prohibition in the 20's was clearly massive).</p>\n<p>As a practical matter, this is important because many people do things precisely because they are important in their abstract value system, even if they result in little or no hedonic payoff. This, I believe, is an excellent explanation of why success is no guarantee of happiness; success is conducive to getting hedons, but it also tends to cost a lot of hedons, so there is little guarantee that earned wealth will be a net positive (and, with anchoring, hedons will get a lot more expensive than they are for the less successful). On the other hand, earning wealth (or status) is a very common value, so people pursue it irrespective of its hedonistic payoff.</p>\n<p>It may be convenient to argue that the hedonistic payoffs <em>must</em> balance out, but this does not make it the case in reality. Some people work hard on assignments that are practically meaningless to their long-term happiness because they believe they <em>should</em>, not because they have any delusions about their hedonistic payoff. To say, \"If you did X instead of Y because you 'value' X, then the hedonistic cost of breaking your values must exceed Y-X,\" is to win an argument by definition; you have to actually figure out the values and see if that's true. If it's not, then I'm not a hedon-maximizer. You can't then assert that I'm an \"irrational\" hedon maximizer unless you can make some very clear distinction between \"irrationally maximizing hedons\" and \"maximizing something other than hedons.\"</p>\n<p>This dichotomy also describes akrasia fairly well, though I'd hesitate to say it truly explains it. Akrasia is what happens when we maximize our hedons at the expense of our utilons. We play video games/watch TV/post on blogs because it feels good, and we feel bad about it because, first, \"it feels good\" is not recognized as a major positive value in most of our utilon-functions, and second, because doing our homework <em>is</em> recognized as a major positive value in our utilon functions. The experience makes us procrastinate and our values make us feel guilty about it. Just as we should not needlessly multiply causes, neither should we erroneously merge them.</p>\n<p>Furthermore, this may cause our intuition to seriously interfere with utility-based hypotheticals, such <a href=\"/lw/108/exterminating_ourselves_may_be_rational/\">as</a> <a href=\"/lw/108/exterminating_life_is_rational/10qv\">these</a>. Basically, you choose to draw cards, one at a time, that have a 10% chance of killing you and a 90% chance of doubling your utility. Logically, if your current utility is positive and you assign a utility of zero<sup>2</sup> (or greater) to your death (which makes sense in hedons, but not necessarily in utilons), you should draw cards until you die. The problem of course being that if you draw a card a second, you will be dead in a minute with P= ~.9982, and dead in an hour with P=~1-1.88*10<sup>-165</sup>.</p>\n<p>There's a bigger problem that causes our intuition to reject this hypothetical as \"just wrong:\" it leads to major errors in both utilons and hedons. The mind cannot comprehend unlimited doubling of hedons. I doubt you can imagine being 2<sup>60</sup> times as happy as you are now; indeed, I doubt it is meaningfully possible to be so happy. As for utilons, most people assign a much greater value to \"not dying,\" compared with having more hedons. Thus, a hedonic reading of the problem returns an error because repeated doubling feels meaningless, and a utilon reading (may) return an error if we assign a significant enough negative value to death. But if we look at it purely in terms of numbers, we end up very, very happy right up until we end up very, very dead.</p>\n<p>Any useful utilitarian calculus need take into account that hedonic utility is, for most people, incomplete. Value utility is often a major motivating factor, and it need not translate perfectly into hedonic terms. Incorporating value utility seems necessary to have a map of human happiness that actually matches the territory. It also may be good that it can be easier to change values than it is to change hedonic experiences. But assuming people maximize hedons, and then assuming quantitative values that conform to this assumption, proves nothing about what actually motivates people and risks serious systematic error in furthering human happiness.</p>\n<p>We know that our experiential utility cannot encompass all that really matters to us, so we have a value system that we place above it precisely to avoid risking destroying the whole world to make ourselves marginally happier, or to avoid pursuing any other means of gaining happiness that carries tremendous potential expense.</p>\n<p>1- Apparently Omega has started a firm due to excessive demand for its services, or to avoid having to talk to me.</p>\n<p>2- This assumption is rather problematic, though zero seems to be the only correct value of death in hedons. But imagine that you just won the lottery (without buying a ticket, presumably) and got selected as the most important, intelligent, attractive person in whatever field or social circle you care most about. How bad would it be to drop dead? Now, imagine you just got captured by some psychopath and are going to be tortured for years until you eventually die. How bad would it be to drop dead? Assigning zero (or the same value, period) to both outcomes seems wrong. I realize that you can say that death in one is negative and in the other is positive relative to expected utility, but still, the value of death does not seem identical, so I'm suspicious of assigning it the same value in both cases. I realize this is hand-wavy; I think I'd need a separate post to address this issue properly.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xknvtHwqvqhwahW8Q": 2, "xexCWMyds6QLWognu": 2, "5f5c37ee1b5cdee568cfb186": 2, "AHK82ypfxF45rqh9D": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yYTv6J6wFL8XK3Q7o", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 35, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "1493", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 119, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["z3cTkXbA7jgwGWPcv", "LkCeA4wu8iLmetb28"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-11T02:05:45.264Z", "modifiedAt": null, "url": null, "title": "Cache invalidation test", "slug": "cache-invalidation-test", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wmoore", "createdAt": "2009-02-17T05:49:50.396Z", "isAdmin": false, "displayName": "wmoore"}, "userId": "EgQZcMBqxf6sGmKfi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bjPL428k8STPwp3aD/cache-invalidation-test", "pageUrlRelative": "/posts/bjPL428k8STPwp3aD/cache-invalidation-test", "linkUrl": "https://www.lesswrong.com/posts/bjPL428k8STPwp3aD/cache-invalidation-test", "postedAtFormatted": "Tuesday, August 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cache%20invalidation%20test&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACache%20invalidation%20test%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbjPL428k8STPwp3aD%2Fcache-invalidation-test%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cache%20invalidation%20test%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbjPL428k8STPwp3aD%2Fcache-invalidation-test", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbjPL428k8STPwp3aD%2Fcache-invalidation-test", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4, "htmlBody": "<p>Cache should be cleared</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bjPL428k8STPwp3aD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 5.141082092500791e-07, "legacy": true, "legacyId": "1495", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-11T04:27:43.258Z", "modifiedAt": null, "url": null, "title": "Deleting paradoxes with fuzzy logic", "slug": "deleting-paradoxes-with-fuzzy-logic", "viewCount": null, "lastCommentedAt": "2018-06-22T01:22:39.769Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "odm3FHPzgbiGpWsSg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YWtZpAynkxth3KXjN/deleting-paradoxes-with-fuzzy-logic", "pageUrlRelative": "/posts/YWtZpAynkxth3KXjN/deleting-paradoxes-with-fuzzy-logic", "linkUrl": "https://www.lesswrong.com/posts/YWtZpAynkxth3KXjN/deleting-paradoxes-with-fuzzy-logic", "postedAtFormatted": "Tuesday, August 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Deleting%20paradoxes%20with%20fuzzy%20logic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADeleting%20paradoxes%20with%20fuzzy%20logic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYWtZpAynkxth3KXjN%2Fdeleting-paradoxes-with-fuzzy-logic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Deleting%20paradoxes%20with%20fuzzy%20logic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYWtZpAynkxth3KXjN%2Fdeleting-paradoxes-with-fuzzy-logic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYWtZpAynkxth3KXjN%2Fdeleting-paradoxes-with-fuzzy-logic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1155, "htmlBody": "<p>You've all seen it. Sentences like \"this sentence is false\": if they're false, they're true, and vice versa, so they can't be either true or false. Some people solve this problem by doing something really complicated: they introduce infinite type hierarchies wherein every sentence you can express is given a \"type\", which is an ordinal number, and every sentence can only refer to sentences of lower type. \"This sentence is false\" is not a valid sentence there, because it refers to itself, but no ordinal number is less than itself. Eliezer Yudkowsky <a href=\"/lw/gm/anime_explains_the_epimenides_paradox/\">mentions but says little about</a> such things. What he does say, I agree with: <em>ick!</em></p>\n<p>In addition to the sheer icky factor involved in this complicated method of making sure sentences can't refer to themselves, we have deeper problems. In English, sentences <em>can</em> refer to themselves. Heck, <em>this</em> sentence refers to itself. And this is not a flaw in English, but something useful: sentences ought to be able to refer to themselves. I want to be able to write stuff like \"All complete sentences written in English contain at least one vowel\" without having to write it in Spanish or as an incomplete sentence.<sup><a name=\"warriefnm1\"></a><a href=\"#warriefn1\">1</a></sup> How can we have self-referential sentences without having paradoxes that result in the universe doing what cheese does at the bottom of the oven? Easy: use fuzzy logic.</p>\n<p><a id=\"more\"></a>Now, take a nice look at the sentence \"this sentence is false\". If your intuition is like mine, this sentence seems false. (If your intuition is unlike mine, it doesn't matter.) But obviously, it isn't false. At least, it's not <em>completely</em> false. Of course, it's not true, either. So it's not true or false. Nor is it the mythical third truth value, <em>clem</em><sup><a name=\"warriefnm2\"></a><a href=\"#warriefn2\">2</a></sup>, as clem is not false, making the sentence indeed false, which is a paradox again. Rather, it's something in between true and false--\"of medium truth\", if you will.</p>\n<p>So, how do we represent \"of medium truth\" formally? Well, the obvious way to do that is using a real number. Say that a completely false sentence has a truth value of 0, a completely true sentence has a truth value of 1, and the things in between have truth values in between.<sup><a name=\"warriefnm3\"></a><a href=\"#warriefn3\">3</a></sup> Will this work? Why, yes, and I can prove it! Well, no, I actually can't. Still, the following, trust me, <em>is</em> a theorem:</p>\n<p style=\"padding-left: 30px;\">Suppose there is a set of sentences, and there are N of them, where N is some (possibly infinite) cardinal number, and each sentence's truth value is a continuous function of the other sentences' truth values. Then there is a consistent assignment of a truth value to every sentence. (More tersely, every continuous function [0,1]^N -&gt; [0,1]^N for every cardinal number N has at least one fixed point.)</p>\n<p>So for every set of sentences, no matter how wonky their self- and cross-references are, there is some consistent assignment of truth values to them. At least, this is the case <em>if</em> all their truth values vary continuously with each other. This won't happen under strict interpretations of sentences such as \"this sentence's truth value is less than 0.5\": this sentence, interpreted as black and white, has a truth value of 1 when its truth value is below 0.5 and a truth value of 0 when it's not. This <em>is</em> inconsistent. So, we'll ban such sentences. No, I don't mean ban sentences that refer to themselves; that would just put us back where we started. I mean we should ban sentences whose truth values have \"jumps\", or discontinuities. The sentence \"this sentence's truth value is less than 0.5\" has a sharp jump in truth value at 0.5, but the sentence \"this sentence's truth value is significantly less than 0.5\" does not: as its truth value goes down from 0.5 down to 0.4 or so, it also goes up from 0.0 up to 1.0, leaving us a consistent truth value for that sentence around 0.49.</p>\n<p><em>Edit: I accidentally said \"So, we'll not ban such sentences.\" That's almost the opposite of what I wanted to say.</em></p>\n<p>Now, at this point, you probably have some ideas. I'll get to those one at a time. First, is all this truth value stuff really necessary? To that, I say yes. Take the sentence \"the Leaning Tower of Pisa is short\". This sentence is certainly not completely true; if it were, the Tower would have to have a height of zero. It's not completely false, either; if it were, the Tower would have to be infinitely tall. If you tried to come up with any binary assignment of \"true\" and \"false\" to sentences such as these, you'd run into the Sorites paradox: how tall would the Tower be if any taller tower were \"tall\" and any shorter tower were \"short\"? A tower a millimeter higher than what you say would be \"tall\", and a tower a millimeter shorter would be \"short\", which we find absurd. It would make a lot more sense if a change of height of one millimeter simply changed the truth value of \"it's short\" by about 0.00001.</p>\n<p>Second, isn't this just probability, which we already know and love? No, it isn't. If I say that \"the Leaning Tower of Pisa is extremely short\", I don't mean that I'm very, very sure that it's short. If I say \"my mother was half Irish\", I don't mean that I have no idea whether she was Irish or not, and might find evidence later on that she was completely Irish. Truth values are separate from probabilities.</p>\n<p>Third and finally, how can this be treated formally? I say, to heck with it. Saying that truth values are real numbers from 0 to 1 is sufficient; regardless of whether you say that \"X and Y\" is as true as the product of the truth values of X and Y or that it's as true as the less true of the two, you have an operation that behaves like \"and\". If two people have different interpretations of truth values, you can feel free to just add more functions that convert between the two. I don't know of any \"laws of truth values\" that fuzzy logic ought to conform to. If you come up with a set of laws that happen to work particularly well or be particularly elegant (percentiles? decibels of evidence?), feel free to make it known.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p><a name=\"warriefn1\"></a>1. <a href=\"#warriefnm1\">^</a> The term \"sentence fragment\" is considered politically incorrect nowadays due to protests by incomplete sentences. \"Only a fragment? Not us! One of us standing alone? Nothing wrong with that!\"</p>\n<p><a name=\"warriefn2\"></a>2. <a href=\"#warriefnm2\">^</a> I made this word up. I'm so proud of it. Don't you think it's cute?</p>\n<p><a name=\"warriefn3\"></a>3. <a href=\"#warriefnm3\">^</a> Sorry, Eliezer, but this <em>cannot </em>be consistently interpreted such that 0 and 1 are not valid truth values: if you did that, then the <em>modest sentence</em> \"this sentence is at least somewhat true\" would always be truer than itself, whereas if 1 is a valid truth value, it is a consistent truth value of that sentence.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YWtZpAynkxth3KXjN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 7, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "1496", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 76, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["y3HAxPN8WHqX7zhkv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-11T12:47:06.014Z", "modifiedAt": null, "url": null, "title": "Sense, Denotation and Semantics", "slug": "sense-denotation-and-semantics", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:20.398Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zFZicEFAxTfSEbK69/sense-denotation-and-semantics", "pageUrlRelative": "/posts/zFZicEFAxTfSEbK69/sense-denotation-and-semantics", "linkUrl": "https://www.lesswrong.com/posts/zFZicEFAxTfSEbK69/sense-denotation-and-semantics", "postedAtFormatted": "Tuesday, August 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sense%2C%20Denotation%20and%20Semantics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASense%2C%20Denotation%20and%20Semantics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzFZicEFAxTfSEbK69%2Fsense-denotation-and-semantics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sense%2C%20Denotation%20and%20Semantics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzFZicEFAxTfSEbK69%2Fsense-denotation-and-semantics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzFZicEFAxTfSEbK69%2Fsense-denotation-and-semantics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 696, "htmlBody": "<p>J. Y. Girard, et al. (1989). <em>Proofs and types</em>. Cambridge University Press, New York, NY, USA. (<a href=\"http://www.paultaylor.eu/stable/prot.pdf\">PDF</a>)</p>\n<p>I found introductory description of a number of ideas given in the beginning of this book very intuitively clear, and these ideas should be relevant to our discussion, preoccupied with the meaning of meaning as we are. Though the book itself is quite technical, the first chapter should be accessible to many readers.</p>\n<p>From the beginning of the chapter:</p>\n<blockquote>\n<p>Let us start with an example. There is a standard procedure for multiplication, which yields for the inputs 27 and 37 the result 999. What can we say about that?</p>\n<p>A first attempt is to say that we have an <em>equality</em></p>\n<p style=\"padding-left: 30px;\">27 &times; 37 = 999</p>\n<p>This equality makes sense in the mainstream of mathematics by saying that the two sides <em>denote</em> the same integer and that &times; is a <em>function</em> in the Cantorian sense of a graph.</p>\n<p>This is the denotational aspect, which is undoubtedly correct, but it misses the essential point:</p>\n<p>There is a finite <em>computation</em> process which shows that the denotations are equal. It is an abuse (and this is not cheap philosophy &mdash; it is a concrete question) to say that 27 &times; 37 <em>equals</em> 999, since if the two things we have were <em>the same</em> then we would never feel the need to state their equality. Concretely we ask a <em>question</em>, 27 &times; 37, and get an <em>answer</em>, 999. The two expressions have different <em>senses</em> and we must <em>do</em> something (make a proof or a calculation, or at least look in an encyclopedia) to show that these two <em>senses</em> have the same <em>denotation</em>.<a id=\"more\"></a></p>\n<p>Concerning &times;, it is incorrect to say that this is a function (as a graph) since the computer in which the program is loaded has no room for an infinite graph. Hence we have to conclude that we are in the presence of a <em>finitary</em> dynamics related to this question of sense.</p>\n<p>Whereas denotation was modelled at a very early stage, sense has been pushed towards <em>subjectivism</em>, with the result that the present mathematical treatment of sense is more or less reduced to <em>syntactic</em> manipulation. This is not <em>a priori</em> in the essence of the subject, and we can expect in the next decades to find a treatment of computation that would combine the advantages of denotational semantics (mathematical clarity) with those of syntax (finite dynamics). This book clearly rests on a tradition that is based on this unfortunate current state of affairs: in the dichotomy between <em>infinite, static denotation</em> and <em>finite, dynamic sense</em>, the denotational side is much more developed than the other.</p>\n<p>So, one of the most fundamental distinctions in logic is that made by Frege: given a sentence A, there are two ways of seeing it:</p>\n<ul>\n<li>as a sequence of <em>instructions</em>, which determine its <em>sense</em>, for example A &or; B means \"A or B\", etc. </li>\n</ul>\n<ul>\n<li>\n<p>as the <em>ideal result</em> found by these operations: this is its <em>denotation</em>.</p>\n<p>\"Denotation\", as opposed to \"notation\", is what <em>is denoted</em>, and not what <em>denotes</em>. For example the denotation of a logical sentence is t (true) or f (false), and the denotation of A &or; B can be obtained from the denotations of A and B by means of the truth table for disjunction.</p>\n</li>\n</ul>\n<p>Two sentences which have the same sense have the same denotation, that is obvious; but two sentences with the same denotation rarely have the same sense. For example, take a complicated mathematical equivalence A&hArr;B. The two sentences have the same denotation (they are true at the same time) but surely not the same sense, otherwise what is the point of showing the equivalence?</p>\n<p>This example allows us to introduce some associations of ideas:</p>\n<ul>\n<li> sense, syntax, proofs; </li>\n</ul>\n<ul>\n<li> denotation, truth, semantics, algebraic operations.</li>\n</ul>\n<p>That is the fundamental dichotomy in logic. Having said that, the two sides hardly play symmetrical roles!</p>\n</blockquote>\n<p>Read the whole chapter (or the first three chapters). <a href=\"http://www.paultaylor.eu/stable/prot.pdf\">Download the book here (PDF)</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zFZicEFAxTfSEbK69", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 12, "extendedScore": null, "score": 5.142119678099202e-07, "legacy": true, "legacyId": "1499", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-13T05:31:41.197Z", "modifiedAt": null, "url": null, "title": "Towards a New Decision Theory", "slug": "towards-a-new-decision-theory", "viewCount": null, "lastCommentedAt": "2021-07-13T21:59:21.413Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Wei_Dai", "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/de3xjFaACCAk6imzv/towards-a-new-decision-theory", "pageUrlRelative": "/posts/de3xjFaACCAk6imzv/towards-a-new-decision-theory", "linkUrl": "https://www.lesswrong.com/posts/de3xjFaACCAk6imzv/towards-a-new-decision-theory", "postedAtFormatted": "Thursday, August 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Towards%20a%20New%20Decision%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATowards%20a%20New%20Decision%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fde3xjFaACCAk6imzv%2Ftowards-a-new-decision-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Towards%20a%20New%20Decision%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fde3xjFaACCAk6imzv%2Ftowards-a-new-decision-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fde3xjFaACCAk6imzv%2Ftowards-a-new-decision-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1718, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>ZH-CN</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> <w:UseFELayout /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin;} --> <!--[endif]--></p>\n<p class=\"MsoNormal\">It commonly acknowledged here that current decision theories have deficiencies that show up in the form of various paradoxes. Since there seems to be little hope that Eliezer will publish his <a href=\"/lw/135/timeless_decision_theory_problems_i_cant_solve/\">Timeless Decision Theory</a> any time soon, I decided to try to synthesize some of the ideas discussed in this forum, along with a few of my own, into a coherent alternative that is hopefully not so paradox-prone.</p>\n<p>I'll start with a way of framing the question. Put yourself in the place of an AI, or more specifically, the decision algorithm of an AI. You have access to your own source code S, plus a bit string X representing all of your memories and sensory data. You have to choose an output string Y. That&rsquo;s the decision. The question is, how? (The answer isn't &ldquo;Run S,&rdquo; because what we want to know is what S should be in the first place.)</p>\n<p class=\"MsoNormal\">Let&rsquo;s proceed by asking the question, &ldquo;What are the consequences of S, on input X, returning Y as the output, instead of Z?&rdquo; To begin with, we'll consider just the consequences of that choice in the realm of abstract computations (i.e. computations considered as mathematical objects rather than as implemented in physical systems). The most immediate consequence is that any program that calls S as a subroutine with X as input, will receive Y as output, instead of Z. What happens next is a bit harder to tell, but supposing that you know something about a program P that call S as a subroutine, you can further deduce the effects of choosing Y versus Z by tracing the difference between the two choices in P&rsquo;s subsequent execution. We could call these the computational consequences of Y. Suppose you have preferences about the execution of a set of programs, some of which call S as a subroutine, then you can satisfy your preferences directly by choosing the output of S so that those programs will run the way you most prefer.</p>\n<p class=\"MsoNormal\"><a id=\"more\"></a>A more general class of consequences might be called logical consequences. Consider a program P&rsquo; that doesn&rsquo;t call S, but a different subroutine S&rsquo; that&rsquo;s logically equivalent to S. In other words, S&rsquo; always produces the same output as S when given the same input. Due to the logical relationship between S and S&rsquo;, your choice of output for S must also affect the subsequent execution of P&rsquo;. Another example of a logical relationship is an S' which always returns the first bit of the output of S when given the same input, or one that returns the same output as S on some subset of inputs.</p>\n<p class=\"MsoNormal\">In general, you can&rsquo;t be certain about the consequences of a choice, because you&rsquo;re not logically omniscient. How to handle logical/mathematical uncertainty is an <a href=\"/lw/f9/a_request_for_open_problems/bun\">open problem</a>, so for now we'll just assume that you have access to a \"mathematical intuition subroutine\" that somehow allows you to form beliefs about the likely consequences of your choices.</p>\n<p class=\"MsoNormal\">At this point, you might ask, &ldquo;That&rsquo;s well and good, but what if my preferences extend beyond abstract computations? What about consequences on the physical universe?&rdquo; The answer is, we can view the physical universe as a program that runs S as a subroutine, or more generally, view it as a mathematical object which has S embedded within it. (From now on I&rsquo;ll just refer to programs for simplicity, with the understanding that the subsequent discussion can be generalized to non-computable universes.) Your preferences about the physical universe can be translated into preferences about such a program P and programmed into the AI. The AI, upon receiving an input X, will look into P, determine all the instances where it calls S with input X, and choose the output that optimizes its preferences about the execution of P. If the preferences were translated faithfully, the the AI's decision should also optimize your preferences regarding the physical universe. This faithful translation is a second major open problem.</p>\n<p class=\"MsoNormal\">What if you have some uncertainty about which program our universe corresponds to? In that case, we have to specify preferences for the entire set of programs that our universe may correspond to. If your preferences for what happens in one such program is independent of what happens in another, then we can represent them by a probability distribution on the set of programs plus a utility function on the execution of each individual program. More generally, we can always represent your preferences as a utility function on vectors of the form &lt;E1, E2, E3, &hellip;&gt; where E1 is an execution history of P1, E2 is an execution history of P2, and so on.</p>\n<p class=\"MsoNormal\">These considerations lead to the following design for the decision algorithm S. S is coded with a vector &lt;P1, P2, P3, ...&gt; of programs that it cares about, and a utility function on vectors of the form &lt;E1, E2, E3, &hellip;&gt; that defines its preferences on how those programs should run. When it receives an input X, it looks inside the programs P1, P2, P3, ..., and uses its \"mathematical intuition\" to form a probability distribution P_Y over the set of vectors &lt;E1, E2, E3, &hellip;&gt; for each choice of output string Y. Finally, it outputs a string Y* that maximizes the expected utility Sum P_Y(&lt;E1, E2, E3, &hellip;&gt;) U(&lt;E1, E2, E3, &hellip;&gt;). (This specifically assumes that expected utility maximization is the right way to deal with mathematical uncertainty. Consider it a temporary placeholder until that problem is solved. Also, I'm describing the algorithm as a brute force search for simplicity. In reality, you'd probably want it to do something cleverer to find the optimal Y* more quickly.)</p>\n<h4>Example 1: Counterfactual Mugging</h4>\n<p class=\"MsoNormal\">Note that Bayesian updating is not done explicitly in this decision theory. When the decision algorithm receives input X, it may determine that a subset of programs it has preferences about never calls it with X and are also logically independent of its output, and therefore it can safely ignore them when computing the consequences of a choice. There is no need to set the probabilities of those programs to 0 and renormalize.</p>\n<p class=\"MsoNormal\">So, with that in mind, we can model <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">Counterfactual Mugging</a> by the following Python program:</p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\">def P(coin):<br />&nbsp;&nbsp;&nbsp; AI_balance = 100<br />&nbsp;&nbsp;&nbsp; if coin == \"heads\":<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if S(\"heads\") == \"give $100\":<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AI_balance -= 100<br />&nbsp;&nbsp;&nbsp; if coin == \"tails\":<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if Omega_Predict(S, \"heads\") == \"give $100\":<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AI_balance += 10000</p>\n<p class=\"MsoNormal\">The AI&rsquo;s goal is to maximize expected utility = .5 * U(AI_balance after P(\"heads\")) + .5 * U(AI_balance after P(\"tails\")). Assuming U(AI_balance)=AI_balance, it&rsquo;s easy to determine U(AI_balance after P(\"heads\")) as a function of S&rsquo;s output. It equals 0 if S(&ldquo;heads&rdquo;) == &ldquo;give $100&rdquo;, and 100 otherwise. To compute U(AI_balance after P(\"tails\")), the AI needs to look inside the Omega_Predict function (not shown here), and try to figure out how accurate it is. Assuming the mathematical intuition module says that choosing &ldquo;give $100&rdquo; as the output for S(&ldquo;heads&rdquo;) makes it more likely (by a sufficiently large margin) for Omega_Predict(S, \"heads\") to output &ldquo;give $100&rdquo;, then that choice maximizes expected utility.</p>\n<h4 class=\"MsoNormal\">Example 2: Return of Bayes<br /></h4>\n<p class=\"MsoNormal\">This example is based on case 1 in Eliezer's post <a href=\"/lw/hk/priors_as_mathematical_objects/\">Priors as Mathematical Objects</a>. An urn contains 5 red balls and 5 white balls. The AI is asked to predict the probability of each ball being red as it as drawn from the urn, its goal being to maximize the expected logarithmic score of its predictions. The main point of this example is that this decision theory can reproduce the effect of Bayesian reasoning when the situation calls for it. We can model the scenario using preferences on the following Python program:</p>\n<p style=\"padding-left: 30px;\">def P(n):<br />&nbsp;&nbsp;&nbsp; urn = ['red', 'red', 'red', 'red', 'red', 'white', 'white', 'white', 'white', 'white']<br />&nbsp;&nbsp;&nbsp; history = []<br />&nbsp;&nbsp;&nbsp; score = 0<br />&nbsp;&nbsp;&nbsp; while urn:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; i = n%len(urn)<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; n = n/len(urn)<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ball = urn[i]<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; urn[i:i+1] = []<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; prediction = S(history)<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if ball == 'red':<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; score += math.log(prediction, 2)<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; score += math.log(1-prediction, 2)<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print (score, ball, prediction)<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; history.append(ball)</p>\n<p>Here is a printout from a sample run, using n=1222222:</p>\n<p style=\"padding-left: 30px;\">-1.0 red 0.5<br />-2.16992500144 red 0.444444444444<br />-2.84799690655 white 0.375<br />-3.65535182861 white 0.428571428571<br />-4.65535182861 red 0.5<br />-5.9772799235 red 0.4<br />-7.9772799235 red 0.25<br />-7.9772799235 white 0.0<br />-7.9772799235 white 0.0<br />-7.9772799235 white 0.0</p>\n<p>S should use deductive reasoning to conclude that returning (number of red balls remaining / total balls remaining) maximizes the average score across the range of possible inputs to P, from n=1 to 10! (representing the possible orders in which the balls are drawn), and do that. Alternatively, S can approximate the correct predictions using brute force: generate a random function from histories to predictions, and compute what the average score would be if it were to implement that function. Repeat this a large number of times and it is likely to find a function that returns values close to the optimum predictions.</p>\n<h4>Example 3: Level IV Multiverse</h4>\n<p>In Tegmark's Level 4 <a href=\"http://space.mit.edu/home/tegmark/multiverse.html\">Multiverse</a>, all structures that exist mathematically also exist physically. In this case, we'd need to program the AI with preferences over all mathematical structures, perhaps represented by an ordering or utility function over conjunctions of well-formed sentences in a formal set theory. The AI will then proceed to \"optimize\" all of mathematics, or at least the parts of math that (A) are logically dependent on its decisions and (B) it can reason or form intuitions about.</p>\n<p>I suggest that the Level 4 Multiverse should be considered the default setting for a general decision theory, since we cannot rule out the possibility that all mathematical structures do indeed exist physically, or that we have direct preferences on mathematical structures (in which case there is no need for them to exist \"physically\"). Clearly, application of decision theory to the Level 4 Multiverse requires that the previously mentioned open problems be solved in their most general forms: how to handle logical uncertainty in any mathematical domain, and how to map fuzzy human preferences to well-defined preferences over the structures of mathematical objects.</p>\n<p><strong>Added:</strong> For further information and additional posts on this decision theory idea, which came to be called \"Updateless Decision Theory\", please see <a href=\"https://wiki.lesswrong.com/wiki/Updateless_decision_theory\">its entry</a>&nbsp;in the LessWrong Wiki.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 6, "5f5c37ee1b5cdee568cfb1dc": 7}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "de3xjFaACCAk6imzv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 59, "baseScore": 79, "extendedScore": null, "score": 0.000127, "legacy": true, "legacyId": "1498", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 80, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>ZH-CN</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> <w:UseFELayout /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin;} --> <!--[endif]--></p>\n<p class=\"MsoNormal\">It commonly acknowledged here that current decision theories have deficiencies that show up in the form of various paradoxes. Since there seems to be little hope that Eliezer will publish his <a href=\"/lw/135/timeless_decision_theory_problems_i_cant_solve/\">Timeless Decision Theory</a> any time soon, I decided to try to synthesize some of the ideas discussed in this forum, along with a few of my own, into a coherent alternative that is hopefully not so paradox-prone.</p>\n<p>I'll start with a way of framing the question. Put yourself in the place of an AI, or more specifically, the decision algorithm of an AI. You have access to your own source code S, plus a bit string X representing all of your memories and sensory data. You have to choose an output string Y. That\u2019s the decision. The question is, how? (The answer isn't \u201cRun S,\u201d because what we want to know is what S should be in the first place.)</p>\n<p class=\"MsoNormal\">Let\u2019s proceed by asking the question, \u201cWhat are the consequences of S, on input X, returning Y as the output, instead of Z?\u201d To begin with, we'll consider just the consequences of that choice in the realm of abstract computations (i.e. computations considered as mathematical objects rather than as implemented in physical systems). The most immediate consequence is that any program that calls S as a subroutine with X as input, will receive Y as output, instead of Z. What happens next is a bit harder to tell, but supposing that you know something about a program P that call S as a subroutine, you can further deduce the effects of choosing Y versus Z by tracing the difference between the two choices in P\u2019s subsequent execution. We could call these the computational consequences of Y. Suppose you have preferences about the execution of a set of programs, some of which call S as a subroutine, then you can satisfy your preferences directly by choosing the output of S so that those programs will run the way you most prefer.</p>\n<p class=\"MsoNormal\"><a id=\"more\"></a>A more general class of consequences might be called logical consequences. Consider a program P\u2019 that doesn\u2019t call S, but a different subroutine S\u2019 that\u2019s logically equivalent to S. In other words, S\u2019 always produces the same output as S when given the same input. Due to the logical relationship between S and S\u2019, your choice of output for S must also affect the subsequent execution of P\u2019. Another example of a logical relationship is an S' which always returns the first bit of the output of S when given the same input, or one that returns the same output as S on some subset of inputs.</p>\n<p class=\"MsoNormal\">In general, you can\u2019t be certain about the consequences of a choice, because you\u2019re not logically omniscient. How to handle logical/mathematical uncertainty is an <a href=\"/lw/f9/a_request_for_open_problems/bun\">open problem</a>, so for now we'll just assume that you have access to a \"mathematical intuition subroutine\" that somehow allows you to form beliefs about the likely consequences of your choices.</p>\n<p class=\"MsoNormal\">At this point, you might ask, \u201cThat\u2019s well and good, but what if my preferences extend beyond abstract computations? What about consequences on the physical universe?\u201d The answer is, we can view the physical universe as a program that runs S as a subroutine, or more generally, view it as a mathematical object which has S embedded within it. (From now on I\u2019ll just refer to programs for simplicity, with the understanding that the subsequent discussion can be generalized to non-computable universes.) Your preferences about the physical universe can be translated into preferences about such a program P and programmed into the AI. The AI, upon receiving an input X, will look into P, determine all the instances where it calls S with input X, and choose the output that optimizes its preferences about the execution of P. If the preferences were translated faithfully, the the AI's decision should also optimize your preferences regarding the physical universe. This faithful translation is a second major open problem.</p>\n<p class=\"MsoNormal\">What if you have some uncertainty about which program our universe corresponds to? In that case, we have to specify preferences for the entire set of programs that our universe may correspond to. If your preferences for what happens in one such program is independent of what happens in another, then we can represent them by a probability distribution on the set of programs plus a utility function on the execution of each individual program. More generally, we can always represent your preferences as a utility function on vectors of the form &lt;E1, E2, E3, \u2026&gt; where E1 is an execution history of P1, E2 is an execution history of P2, and so on.</p>\n<p class=\"MsoNormal\">These considerations lead to the following design for the decision algorithm S. S is coded with a vector &lt;P1, P2, P3, ...&gt; of programs that it cares about, and a utility function on vectors of the form &lt;E1, E2, E3, \u2026&gt; that defines its preferences on how those programs should run. When it receives an input X, it looks inside the programs P1, P2, P3, ..., and uses its \"mathematical intuition\" to form a probability distribution P_Y over the set of vectors &lt;E1, E2, E3, \u2026&gt; for each choice of output string Y. Finally, it outputs a string Y* that maximizes the expected utility Sum P_Y(&lt;E1, E2, E3, \u2026&gt;) U(&lt;E1, E2, E3, \u2026&gt;). (This specifically assumes that expected utility maximization is the right way to deal with mathematical uncertainty. Consider it a temporary placeholder until that problem is solved. Also, I'm describing the algorithm as a brute force search for simplicity. In reality, you'd probably want it to do something cleverer to find the optimal Y* more quickly.)</p>\n<h4 id=\"Example_1__Counterfactual_Mugging\">Example 1: Counterfactual Mugging</h4>\n<p class=\"MsoNormal\">Note that Bayesian updating is not done explicitly in this decision theory. When the decision algorithm receives input X, it may determine that a subset of programs it has preferences about never calls it with X and are also logically independent of its output, and therefore it can safely ignore them when computing the consequences of a choice. There is no need to set the probabilities of those programs to 0 and renormalize.</p>\n<p class=\"MsoNormal\">So, with that in mind, we can model <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">Counterfactual Mugging</a> by the following Python program:</p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\">def P(coin):<br>&nbsp;&nbsp;&nbsp; AI_balance = 100<br>&nbsp;&nbsp;&nbsp; if coin == \"heads\":<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if S(\"heads\") == \"give $100\":<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AI_balance -= 100<br>&nbsp;&nbsp;&nbsp; if coin == \"tails\":<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if Omega_Predict(S, \"heads\") == \"give $100\":<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AI_balance += 10000</p>\n<p class=\"MsoNormal\">The AI\u2019s goal is to maximize expected utility = .5 * U(AI_balance after P(\"heads\")) + .5 * U(AI_balance after P(\"tails\")). Assuming U(AI_balance)=AI_balance, it\u2019s easy to determine U(AI_balance after P(\"heads\")) as a function of S\u2019s output. It equals 0 if S(\u201cheads\u201d) == \u201cgive $100\u201d, and 100 otherwise. To compute U(AI_balance after P(\"tails\")), the AI needs to look inside the Omega_Predict function (not shown here), and try to figure out how accurate it is. Assuming the mathematical intuition module says that choosing \u201cgive $100\u201d as the output for S(\u201cheads\u201d) makes it more likely (by a sufficiently large margin) for Omega_Predict(S, \"heads\") to output \u201cgive $100\u201d, then that choice maximizes expected utility.</p>\n<h4 class=\"MsoNormal\" id=\"Example_2__Return_of_Bayes\">Example 2: Return of Bayes<br></h4>\n<p class=\"MsoNormal\">This example is based on case 1 in Eliezer's post <a href=\"/lw/hk/priors_as_mathematical_objects/\">Priors as Mathematical Objects</a>. An urn contains 5 red balls and 5 white balls. The AI is asked to predict the probability of each ball being red as it as drawn from the urn, its goal being to maximize the expected logarithmic score of its predictions. The main point of this example is that this decision theory can reproduce the effect of Bayesian reasoning when the situation calls for it. We can model the scenario using preferences on the following Python program:</p>\n<p style=\"padding-left: 30px;\">def P(n):<br>&nbsp;&nbsp;&nbsp; urn = ['red', 'red', 'red', 'red', 'red', 'white', 'white', 'white', 'white', 'white']<br>&nbsp;&nbsp;&nbsp; history = []<br>&nbsp;&nbsp;&nbsp; score = 0<br>&nbsp;&nbsp;&nbsp; while urn:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; i = n%len(urn)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; n = n/len(urn)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ball = urn[i]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; urn[i:i+1] = []<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; prediction = S(history)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if ball == 'red':<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; score += math.log(prediction, 2)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; score += math.log(1-prediction, 2)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print (score, ball, prediction)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; history.append(ball)</p>\n<p>Here is a printout from a sample run, using n=1222222:</p>\n<p style=\"padding-left: 30px;\">-1.0 red 0.5<br>-2.16992500144 red 0.444444444444<br>-2.84799690655 white 0.375<br>-3.65535182861 white 0.428571428571<br>-4.65535182861 red 0.5<br>-5.9772799235 red 0.4<br>-7.9772799235 red 0.25<br>-7.9772799235 white 0.0<br>-7.9772799235 white 0.0<br>-7.9772799235 white 0.0</p>\n<p>S should use deductive reasoning to conclude that returning (number of red balls remaining / total balls remaining) maximizes the average score across the range of possible inputs to P, from n=1 to 10! (representing the possible orders in which the balls are drawn), and do that. Alternatively, S can approximate the correct predictions using brute force: generate a random function from histories to predictions, and compute what the average score would be if it were to implement that function. Repeat this a large number of times and it is likely to find a function that returns values close to the optimum predictions.</p>\n<h4 id=\"Example_3__Level_IV_Multiverse\">Example 3: Level IV Multiverse</h4>\n<p>In Tegmark's Level 4 <a href=\"http://space.mit.edu/home/tegmark/multiverse.html\">Multiverse</a>, all structures that exist mathematically also exist physically. In this case, we'd need to program the AI with preferences over all mathematical structures, perhaps represented by an ordering or utility function over conjunctions of well-formed sentences in a formal set theory. The AI will then proceed to \"optimize\" all of mathematics, or at least the parts of math that (A) are logically dependent on its decisions and (B) it can reason or form intuitions about.</p>\n<p>I suggest that the Level 4 Multiverse should be considered the default setting for a general decision theory, since we cannot rule out the possibility that all mathematical structures do indeed exist physically, or that we have direct preferences on mathematical structures (in which case there is no need for them to exist \"physically\"). Clearly, application of decision theory to the Level 4 Multiverse requires that the previously mentioned open problems be solved in their most general forms: how to handle logical uncertainty in any mathematical domain, and how to map fuzzy human preferences to well-defined preferences over the structures of mathematical objects.</p>\n<p><strong>Added:</strong> For further information and additional posts on this decision theory idea, which came to be called \"Updateless Decision Theory\", please see <a href=\"https://wiki.lesswrong.com/wiki/Updateless_decision_theory\">its entry</a>&nbsp;in the LessWrong Wiki.</p>", "sections": [{"title": "Example 1: Counterfactual Mugging", "anchor": "Example_1__Counterfactual_Mugging", "level": 1}, {"title": "Example 2: Return of Bayes", "anchor": "Example_2__Return_of_Bayes", "level": 1}, {"title": "Example 3: Level IV Multiverse", "anchor": "Example_3__Level_IV_Multiverse", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "146 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 146, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["c3wWnvgzdbRhNnNbQ", "jzf4Rcienrm6btRyt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 15, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-14T19:48:29.420Z", "modifiedAt": null, "url": null, "title": "Fighting Akrasia:  Survey Design Help Request", "slug": "fighting-akrasia-survey-design-help-request", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:20.558Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gworley", "createdAt": "2009-03-26T17:18:20.404Z", "isAdmin": false, "displayName": "G Gordon Worley III"}, "userId": "gjoi5eBQob27Lww62", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pzsdtz6bRoDrsrkLP/fighting-akrasia-survey-design-help-request", "pageUrlRelative": "/posts/pzsdtz6bRoDrsrkLP/fighting-akrasia-survey-design-help-request", "linkUrl": "https://www.lesswrong.com/posts/pzsdtz6bRoDrsrkLP/fighting-akrasia-survey-design-help-request", "postedAtFormatted": "Friday, August 14th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fighting%20Akrasia%3A%20%20Survey%20Design%20Help%20Request&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFighting%20Akrasia%3A%20%20Survey%20Design%20Help%20Request%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpzsdtz6bRoDrsrkLP%2Ffighting-akrasia-survey-design-help-request%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fighting%20Akrasia%3A%20%20Survey%20Design%20Help%20Request%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpzsdtz6bRoDrsrkLP%2Ffighting-akrasia-survey-design-help-request", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpzsdtz6bRoDrsrkLP%2Ffighting-akrasia-survey-design-help-request", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 230, "htmlBody": "<p><strong>Follow-up to:</strong>&nbsp; <a href=\"/lw/e9/fighting_akrasia_finding_the_source/\">Fighting Akrasia:&nbsp; Finding the Source</a></p>\n<p>In the last post in this series I posted a link to a Google Docs survey to try to gather some data on what techniques, if any, work for people in conquering akrasia, but we haven't gotten very much information so far:&nbsp; the response pool is fairly homogeneous in terms of age, sex, and personality type.&nbsp; In part this is because we need to get more responses outside of the LW readership, but probably also because I'm not asking the right questions.&nbsp; So, my challenge this weekend is to come up with some good revisions for <a href=\"http://spreadsheets.google.com/viewform?hl=en&amp;formkey=cmpGck9ZSDFqUk0yNUQzRHlyZmlDSFE6MA..\">the survey</a>.</p>\n<p>In order to maximize comment usefulness, please suggest one revision per top level comment and then any discussion of that revision can take place in the replies.</p>\n<p>In the interest of keeping the comments on topic, I request a moratorium on discussions of whether or not akrasia exists and whether or not we can or should do something about it in the comments on <em>this</em> article.&nbsp; It's not that I want to exclude or silence opinions contrary to what I'm trying to accomplish:&nbsp; it's just that I would like to keep <em>this article</em> on the topic of revising the akrasia fighting survey.&nbsp; By all means, if my posting about akrasia really bothers you, write up an article explaining why I'm wrong and we'll discuss the issue more there.</p>\n<p>Thanks!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pzsdtz6bRoDrsrkLP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 1, "extendedScore": null, "score": 5.149808810326975e-07, "legacy": true, "legacyId": "1502", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BH8BcP7d2sbmzZig8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-15T14:11:34.964Z", "modifiedAt": null, "url": null, "title": "Minds that make optimal use of small amounts of sensory data", "slug": "minds-that-make-optimal-use-of-small-amounts-of-sensory-data", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:20.695Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SforSingularity", "createdAt": "2009-08-10T19:17:46.580Z", "isAdmin": false, "displayName": "SforSingularity"}, "userId": "cS3uqQdsTzLnHwtKa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zQT2pAf9zxCCYMXkC/minds-that-make-optimal-use-of-small-amounts-of-sensory-data", "pageUrlRelative": "/posts/zQT2pAf9zxCCYMXkC/minds-that-make-optimal-use-of-small-amounts-of-sensory-data", "linkUrl": "https://www.lesswrong.com/posts/zQT2pAf9zxCCYMXkC/minds-that-make-optimal-use-of-small-amounts-of-sensory-data", "postedAtFormatted": "Saturday, August 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Minds%20that%20make%20optimal%20use%20of%20small%20amounts%20of%20sensory%20data&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMinds%20that%20make%20optimal%20use%20of%20small%20amounts%20of%20sensory%20data%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzQT2pAf9zxCCYMXkC%2Fminds-that-make-optimal-use-of-small-amounts-of-sensory-data%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Minds%20that%20make%20optimal%20use%20of%20small%20amounts%20of%20sensory%20data%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzQT2pAf9zxCCYMXkC%2Fminds-that-make-optimal-use-of-small-amounts-of-sensory-data", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzQT2pAf9zxCCYMXkC%2Fminds-that-make-optimal-use-of-small-amounts-of-sensory-data", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 846, "htmlBody": "<p>In <a href=\"/lw/qk/that_alien_message/\">That alien message</a>, Eliezer made some pretty wild claims:</p>\n<blockquote>\n<p><em>My moral - that even Einstein did not come within a million light-years of making efficient use of sensory data.</em></p>\n<p><em>Riemann invented his geometries before Einstein had a use for them; the physics of our universe is not that complicated in an absolute sense.&nbsp; A Bayesian superintelligence, hooked up to a webcam, would invent General Relativity as a hypothesis - perhaps not the dominant hypothesis, compared to Newtonian mechanics, but still a hypothesis under direct consideration - by the time it had seen the third frame of a falling apple.&nbsp; It might guess it from the first frame, if it saw the statics of a bent blade of grass.</em></p>\n<p><em>They never suspected a thing.&nbsp; They weren't very smart, you see, even before taking into account their slower rate of time.&nbsp; Their primitive equivalents of rationalists went around saying things like, \"There's a bound to how much information you can extract from sensory data.\"&nbsp; And they never quite realized what it meant, that we were smarter than them, and thought faster.</em></p>\n</blockquote>\n<p>In the comments, Will Pearson asked for \"some form of proof of concept\". It seems that researchers at Cornell - Schmidt and Lipson - have done exactly that. See their video on Guardian Science:</p>\n<blockquote>\n<p><a href=\"http://www.guardian.co.uk/science/video/2009/apr/02/eureka-machine-artificial-intelligence\">'Eureka machine' can discover laws of nature</a> - The machine formulates laws by observing the world and detecting patterns in the vast quantities of data it has collected</p>\n</blockquote>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<p>Researchers at Cambridge and Aberystwith have <a href=\"http://www.scientificamerican.com/article.cfm?id=robots-adam-and-eve-ai\">gone one step further</a> and implemented an AI system/robot to perform scientific experiments:</p>\n<blockquote>\n<p><em>Researchers at Aberystwyth University in Wales and England's University of Cambridge report in Science today that they designed Adam - they describe how the bot operates by relating how he carried out one of his tasks, in this case to find out more about the genetic makeup of baker's yeast Saccharomyces cerevisiae, an organism that scientists use to model more complex life systems. Using artificial intelligence, Adam hypothesized that certain genes in baker's yeast code for specific enzymes that catalyze biochemical reactions. The robot devised experiments to test these beliefs, ran the experiments, and interpreted the results.</em></p>\n</blockquote>\n<p><em><span style=\"font-style: normal; \">The crucial question is: what can we learn about the likely effectiveness of a \"superintelligent\" AI from the behavior of these AI programs? First of all, let us be clear: this AI is *<strong>not</strong>* a \"superintellgience\", so we shouldn't expect it to perform at that level. The problem we face is analogous to the problem of extrapolating how fast an olympic sprinter can run from looking at a baby crawling around on the floor. Furthermore, the Cornell machine was given a physical system that was specifically chosen to be easy to analyze, and a representation (equations) that is known to be suited to the problem.&nbsp;</span></em></p>\n<p><em><span style=\"font-style: normal; \">We can certainly state that the program analyzed some data much faster than any human could have done. In a running time probably measured in hours or minutes, it took a huge stream of raw position and velocity data and found the underlying conserved quantities. And given likely algorithmic optimizations and another 10 years' of Moore's law, we can safely say that in 10 years' time, that particular program will run in seconds on a $500 machine or milliseconds on a supercomputer. These results actually surprise me: an AI can automatically and instantly analyze a physical system (albeit a rigged one).&nbsp;</span></em></p>\n<p>But, of course, one has to ask: how much more narrow-AI work would it take to actually look at video of some bouncing, falling and whirling objects and deduce a general physical law such as the earth's gravity and the laws governing air resistance, where the objects are not hand-picked to be easy to analyze? This is unclear. But I can see mechanisms whereby this would work, rather than merely having to submit to the overwhelming power of the word \"superintelligence\". My suspicion is that with current state-of-the-art object identification technology, video footage of a system of bouncing balls and pendulums and springs would be amenable to this kind of analysis. There may even be a research project in that proposition.&nbsp;</p>\n<p>As far as extrapolating the behavior of a superintelligence from the behavior of the Cornell AI or the Adam robot, we should note that no human can look at a complex physical system for a few seconds and just write down the physical law or equation that it obeys. A simple narrow AI has already outperformed humans at one specific task; though it still cannot do most of what a scientist does. We should therefore update our beliefs to assign more weight to the hypothesis that on some particular narrow physical modelling task, a \"superintelligence\" would vastly outperform us. Personally I was surprised at what such a simple system can do, though with hindsight it is obvious: data from a physical system follows patterns, and statistics can indentify those patterns. Science is not a magic ritual that only humans can perform, rather it is a specific kind of algorithm, and we should expect there to be no special injunction against silicon minds from doing it.&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zQT2pAf9zxCCYMXkC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 12, "extendedScore": null, "score": 5.151600426430657e-07, "legacy": true, "legacyId": "1503", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5wMcKNAwB6X4mp9og"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-16T16:06:18.646Z", "modifiedAt": null, "url": null, "title": "Bloggingheads: Yudkowsky and Aaronson talk about AI and Many-worlds", "slug": "bloggingheads-yudkowsky-and-aaronson-talk-about-ai-and-many", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:23.522Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8njamAu4vgJYxbJzN/bloggingheads-yudkowsky-and-aaronson-talk-about-ai-and-many", "pageUrlRelative": "/posts/8njamAu4vgJYxbJzN/bloggingheads-yudkowsky-and-aaronson-talk-about-ai-and-many", "linkUrl": "https://www.lesswrong.com/posts/8njamAu4vgJYxbJzN/bloggingheads-yudkowsky-and-aaronson-talk-about-ai-and-many", "postedAtFormatted": "Sunday, August 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bloggingheads%3A%20Yudkowsky%20and%20Aaronson%20talk%20about%20AI%20and%20Many-worlds&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABloggingheads%3A%20Yudkowsky%20and%20Aaronson%20talk%20about%20AI%20and%20Many-worlds%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8njamAu4vgJYxbJzN%2Fbloggingheads-yudkowsky-and-aaronson-talk-about-ai-and-many%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bloggingheads%3A%20Yudkowsky%20and%20Aaronson%20talk%20about%20AI%20and%20Many-worlds%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8njamAu4vgJYxbJzN%2Fbloggingheads-yudkowsky-and-aaronson-talk-about-ai-and-many", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8njamAu4vgJYxbJzN%2Fbloggingheads-yudkowsky-and-aaronson-talk-about-ai-and-many", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 70, "htmlBody": "<p><a href=\"http://www.bloggingheads.tv/diavlogs/21857\">Eliezer Yudkowsky and Scott Aaronson - Percontations: Artificial Intelligence and Quantum Mechanics</a></p>\n<p>Sections of the diavlog:</p>\n<ul>\n<li>When will we build the first superintelligence?</li>\n<li>Why quantum computing isn&rsquo;t a recipe for robot apocalypse</li>\n<li>How to guilt-trip a machine</li>\n<li>The evolutionary psychology of artificial intelligence</li>\n<li>Eliezer contends many-worlds is obviously correct</li>\n<li>Scott contends many-worlds is ridiculous (but might still be true)</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1c9": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8njamAu4vgJYxbJzN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 22, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "1505", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 103, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-16T19:59:33.959Z", "modifiedAt": null, "url": null, "title": "My God! It's full of Nash equilibria!", "slug": "my-god-it-s-full-of-nash-equilibria", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:21.156Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cyan", "createdAt": "2009-02-27T22:31:08.528Z", "isAdmin": false, "displayName": "Cyan"}, "userId": "eGtDNuhj58ehX9Wgf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GySAm6fTjjDEPAXF8/my-god-it-s-full-of-nash-equilibria", "pageUrlRelative": "/posts/GySAm6fTjjDEPAXF8/my-god-it-s-full-of-nash-equilibria", "linkUrl": "https://www.lesswrong.com/posts/GySAm6fTjjDEPAXF8/my-god-it-s-full-of-nash-equilibria", "postedAtFormatted": "Sunday, August 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20My%20God!%20It's%20full%20of%20Nash%20equilibria!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMy%20God!%20It's%20full%20of%20Nash%20equilibria!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGySAm6fTjjDEPAXF8%2Fmy-god-it-s-full-of-nash-equilibria%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=My%20God!%20It's%20full%20of%20Nash%20equilibria!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGySAm6fTjjDEPAXF8%2Fmy-god-it-s-full-of-nash-equilibria", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGySAm6fTjjDEPAXF8%2Fmy-god-it-s-full-of-nash-equilibria", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 92, "htmlBody": "<p>Speaking of <a href=\"/lw/15t/bloggingheads_yudkowsky_and_aaronson_talk_about/\">Scott Aaronson</a>, his <a href=\"http://scottaaronson.com/blog/?p=418\">latest post at Shtetl-Optimized</a> seems worthy of&nbsp;some linky love.</p>\r\n<blockquote>\r\n<p>Why do native speakers of the language you&rsquo;re studying talk too fast for you to understand them?&nbsp; Because otherwise, they could talk faster and still understand each other.</p>\r\n<p>...</p>\r\n<p>Again and again, I&rsquo;ve undergone the humbling experience of first lamenting how badly something sucks, then only much later having the crucial insight that <em>its not sucking wouldn&rsquo;t have been a Nash equilibrium</em>.&nbsp; Clearly, then, I haven&rsquo;t yet gotten good enough at Malthusianizing my daily life&mdash;have you?</p>\r\n</blockquote>\r\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GySAm6fTjjDEPAXF8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 19, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "1506", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8njamAu4vgJYxbJzN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-16T21:12:39.466Z", "modifiedAt": null, "url": null, "title": "Happiness is a Heuristic", "slug": "happiness-is-a-heuristic", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:49.761Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "pwZ6qMgzoKr3JPqx4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/anS4CNRhddny9Snz2/happiness-is-a-heuristic", "pageUrlRelative": "/posts/anS4CNRhddny9Snz2/happiness-is-a-heuristic", "linkUrl": "https://www.lesswrong.com/posts/anS4CNRhddny9Snz2/happiness-is-a-heuristic", "postedAtFormatted": "Sunday, August 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Happiness%20is%20a%20Heuristic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHappiness%20is%20a%20Heuristic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FanS4CNRhddny9Snz2%2Fhappiness-is-a-heuristic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Happiness%20is%20a%20Heuristic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FanS4CNRhddny9Snz2%2Fhappiness-is-a-heuristic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FanS4CNRhddny9Snz2%2Fhappiness-is-a-heuristic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1293, "htmlBody": "<p><!-- BODY { FONT-FAMILY:Tahoma; FONT-SIZE:10pt } P { FONT-FAMILY:Tahoma; FONT-SIZE:10pt } DIV { FONT-FAMILY:Tahoma; FONT-SIZE:10pt } TD { FONT-FAMILY:Tahoma; FONT-SIZE:10pt } -->Whenever the topic of happiness is mentioned, it's always discussed like it's the most important thing in the world. People talk about it like they would a hidden treasure or a rare beast - you have to seek it, hunt it, ensnare it and hold it tight, or it'll slip through your fingers. Perhaps it's just the contrarian in me, but this seems misguided - &nbsp;happiness shouldn't be searched for like the holy grail. Not that I don't want to be happy, but is that really the purpose of my life - to have my neurons stimulated in a way that feels good, and try to keep that up until I die? Why don't I just slip myself into a Soma-coma then? Of course, anything I do boils down to a particular stimulation of neurons, but that doesn't mean there's not something better to aspire to. To pursue happiness as an end itself I think, is backwards. It wasn't built into our brains because evolution was being nice - it's there because it increases our fitness. Happiness is designed to get us somewhere, not to be a destination in itself.</p>\n<p><a id=\"more\"></a></p>\n<div>Fortunately, we're not obligated to follow evolution's whims. But this confusion might be the reason that, in general, we're crappy at predicting what will make us happy. A few data points:</div>\n<div>-In an oft-quoted study, lottery winners are less happy than they predicted a year afterwards, and parapalegics are more happy.[1]</div>\n<div>-People tend to enjoy potato chips the same amount despite predicting (after being primed) that they'll either love them or hate them.[2]</div>\n<div>-Both assistant professors who do and do not receive tenure over-emphasize the impact the result will have on their happiness.</div>\n<div>Our wanting system is seperate from our liking system[3], and, contrary to intuition, we don't seem to be wired to pursue things that will make us happy. Rather, we seem to be wired to pursue things, and then get a fixed amount of happiness from whatever we end up with. We try to get what we like, but we tend to end up liking what we get. Dan Gilbert refers to this as 'synthetic' happiness, and posits it as the reason people who miss big opportunities tend to say that \"it worked out for the best.\" It's tempting to assume this is just a rationalization technique, but there's evidence it happens on a subconscious level. When amnesiacs are given a painting as a gift, later they tend to pick it as their favorite out of a group of paintings, despite not being able to remember owning it.[4]</div>\n<div>Where this system seems to get derailed is when we have to make extensive use of our higher brain functions. When have to make a complex decision or deliberate over a choice, we tend to be less satisfied with it. A few more data points:</div>\n<div>-People are more satisfied with chocolate when they are choosing from six different kinds than when choosing from thirty different kinds. [5]<br /></div>\n<div>-People are happier with a photo when they have to immediately choose the one they want than when they're given three days to make their choice.[4]</div>\n<div>-People who simply rated a group of posters on a scale of 1-9 (and then received their favorite one) were happier than people who had to list the reasons they liked the posters first. [6]<br /></div>\n<div>-When given a choice of jams, people are more likely to make a purchase when choosing from six than when choosing from twenty-four.[7]</div>\n<div>Happiness is tied to affect, the subconscious system responsible for our 'gut' reactions. It's much easier (mentally, anyway) for us to make a choice and then decide it was the correct one than to weigh all the options and then be happy with whatever is 'best'. Barry Schwartz refers to this as \"the paradox of choice\". &nbsp;When we can't rely on a gut reaction, or are forced to supercede it by consciously deliberating, our happiness seems to suffer.</div>\n<div>Though it's usually refered to as one emotion, 'happiness' is just an umbrella term for a variety of positive emotional states. These tend to fall into two groups - hedonia and eudamonia. Hedonia is pleasure - pure sensory stimulation. Eating a cookie, buying a car, having an orgasm are all hedonic pleasures. Hedonia comes from external stimulation, things outside ourselves that we pursue. Because we're wired to pursue things regardless of how much we already have, hedonia doesn't get sated. We quickly adapt to changes in our environment, always wanting more - more money, more food, and more sex. We get caught on a hedonic treadmill [8], needing more and more stimulation to produce the same amount of pleasure. Since we quickly return to our hedonic set points, this pleasure never lasts - nor should we expect it to. The ancient humans who were satisfied with what they had all got outcompeted by the ones who wanted more. But that means for sustained happiness, you have to look elsewhere.</div>\n<div>Eudamonia, on the other hand, does produce lasting happiness. Eudamonia is 'well-being', and it comes, according to Martin Seligman, from identifying your strengths and orienting your life around them. Eudamonia isn't based on pleasure per-se, but on orienting yourself so you interact with the world in a positive way. For example, having fulfilling social relationships - being the type of person that cares about others - is consistently one of the best predictors of happiness. &nbsp;More data points:</div>\n<div>-People who get married tend to be happier than single people.[9]</div>\n<div>-Spending twenty dollars on a gift for another person produces more happiness than spending twenty dollars on yourself.[10]</div>\n<div>-Writing a long, grateful letter to a person to whom you're thankful, visiting them, and reading it outloud produces extreme increases in happiness even months afterwards.[10]</div>\n<div>Other eudamonic states are Cs&iacute;kszentmih&aacute;lyi's flow state [11], and the states of happiness reached by buddhist monks [12]. All come from not from pure pleasure, but from constructing your life in a way that matches what's most important to you. It's eudamonic changes that center our lives around our values that are capable of raising our hedonic set points. And so, in the rich tradition of scientific studies being boiled down to aphorisms, \"happiness comes from within\".</div>\n<div>Perhaps it's tacit knowledge of this that makes us suspicious of supposed extreme happiness offered by an external source. People generally reject eternal happiness if it means taking Soma for the rest of your life, or being locked into an experience machine, or being reduced to an orgasmium, or any one of a myriad of possible ways to max out those pleasure neurons. I don't see this as a bug - I see it as a feature. It means our desire for happiness is conflicting with some higher value. The solution to a conflict like this is, I think, straightforward - go with the higher value! Happiness is designed to help you achieve what's imporant to you - &nbsp;to take you somewhere your brain thinks you're supposed to go. A conflict means that your subconscious doesn't like where it's headed. And if you don't like it, you don't have to go there. [13]</div>\n<div>The happiness heuristic seems to drive us to the best possible future outcomes, by motivating us to seek resources (hedonia) and have a life that fufills what's important to us (eudamonia). Our brains don't assign happiness intrinsic value - it has delegated value, in that acquiring it lets us acquire other things. Seeking happiness seems to be unnecessary; if you orient your life around your desires and values, happiness will generally follow one way or another.</div>\n<div><br /></div>\n<div>Links (note: if anyone can provide a link to the actual studies referenced, rather than just newspaper articles, I'll replace them)<br /></div>\n<div>1: http://education.ucsb.edu/janeconoley/ed197/documents/brickman_lotterywinnersandaccidentvictims.pdf</div>\n<div>2: http://www.psychologicalscience.org/observer/getArticle.cfm?id=2188<br /></div>\n<div>3: http://wireheading.com/pleasure/liking-wanting.html</div>\n<div>4: http://www.ted.com/talks/dan_gilbert_asks_why_are_we_happy.html</div>\n<div>5: http://www.dynamist.com/articles-speeches/forbes/choice.html</div>\n<div>6: http://scienceblogs.com/cortex/2007/02/post_14.php</div>\n<div>7: http://scienceblogs.com/cortex/2007/06/the_paradox_of_choice_internet.php</div>\n<div>8: http://en.wikipedia.org/wiki/Hedonic_treadmill</div>\n<div>9: http://www.apa.org/releases/married_happy.html</div>\n<div>10: http://www.ted.com/index.php/talks/martin_seligman_on_the_state_of_psychology.html</div>\n<div>11: http://en.wikipedia.org/wiki/Flow_(psychology)</div>\n<div>12: http://www.timesonline.co.uk/tol/comment/columnists/libby_purves/article1136398.ece<br /></div>\n<div>13: http://lesswrong.com/lw/wv/prolegomena_to_a_theory_of_fun/<br /></div>\n<div><br /></div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb186": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "anS4CNRhddny9Snz2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 12, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "1507", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-16T21:23:10.050Z", "modifiedAt": null, "url": null, "title": "Experiential Pica", "slug": "experiential-pica", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:55.807Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9ZodFr54FtpLThHZh/experiential-pica", "pageUrlRelative": "/posts/9ZodFr54FtpLThHZh/experiential-pica", "linkUrl": "https://www.lesswrong.com/posts/9ZodFr54FtpLThHZh/experiential-pica", "postedAtFormatted": "Sunday, August 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Experiential%20Pica&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExperiential%20Pica%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9ZodFr54FtpLThHZh%2Fexperiential-pica%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Experiential%20Pica%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9ZodFr54FtpLThHZh%2Fexperiential-pica", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9ZodFr54FtpLThHZh%2Fexperiential-pica", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 887, "htmlBody": "<p>tl;dr version: Akrasia might be like an eating disorder!</p>\n<p>When I was a teenager, I ate ice.&nbsp; Lots of ice.&nbsp; Cups and cups and cups of ice, constantly, all day long, when it was freely available.&nbsp; This went on for years, during which time I ignored the fact that others found it peculiar. (\"Oh,\" I would joke to curious people at the school cafeteria, ignoring the opportunity to detect the strangeness of my behavior, \"it's for my pet penguin.\")&nbsp; I had my cache of excuses: it keeps my mouth occupied.&nbsp; It's so nice and cool in the summer.&nbsp; I don't drink enough water anyway, it keeps me hydrated.&nbsp; Yay, zero-calorie snack!</p>\n<p>Then I turned seventeen and attempted to donate blood, and was basically told, when they did the finger-stick test, \"Either this machine is broken or you should be in a dead faint.\"&nbsp; I got some more tests done, confirmed that extremely scary things were wrong with my blood, and started taking iron supplements.&nbsp; I stopped eating ice.&nbsp; I stopped having any interest in eating ice at all.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Pica_%28disorder%29\">Pica</a> is an impulse to eat things that are not actually food.&nbsp; Compared to some of the things that people with pica eat, I got off very easy: ice did not do me any harm on its own, and was merely a symptom.&nbsp; But here's the kicker: What I needed was iron.&nbsp; If I'd been consciously aware of that need, I'd have responded to it with the supplements far earlier, or with steak<sup>1</sup> and spinach and cereals fortified with 22 essential vitamins &amp; minerals.&nbsp; Ice does not contain iron.&nbsp; And yet when what I needed was iron, what I wanted was ice.</p>\n<p>What if akrasia is <em>experiential pica</em>?<em>&nbsp; </em>What if, when you want to play Tetris or watch TV or tat doilies instead of doing your Serious Business, that <em>means</em> that you aren't going to art museums enough, or that you should get some exercise, or that what your brain really craves is the chance to write a symphony?<a id=\"more\"></a></p>\n<p>The existence - indeed, prevalence - of pica is a perfect example of how the brain is <em>very bad</em> at communicating certain needs to the systems that can get those needs met.&nbsp; Even when the same mechanism - that of instilling the desire to eat something, in the case of pica - could be used to meet the need, the brain misfires<sup>2</sup>.&nbsp; It didn't make me crave liver and shellfish and molasses, it made me crave water in frozen form.&nbsp; A substance which did nothing to help, and was very inconvenient to continually keep around and indulge in, and which made people look at me funny when I held up the line at the drink dispenser for ten minutes filling up half a dozen paper cups.</p>\n<p>So why shouldn't I believe that, for lack of some <em>non</em>-food X, my brain just might force me to seek out unrelated non-food Y and make me think it was all my own brilliant idea?&nbsp; (\"Yay, zero-calorie snack that hydrates, cools, and is free or mere pennies from fast food outlets when I have completely emptied the icemaker!&nbsp; I'm <em>so clever!</em>\")</p>\n<p>The trouble, if one hopes to take this hypothesis any farther, is that it's hard to tell what your experiential deficiencies might be<sup>3</sup>.&nbsp; The baseline needs for figure-skating and flan-tasting probably vary person-to-person a lot more than nutrient needs do.&nbsp; You can't stick your finger, put a drop of blood into a little machine that goes \"beep\", and see if it says that you spend too little time weeding peonies.&nbsp; I also have no way to solve the problem of being akratic about attempted substitutions for akrasia-related activities: even if you discovered for sure that by baking a batch of muffins once a week, you would lose the crippling desire to play video games constantly, nothing's stopping the desire to play video games from obstructing initial attempts at muffin-baking.</p>\n<p>Possible next steps to explore the experiential pica idea and see how it pans out:</p>\n<ul>\n<li>Study the habits of highly effective people.&nbsp; Do you know somebody who seems unplagued by akrasia?&nbsp; What does (s)he do during downtime?&nbsp; Maybe someone you're acquainted with has hit on a good diet of experience that we could try to emulate.</li>\n<li>If you are severely plagued by akrasia, and there is some large class of experiences that you completely leave out of your life, attempt to find a way to incorporate something from that class.&nbsp; See if it helps.&nbsp; For instance, if you are practically never outdoors, take a short walk or just sit in the yard; if you practically never do anything for aesthetic reasons, find something pretty to look at or listen to; etc.</li>\n<li>You might already have periods when you are less akratic than usual.&nbsp; Notice what experiences you have had around those times that could have contributed.</li>\n</ul>\n<p>&nbsp;</p>\n<p><sup>1</sup>I was not a vegetarian until I had already been eating ice for a very long time.&nbsp; The switch can only have exacerbated the problem.</p>\n<p><sup>2</sup>Some pica sufferers do in fact eat things that contain the mineral they're deficient in, but not all.</p>\n<p><sup>3</sup>Another problem is that this theory only covers what might be called \"enticing\" akrasia, the positive desire to do non-work things.&nbsp; It has nothing to say about aversive akrasia, where you would do <em>anything but</em> what you metawant to do.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5BvRW4FxdD8DFhiew": 14, "r7qAjcbfhj2256EHH": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9ZodFr54FtpLThHZh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 112, "baseScore": 120, "extendedScore": null, "score": 0.00019, "legacy": true, "legacyId": "1508", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 121, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 113, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-16T21:29:56.353Z", "modifiedAt": null, "url": null, "title": "Friendlier AI through politics", "slug": "friendlier-ai-through-politics", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:22.656Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jonathan_Graehl", "createdAt": "2009-02-27T23:21:15.671Z", "isAdmin": false, "displayName": "Jonathan_Graehl"}, "userId": "eKsWtKKceoRYwcc7s", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/frcaDCCBv4ZKET7mH/friendlier-ai-through-politics", "pageUrlRelative": "/posts/frcaDCCBv4ZKET7mH/friendlier-ai-through-politics", "linkUrl": "https://www.lesswrong.com/posts/frcaDCCBv4ZKET7mH/friendlier-ai-through-politics", "postedAtFormatted": "Sunday, August 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Friendlier%20AI%20through%20politics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFriendlier%20AI%20through%20politics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfrcaDCCBv4ZKET7mH%2Ffriendlier-ai-through-politics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Friendlier%20AI%20through%20politics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfrcaDCCBv4ZKET7mH%2Ffriendlier-ai-through-politics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfrcaDCCBv4ZKET7mH%2Ffriendlier-ai-through-politics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 177, "htmlBody": "<p><a href=\"http://davidbrin.blogspot.com/2009/08/real-way-to-feel-safe-with-artificial.html\">David Brin suggests</a> that some kind of political system populated with humans and diverse but imperfectly rational and friendly AIs would evolve in a satisfactory direction for humans.</p>\n<p>I don't know whether creating an <em>imperfectly</em>&nbsp;rational general AI is any easier, except that limited perceptual and computational resources obviously imply less than optimal outcomes; still, why shouldn't we hope for optimal given those constraints? &nbsp;I imagine the question will become more settled before anyone nears unleashing a self-improving superhuman AI.</p>\n<p>An imperfectly friendly AI, perfectly rational or not, is a very likely scenario. &nbsp;Is it sufficient to create diverse singleton value-systems (demographically representative of humans' values) rather than a consensus (over all humans' values) monolithic Friendly? &nbsp;</p>\n<p>What kind of competitive or political system would make fragmented squabbling AIs safer than an attempt to get the monolithic approach right? &nbsp;Brin seems to have some hope of improving politics regardless of AI participation, but I'm not sure exactly what his dream is or how to get there - perhaps his \"<a href=\"http://www.davidbrin.com/disputation.htm\">disputation arenas</a>\" would work if the participants were rational and altruistically honest).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "frcaDCCBv4ZKET7mH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 2, "extendedScore": null, "score": 5.154653746586066e-07, "legacy": true, "legacyId": "1509", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-16T23:29:40.282Z", "modifiedAt": null, "url": null, "title": "Singularity Summit 2009 (quick post)", "slug": "singularity-summit-2009-quick-post", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:33.847Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Zwbz6Wv7yaLM6J36r/singularity-summit-2009-quick-post", "pageUrlRelative": "/posts/Zwbz6Wv7yaLM6J36r/singularity-summit-2009-quick-post", "linkUrl": "https://www.lesswrong.com/posts/Zwbz6Wv7yaLM6J36r/singularity-summit-2009-quick-post", "postedAtFormatted": "Sunday, August 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Singularity%20Summit%202009%20(quick%20post)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingularity%20Summit%202009%20(quick%20post)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZwbz6Wv7yaLM6J36r%2Fsingularity-summit-2009-quick-post%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Singularity%20Summit%202009%20(quick%20post)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZwbz6Wv7yaLM6J36r%2Fsingularity-summit-2009-quick-post", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZwbz6Wv7yaLM6J36r%2Fsingularity-summit-2009-quick-post", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 119, "htmlBody": "<p>Someone else may do a more formal announcement later, but since early registration expires on August 20th, I'm doing a quick heads-up to Less Wrong readers:</p>\n<p>The <strong>Singularity Summit 2009</strong> is in <strong>New York</strong> on <strong>Oct 3-4</strong>.</p>\n<p>There are discounts for students, blog mentions, referrals, and registration before August 20th.</p>\n<p>Speakers of note to rationalists will include Robin Hanson, Gary Drescher (author of <em>Good and Real, </em>one of the few master-level works of reductionism out there), and David Chalmers.&nbsp; Also speaking will be Marcus Hutter and Juergen Schmidhuber, as well as some of the usual suspects:&nbsp; Aubrey de Grey, Peter Thiel, Ben Goertzel, and Ray Kurzweil.</p>\n<p>They're really trying to raise the intellectual level this year.</p>\n<p>Singularity Summit 2009 <a href=\"http://www.singularitysummit.com/summit\">home page</a>, <a href=\"http://www.singularitysummit.com/program\">program</a>, and <a href=\"https://www.singularitysummit.com/registration/\">registration</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DQHWBcKeiLnyh9za9": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Zwbz6Wv7yaLM6J36r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 18, "extendedScore": null, "score": 5.154848482967616e-07, "legacy": true, "legacyId": "1501", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-18T19:17:07.019Z", "modifiedAt": null, "url": null, "title": "Scott Aaronson's \"On Self-Delusion and Bounded Rationality\"", "slug": "scott-aaronson-s-on-self-delusion-and-bounded-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:57.750Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qHDNab3hnRFNJFsiE/scott-aaronson-s-on-self-delusion-and-bounded-rationality", "pageUrlRelative": "/posts/qHDNab3hnRFNJFsiE/scott-aaronson-s-on-self-delusion-and-bounded-rationality", "linkUrl": "https://www.lesswrong.com/posts/qHDNab3hnRFNJFsiE/scott-aaronson-s-on-self-delusion-and-bounded-rationality", "postedAtFormatted": "Tuesday, August 18th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Scott%20Aaronson's%20%22On%20Self-Delusion%20and%20Bounded%20Rationality%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScott%20Aaronson's%20%22On%20Self-Delusion%20and%20Bounded%20Rationality%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqHDNab3hnRFNJFsiE%2Fscott-aaronson-s-on-self-delusion-and-bounded-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Scott%20Aaronson's%20%22On%20Self-Delusion%20and%20Bounded%20Rationality%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqHDNab3hnRFNJFsiE%2Fscott-aaronson-s-on-self-delusion-and-bounded-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqHDNab3hnRFNJFsiE%2Fscott-aaronson-s-on-self-delusion-and-bounded-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 105, "htmlBody": "<p>Poignant <a href=\"http://www.scottaaronson.com/writings/selfdelusion.html\">short story</a> about truth-seeking that I just found. Quote:</p>\n<blockquote>\n<p>\"No,\" interjected an internal voice. \"You need to prove that your dad will appear by a direct argument from the length of your nails, one that does not invoke your subsisting in a dream state as an intermediate step.\"</p>\n<p>\"Nonsense,\" retorted another voice. \"That we find ourselves in a dream state was never assumed; rather, it follows so straightforwardly from the long-nail counterfactual that the derivation could be done, I think, even in an extremely weak system of inference.\"</p>\n</blockquote>\n<p>The full thing reads like a flash tour of OB/LW, except it was written in 2001.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qHDNab3hnRFNJFsiE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 22, "extendedScore": null, "score": 5.159125128734977e-07, "legacy": true, "legacyId": "1513", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-19T01:10:11.862Z", "modifiedAt": null, "url": null, "title": "Ingredients of Timeless Decision Theory", "slug": "ingredients-of-timeless-decision-theory", "viewCount": null, "lastCommentedAt": "2021-02-15T10:23:58.047Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/szfxvS8nsxTgJLBHs/ingredients-of-timeless-decision-theory", "pageUrlRelative": "/posts/szfxvS8nsxTgJLBHs/ingredients-of-timeless-decision-theory", "linkUrl": "https://www.lesswrong.com/posts/szfxvS8nsxTgJLBHs/ingredients-of-timeless-decision-theory", "postedAtFormatted": "Wednesday, August 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ingredients%20of%20Timeless%20Decision%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIngredients%20of%20Timeless%20Decision%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FszfxvS8nsxTgJLBHs%2Fingredients-of-timeless-decision-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ingredients%20of%20Timeless%20Decision%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FszfxvS8nsxTgJLBHs%2Fingredients-of-timeless-decision-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FszfxvS8nsxTgJLBHs%2Fingredients-of-timeless-decision-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2125, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Newcomb's Problem and Regret of Rationality</a>, <a href=\"/lw/15m/towards_a_new_decision_theory/\">Towards a New Decision Theory<br /></a></p>\n<p>Wei Dai <a href=\"/lw/15m/towards_a_new_decision_theory/11ji\">asked</a>:</p>\n<blockquote>\n<p>\"Why didn't you mention earlier that your timeless decision theory mainly had to do with logical uncertainty? It would have saved people a lot of time trying to guess what you were talking about.\"</p>\n</blockquote>\n<p>...</p>\n<p>All right, fine, here's a fast summary of the most important ingredients that go into my \"timeless decision theory\".&nbsp; This isn't so much an explanation of TDT, as a list of starting ideas that you could use to recreate TDT given sufficient background knowledge.&nbsp; It seems to me that this sort of thing really takes a mini-book, but perhaps I shall be proven wrong.</p>\n<p>The one-sentence version is:&nbsp; Choose as though controlling the logical output of the abstract computation you implement, including the output of all other instantiations and simulations of that computation.</p>\n<p>The three-sentence version is:&nbsp; Factor your uncertainty over (impossible) possible worlds into a causal graph that includes nodes corresponding to the unknown outputs of known computations; condition on the known initial conditions of your decision computation to screen off factors influencing the decision-setup; compute the counterfactuals in your expected utility formula by surgery on the node representing the logical output of that computation.<a id=\"more\"></a></p>\n<p>To obtain the background knowledge if you don't already have it, the two main things you'd need to study are the classical debates over Newcomblike problems, and the Judea Pearl synthesis of causality.&nbsp; Canonical sources would be \"Paradoxes of Rationality and Cooperation\" for Newcomblike problems and \"Causality\" for causality.</p>\n<p>For those of you who don't condescend to buy physical books, <a href=\"http://kops.ub.uni-konstanz.de/volltexte/2000/524/\">Marion Ledwig's thesis on Newcomb's Problem</a> is a good summary of the existing attempts at decision theories, evidential decision theory and causal decision theory.&nbsp; You need to know that causal decision theories two-box on Newcomb's Problem (which loses) and that evidential decision theories refrain from smoking on the <a href=\"http://www.geocities.com/eganamit/NoCDT.pdf\">smoking lesion</a> problem (which is even crazier).&nbsp; You need to know that the expected utility formula is actually over a counterfactual on our actions, rather than an ordinary probability update on our actions.</p>\n<p>I'm not sure what you'd use for online reading on causality.&nbsp; Mainly you need to know:</p>\n<ul>\n<li>That a causal graph <em>factorizes </em>a correlated probability distribution into a deterministic mechanism of chained functions plus a set of <em>uncorrelated</em> unknowns as background factors.</li>\n<li>Standard ideas about \"screening off\" variables (D-separation).</li>\n<li>The standard way of computing counterfactuals (through surgery on causal graphs).</li>\n</ul>\n<p>It will be helpful to have the standard Less Wrong background of defining rationality in terms of processes that systematically discover truths or achieve preferred outcomes, rather than processes that sound reasonable; understanding that you are <em>embedded within</em> physics; understanding that your philosophical intutions are how some particular cognitive algorithm <a href=\"http://wiki.lesswrong.com/wiki/How_an_algorithm_feels\">feels from inside</a>; and so on.</p>\n<hr />\n<p>The first lemma is that a factorized probability distribution which includes <em>logical </em>uncertainty - uncertainty about the unknown output of known computations - appears to need <em>cause-like nodes</em> corresponding to this uncertainty.</p>\n<p>Suppose I have a calculator on Mars and a calculator on Venus.&nbsp; Both calculators are set to compute 123 * 456.&nbsp; Since you know their exact initial conditions - perhaps even their exact initial physical state - a standard reading of the causal graph would insist that any uncertainties we have about the output of the two calculators, should be uncorrelated.&nbsp; (By standard D-separation; if you have observed all the ancestors of two nodes, but have not observed any common descendants, the two nodes should be independent.)&nbsp; However, if I tell you that the calculator at Mars flashes \"56,088\" on its LED display screen, you will conclude that the Venus calculator's display is also flashing \"56,088\".&nbsp; (And you will conclude this before any ray of light could communicate between the two events, too.)</p>\n<p>If I was giving a long exposition I would go on about how if you have two envelopes originating on Earth and one goes to Mars and one goes to Venus, your conclusion about the one on Venus from observing the one on Mars does not of course indicate a faster-than-light physical event, but standard ideas about D-separation indicate that completely observing the <em>initial state of the calculators</em> ought to <em>screen off</em> any remaining uncertainty we have about their causal descendants so that the descendant nodes are uncorrelated, and the fact that they're still correlated indicates that there is a common unobserved factor, and this is our logical uncertainty about the result of the abstract computation.&nbsp; I would also talk for a bit about how if there's a small random factor in the transistors, and we saw three calculators, and two showed 56,088 and one showed 56,086, we would probably treat these as likelihood messages going up from nodes descending from the \"Platonic\" node standing for the <em>ideal</em> result of the computation - in short, it looks like our uncertainty about the unknown logical results of known computations, really does behave like a standard causal node from which the physical results descend as child nodes.</p>\n<p>But this is a short exposition, so you can fill in that sort of thing yourself, if you like.</p>\n<p>Having realized that our causal graphs contain nodes corresponding to logical uncertainties / the ideal result of Platonic computations, we next construe the counterfactuals of our expected utility formula to be counterfactuals over the <em>logical result</em> of the abstract computation corresponding to the expected utility calculation, rather than counterfactuals over any particular physical node.</p>\n<p>You treat your choice as determining the result of the logical computation, and hence all instantiations of that computation, and all instantiations of other computations dependent on that logical computation.</p>\n<p>Formally you'd use a Godelian diagonal to write:</p>\n<p>Argmax[A in Actions] in Sum[O in Outcomes](Utility(O)*P(<em>this computation</em> yields A []-&gt; O|rest of universe))</p>\n<p>(where P( X=x []-&gt; Y | Z ) means computing the counterfactual on the factored causal graph P, that surgically setting node X to x, leads to Y, given Z)</p>\n<p>Setting this up <em>correctly </em>(in accordance with standard constraints on causal graphs, like noncircularity) will solve (yield reflectively consistent, epistemically intuitive, systematically winning answers to) 95% of the Newcomblike problems in the literature I've seen, including Newcomb's Problem and other problems causing CDT to lose, the Smoking Lesion and other problems causing EDT to fail, Parfit's Hitchhiker which causes both CDT and EDT to lose, etc.</p>\n<p>Note that this does not solve the <a href=\"/lw/135/timeless_decision_theory_problems_i_cant_solve/\">remaining open problems</a> in TDT (though Nesov and Dai may have solved one such problem with their <a href=\"/lw/15m/towards_a_new_decision_theory/\">updateless decision theory</a>).&nbsp; Also, although this theory goes into much more detail about how to compute its counterfactuals than classical CDT, there are still some visible incompletenesses when it comes to generating causal graphs that include the uncertain results of computations, computations dependent on other computations, computations uncertainly correlated to other computations, computations that reason abstractly about other computations without simulating them exactly, and so on.&nbsp; On the other hand, CDT just has the entire counterfactual distribution rain down on the theory as mana from heaven (e.g. James Joyce, <em>Foundations of Causal Decision Theory</em>), so TDT is at least an improvement; and standard classical logic and standard causal graphs offer quite a lot of pre-existing structure here.&nbsp; (In general, understanding the causal structure of reality is an <a href=\"http://en.wikipedia.org/wiki/AI-complete\">AI-complete</a> problem, and so in philosophical dilemmas the causal structure of the problem is implicitly given in the story description.)</p>\n<p>Among the many other things I am skipping over:</p>\n<ul>\n<li>Some actual examples of where CDT loses and TDT wins, EDT loses and TDT wins, both lose and TDT wins, what I mean by \"setting up the causal graph correctly\" and some potential pitfalls to avoid, etc.</li>\n<li>A rather huge amount of reasoning which defines reflective consistency on a problem class; explains why reflective consistency is a rather strong desideratum for self-modifying AI; why the need to make \"precommitments\" is an expensive retreat to second-best and shows lack of reflective consistency; explains why it is desirable to win and get lots of money rather than just be \"reasonable\" (that is conform to pre-existing intuitions generated by a pre-existing algorithm); which notes that, considering the many pleas from people who <a href=\"http://www.geocities.com/eganamit/NoCDT.pdf\">want, but can't find</a> any good intermediate stage between CDT and EDT, it's a fascinating little fact that if you were rewriting your own source code, you'd rewrite it to one-box on Newcomb's Problem and smoke on the smoking lesion problem...</li>\n<li>...and so, having given many considerations of desirability in a decision theory, shows that the behavior of TDT corresponds to reflective consistency on a problem class in which your payoff is determined by the type of decision you make, but not sensitive to the exact algorithm you use apart from that - that TDT is the compact way of computing this desirable behavior we have previously defined in terms of reflectively consistent systematic winning.</li>\n<li>Showing that classical CDT, given self-modification ability, modifies into a crippled and inelegant form of TDT.</li>\n<li>Using TDT to fix the non-naturalistic behavior of Pearl's version of classical causality in which we're supposed to pretend that our actions are divorced from the rest of the universe - the counterfactual surgery, written out Pearl's way, will actually give poor predictions for some problems (like someone who two-boxes on Newcomb's Problem and believes that box B has a base-rate probability of containing a million dollars, because the counterfactual surgery says that box B's contents have to be independent of the action).&nbsp; TDT not only gives the correct prediction, but explains why the counterfactual surgery can have the form it does - if you condition on the initial state of the computation, this should screen off all the information you could get about outside things that affect your decision; then your actual output can be <em>further </em>determined <em>only</em> by the Godel-diagonal formula written out above, permitting the formula to contain a counterfactual surgery that <em>assumes</em> its own output, so that the formula does not need to infinitely recurse on calling itself.</li>\n<li>An account of some brief ad-hoc experiments I performed on IRC to show that a majority of respondents exhibited a decision pattern best explained by TDT rather than EDT or CDT.</li>\n<li>A rather huge amount of exposition of what TDT decision theory actually corresponds to in terms of philosophical intuitions, especially those about \"<a href=\"http://wiki.lesswrong.com/wiki/Free_will\">free will</a>\".&nbsp; For example, this is the theory I was using as hidden background when I wrote in \"<a href=\"/lw/ra/causality_and_moral_responsibility/\">Causality and Moral Responsibility</a>\" that factors like education and upbringing can be thought of as determining <em>which</em> person makes a decision - that <em>you</em> rather than someone else makes a decision - but that <em>the decision made by that particular person</em> is up to you.&nbsp; This corresponds to conditioning on the known initial state of the computation, and performing the counterfactual surgery over its output.&nbsp; I've actually done a lot of this exposition on OBLW without explicitly mentioning TDT, like <a href=\"/lw/r1/timeless_control/\">Timeless Control</a> and <a href=\"/lw/r0/thou_art_physics/\">Thou Art Physics</a> for reconciling determinism with choice (actually effective choice <em>requires </em>determinism, but this confuses humans for reasons given in <a title=\"http://lesswrong.com/lw/rb/possibility_and_couldness/\" rel=\"nofollow\" href=\"/lw/rb/possibility_and_couldness/\">Possibility and Could-ness</a>).&nbsp; But if you read the other parts of the <a href=\"http://wiki.lesswrong.com/wiki/Free_will_%28solution%29\">solution to \"free will\"</a>, and <em>then furthermore</em> explicitly formulate TDT, then this is what utterly, finally, completely, and without even a tiny trace of confusion or dissatisfaction or a sense of lingering questions, kills off entirely the question of \"free will\".</li>\n<li>Some concluding chiding of those philosophers who blithely decided that the \"rational\" course of action systematically loses; that rationalists defect on the Prisoner's Dilemma and hence we need a separate concept of \"social rationality\"; that the \"reasonable\" thing to do is determined by consulting pre-existing intuitions of reasonableness, rather than first looking at which agents walk away with huge heaps of money and then working out how to do it systematically; people who take their intuitions about free will at face value; assuming that counterfactuals are fixed givens raining down from the sky rather than non-observable constructs which we can construe in whatever way generates a winning decision theory; et cetera.&nbsp; And celebrating of the fact that rationalists can cooperate with each other, vote in elections, and do many other nice things that philosophers have claimed they can't.&nbsp; And suggesting that perhaps next time one should extend \"rationality\" a bit more credit before sighing and nodding wisely about its limitations.</li>\n<li>In conclusion, rational agents are not incapable of cooperation, rational agents are not constantly fighting their own source code, rational agents do not go around helplessly wishing they were less rational, and finally, rational agents <em>win</em>.</li>\n</ul>\n<p>Those of you who've read the <a href=\"/lw/r5/the_quantum_physics_sequence/\">quantum mechanics sequence</a> can extrapolate from past experience that I'm not bluffing.&nbsp; But it's not clear to me that writing this book would be my best possible expenditure of the required time.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 2, "5f5c37ee1b5cdee568cfb1db": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "szfxvS8nsxTgJLBHs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 52, "baseScore": 54, "extendedScore": null, "score": 8.9e-05, "legacy": true, "legacyId": "1511", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 54, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 227, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6ddcsdA2c2XpNpE5x", "de3xjFaACCAk6imzv", "c3wWnvgzdbRhNnNbQ", "FqJGfSrXphrcwpiZe", "YYLmZFEGKsjCKQZut", "NEeW7eSXThPz7o4Ne", "3buXtNiSK8gcRLMSG", "hc9Eg6erp6hk9bWhn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-19T22:24:38.805Z", "modifiedAt": null, "url": null, "title": "You have just been Counterfactually Mugged!", "slug": "you-have-just-been-counterfactually-mugged", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:05.306Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CronoDAS", "createdAt": "2009-02-27T04:42:19.587Z", "isAdmin": false, "displayName": "CronoDAS"}, "userId": "Q2oaNonArzibx5cQN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uHc2mCfJGiGvBc2Zo/you-have-just-been-counterfactually-mugged", "pageUrlRelative": "/posts/uHc2mCfJGiGvBc2Zo/you-have-just-been-counterfactually-mugged", "linkUrl": "https://www.lesswrong.com/posts/uHc2mCfJGiGvBc2Zo/you-have-just-been-counterfactually-mugged", "postedAtFormatted": "Wednesday, August 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20You%20have%20just%20been%20Counterfactually%20Mugged!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYou%20have%20just%20been%20Counterfactually%20Mugged!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuHc2mCfJGiGvBc2Zo%2Fyou-have-just-been-counterfactually-mugged%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=You%20have%20just%20been%20Counterfactually%20Mugged!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuHc2mCfJGiGvBc2Zo%2Fyou-have-just-been-counterfactually-mugged", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuHc2mCfJGiGvBc2Zo%2Fyou-have-just-been-counterfactually-mugged", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 142, "htmlBody": "<p>I'm going to test just how much the people here are committed to paying a Counterfactual Mugger, by playing Omega.</p>\n<p>I'm going to roll a die. If it doesn't come up 5 or 6, I'm going to ask Eliezer Yudkowsky to reply to this article with the comment \"I am a poopy head.\" If I roll a 5 or 6, I'm going to donate $20 to SIAI if I predict that Eliezer Yudkowsky will post the above comment.</p>\n<p>Because Eliezer has indicated that he would pay up when counterfactually mugged, I do predict that, if I roll a 5 or 6, he'll respond.</p>\n<p>::rolls die::</p>\n<p>Darn it! It's a 5. Well, I'm a man of my word, so...</p>\n<p>::donates::</p>\n<p>Um, let's try that again. (At least I've proven my honesty!)</p>\n<p>::rolls die::</p>\n<p>Okay, this time it's a 1.</p>\n<p>So, Eliezer, will you post a comment admitting that you're a poopy head?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YpHkTW27iMFR2Dkae": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uHc2mCfJGiGvBc2Zo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 7, "extendedScore": null, "score": 5.161775731676093e-07, "legacy": true, "legacyId": "1514", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-20T14:54:18.626Z", "modifiedAt": null, "url": null, "title": "Evolved Bayesians will be biased", "slug": "evolved-bayesians-will-be-biased", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:22.426Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dzr4GzB6cA3ARPPP9/evolved-bayesians-will-be-biased", "pageUrlRelative": "/posts/dzr4GzB6cA3ARPPP9/evolved-bayesians-will-be-biased", "linkUrl": "https://www.lesswrong.com/posts/dzr4GzB6cA3ARPPP9/evolved-bayesians-will-be-biased", "postedAtFormatted": "Thursday, August 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Evolved%20Bayesians%20will%20be%20biased&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEvolved%20Bayesians%20will%20be%20biased%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdzr4GzB6cA3ARPPP9%2Fevolved-bayesians-will-be-biased%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Evolved%20Bayesians%20will%20be%20biased%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdzr4GzB6cA3ARPPP9%2Fevolved-bayesians-will-be-biased", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdzr4GzB6cA3ARPPP9%2Fevolved-bayesians-will-be-biased", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 474, "htmlBody": "<p>I have a small theory which strongly implies that getting less biased is likely to make \"winning\" more difficult.</p>\n<p>Imagine some sort of evolving agents that follow vaguely Bayesianish logic. They don't have infinite resources, so they use a lot of heuristics, not direct Bayes rule with priors based on Kolmogorov complexity. Still, they employ a procedure A to estimate what the world is like based on data available, and a procedure D to make decisions based on their estimations, both of vaguely Bayesian kind.</p>\n<p>Let's be kind to our agents and grant that for every possible data and every possible decision they might have encountered in their ancestral environment, they make exactly the same decision as an ideal Bayesian agent would. A and D have been fine-tuned to work perfectly together.</p>\n<p>That doesn't mean that either A or D are perfect even within this limited domain. Evolution wouldn't care about that at all. Perhaps different biases within A cancel each other. For example an agent might overestimate snakes' dangerousness and also overestimate his snake-dodging skills - resulting in exactly the right amount of fear of snakes.</p>\n<p>Or perhaps a bias in A cancels another bias in D. For example an agent might overestimate his chance of success at influencing tribal policy, what neatly cancels his unreasonably high threshold for trying to do so.</p>\n<p>And then our agents left their ancestral environment, and found out that for some of the new situations their decisions aren't that great. They thought about it a lot, noticed how biased they are, and started a website on which they teach each other how to make their A more like perfect Bayesian's A. They even got quite good at it.</p>\n<p>Unfortunately they have no way of changing their D. So biases in their decisions which used to neatly counteract biases in their estimation of the world now make them commit a lot of mistakes even in situations where naive agents do perfectly well.</p>\n<p>The problem is that for virtually every A and D pair that could have possibly evolved, no matter how good the pair is together, neither A nor D would be perfect in isolation. In all likelihood both A and D are ridiculously wrong, just in a special way that never hurts. Improving one without improving the other, or improving just part of either A or D, will lead to much worse decisions, even if your idea of what the world is like gets better.</p>\n<p>I think humans might be a lot like that. As an artifact of evolution we make incorrect guesses about the world, and choices that would be incorrect given our guesses - just in a way that worked really well in ancestral environment, and works well enough most of the time even now. <a title=\"Depressive realism\" href=\"http://en.wikipedia.org/wiki/Depressive_realism\">Depressive realism</a> is a special case of this effect, but the problem is much more general.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4R8JYu4QF2FqzJxE5": 1, "LhX3F2SvGDarZCuh6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dzr4GzB6cA3ARPPP9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 28, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "1515", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-20T21:42:41.869Z", "modifiedAt": null, "url": null, "title": "How inevitable was modern human civilization - data", "slug": "how-inevitable-was-modern-human-civilization-data", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:59.808Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2utSryKeZ8hMdirhp/how-inevitable-was-modern-human-civilization-data", "pageUrlRelative": "/posts/2utSryKeZ8hMdirhp/how-inevitable-was-modern-human-civilization-data", "linkUrl": "https://www.lesswrong.com/posts/2utSryKeZ8hMdirhp/how-inevitable-was-modern-human-civilization-data", "postedAtFormatted": "Thursday, August 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20inevitable%20was%20modern%20human%20civilization%20-%20data&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20inevitable%20was%20modern%20human%20civilization%20-%20data%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2utSryKeZ8hMdirhp%2Fhow-inevitable-was-modern-human-civilization-data%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20inevitable%20was%20modern%20human%20civilization%20-%20data%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2utSryKeZ8hMdirhp%2Fhow-inevitable-was-modern-human-civilization-data", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2utSryKeZ8hMdirhp%2Fhow-inevitable-was-modern-human-civilization-data", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 867, "htmlBody": "<p>We have a sample of one modern human civilization, but there are some hints on how likely it was to happen.<br /><br />Major types of hints are:</p>\n<ul>\n<li> Time - if something happened extremely quickly; or extremely late, it suggests how likely it was.</li>\n<li>Independent invention - something that was invented independently multiple times is likelier; something invented only once in spite of plenty of time, isolation, and prerequisites is less likely.</li>\n</ul>\n<p>Data for:<a id=\"more\"></a></p>\n<ul>\n<li>Life seems to have developed extremely quickly after creation of Earth. <a href=\"http://en.wikipedia.org/wiki/Origin_of_life\">[Origin of life]</a></li>\n<li>Multicellularity seems to have evolved multiple times independently, at least in animals, fungi, and plants. <a href=\"http://en.wikipedia.org/wiki/Evolution_of_multicellularity\">[Evolution of multicellularity]</a></li>\n<li>Similar process also happened multiple time on higher level - eusociality developed in aphids, thrips, mole rats, termites, and at least 11 times in Hymenoptera (ants, bees, and wasps). <a href=\"http://en.wikipedia.org/wiki/Eusociality\">[Eusociality]</a></li>\n<li>Life did not die out on Earth, or on any particular environment where it previously thrived, in spite of major changes in temperature, composition of atmosphere, and multiple large scale disasters. This suggests life is very resilient. Every time life is wiped out in some part of Earth, it is quickly recolonized.</li>\n<li>Many different lineages of animals developed societies. <a href=\"http://en.wikipedia.org/wiki/Social_animal\">[Social animal]</a></li>\n<li>Many different lineages of animals developed communication. <a href=\"http://en.wikipedia.org/wiki/Animal_communication\">[Animal communication]</a></li>\n<li>All transitions from Middle Paleolithic onwards happened relatively fast to extremely fast on evolutionary scale. <a href=\"http://en.wikipedia.org/wiki/Paleolithic\">[Paleolithic]</a></li>\n<li>Invention of Mesolithic and Neolithic culture including agriculture, bow, boats, animal husbandry, pottery were all invented multiple times independently, in Afroeurasia, and Americas. <a href=\"http://en.wikipedia.org/wiki/Stone_Age\">[Stone Age]</a></li>\n<li>Likewise many of latter inventions including metallurgy, writing, money, and state were developed multiple times independently.</li>\n</ul>\n<p>Data against:</p>\n<ul>\n<li>Universe is not filled with technical civilizations. Some (dubious due to zero empirical evidence) models suggest once such civilization develops anywhere in the galaxy, it is very likely to colonize the entire galaxy in relatively short period of time. As it didn't happen, it's a strong evidence that there are very few, perhaps no, advanced technical civilizations in our galaxy; or anywhere else in the universe if our galaxy is a good representative. <a href=\"http://en.wikipedia.org/wiki/Fermi_paradox\">[Fermi paradox]</a></li>\n<li>Life can survive in a very wide range of circumstances, so there are plenty of places where we might expect to find life if its development was also likely. Mars, Venus, moons of Jupiter and Saturn, and perhaps some other places just in the Solar System might be sufficiently friendly to life. Yet, as far as we know, none ever developed in any of them, what puts strong limits on inevitability of life. <a href=\"http://en.wikipedia.org/wiki/Extremophile\">[Extremophile]</a></li>\n<li>In spite of all the theories proposed, we know of no mechanism under which creation of life seems even remotely plausible. Somewhere between the primordial soup (or equivalent) to the first replicator with reasonably stable heredity and metabolism (or equivalent), there's a large number of unknown steps of unknown but most likely extremely low probability. <a href=\"http://en.wikipedia.org/wiki/Origin_of_life\">[Origin of life]</a></li>\n<li>Nervous system evolved only once, about 3 billion years after life started, and nothing analogous to it ever evolved in any other lineage. <a href=\"http://en.wikipedia.org/wiki/Urbilaterian\">[Urbilaterian]</a></li>\n<li>It took life 3 billion years to reach stage of reasonably complex animals, what suggests it is not very likely. <a href=\"http://en.wikipedia.org/wiki/Cambrian_explosion\">[Cambrian explosion]</a></li>\n<li>Almost all animals seem to have very low encephalization quotients, suggesting that high intelligence is unlikely to develop. The only two major exceptions are primates and dolphins. <a href=\"http://www.aquaticape.org/eq.html\">[Brain size and EQ]</a></li>\n<li>Anything resembling human language developed only once. <a href=\"http://en.wikipedia.org/wiki/Origin_of_language\">[Origin of language]</a></li>\n<li>It is far from certain, but it seems that Neanderthals had the same capacity for speaking language as modern humans. This pushes development of language very far back, and suggest development of civilization even given language is unlikely. <a href=\"http://en.wikipedia.org/wiki/Neanderthal#Language\">[Neanderthal]</a></li>\n<li>Transition from animal life to something as complex as early Homo life (Lower Paleolithic), like manufacturing of tools, control of fire etc. seem to have happened only once in history of life, and extremely late. <a href=\"http://en.wikipedia.org/wiki/Human_evolution#Use_of_tools\">[Human evolution]</a></li>\n<li>Likewise transitions to Middle Paleolithic, and Upper Paleolithic seem to have happened only once. It could be argued that if it was&nbsp; isolated human populations had chance of developing innovations contained in them independently, but didn't.</li>\n<li>Some inventions like wheel, and iron smelting were invented only once. However by this time the world was going so fast and globalized enough that it's very weak evidence for their difficulty. Inventions later than antiquity also provide little evidence due to little time and little isolation.</li>\n</ul>\n<p>To me it looks like life, animals with nervous systems, Upper Paleolithic-style Homo, language, and behavioral modernity were all extremely unlikely events (notice how far ago they are - vaguely ~3.5bln, ~600mln, ~3mln, ~200k or ~600k, ~50k years ago) - except perhaps language and behavioral modernity might have been linked with each other, if language was relatively late (Homo sapiens only) and behavioral modernity more gradual (and its apparent suddenness is an artifact). Once we have behavioral modernity, modern civilization seems almost inevitable. Your interpretation might vary of course, but at least now you have a lot of data to argue for your position, in convenient format.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"25oxqHiadqM6Hf7Gn": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2utSryKeZ8hMdirhp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 32, "extendedScore": null, "score": 5.164055945502626e-07, "legacy": true, "legacyId": "1517", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 104, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-20T22:07:46.662Z", "modifiedAt": null, "url": null, "title": "Timeless Decision Theory and Meta-Circular Decision Theory", "slug": "timeless-decision-theory-and-meta-circular-decision-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:31.531Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fQv85Rd3pw789MHaX/timeless-decision-theory-and-meta-circular-decision-theory", "pageUrlRelative": "/posts/fQv85Rd3pw789MHaX/timeless-decision-theory-and-meta-circular-decision-theory", "linkUrl": "https://www.lesswrong.com/posts/fQv85Rd3pw789MHaX/timeless-decision-theory-and-meta-circular-decision-theory", "postedAtFormatted": "Thursday, August 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Timeless%20Decision%20Theory%20and%20Meta-Circular%20Decision%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATimeless%20Decision%20Theory%20and%20Meta-Circular%20Decision%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfQv85Rd3pw789MHaX%2Ftimeless-decision-theory-and-meta-circular-decision-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Timeless%20Decision%20Theory%20and%20Meta-Circular%20Decision%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfQv85Rd3pw789MHaX%2Ftimeless-decision-theory-and-meta-circular-decision-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfQv85Rd3pw789MHaX%2Ftimeless-decision-theory-and-meta-circular-decision-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3139, "htmlBody": "<p>(This started as a reply to <a href=\"/lw/15z/ingredients_of_timeless_decision_theory/1217\">Gary Drescher's comment here</a> in which he proposes a Metacircular Decision Theory (MCDT); but it got way too long so I turned it into an article, which also contains some amplifications on TDT which may be of general interest.)<a id=\"more\"></a></p>\n<p><em>Part 1:</em>&nbsp; How timeless decision theory does under the sort of problems that Metacircular Decision Theory talks about.</p>\n<blockquote>\n<p>Say we have an agent embodied in the universe. The agent knows some facts about the universe (including itself), has an inference system of some sort for expanding on those facts, and has a preference scheme that assigns a value to the set of facts, and is wired to select an action--specifically, the/an action that implies (using its inference system) the/a most-preferred set of facts.</p>\n<p>But without further constraint, this process often leads to a contradiction. Suppose the agent's repertoire of actions is A1, ...An, and the value of action Ai is simply i. Say the agent starts by considering the action A7, and dutifully evaluates it as 7. Next, it contemplates the action A6, and reasons as follows: \"Suppose I choose A6. I know I'm a utility-maximizing agent, and I already know there's another choice that has value 7. Therefore, if follows from my (hypothetical) choice of A6 that A6 has a value of at least 7.\" But that inference, while sound, contradicts the fact that A6's value is 6.</p>\n</blockquote>\n<p>This is why timeless decision theory is a causality-based decision theory.&nbsp; I don't recall if you've indicated that you've studied Pearl's synthesis of Bayesian networks and causal graphs(?) (though if not you should be able to come up to speed on them pretty quickly).</p>\n<p>So in the (standard) formalism of causality - just causality, never mind decision theory as yet - causal graphs give us a way to formally compute counterfactuals:&nbsp; We set the value of a particular node <em>surgically</em>.&nbsp; This means we <em>delete </em>the structural equations that would ordinarily give us the value at the node N_i as a function of the parent values P_i and the background uncertainty U_i at that node (which U_i must be uncorrelated to all other U, or the causal graph has not been fully factored).&nbsp; We delete this structural equation for N_i and make N_i parentless, so we don't send any likelihood messages up to the former parents when we update our knowledge of the value at N_i.&nbsp; However, we do send prior-messages from N_i to all of <em>its</em> descendants, maintaining the structural equations for the children of which N_i is a parent, and their children, and so on.</p>\n<p>That's the standard way of computing counterfactuals in the Pearl/Spirtes/Verma synthesis of causality, as found in \"Causality: Models, Reasoning, and Inference\" and \"Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference\".</p>\n<p>Classical causal decision theory says that your expected utility formula is over the <em>counterfactual </em>expectation of your <em>physical</em> act.&nbsp; Now, although the CDTs I've read have <em>not </em>in fact talked about Pearl - perhaps because it's a relatively recent mathematical technology, or perhaps because I last looked into the literature a few years back - and have just taken the counterfactual distribution as intuitively obvious mana rained from heaven - nonetheless it's pretty clear that their intuitions are operating pretty much the Pearlian way, via counterfactual surgery on the physical act.</p>\n<p>So in <em>calculating </em>the \"expected utility\" of an act - the computation that classical CDT uses to <em>choose </em>an action - CDT assumes the act to be <em>severed from its physical causal parents</em>.&nbsp; Let's say that there's a Smoking Lesion problem, where the same gene causes a taste for cigarettes and an increased probability of cancer.&nbsp; Seeing someone else smoke, we would infer that they have an increased probability of cancer - this sends a likelihood-message upward to the node which represents the probability of having the gene, and this node in turns sends a prior-message downward to the node which represents the probability of getting cancer.&nbsp; But the counterfactual surgery that CDT performs on its physical acts, means that it calculates the expected utility as though the physical act is severed from its parent nodes.&nbsp; So CDT calculates the expected utility as though it has the base-rate probability of having the cancer gene regardless of its act, and so chooses to smoke, since it likes cigarettes.&nbsp; This is the common-sense and reflectively consistent action, so CDT appears to \"win\" here in terms of giving the winning answer - but it's worth noting that the <em>internal </em>calculation performed is <em>wrong</em>; if you act to smoke cigarettes, your probability of getting cancer is <em>not </em>the base rate.</p>\n<p>And on Newcomb's Problem this internal error comes out into the open; the inside of CDT's counterfactual expected utility calculation, expects box B to contain a million dollars at the base rate, since it surgically severs the act of taking both boxes from the parent variable of your source code, which correlates to your previous source code at the moment Omega observed it, which correlates to Omega's decision whether to leave box B empty.</p>\n<p>Now turn to timeless decision theory, in which the (Godelian diagonal) expected utility formula is written as follows:</p>\n<blockquote>\n<p>Argmax[A in Actions] in Sum[O in Outcomes](Utility(O)*P(<em>this computation </em>yields A []-&gt; O|rest of universe))</p>\n</blockquote>\n<p>The interior of this formula performs counterfactual surgery to sever the <em>logical output </em>of the expected utility formula, from the <em>initial conditions </em>of the expected utility formula.&nbsp; So we do <em>not </em>conclude, <em>in the inside of the formula as it performs the counterfactual surgery</em>, that if-counterfactually A_6 is chosen over A_7 then A_6 must have higher expected utility.&nbsp; If-evidentially A_6 is chosen over A_7, then A_6 has higher expected utility - but this is not what the interior of the formula computes.&nbsp; As we <em>compute</em> the formula, the logical output is divorced from all parents; we cannot infer anything about its immediately logical precedents.&nbsp; This counterfactual surgery may be <em>necessary</em>, in fact, to stop an infinite regress in the formula, as it tries to model its own output in order to decide its own output; and this, arguably, is exactly <em>why </em>the decision counterfactual has the form it does - it is <em>why </em>we have to talk about counterfactual surgery within decisions in the first place.</p>\n<p><em>Descendants </em>of the logical output, however, continue to update their values within the counterfactual, which is why TDT one-boxes on Newcomb's Problem - both your current self's physical act, and Omega's physical act in the past, are logical-causal <em>descendants </em>of the computation, and are recalculated accordingly inside the counterfactual.</p>\n<p>If you desire to smoke cigarettes, this would be observed and screened off by conditioning on the <em>fixed initial conditions </em>of the computation - the fact that the utility function had a positive term for smoking cigarettes, would already tell you that you had the gene.&nbsp; (Eells's \"tickle\".)&nbsp; If you can't observe your own utility function then you are actually taking a step outside the timeless decision theory as formulated.</p>\n<p>So from the perspective of Metacircular Decision Theory - what is done with various facts - timeless decision theory can state very definitely how it treats the various facts, within the interior of its expected utility calculation.&nbsp; It does not <em>update </em>any physical or logical parent of the logical output - rather, it <em>conditions</em> on the initial state of the computation, in order to screen off outside influences; then no further inferences about them are made.&nbsp; And if you already know anything about the consequences of your logical output - its descendants in the logical causal graph - you will <em>re</em>compute what they <em>would have been</em> if you'd had a different output.</p>\n<p>This last codicil is important for cases like Parfit's Hitchhiker, in which Omega (or perhaps Paul Ekman), driving a car through the desert, comes across yourself dying of thirst, and will give you a ride to the city only if they expect you to pay them $100 <em>after</em> you arrive in the city.&nbsp; (With the whole scenario being <a href=\"/lw/tn/the_true_prisoners_dilemma/\">trued</a> by strict selfishness, no knock-on effects, and so on.)&nbsp; There is, of course, no way of forcing the agreement - so will you compute, <em>in the city</em>, that it is <em>better for you</em> to give $100 to Omega, after having <em>already </em>been saved?&nbsp; Both evidential decision theory and causal decision theory will give the losing (dying in the desert, hence reflectively inconsistent) answer here; but TDT answers, \"<em>If I had decided not to pay,</em> then Omega <em>would have</em> left me in the desert.\"&nbsp; So the expected utility of not paying $100 remains lower, <em>even after you arrive in the city,</em> given the way TDT computes its counterfactuals inside the formula - which is the dynamically and reflectively consistent and winning answer..&nbsp; And note that this answer is arrived at in one natural step, without needing explicit reflection, let alone precommitment - you will answer this way even if the car-driver Omega made its prediction without you being aware of it, so long as Omega can credibly establish that it was predicting you with reasonable accuracy rather than making a pure uncorrelated guess.&nbsp; (And since it's not a very complicated calculation, Omega knowing that you are a timeless decision theorist is credible enough.)</p>\n<blockquote>\n<p>I wonder if it might be open to the criticism that you're effectively postulating the favored answer to Newcomb's Problem (and other such scenarios) by postulating that when you surgically alter one of the nodes, you correspondingly alter the nodes for the other instances of the computation.</p>\n</blockquote>\n<p>This is where one would refer to the omitted extended argument about a calculator on Mars and a calculator on Venus, where both calculators were manufactured at the same factory on Earth and observed before being transported to Mars and Venus.&nbsp; If we manufactured two envelopes on Earth, containing the same letter, and transported them to Mars and Venus without observing them, then indeed the contents of the two envelopes would be correlated in our probability distribution, even though the Mars-envelope is not a cause of the Venus-envelope, nor the Venus-envelope a cause of the Mars-envelope, because they have a common cause in the background.&nbsp; But if we <em>observe </em>the common cause - look at the message as it is written, before being Xeroxed and placed into the two envelopes - then the standard theory of causality <em>requires </em>that our remaining uncertainty about the two envelopes be <em>uncorrelated</em>; we have observed the common cause and screened it off.&nbsp; If N_i is not a cause of N_j or vice versa, and you <em>know </em>the state of all the common ancestors A_ij of N_i and N_j, and you do <em>not </em>know the state of any mutual descendants D_ij of N_i and N_j, then the standard rules of causal graphs (D-separation) show that your probabilities at N_i and N_j must be independent.</p>\n<p>However, if you manufacture on Earth two calculators both set to calculate 123 * 456, and you have not yet performed this calculation in your head, then you can <em>observe completely the physical state of the two calculators</em> before they leave Earth, and yet still have <em>correlated </em>uncertainty about what result will flash on the screen on Mars and the screen on Venus.&nbsp; So this situation is simply <em>not </em>compatible with the mathematical axioms on causal graphs if you draw a causal graph in which the only common ancestor of the two calculators is the physical factory that made them and produced their correlated initial state.&nbsp; If you are to preserve the rules of causal graphs at all, you must have an additional node - which would logically seem to represent one's logical uncertainty about the abstract computation 123 * 456 - which is the parent of both calculators.&nbsp; Seeing the Venusian calculator flash the result 56,088, this physical event sends a likelihood-message to its parent node representing the logical result of 123 * 456, which sends a prior-message to its child node, the physical message flashed on the screen at Mars.</p>\n<p>A similar argument shows that if we have completely observed our own <em>initial </em>source code, and perhaps observed Omega's <em>initial</em> source code which contains a copy of our source code and the intention to simulate it, but we do not yet know our own decision, then the only way in which our uncertainty about our own physical act can possibly be correlated <em>at all </em>with Omega's past act to fill or leave empty the box B - given that neither act physically causes the other - is if there is some common ancestor node unobserved; and having already seen that our causal graph must include logical uncertainty if it is to stay factored, we can (must?) interpret this unobserved common node as the logical output of the known expected utility calculation.</p>\n<p>From this, I would argue, TDT follows.&nbsp; But of course it's going to be difficult to exhibit an algorithm that computes this - guessing unknown causal networks is an extremely difficult problem in machine learning, and only small such networks can be learned.&nbsp; In general, determining the causal structure of reality is AI-complete.&nbsp; And by interjecting logical uncertainty into the problem, we really are heading far beyond the causal networks that known machine algorithms can <em>learn.</em>&nbsp; But it <em>is</em> the case that if you rely on humans to learn the causal algorithm, then it is pretty clear that the Newcomb's Problem setup, if it is to be analyzed in causal terms at all, must have nodes corresponding to logical uncertainty, on pain of violating the axioms governing causal graphs.&nbsp; Furthermore, in being told that Omega's leaving box B full or empty correlates to our <em>decision</em> to take only one box or both boxes, <em>and</em> that Omega's act lies in the past, <em>and</em> that Omega's act is not directly influencing us, <em>and </em>that we have not found any other property which would screen off this uncertainty even when we inspect our own source code / psychology in advance of knowing our actual decision, <em>and</em> that our computation is the only <em>direct</em> ancestor of our logical output, then we're being told in unambiguous terms (I think) to make our own physical act and Omega's act a common descendant of the unknown logical output of our known computation.&nbsp; (A counterexample in the form of another causal graph compatible with the same data is welcome.)&nbsp; And of course we could make the problem very clear by letting the agent be a computer program and letting Omega have a copy of the source code with superior computing power, in which case the logical interpretation is very clear.</p>\n<p>So these are the facts which TDT takes into account, and the facts which it ignores.&nbsp; The Nesov-Dai updateless decision theory is even stranger - as far as I can make out, it ignores <em>all</em> facts except for the fact about which inputs have been received by the logical version of the computation it implements.&nbsp; If combined with TDT, we would interpret UDT as having a never-updated weighting on all possible universes, and a causal structure (causal graph, presumably) on those universes.&nbsp; Any given logical computation in UDT will count all instantiations of itself in all universes which have received exactly the same inputs - even if those instantiations are being imagined by Omega in universes which UDT would ordinarily be interpreted as \"known to be logically inconsistent\", like universes in which the third decimal digit of pi is 3.&nbsp; Then UDT calculates the counterfactual consequences, weighted across all imagined universes, using its causal graphs on each of those universes, of setting the logical act to A_i.&nbsp; Then it maximizes on A_i.</p>\n<p>I would ask if, applying Metacircular Decision Theory from a \"common-sense human base level\", you see any case in which additional facts should be taken into account, or other facts ignored, apart from those facts used by TDT (UDT).&nbsp; If not, and if TDT (UDT) are reflectively consistent, then TDT (UDT) is the fixed point of MCDT starting from a human baseline decision theory.&nbsp; Of course this can't actually be the case because TDT (UDT) are incomplete with respect to the <a href=\"/lw/135/timeless_decision_theory_problems_i_cant_solve/\">open problems</a> cited earlier, like logical ordering of moves, and choice of conditional strategies in response to conditional strategies.&nbsp; But it would be the way I'd pose the problem to you, Gary Drescher - MCDT is an interesting way of looking things, but I'm still trying to wrap my mind around it.</p>\n<p><em>Part 2: Metacircular Decision Theory as reflection criterion.</em></p>\n<blockquote>\n<p>MCDT's proposed criterion is this: the agent makes a meta-choice about which facts to omit when making inferences about the hypothetical actions, and selects the set of facts which lead to the best outcome if the agent then evaluates the original candidate actions with respect to that choice of facts. The agent then iterates that meta-evaluation as needed (probably not very far) until a fixed point is reached, i.e. the same choice (as to which facts to omit) leaves the first-order choice unchanged. (It's ok if that's intractable or uncomputable; the agent can muddle through with some approximate algorithm.)</p>\n<p>...In other words, metacircular consistency isn't just a <em>test</em> that we'd like the decision theory to pass. Metacircular consistency <em>is</em> the theory; it <em>is</em> the algorithm.</p>\n</blockquote>\n<p>But it looks to me like MCDT has to start from some particular base theory, and different base theories may have different fixed points (or conceivably, cycles).&nbsp; In which case we can't yet call MCDT itself a complete theory specification.&nbsp; When you talk about which facts <em>would</em> be wise to take into account, or ignore, (or recompute counterfactually even if they already have known values?), then you're imagining different source codes (or MCDT specifications?) that an agent could have; and calculating the benefits of adopting these different source codes, relative to the way the <em>current </em>base theory computes \"adopting\" and \"benefit\"</p>\n<p>For example, if you start with CDT and apply MCDT at 7am, it looks to me like \"use TDT (UDT) for all cases where my source code has a physical effect after 7am, and use CDT for all cases where the source code had a physical effect before 7am or a correlation stemming from common ancestry\" is a reflectively stable fixed point of MCDT.&nbsp; Whenever CDT asks \"<em>What if</em> I took into account these different facts?\", it will say, \"But Omega would not be physically affected by my self-modification, so clearly it can't benefit me in any way.\"&nbsp; If the MCDT criterion is to be applied in a different and intuitively appealing way that has only one fixed point (up to different utility functions) then this would establish MCDT as a good candidate for <em>the</em> decision theory, but right now it does look to me like <em>a</em> reflective consistency test.&nbsp; But maybe this is because I haven't yet wrapped my mind around the MCDT's fact-treatment-based decomposition of decision theories, or because you've already specified further mandatory structure in the base theory how the <em>effect of</em> ignoring or taking into account some particular fact is to be computed.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1, "YpHkTW27iMFR2Dkae": 1, "MP8NqPNATMqPrij4n": 1, "5f5c37ee1b5cdee568cfb1db": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fQv85Rd3pw789MHaX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 31, "extendedScore": null, "score": 5.4e-05, "legacy": true, "legacyId": "1516", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HFyWNBnDNEDsDNLrZ", "c3wWnvgzdbRhNnNbQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-21T09:26:24.196Z", "modifiedAt": null, "url": null, "title": "ESR's New Take on Qualia", "slug": "esr-s-new-take-on-qualia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:37.157Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "billswift", "createdAt": "2009-02-28T05:59:56.578Z", "isAdmin": false, "displayName": "billswift"}, "userId": "WBoeSNFkZ4q8nRenK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Jou2pbxseb5beQoGE/esr-s-new-take-on-qualia", "pageUrlRelative": "/posts/Jou2pbxseb5beQoGE/esr-s-new-take-on-qualia", "linkUrl": "https://www.lesswrong.com/posts/Jou2pbxseb5beQoGE/esr-s-new-take-on-qualia", "postedAtFormatted": "Friday, August 21st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20ESR's%20New%20Take%20on%20Qualia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AESR's%20New%20Take%20on%20Qualia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJou2pbxseb5beQoGE%2Fesr-s-new-take-on-qualia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=ESR's%20New%20Take%20on%20Qualia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJou2pbxseb5beQoGE%2Fesr-s-new-take-on-qualia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJou2pbxseb5beQoGE%2Fesr-s-new-take-on-qualia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 40, "htmlBody": "<p>http://esr.ibiblio.org/?p=1192#more-1192</p>\n<p>ADDED:&nbsp; Even if you disagree with ESR's take, and many will, this is the <strong>clearest</strong> definition I have seen on what qualia is.&nbsp; So it should present a useful starting point, even for those who strongly disagree, to argue from.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8e9e8fzXuW5gGBS3F": 2, "XSryTypw5Hszpa4TS": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Jou2pbxseb5beQoGE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 5, "extendedScore": null, "score": 5.165204359681805e-07, "legacy": true, "legacyId": "1520", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-23T09:15:56.889Z", "modifiedAt": null, "url": null, "title": "The Journal of (Failed) Replication Studies", "slug": "the-journal-of-failed-replication-studies", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:23.290Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Gritsenko", "createdAt": "2009-02-28T23:58:30.709Z", "isAdmin": false, "displayName": "Vladimir_Gritsenko"}, "userId": "Kpd7MYkooiBxKze7C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3imJjn5eDu3xtvG27/the-journal-of-failed-replication-studies", "pageUrlRelative": "/posts/3imJjn5eDu3xtvG27/the-journal-of-failed-replication-studies", "linkUrl": "https://www.lesswrong.com/posts/3imJjn5eDu3xtvG27/the-journal-of-failed-replication-studies", "postedAtFormatted": "Sunday, August 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Journal%20of%20(Failed)%20Replication%20Studies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Journal%20of%20(Failed)%20Replication%20Studies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3imJjn5eDu3xtvG27%2Fthe-journal-of-failed-replication-studies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Journal%20of%20(Failed)%20Replication%20Studies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3imJjn5eDu3xtvG27%2Fthe-journal-of-failed-replication-studies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3imJjn5eDu3xtvG27%2Fthe-journal-of-failed-replication-studies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<p>One of <a href=\"http://seedmagazine.com/\">Seed Magazine</a>'s \"<a href=\"http://revminds.seedmagazine.com/\">Revolutionary Minds</a>\" is <a href=\"http://revminds.seedmagazine.com/revminds/member/moshe_pritsker/\">Moshe Pritsker</a>, who created the <a href=\"http://www.jove.com/\">Journal of Visualized Experiments</a>, which to me looks like a very cool idea. I imagine that early on it may have looked <a href=\"http://www.overcomingbias.com/2008/10/academics-in-cl.html\">somewhat</a> <a href=\"http://www.overcomingbias.com/2008/04/arbitrary-silli.html\">silly</a> (\"he can't implant engineered tissue in a rat heart and he calls himself a <em>scientist</em>?!\"), so it's nice to know JoVE is picking up pace.</p>\n<p>Many folks keep pointing out how published research is itself <a href=\"http://www.overcomingbias.com/2007/09/false-findings-.html\">biased</a> <a href=\"http://www.newscientist.com/article/dn7915\">towards</a> <a href=\"http://www.overcomingbias.com/2009/07/popular-fields-less-accurate.html\">positive</a> <a href=\"http://www.overcomingbias.com/2007/10/health-hope-spr.html\">results</a>, and how replication (and failed replication!) trumps mere \"first!!!11\" publication. If regular journals don't have good incentives to publish \"mere\" (failed) replication studies, why not create a journal that would be dedicated entirely to them? I can't speak about the logistics, but I imagine it can be anything from a start-up (a la JoVE) to an open depository (a la <a href=\"http://arxiv.org/\">arxiv.org</a>).</p>\n<p>I am not part of academia, but I understand that there are a few folks here who are. What do you say?</p>\n<p>[EDIT: Andrew Kemendo notes two such journals in the comments: <a href=\"http://www.jnrbm.com/\">http://www.jnrbm.com/</a> and <a href=\"http://www.jnr-eeb.org/index.php/jnr\">http://www.jnr-eeb.org/index.php/jnr</a>.]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vg4LDxjdwHLotCm8w": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3imJjn5eDu3xtvG27", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 12, "extendedScore": null, "score": 5.169893655748102e-07, "legacy": true, "legacyId": "1504", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-24T22:08:23.183Z", "modifiedAt": null, "url": null, "title": "Working Mantras", "slug": "working-mantras", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:37.187Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uaPRRBGRjxZd6QePE/working-mantras", "pageUrlRelative": "/posts/uaPRRBGRjxZd6QePE/working-mantras", "linkUrl": "https://www.lesswrong.com/posts/uaPRRBGRjxZd6QePE/working-mantras", "postedAtFormatted": "Monday, August 24th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Working%20Mantras&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWorking%20Mantras%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuaPRRBGRjxZd6QePE%2Fworking-mantras%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Working%20Mantras%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuaPRRBGRjxZd6QePE%2Fworking-mantras", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuaPRRBGRjxZd6QePE%2Fworking-mantras", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 657, "htmlBody": "<p>While working with Marcello on AI this summer, I've noticed that I have some standard mantras that I invoke in my inner dialogue (though only on appropriate occasions, not as a literal repeated mantra).&nbsp; This says something about which tropes are most often useful - in my own working life, anyway!<a id=\"more\"></a></p>\n<ul>\n</ul>\n<ol>\n<li>\"If anyone could actually update on the evidence, they would have a power <a href=\"/lw/ur/crisis_of_faith/\">far beyond that of Nobel Prize winners</a>.\"<br />(When encountering a need to discard some idea to which I was attached, or admit loss of a sunk cost on an avenue that doesn't seem to be working out.)</li>\n<li>\"<a href=\"http://wiki.lesswrong.com/wiki/Litany_of_Tarski\">The universe is already like [X], or not</a>; if it is then I can only minimize the embarrassment by <a href=\"http://wiki.lesswrong.com/wiki/Litany_of_Gendlin\">admitting it</a> and adapting as fast as possible.\"<br />(If the first mantra doesn't work; then I actually visualize the universe already being a certain way, so that I can see the penalty for being a universe that works a certain way and yet believing otherwise.)</li>\n<li>\"First understand the problem, then solve it.\"<br />(If getting too caught up in <a href=\"/lw/ka/hold_off_on_proposing_solutions/\">proposing solutions</a>, or discouraged when solutions don't work out - the immediate task at hand is just to <em>understand the problem,</em> and one may ask whether progress has been made on this.&nbsp; From <em>full</em> understanding a solution usually follows quickly.)</li>\n<li>\"<a href=\"http://www.paulgraham.com/head.html\">Load the problem.</a>\"<br />(Try to get your mind involved and processing the various aspects of it.)</li>\n<li>\"<a href=\"/lw/q9/the_failures_of_eld_science/\">Five minutes is enough time to have an insight.</a>\"<br />(If my mind seems to be going empty.)</li>\n<li>\"Ask only one thing of your mind and it may give it to you.\"<br />(Focusing during work, or trying to load the problem into memory before going to sleep each night, in hopes of putting the subconscious to work on it.)</li>\n<li>\"<a href=\"/lw/up/shut_up_and_do_the_impossible/\">Run right up the mountain!</a>\"<br />(My general visualization of the FAI problem; a huge, blank, impossibly high wall, which I have to run up as quickly as possible.&nbsp; Used to accomodate the sense of the problem being much larger than whatever it is I'm working on right now.)</li>\n<li>\"When the problem is solved, that thought will be a wasted motion in retrospect.\"<br />(I first enunciated this as an explicit general principle when explaining to Marcello why e.g. one doesn't worry about people who have failed to solve a problem previously.&nbsp; When you actually solve the problem, those thoughts will predictably not have contributed anything in retrospect.&nbsp; So if your goal is to solve the problem, you should focus on the object-level problem, instead of worrying about whether you have <a href=\"/lw/qs/einsteins_superpowers/\">sufficient status to solve it</a>.&nbsp; The same rule applies to many other habitual worries, <em>or </em><a href=\"http://wiki.lesswrong.com/wiki/Occam%27s_Imaginary_Razor\">reasoning effort expended to reassure against them</a><em>,</em> that would predictably appear as wasted motion in retrospect, after actually solving the problem.)</li>\n<li>\"There's always just enough time when you do something right, no more, no less.\"<br /> (A quote from C. J. Cherryh's <em>Paladin,</em> used when feeling rushed.&nbsp; I don't think it's true literally or otherwise, but it seems to convey an important wordless sentiment.</li>\n<li>\"See the truth, not what you expect or hope.\"<br />(When expecting the answer to go a particular way, or hoping for the answer to go a particular way, is exerting detectable pressure on an ongoing inquiry.)</li>\n</ol> \n<ul>\n</ul>\n<p>I don't listen to music while working, because of studies showing that, e.g., programmers listening to music are equally competent at implementing a given algorithm, but much less likely to notice that the algorithm's output is always equal to its input.&nbsp; However, I sometimes think of the theme <a href=\"http://www.youtube.com/watch?v=ZF29ATE3PTE&amp;fmt=18\">Emiya #0</a> when feeling fatigued or trying to make a special demand on my mind.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hXTqT62YDTTiqJfxG": 1, "udPbn9RthmgTtHMiG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uaPRRBGRjxZd6QePE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 39, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "1525", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BcYBfG8KomcpcxkEg", "uHYYA32CKgKT3FagE", "ZxR8P8hBFQ9kC8wMy", "nCvvhFBaayaXyuBiD", "5o4EZJyqmHY4XgRCY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-25T07:34:52.254Z", "modifiedAt": null, "url": null, "title": "Decision theory: An outline of some upcoming posts", "slug": "decision-theory-an-outline-of-some-upcoming-posts", "viewCount": null, "lastCommentedAt": "2018-07-04T05:58:51.379Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sLxFqs8fdjsPdkpLC/decision-theory-an-outline-of-some-upcoming-posts", "pageUrlRelative": "/posts/sLxFqs8fdjsPdkpLC/decision-theory-an-outline-of-some-upcoming-posts", "linkUrl": "https://www.lesswrong.com/posts/sLxFqs8fdjsPdkpLC/decision-theory-an-outline-of-some-upcoming-posts", "postedAtFormatted": "Tuesday, August 25th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Decision%20theory%3A%20An%20outline%20of%20some%20upcoming%20posts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecision%20theory%3A%20An%20outline%20of%20some%20upcoming%20posts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsLxFqs8fdjsPdkpLC%2Fdecision-theory-an-outline-of-some-upcoming-posts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Decision%20theory%3A%20An%20outline%20of%20some%20upcoming%20posts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsLxFqs8fdjsPdkpLC%2Fdecision-theory-an-outline-of-some-upcoming-posts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsLxFqs8fdjsPdkpLC%2Fdecision-theory-an-outline-of-some-upcoming-posts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1772, "htmlBody": "<p>Last August or so, Eliezer asked <a href=\"/user/Steve_Rayhawk/\">Steve Rayhawk</a> and myself to attempt to solve Newcomb&rsquo;s problem together.&nbsp; This project served a couple of purposes:<br />a.&nbsp; Get an indication as to our FAI research abilities.<br />b.&nbsp; Train our reduction-muscles.<br />c.&nbsp; Check whether Eliezer&rsquo;s (unseen by us) timeless decision theory is a point that outside folks tend to arrive at independently (at least if starting from the rather substantial clues on OB/LW), and whether anything interestingly new came out of an independent attempt.<br /><br />Steve and I (and, briefly but helpfully, <a href=\"/user/Liron/\">Liron Shapira</a>) took our swing at Newcomb.&nbsp; We wrote a great mass of notes that have been sitting on our hard drives, but hadn&rsquo;t stitched them together into a single document.&nbsp; I&rsquo;d like to attempt a Less Wrong sequence on that subject now.&nbsp; Most of this content is stuff that Eliezer, Nesov, and/or Dai developed independently and have been referring to in their posts, but I&rsquo;ll try to present it more fully and clearly.&nbsp; I learned a bit of this from Eliezer/Nesov/Dai&rsquo;s recent posts.</p>\n<p>Here&rsquo;s the outline, to be followed up with slower, clearer blog posts if all goes well:<a id=\"more\"></a></p>\n<p>0.&nbsp; <a href=\"/lw/16i/confusion_about_newcomb_is_confusion_about/\"><strong>Prelude: &ldquo;Should&rdquo; depends on counterfactuals.</strong></a>&nbsp; Newcomb's problem -- the problem of what Joe \"should\" do, to earn most money -- is the problem of which type of counterfactuals best cash out the question \"Should Joe take one box or two?\".&nbsp; Disagreement about Newcomb's problem is disagreement about what sort of counterfactuals we should consider, when we try to figure out what action Joe should take.</p>\n<p><br />1.&nbsp; <strong>My goal in this sequence is to reduce &ldquo;should&rdquo; as thoroughly as I can.</strong>&nbsp; More specifically, I&rsquo;ll make an (incomplete, but still useful) attempt to:</p>\n<ul>\n<li>Make it even more clear that our naive conceptions of &ldquo;could&rdquo; and &ldquo;should&rdquo; are conceptual inventions, and are not Physically Irreducible Existent Things.&nbsp; (Written <a href=\"/lw/174/decision_theory_why_we_need_to_reduce_could_would/\">here</a>.)</li>\n<li>Consider why one might design an agent that uses concepts like &ldquo;could&rdquo; and &ldquo;should&rdquo; (hereafter a &ldquo;Could/Should Agent&rdquo;, or &ldquo;CSA&rdquo;), rather than designing an agent that acts in some other way.&nbsp; Consider what specific concepts of &ldquo;could&rdquo; and &ldquo;should&rdquo; are what specific kinds of useful. (This is meant as a more thorough investigation of the issues treated by Eliezer in &ldquo;<a href=\"/lw/rb/possibility_and_couldness/\">Possibility and Couldness</a>&rdquo;.)</li>\n<li>Consider why evolution ended up creating us as approximate CSAs.&nbsp; Consider what kinds of CSAs are likely to be how common across the multiverse.</li>\n</ul>\n<p>2.&nbsp; <strong>A non-vicious regress.&nbsp;</strong>&nbsp; Suppose we&rsquo;re designing Joe, and we want to maximize his expected winnings.&nbsp; What notion of &ldquo;should&rdquo; <em>should</em> we design Joe to use?&nbsp; There&rsquo;s a regress here, in that creator-agents with different starting decision theories will design agents that have different starting decision theories.&nbsp; But it is a <a href=\"/lw/s0/where_recursive_justification_hits_bottom/\">non-vicious regress</a>.&nbsp; We can gain understanding by making this regress explicit, and asking under what circumstances agents with decision theory X will design future agents with decision theory Y, for different values of X and Y.<br /><br />3a.&nbsp; <strong>When will a CDT-er build agents that use &ldquo;could&rdquo; and &ldquo;should&rdquo;?</strong>&nbsp; Suppose again that you&rsquo;re designing Joe, and that Joe will go out in a world and win utilons on your behalf.&nbsp; What kind of Joe-design will maximize your expected utilons?<br /><br />If we assume nothing about Joe&rsquo;s world, we might find that your best option was to design Joe to act as a bundle of wires which happens to have advantageous physical effects, and which doesn&rsquo;t act like an agent at all.<br /><br />But suppose Joe&rsquo;s world has the following handy property: suppose Joe&rsquo;s actions have effects, and Joe&rsquo;s &ldquo;policy&rdquo;, or the actions he &ldquo;would have taken&rdquo; in response to alternative inputs also have effects, but the details of Joe&rsquo;s internal wiring doesn&rsquo;t otherwise matter.&nbsp; (I'll call this the \"policy-equivalence assumption\").&nbsp; Since Joe&rsquo;s wiring doesn&rsquo;t matter, you can, without penalty, insert whatever computation you like into Joe&rsquo;s insides.&nbsp; And so, if you yourself can think through what action Joe &ldquo;should&rdquo; take, you can build wiring that sits inside Joe, carries out the same computation you would have used to figure out what action Joe &ldquo;should&rdquo; take, and then prompts that action.<br /><br />Joe then inherits his counterfactuals from you: Joe&rsquo;s model of what &ldquo;would&rdquo; happen &ldquo;if he acts on policy X&rdquo; is <em>your</em> model of what &ldquo;would&rdquo; happen <em>if you design an agent, Joe, who</em> acts according to policy X.&nbsp; The result is &ldquo;act according to the policy my creator would have chosen&rdquo; decision theory, now-A.K.A. &ldquo;Updateless Decision Theory&rdquo; (UDT).&nbsp; UDT one-boxes on Newcomb&rsquo;s problem and pays the $100 in the counterfactual mugging problem.<br /><br />3b.&nbsp; But it is only when computational limitations are thrown in that designing Joe to be a CSA leaves you better off than designing Joe to be your top-pick hard-coded policy.&nbsp; So, to understand where CSAs really come from, we&rsquo;ll need eventually to consider how agents can use limited computation.<br /><br />3c.&nbsp; When will a UDT-er build agents that use &ldquo;could&rdquo; and &ldquo;should&rdquo;?&nbsp; The answer is similar to that for a CDT-er.<br /><br />3d.&nbsp; <strong>CSAs are only useful in a limited domain.&nbsp; </strong>In our derivations above, CSAs' usefulness depends on the policy-equivalence assumption.&nbsp; Therefore, if agents&rsquo; computation has important effects apart from its effects on the agents&rsquo; actions, the creator agent may be ill-advised to create any sort of CSA.*&nbsp; For example, if the heat produced by agents&rsquo; physical computation has effects that are as significant as the agent&rsquo;s &ldquo;chosen actions&rdquo;, CSAs may not be useful.&nbsp; This limitation suggests that CSAs may not be useful in a post-singularity world, since in such a world matter may be organized to optimize for computation in a manner far closer to physical efficiency limits, and so the physical side-effects of computation may have more relative significance compared to the computation&rsquo;s output.<br /><br />4.&nbsp; <strong>What kinds of CSAs make sense?</strong>&nbsp; More specifically, what kinds of counterfactual &ldquo;coulds&rdquo; make sense as a basis for a CSA?<br /><br />In part 4, we noted that when Joe&rsquo;s policy is all that matters, you can stick your &ldquo;What policy should Joe have?&rdquo; computer inside Joe, without disrupting Joe&rsquo;s payoffs.&nbsp; Thus, you can build Joe to be a &ldquo;carry out the policy my creator would think best&rdquo; CSA.<br /><br />It turns out this trick can be extended.<br /><br />Suppose you aren't a CDT-er.&nbsp; Suppose you are more like one of Eliezer's \"timeless\" agents.&nbsp; When you think about what you &ldquo;could&rdquo; and &ldquo;should&rdquo; do, you do your counterfactuals, not over what you alone will do, but over what you and a whole set of other agents &ldquo;running the same algorithm you are running&rdquo; will simultaneously do.&nbsp; For example, you may (in your model) be choosing what algorithm you and <a href=\"/lw/tn/the_true_prisoners_dilemma/#n16\">Clippy</a> will both send into a one-shot prisoner&rsquo;s dilemma.<br /><br />Much as was the case with CDT-ers, so long as your utility estimate depends only on the algorithm&rsquo;s outputs and not its details you can choose the algorithm you&rsquo;re creating to be an &ldquo;updateless&rdquo;, &ldquo;act according to the policy your creator would have chosen&rdquo; CSA.<br /><br />5. <strong>Which types of CSAs will create which other types of CSAs under what circumstances?&nbsp;</strong> I go through the list above.<br /><br />6.&nbsp; A partial list of remaining problems, and of threads that may be useful to pull on.<br /><br />6.a.&nbsp; <strong>Why design CSAs at all, rather than look-up tables or non-agent-like jumbles of wires?</strong>&nbsp; Computational limitations are part of the answer: if I design a CSA to play chess with me, it knows what move it has to respond to, and so can focus its computation on that specific situation.&nbsp; Does CSAs&rsquo; usefulness in focussing computation shed light on what type of CSAs to design?<br /><br />6.b.&nbsp; <strong>More generally, how did evolution come to build us humans as approximate CSAs?&nbsp;</strong> And what kinds of decision theory should other agent-design processes, in other parts of the multiverse, be expected to create?<br /><br />6.c.&nbsp; <strong>What kind of a CSA are you?</strong>&nbsp; What are you really asking, when you ask what you &ldquo;should&rdquo; do in Newcomb&rsquo;s problem?&nbsp; What algorithm do <em>you</em> actually run, and want to run, there?<br /><br />6.d.&nbsp; <strong>Two-player games: avoiding paradoxes and infinite types.</strong>&nbsp; I used a simplification above: I assumed that agents took in inputs from a finite list, and produced outputs from a finite list.&nbsp; This simplification does not allow for two general agents to, say, play one another in prisoner&rsquo;s dilemma while seeing one another&rsquo;s policy.&nbsp; If I can choose any policy that is a function from the set of you policy options to {C, D}, and you can choose any policy that is a function from the set of <em>my</em> policy options to {C, D}, each of us must have more policy-options than the other.<br /><br />Some other formalism is needed for two-player games.&nbsp; (Eliezer lists this problem, and the entangled problem 6.e, in his Timeless decision theory: problems I can&rsquo;t solve.)<br /><br />6.e.&nbsp; <strong>What do real agents do in the situations Eliezer has been calling &ldquo;problems of logical priority before time&rdquo;?</strong>&nbsp; Also, what are the natural alternative decision theories to use for such problems, and is there one which is so much more natural than others that we might expect it to populate the universe, just as Eliezer hopes his &ldquo;timeless decision theory&rdquo; might accurately describe the bulk of decision agents in the universe?<br /><br />Note that this question is related to, but harder than, the more limited question in 7.b.&nbsp; It is harder because we are now asking our CSAs to produce actions/outputs in more complicated situations.</p>\n<p>6.f.&nbsp; <strong>Technical machinery for dealing with timeless decision theories.</strong>&nbsp; [Steve Rayhawk added this item.]&nbsp; As noted in 5 above, and as Eliezer noted in <a href=\"/lw/15z/ingredients_of_timeless_decision_theory/\">Ingredients of Timeless Decision Theory</a>, we may wish to use a decision theory where we are &ldquo;choosing&rdquo; the answer to a particular math problem, or the output of a particular algorithm. Since the output of an algorithm is a logical question, this requires reasoning under uncertainty about the answers to logical questions. Setting this up usefully, without paradoxes or inconsistencies, requires some work. Steve has a gimmick for treating part of this problem. (The gimmick starts from the hierarchical Bayesian modeling idea of a hyperprior, used in its most general form: a prior belief about conditional probability tables for other variables.)</p>\n<hr />\n<p>*More precisely: CSAs&rsquo; usefulness breaks down if the creator&rsquo;s world-model includes important effects from its choice of which agent it creates, apart from the effects of that agent&rsquo;s policy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sLxFqs8fdjsPdkpLC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 31, "extendedScore": null, "score": 5.1e-05, "legacy": true, "legacyId": "1527", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["B7bMmhvaufdtxBtLW", "gxxpK3eiSQ3XG3DW7", "3buXtNiSK8gcRLMSG", "C8nEXTcjZb9oauTCW", "szfxvS8nsxTgJLBHs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-25T18:54:32.609Z", "modifiedAt": null, "url": null, "title": "How does an infovore manage information overload?", "slug": "how-does-an-infovore-manage-information-overload", "viewCount": null, "lastCommentedAt": "2021-07-19T23:31:59.909Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "haig", "createdAt": "2009-02-27T10:14:21.038Z", "isAdmin": false, "displayName": "haig"}, "userId": "vTx26uFumCryKRuFQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/83WNLnCgyYDdEv6Nu/how-does-an-infovore-manage-information-overload", "pageUrlRelative": "/posts/83WNLnCgyYDdEv6Nu/how-does-an-infovore-manage-information-overload", "linkUrl": "https://www.lesswrong.com/posts/83WNLnCgyYDdEv6Nu/how-does-an-infovore-manage-information-overload", "postedAtFormatted": "Tuesday, August 25th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20does%20an%20infovore%20manage%20information%20overload%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20does%20an%20infovore%20manage%20information%20overload%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F83WNLnCgyYDdEv6Nu%2Fhow-does-an-infovore-manage-information-overload%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20does%20an%20infovore%20manage%20information%20overload%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F83WNLnCgyYDdEv6Nu%2Fhow-does-an-infovore-manage-information-overload", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F83WNLnCgyYDdEv6Nu%2Fhow-does-an-infovore-manage-information-overload", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 219, "htmlBody": "<p>I am, and have been for most of my life, an information glutton. &nbsp;The internet has made my affliction worse by providing me with the equivalent of an unlimited buffet of both nutritious as well as junk food for my brain which never leaves my side. &nbsp;A fire hose of data focused straight into my mind's mouth. &nbsp;If the brain food is mostly high quality, and I'm exercising my grey matter vigorously enough to warrant such high volumes of knowledge, then it's not that much of a problem. &nbsp;However, I've recently crossed a threshold where I seem to be spending more time navigating this buffet rather than consuming the food. &nbsp;</p>\n<p>Ok, dropping the metaphor and getting to the point, I need to know how I can efficiently minimize the amount of time I spend staying abreast of the things I should now so I can maximizing the time I spend actually learning them and hopefully having ample time left over to be productive at applying that knowledge. &nbsp;Mind you, I am pretty diligent when it comes to avoiding the frivolous youtube clips, emails, and reddit/slashdot/etc. refreshes. &nbsp;That isn't the problem. &nbsp;The problem is figuring out which books, research papers, and blogs to stay aware of, and how to automate such a system. &nbsp;Any techniques you would like to share? &nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 1, "DbMQGrxbhLxtNkmca": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "83WNLnCgyYDdEv6Nu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 3, "extendedScore": null, "score": 5.175553577721583e-07, "legacy": true, "legacyId": "1529", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-25T20:01:21.664Z", "modifiedAt": null, "url": null, "title": "Confusion about Newcomb is confusion about counterfactuals", "slug": "confusion-about-newcomb-is-confusion-about-counterfactuals", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:24.305Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B7bMmhvaufdtxBtLW/confusion-about-newcomb-is-confusion-about-counterfactuals", "pageUrlRelative": "/posts/B7bMmhvaufdtxBtLW/confusion-about-newcomb-is-confusion-about-counterfactuals", "linkUrl": "https://www.lesswrong.com/posts/B7bMmhvaufdtxBtLW/confusion-about-newcomb-is-confusion-about-counterfactuals", "postedAtFormatted": "Tuesday, August 25th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Confusion%20about%20Newcomb%20is%20confusion%20about%20counterfactuals&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConfusion%20about%20Newcomb%20is%20confusion%20about%20counterfactuals%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB7bMmhvaufdtxBtLW%2Fconfusion-about-newcomb-is-confusion-about-counterfactuals%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Confusion%20about%20Newcomb%20is%20confusion%20about%20counterfactuals%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB7bMmhvaufdtxBtLW%2Fconfusion-about-newcomb-is-confusion-about-counterfactuals", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB7bMmhvaufdtxBtLW%2Fconfusion-about-newcomb-is-confusion-about-counterfactuals", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 582, "htmlBody": "<p>(This is the first, and most newcomer-accessible, post in a planned <a href=\"/lw/16f/decision_theory_an_outline_of_some_upcoming_posts/\">sequence</a>.)</p>\n<p>Newcomb's Problem:<em></em></p>\n<p style=\"padding-left: 30px;\"><em>Joe walks out onto the square.&nbsp; As he walks, a majestic being flies by Joe's head with a box labeled \"brain scanner\", drops two boxes on the ground, and departs the scene.&nbsp; A passerby, known to be trustworthy, comes over and <a href=\"http://wiki.lesswrong.com/wiki/Newcomb%27s_problem\">explains</a>...</em></p>\n<p style=\"padding-left: 30px;\"><em>If Joe aims to get the most money, should Joe take one box or two?</em></p>\n<p>What are we asking when we ask what Joe \"should\" do?&nbsp; It is common to cash out \"should\" claims as counterfactuals: \"If Joe were to one-box, he would make more money\".&nbsp;&nbsp; This method of translating \"should\" questions does seem to capture something of what we mean: we do seem to be asking how much money Joe can expect to make \"if he one-boxes\" vs. \"if he two-boxes\".&nbsp; The trouble with this translation, however, is that it is not clear what world \"if Joe were to one-box\" should refer to -- and, therefore, it is not clear how much money we should say Joe would make, \"if he were to one-box\".&nbsp; After all, Joe is a deterministic physical system; his current state (together with the state of his future self's past light-cone) fully determines what Joe's future action will be.&nbsp; There is no Physically Irreducible Moment of Choice, where this same Joe, with his own exact actual past, \"can\" go one way or the other.</p>\n<p><a id=\"more\"></a>To restate the situation more clearly: let us suppose that this Joe, standing here, is poised to two-box.&nbsp; In order to determine how much money Joe \"would have made if he had one-boxed\", let us say that we imagine reaching in, with a magical sort of world-surgery, and altering the world so that Joe one-boxes instead.&nbsp; We then watch to see how much money Joe receives, in this surgically altered world.&nbsp; <br /><br />The question before us, then, is what sort of magical world-surgery to execute, before we watch to see how much money Joe \"would have made if he had one-boxed\".&nbsp; And the difficulty in Newcomb&rsquo;s problem is that there is not one but two obvious world-surgeries to consider.&nbsp; First, we might surgically reach in, after Omega's departure, and alter Joe's box-taking only -- leaving Omega's prediction about Joe untouched.&nbsp; Under this sort of world-surgery, Joe will do better by two-boxing: <br /><br />Expected value ( Joe's earnings if he two-boxes | some unchanged probability distribution on Omega's prediction )&nbsp; &gt; <br />Expected value ( Joe's earnings if he one-boxes | the same unchanged probability distribution on Omega's prediction ).<br /><br />Second, we might surgically reach in, after Omega's departure, and simultaneously alter both Joe's box-taking and Omega's prediction concerning Joe's box-taking.&nbsp; (Equivalently, we might reach in before Omega's departure, and surgically alter the insides of Joe brain -- and, thereby, alter both Joe's behavior and Omega's prediction of Joe's behavior.)&nbsp; Under this sort of world-surgery, Joe will do better by one-boxing:<br /><br />Expected value ( Joe's earnings if he one-boxes | Omega predicts Joe accurately)&nbsp; &gt;<br />Expected value ( Joe's earnings if he two-boxes | Omega predicts Joe accurately).<br /><br /><strong>The point:</strong> Newcomb's problem -- the problem of what Joe \"should\" do, to earn most money -- is the problem which type of world-surgery best cashes out the question \"Should Joe take one box or two?\".&nbsp; Disagreement about Newcomb's problem is disagreement about what sort of world-surgery we should consider, when we try to figure out what action Joe should take.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 2, "fihKHQuS5WZBJgkRm": 2, "Ng8Gice9KNkncxqcj": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B7bMmhvaufdtxBtLW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 47, "baseScore": 53, "extendedScore": null, "score": 9.3e-05, "legacy": true, "legacyId": "1530", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 53, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sLxFqs8fdjsPdkpLC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-26T18:34:25.269Z", "modifiedAt": null, "url": null, "title": "Mathematical simplicity bias and exponential functions", "slug": "mathematical-simplicity-bias-and-exponential-functions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:33.965Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CRDqzWFPRb6jX6b6t/mathematical-simplicity-bias-and-exponential-functions", "pageUrlRelative": "/posts/CRDqzWFPRb6jX6b6t/mathematical-simplicity-bias-and-exponential-functions", "linkUrl": "https://www.lesswrong.com/posts/CRDqzWFPRb6jX6b6t/mathematical-simplicity-bias-and-exponential-functions", "postedAtFormatted": "Wednesday, August 26th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mathematical%20simplicity%20bias%20and%20exponential%20functions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMathematical%20simplicity%20bias%20and%20exponential%20functions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCRDqzWFPRb6jX6b6t%2Fmathematical-simplicity-bias-and-exponential-functions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mathematical%20simplicity%20bias%20and%20exponential%20functions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCRDqzWFPRb6jX6b6t%2Fmathematical-simplicity-bias-and-exponential-functions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCRDqzWFPRb6jX6b6t%2Fmathematical-simplicity-bias-and-exponential-functions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 434, "htmlBody": "<p>One of biases that are extremely prevalent in science, but are rarely talked about anywhere, is bias towards models that are mathematically simple and easier to operate on. Nature doesn't care all that much for mathematical simplicity. In particular I'd say that as a good first approximation, if you think something fits exponential function of either growth or decay, you're wrong. We got so used to exponential functions and how convenient they are to work with, that we completely forgot the nature doesn't work that way.</p>\n<p>But what about nuclear decay, you might be asking now... That's as close you get to real exponential decay as you get... and it's not nowhere close enough. Well, here's a log-log graph of Chernobyl release versus theoretical exponential function, plotted in log-log.</p>\n<p><img src=\"http://upload.wikimedia.org/wikipedia/en/4/46/Chernobylvsru106.png\" alt=\"\" width=\"490\" height=\"299\" /></p>\n<p>Well, that doesn't look all that exponential... The thing is that even if you have perfect exponential decay processes as with single nucleotide decay, when you start mixing a heterogeneous group of such processes, the exponential character is lost. Early in time faster-decaying cases dominate, then gradually those that decay more slowly, somewhere along the way you might have to deal with results of decay (<a href=\"http://en.wikipedia.org/wiki/Depleted_uranium#Radiological_hazards\">pure depleted uranium gets <strong>more</strong> radioactive with time at first, not less, as it decays into low half-life nuclides</a>), and perhaps even some processes you didn't have to consider (like creation of fresh radioactive nuclides via cosmic radiation).</p>\n<p>And that's the ideal case of counting how much radiation a sample produces, where the underlying process is exponential by the basic laws of physics - it still gets us orders of magnitude wrong. When you're measuring something much more vague, and with much more complicated underlying mechanisms, like changes in population, economy, or processing power.</p>\n<p>According to <a href=\"http://en.wikipedia.org/wiki/List_of_countries_by_GDP_%28PPP%29\">IMF</a>, world economy in 2008 was worth 69 trillion $ PPP. Assuming 2% annual growth and naive growth models, the entire world economy produces 12 cents PPP worth of value in entire first century. And assuming fairly stable population, an average person in 3150 will produce more that the entire world does now. And with enough time dollar value of one hydrogen atom will be higher than current dollar value of everything on Earth. And of course with proper time discounting of utility, life of one person now is worth more than half of humanity millennium into the future - exponential growth and exponential decay are both equally wrong.</p>\n<p>To me they all look like clear artifacts of our growth models, but there are people who are so used to them that they treat predictions like that seriously.</p>\n<p>In case you're wondering, <a href=\"http://en.wikipedia.org/wiki/Gross_world_product#Total_World_GDP\">here are some estimates of past world GDP</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CRDqzWFPRb6jX6b6t", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 16, "extendedScore": null, "score": 5.177880354171123e-07, "legacy": true, "legacyId": "1531", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 87, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-26T19:08:30.789Z", "modifiedAt": null, "url": null, "title": "A Rationalist's Bookshelf: The Mind's I (Douglas Hofstadter and Daniel Dennett, 1981)", "slug": "a-rationalist-s-bookshelf-the-mind-s-i-douglas-hofstadter", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:44.174Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "colinmarshall", "createdAt": "2009-07-08T22:54:59.882Z", "isAdmin": false, "displayName": "colinmarshall"}, "userId": "AioHEjiBZshwBwALh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h5K34vLcTEKpMxCZj/a-rationalist-s-bookshelf-the-mind-s-i-douglas-hofstadter", "pageUrlRelative": "/posts/h5K34vLcTEKpMxCZj/a-rationalist-s-bookshelf-the-mind-s-i-douglas-hofstadter", "linkUrl": "https://www.lesswrong.com/posts/h5K34vLcTEKpMxCZj/a-rationalist-s-bookshelf-the-mind-s-i-douglas-hofstadter", "postedAtFormatted": "Wednesday, August 26th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Rationalist's%20Bookshelf%3A%20The%20Mind's%20I%20(Douglas%20Hofstadter%20and%20Daniel%20Dennett%2C%201981)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Rationalist's%20Bookshelf%3A%20The%20Mind's%20I%20(Douglas%20Hofstadter%20and%20Daniel%20Dennett%2C%201981)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh5K34vLcTEKpMxCZj%2Fa-rationalist-s-bookshelf-the-mind-s-i-douglas-hofstadter%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Rationalist's%20Bookshelf%3A%20The%20Mind's%20I%20(Douglas%20Hofstadter%20and%20Daniel%20Dennett%2C%201981)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh5K34vLcTEKpMxCZj%2Fa-rationalist-s-bookshelf-the-mind-s-i-douglas-hofstadter", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh5K34vLcTEKpMxCZj%2Fa-rationalist-s-bookshelf-the-mind-s-i-douglas-hofstadter", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3944, "htmlBody": "<p><img style=\"float: left;\" src=\"http://www.erowid.org/library/books/images/minds_i.jpg\" alt=\"\" width=\"233\" height=\"350\" /></p>\n<p>When the <a href=\"/lw/12d/recommended_reading_for_new_rationalists\">call to compile a reading list for new rationalists</a> went out, contributor <a href=\"/user/djcb\">djcb</a> responded by suggesting <em>The Mind's I: Fantasies and Reflections on Self and Soul</em>, a compilation of essays, fictions and excerpts \"composed and arranged\" by Douglas Hofstadter and Daniel Dennett. Cut to me peering guiltily over my shoulder, my own copy sitting unread on the shelf, peering back.<br /><br />The book presents Hofstadter and Dennett's co-curation of 27 pieces, some penned by the curators themselves, meant to \"reveal\" and \"make vivid\" a set of \"perplexities,\" to wit: \"What is the mind?\" \"Who am I?\" \"Can mere matter think or feel?\" \"Where is the soul?\" Two immediate concerns arise. First, <em>The Mind's I</em>'s 1981 publication date gives it access to the vast majority of what's been thought and said about these questions, but robs it of of any intellectual progress toward the answers made in the nearly three decades since. (This turns out not to be an issue, as most of the answers seem to have drawn no closer in the 1980s, 1990s or 2000s.) Second, those sound suspiciously similar to questions hazily articulated by college freshmen, less amenable to \"rational inquiry\" than to \"dorm furniture and bad weed.\" They don't quite pass the \"<em>man</em> test,\" an reversal of the fortune cookie \"in bed\" game: simply tack \"<em>man</em>\" onto the <em>beginning</em> of each question and see who laughs. \"<em>Man</em>, who am I?\" \"<em>Man</em>, where is the soul?\" \"<em>Man</em>, can matter think or feel?\"<br /><br />Hofstadter and Dennett's fans know, however, that their analyses rise a cut above, engaged as they are in the admirable struggle to excise the navel-gazing from traditionally navel-gazey topics. The beauty is that they've always accomplished this, together and separately, not by making these issues less exciting but by making them <em>more</em> so. Their clear, stimulating exegeses, explorations and speculations brim with both the enthusiasm of the thrilled neophyte and the levelheadedness of the seasoned surveyor. They even do it humorously, Hofstadter with his zig-zaggy punniness and Dennett with his wit that somehow stays just north of goofy. Thus armed, they've taken on such potentially dangerous topics as whether words and thoughts follow rules, how the animate emerges from the inanimate (Hofstader's rightly celebrated <em>G&ouml;del, Escher, Bach: An Eternal Golden Braid</em>) and consciousness (most of Dennett's career), on the whole safely.<br /><br />But obviously this is not a \"pure\" (whatever that might mean) Hofstadter-Dennett joint; rather, their editorial choices compose one half and their personal commentaries &mdash; \"reflections,\" they banner them &mdash; on the fruits of those choices compose the other. Nearly every selection, whether a short story, article, novel segment or dialogue, leads into an original discussion and evaluation by, as they sign them, D.R.H. and/or D.C.D. They affirm, they contradict, they expand, they question, they veer off in their own directions; the reflections would make a neat little book on the topics at hand by themselves.<br /><br /><a id=\"more\"></a>Terribly inelegant a strategy as this is, perhaps I'll cover the pieces one-by-one:</p>\n<ol>\n<li> The first section, on self and identity, opens strong with Jorge Luis Borges, for my money the finest short fictionalist of ideas... ever, probably. His well-known \"Borges and I\" plays with the distinction between Borges the man and Borges the public author, treating the two as ontologically distinct. Even if that idea has passed into the realm of old hat, the story containing it holds up by the razor-sharpness of its language, even in translation: \"It would be an exaggeration to say that ours is a hostile relationship: I live, let myself go on living, so that Borges may contrive his literature, and this literature justifies me.\"</li>\n<li> The mystic Douglas Harding, in \"On Having No Head\", recounts the moment he discovered he had no head. As he describes the various consequences of this realization, ht essay becomes essentially a riff on the fact that it's impossible for anybody to directly see their own, physical head and thus that they know of its existence that much less definitively. At some point, this wears out its welcome; Harding stretches an intellectual snack into a dinner, following the meal with a coda about how, aw, it's all just semantic confusion over the verb <em>to see</em>.</li>\n<li> Harold Morowitz's \"Rediscovering the Mind\" has not, it must be said, stuck deeply in my own. My forgetfulness may be due in part to the fact that reductionist examination of the mind and the challenges such an approach faces have entered, and remained in, common discourse since the article saw <em>Psychology Today</em> publication in 1980, so its ideas couldn't strike me with what I assume to be the intended force of novelty. As a brief introduction to the problems of reductionism and the mind, though, I imagine it's pretty effective.</li>\n<li> Kicking off the section on the concept of the soul, Alan Turing's groundbreaking 1950 <em>Mind</em> article \"Computing Machinery and Intelligence\" proposes his now-eponymous test for machine intelligence. One might assume the years have been especially unkind to Turing's (at least nominally) technology-minded essay and Dennett and Hofstadter's accompanying commentary, but no, machine intelligence remains elusive, and thus both texts merit continued digestion. </li>\n<li> Hofstadter extends the Turing talk with \"The Turing Test: A Coffeehouse Conversation\", setting up an intellectual triangle between \"Chris, a physics student; Pat, a biology Student; and Sandy, a philosophy student.\" (The unisex names turn out to fold into one of the discussion's main points, though I found keeping everyone straight a tad difficult.) The three throw down their collective six cents on the possibilities, implications and validity of the famous test. While illuminating, the piece spreads its content way too thin, its 23 pages littered with conversational detritus: \"That's a sad story.\" \"Good question.\" \"How so?\" But to be fair, these problems hamper most written dialogues, as does the reader's sneaking suspicion that they're being somehow led down the garden path. As dialogues &mdash; trialogues? &mdash; go, though, this one serves a nutritional portion.</li>\n<li> \"The Princess Ineffabelle\", the first of the collection's three imaginings by Polish science fiction writer Stanislaw Lem, envisions a sort of proto-virtual-reality device that can load up an entire era and its people on punch cards (!) and simulate it with find-grained precision. A king, seeking a princess extant only within the machine's world, inquires as to how he might go about having himself digitized and inserted into said world. But the digital king wouldn't really be the <em>king</em> king, right? Or would that matter?</li>\n<li> Terrell Miedaner's eerie \"The Soul of Martha, A Beast\" envisions a courtroom demonstration wherein a chimpanzee, wired to a device that translates its brain's neural patterns into a simple English vocabulary. A discussion ensues about whether the animal, \"uttering\" strings like \"Hello! Hello! I Martha Happy Happy Chimp,\" truly merits the designation \"intelligent,\" after which the researcher puts his charge to death:\n<blockquote>As the unsuspecting chimpanzee placed the poisoned gift into her mouth and bit, Belinsky conceived of an experiment he had never before considered. He turned on the switch. \"Candy Candy Thank You Belinsky Happy Happy Martha.\"<br /><br />Then her voice stopped of its own accord. She stiffened, then relaxed in her master's arms, dead.<br /><br />But brain death is not immediate. The final sensory discharge of some circuit within her inert body triggered a brief burst of neural pulsations decoded as \"Hurt Martha Hurt Martha.\"<br /><br />Nothing happened for another two seconds. Then randomly triggered neural discharges no longer having anything to do with the animal's lifeless body send one last pulsating signal to the world of men.<br /><br />\"Why Why Why Why &mdash;\"<br /><br />A soft electrical click stopped the testimony.</blockquote>\nThe operative concept, discussed in Hofstadter's reflection, emerges as the determination of what degree of linguistic evidence, if any, indicates the presence of \"intelligence,\" \"consciousness,\" a \"soul\" &mdash; pick one or more of your favorite fuzzily-defined concept and attempt to determine what separates them. All the book's pieces present more questions than answers, and Miedaner's first especially so. Still, it stays with you, as does his next piece...</li>\n<li> \"The Soul of the Mark III Beast\", in which a lawyer invites a timid woman to \"kill\" a robot. The mechanical creature, a steely cross between a mouse and a beetle, \"eats\" electrical current from the wall, \"flees\" its pursuer's hammer blows and \"bleeds\" oil when damages. These points of superficial congruence with the animal kingdom seriously freak the woman out, and she's really got to maintain to finish the job. In short: the fuzzy-to-nonexistent boundary between the sentient and the nonsentient, illustrated (in prose).</li>\n<li> Allen Wheelis' \"Spirit\", which heads the section on the mind's physical foundation (also known as the brain), comes off as relatively insubstantial but addresses concerns certain readers may harbor. To wit: it feels as if we humans possess some ineffable \"spirit.\" But it's tough to pin down, though it may animate the rest of the natural world as well. Hofstadter boils it down skillfully in the reflection: \"Wheelis portrays the eerie, disorienting view that modern science has given us of our place in the scheme of things. Many scientists, not to mention humanists, find this a very difficult view to swallow and look for some kind of spiritual essence, perhaps intangible, that would distinguish living beings, particularly humans, from the inanimate rest of the universe. How does anima come from atoms?\" A Big Question indeed.</li>\n<li> \"Selfish Genes and Selfish Memes\" is a selection from Richard Dawkins' <em>The Selfish Gene</em>. If you have not read this book, minimize your browser and do so. I'll wait.</li>\n<li> \"Prelude... Ant Fugue\" is a selection from Douglas Hofstadter's <em>G&ouml;del, Escher, Bach</em>. If you have not read this book, minimize your browser and do so. I'll wait. (It's the dialogue comparing the human brain to an ant farm, which I still find ever-so-slightly mindblowing to this day.)</li>\n<li> Our mental hardware undergoes the severest possible parting-out in Arnold Zuboff's \"<a href=\"http://themindi.blogspot.com/2007/02/147.html\">The Story of a Brain</a>\", a fiction and thought experiment &mdash; in several senses of the term &mdash; where a group of scientists remove the healthy brain from a young man's otherwise abnormally decaying body, stick it in a vat and give it \"experiences\" by way of electrical stimulation. But then a drunken night watchman accidentally separates the brain's hemispheres, damage the scientists attempt to repair with remote communication devices allowing neurons from one half to stimulate the others'. Over the next thousand years, thanks to widespread scientific-community interest, fiddly readjustment of the apparatus and a general shortage of brains in vats, each of this brain's individual neurons finds it way, step by logical step, to a separate laboratory, all supposedly linked together. And the labs occasionally replace their neurons. It's the brain as Abraham Lincoln's proverbial original axe: the blade's been replaced once and the handle twice. At what exact point can we no longer call it a brain, as we normally understand the concept? As with many of the other concepts on which the book touches, discrete boundaries remain elusive.</li>\n<li> Daniel Dennett's \"<a href=\"http://www.newbanner.com/SecHumSCM/WhereAmI.html\">Where Am I</a>?\" leads into the section on mind-as-software. (See also the <a href=\"http://www.youtube.com/watch?v=U_8yo5hacKM\">video dramatization</a>!) The story follows a fictionalized version of Dennett himself as he's hired on to a secret government project to dig up a brain-destroying underground warhead. Specifically, Dennett's meant to go down there and dig it up by hand. Removing his own brain and installing it safely in a vat, the government dudes set it up so Dennett can remotely control his own body, in a way, but feel, more or less &mdash; he compellingly describes the newly-introduced little technical quirks &mdash; as if he's still a brain and body organically united. But who's the \"real\" Dennett? Shades of first-year philosophy classes' rhetorical questions about who you'd be if your divided brain was split between two bodies, I know, but Dennett presents it in a delightfully entertaining way, as is his wont.</li>\n<li> With \"<a href=\"http://themindi.blogspot.com/2007/02/chapter-14-where-was-i.html\">Where Was I?</a>\", David Hawley Sanford takes another angle on Dennett's concept, positing a different government operation &mdash; again, top-secret &mdash; to develop devices that transfer remotely-gathered sense experiences so accurately to the local user's body that the meaning of reference to his actual location &mdash; and how one goes about determining his actual location &mdash; grows muddled, questionable, a matter of unsettlable debate.</li>\n<li> The next chapter excerpts Justin Leibler's <em>Beyond Rejection</em>, a sci-fi novel about a murdered man who wakes up to find his brain loaded &mdash; via brain-backup tapes, a standard piece of personal technology in Liebler's imagined future &mdash; into a new body: specifically, a woman's. (More specifically, a woman with a tail's. The tail is not explained, at least in the reprinted segment.) The ten pages include a suitably creepy sequence wherein the protagonist wakes up, disoriented due to incomplete brain-body synchronization and disturbed by the two new \"dead cancerous mounds\" of \"disconnected, nerveless jelly\" &mdash; breasts, in other words &mdash; he'll have to learn to live with. While not especially striking technically or biologically, the passage definitely evokes the right set of feelings.</li>\n<li> A selection from Rudy Rucker's slightly goofy-sounding novel <a href=\"http://themindi.blogspot.com/2007/02/chapter-16-software.html\"><em>Software</em></a> illustrates, after a fashion, the questions of what specific component or components, if any, drive consciousness, and what <em>self</em>-consciousness has to do with that consciousness. And, as Dennett's reflection clarifies, if a supposedly conscious entity's consciousness were to cease existing, how would we know?</li>\n<li> Christopher Cherniak's short story \"The Riddle of the Universe and Its Solution\" posits a computer program whose output, when viewed in full by a human, forces that human's brain into an infinite loop &mdash; \"perhaps even the ultimate Zen state of <em>satori</em>,\" Hofstadter reflects &mdash; \"locking it up\" for good. Before slipping into this coma, each victim utters the word \"Aha!\" This analogizes the human brain &mdash; and only the human brain, since the program, \"the G&ouml;del sentence for human Turing machine,\" is shown not to induce the coma in apes &mdash; to an actual computer in terms of operating with enough logical strictness to wilfully &mdash; loaded word, I know, and so do all the authors involved &mdash; incapacitate itself. Hofstadter ties this into the broader topic of self-referential loops and what they might already have to do with the mind.</li>\n<li> The book's second Stanislaw Lem selection, \"The Seventh Sally or How Trurl's Own Perfection Led to No Good\", opens the section on created selves and free will. I found it just slightly too weirdly-written to draw much from directly, but Dennett and Hofstadter's much clearer reflection &mdash; no pun intended &mdash; drops a few intriguing thoughts about looking for \"souls\" inhabiting simulated worlds.</li>\n<li> The third Lem piece, \"Non Serviam\", comes immediately after. Though thematically similar to its predecessor &mdash; the nature of simulation, the parallels between simulated world and the non-simulated world &mdash; it's also slightly less opaque. (<em>Slightly</em> less.)</li>\n<li> Raymond Smullyan's dialogue \"Is God a Taoist?\" has a mortal pleading with his creator to strip him of free will:\n<blockquote>GOD: Why would you wish not to have free will?<br /><br />MORTAL: Because free will means moral responsibility, and moral responsibility is more than I can bear.<br /><br />GOD: Why do you find moral responsibility so unbearable?<br /><br />MORTAL: Why? I honestly can&rsquo;t analyze why; all I know is that I do.<br /><br />GOD: All right, in that case suppose I absolve you from all moral responsibility, but still leave you with free will. Will this be satisfactory?<br /><br />MORTAL: (after a pause): No, I am afraid not.</blockquote>\nAnd it goes on like this, the mortal desperately trying to reason with the god and find a means of being freed from what's bothering him about morality, goodness, responsibility and choice. Eventually, matters either evolve or devolve, depending upon how you look at it, to whether the god or the mortal exists, how one can know the other exists, whether the god is the mortal or the mortal the god, who's on first, what's on second and so on and so forth. In his reflection, Hofstadter references an apropos Marvin Minsky quote: \"Logic doesn&rsquo;t apply to the real world.\"</li>\n<li> A second dose of Borges comes in \"The Circular Ruins\", the story of an isolated wizard who dreams up an actual human being. When he's imagined this potential boy's every possible detail, he requests that the god Fire create him. Fire complies, incarnating the wizard's vision, but in such a way that he's still not quit real enough to be burned by fire (the element). When the wizard walks into a fire, he find's that he doesn't burn &mdash; and thus is, himself, someone else's dream. We're back in Intro to Philosophy's territory, in a way: are you dreaming right now, or are you not? How do you know? \"Is this philosophical play with the ideas of dreaming and reality just idle?\" Dennett asks. \"Isn't there a no-nonsense 'scientific' stance from which we objectively distinguish between the things that are really there and mere fictions? Perhaps there is, but then on which side of the divide we put ourselves? Not our physical bodies, but our selves?\" The answers appear to be \"nah\" and \"we don't know,\" or maybe \"<em>mu</em>.\"</li>\n<li> John Searle's \"Minds, Brains, and Programs\" searches for the seat of intelligence with what's now called the \"Chinese room\" thought experiment, in which one imagines a human sealed in a room under whose door an unseen interlocutor passes slips of paper with sentences written in Chinese. With no understanding of the Chinese language, the man in the room follows a series of mechanistic procedures to write out a reply on another slip and pass it back under the door. Repeat. If the fellow on the door's other side believes he's conducting a conversation in writing with a genuine Chinese speaker &mdash; the rule-following scribbler inside having thus passed a sort of Turing test &mdash; who's to say that somewhere in the man, the rules and the slips of paper, there is <em>not</em> a genuine understanding of Chinese? But of course we find that ridiculous, so there's got to be something within the brain that we can use as a line of demarcation. Nothing we've identified yet or that may be identifiable at all &mdash;, but <em>something</em>. Dennett and Hofstadter don't find this line of thought convincing, identifying a few sleight-of-hand points in their reflection, but I didn't feel it a waste of time to hear the notion proposed. Proposed rather unconvincingly, sure, but quite articulately! (More so than my summary gives it credit for, certainly.)</li>\n<li> The brief but piquant \"An Unfortunate Dualist\" by Raymond Smullyan envisions a devout dualist in great pain. Though he'd like to kill himself, he fears hurting others, commiting moral crime and/or enduring punishment in the afterlife. Fortunately, he finds a drug that destroys only the soul, leaving the body intact and operational as before. A friend secretly injects him with the drug the night before he goes out to pick up a dosage himself. Upon ingesting it of his own volition, the dualist, of course, feels no different: disappointed, he believes himself to still possess a soul and endure suffering. \"Doesn't all this suggest,\" Smullyan asks, \"that perhaps there might be something just a little wrong with dualism?\" Indeed, but who's really a dualist anymore?</li>\n<li> Thomas Nagel answers his essay's title question \"What Is It Like to Be a Bat?\" with the argument that we can't know, because we're humans, inescapably, and they're bats. So we could well ask what it would be like for a <em>human</em> to be a bat &mdash; what it would be like to have our human senses and perceptions transformed into human senses and perceptions that more closely resemble what we think bats have &mdash; but not what it's like to simply <em>be</em> a bat. Hofstadter takes this pretty far in his reflection, asking such questions as \"What is it like to hear one's native language without understanding it?\" and \"What is it like to hate chocolate (or your personal favorite flavor)?\" Fans will enjoy his punning of Nagel's title, \"What is it like to bat a bee? What is it like to be a bee being batted? What is it like to be a batted bee?\" (Illustration of baseball player and bee included.)</li>\n<li> Completing the Smullyan hat trick, \"An Epistemological Nightmare\" depicts a man's consultations with an \"experimental epistemologist.\" Infatuated with his latest piece of in-office gear, a \"cerebroscope\" that supposedly reads the patient's every neuron, the epistemologist puts the poor fellow through the ringer by using the device to reject his every statement about his beliefs, his beliefs about his beliefs, and his beliefs about his beliefs about his beliefs. Like the book's first Smullyan selection, this dialogue isn't without its Abbott-and-Costello elements: the absurdity reaches such a height that the epistemologist must eventually forsake the machine in order to break the loop he's created, by virtue of the very trust he's placed in it, between the cerebroscope and his own brain.</li>\n<li> \"A Conversation With Einstein's Brain\" is a selection from Douglas Hofstadter's <em>G&ouml;del, Escher, Bach</em>. If you have not read this book, minimize your browser and do so. I'll wait. (It's the dialogue, even better than the one about the ant farm, that proposes the \"copying\" of a brain into book form, and then letting the books \"interact.\")</li>\n<li> The reflection-less \"Fiction\" by Robert Nozick wraps the book. The piece at first seems to be narrated by a fictional character: indeed, its first sentence is \"I am a fictional character.\" But this character goes on to assert that the reader, too, is a fictional character, and that this piece one fictional character reads and another narrates is, in fact, a work of non-fiction, as are all works &mdash; works within this fictional world in which we live, that is. But who, then, wrote (or currently writes) our world?</li>\n</ol>\n<p>As a book for new rationalists, <em>The Mind's I</em> would be best offered as a jolt, a set of mind-stretching exercises that clear the road for the long, incompletable journey to rationality. A reader expecting any sort of instruction on <em>how</em> to think rationally will find a dry well, but that's not the point; these 27 pieces and their commentaries illustrate that it's possible in the first place to do some thinking in the borderlands of such everyday concepts like as brain, mind, soul, self, I, you, intelligence, sentience, <em>etc.</em> Perhaps the same explanation justifies low-level philosophy courses and the bull sessions students hold in the wee hours after them, but Hofstadter and Dennett manage to use material a great deal more entertaining, more exotic and altogether smarter. Would that we could get a revised and expanded update.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h5K34vLcTEKpMxCZj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 21, "extendedScore": null, "score": 3.2e-05, "legacy": true, "legacyId": "1533", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wfJebLTPGYaK3Gr8W"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-27T16:18:27.792Z", "modifiedAt": null, "url": null, "title": "Pittsburgh Meetup: Survey of Interest", "slug": "pittsburgh-meetup-survey-of-interest", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:23.821Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nick_Tarleton", "createdAt": "2009-03-05T18:07:15.687Z", "isAdmin": false, "displayName": "Nick_Tarleton"}, "userId": "nwrGYcfC4sPPn73Aw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AYKL87S4QRDrsrsvY/pittsburgh-meetup-survey-of-interest", "pageUrlRelative": "/posts/AYKL87S4QRDrsrsvY/pittsburgh-meetup-survey-of-interest", "linkUrl": "https://www.lesswrong.com/posts/AYKL87S4QRDrsrsvY/pittsburgh-meetup-survey-of-interest", "postedAtFormatted": "Thursday, August 27th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pittsburgh%20Meetup%3A%20Survey%20of%20Interest&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APittsburgh%20Meetup%3A%20Survey%20of%20Interest%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAYKL87S4QRDrsrsvY%2Fpittsburgh-meetup-survey-of-interest%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pittsburgh%20Meetup%3A%20Survey%20of%20Interest%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAYKL87S4QRDrsrsvY%2Fpittsburgh-meetup-survey-of-interest", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAYKL87S4QRDrsrsvY%2Fpittsburgh-meetup-survey-of-interest", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<p>I plan to host an OBLW meetup in Pittsburgh, PA, in the next couple of weeks, but would like to know the level of interest first. Please comment if you think you'd attend.</p>\n<p>(The planned location is a house in Squirrel Hill, about a mile and a half from Carnegie Mellon University; but if you attend CMU and would strongly prefer a location on campus, please say so.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AYKL87S4QRDrsrsvY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 10, "extendedScore": null, "score": 5.180018939736717e-07, "legacy": true, "legacyId": "1535", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-27T22:12:58.541Z", "modifiedAt": null, "url": null, "title": "Paper: Testing ecological models", "slug": "paper-testing-ecological-models", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "sFZXfM5397tStmsrK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DE7v8eACTZ2W9z8zu/paper-testing-ecological-models", "pageUrlRelative": "/posts/DE7v8eACTZ2W9z8zu/paper-testing-ecological-models", "linkUrl": "https://www.lesswrong.com/posts/DE7v8eACTZ2W9z8zu/paper-testing-ecological-models", "postedAtFormatted": "Thursday, August 27th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Paper%3A%20Testing%20ecological%20models&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APaper%3A%20Testing%20ecological%20models%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDE7v8eACTZ2W9z8zu%2Fpaper-testing-ecological-models%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Paper%3A%20Testing%20ecological%20models%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDE7v8eACTZ2W9z8zu%2Fpaper-testing-ecological-models", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDE7v8eACTZ2W9z8zu%2Fpaper-testing-ecological-models", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 142, "htmlBody": "<p>You may be interested in a paper of medium age I just read. <a href=\"http://www.asu.edu/clas/csdc/events/pdf/validation_ecology.pdf\">Testing ecological models: the meaning of validation</a> (PDF) tackles a problem many of you are familiar with in a slightly different context.</p>\n<p>To entice you to read it, here are some quotes from its descriptions of other papers:</p>\n<blockquote>\n<p>Holling (1978) pronounced it a fable that the purpose of validation is to establish the truth of the model&hellip;</p>\n</blockquote>\n<blockquote>\n<p>Overton (1977) viewed validation as an integral part of the modelling process&hellip;</p>\n</blockquote>\n<blockquote>\n<p>Botkin (1993) expressed concern that the usage of the terms verification and validation was not consistent with their logical meanings&hellip;</p>\n</blockquote>\n<blockquote>\n<p>Mankin et al. (1977) suggested that the objectives of model-building may be achieved without validating the model&hellip;</p>\n</blockquote>\n<p>I have another reason for posting this; I&rsquo;m looking for more papers on model validation, especially how-to papers. Which ones do you consider most helpful?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DE7v8eACTZ2W9z8zu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 5.180600598566826e-07, "legacy": true, "legacyId": "1536", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-28T09:45:42.067Z", "modifiedAt": null, "url": null, "title": "The Twin Webs of Knowledge", "slug": "the-twin-webs-of-knowledge", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:23.998Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qvnZG2gdBXZ5aWLqL/the-twin-webs-of-knowledge", "pageUrlRelative": "/posts/qvnZG2gdBXZ5aWLqL/the-twin-webs-of-knowledge", "linkUrl": "https://www.lesswrong.com/posts/qvnZG2gdBXZ5aWLqL/the-twin-webs-of-knowledge", "postedAtFormatted": "Friday, August 28th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Twin%20Webs%20of%20Knowledge&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Twin%20Webs%20of%20Knowledge%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqvnZG2gdBXZ5aWLqL%2Fthe-twin-webs-of-knowledge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Twin%20Webs%20of%20Knowledge%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqvnZG2gdBXZ5aWLqL%2Fthe-twin-webs-of-knowledge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqvnZG2gdBXZ5aWLqL%2Fthe-twin-webs-of-knowledge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1016, "htmlBody": "<p>Related to and partially inspired by: <a href=\"/lw/or/joy_in_the_merely_real/\">Joy in the Merely Real</a>; <a href=\"/lw/uw/entangled_truths_contagious_lies/\">Entangled Truths, Contagious Lies</a>.</p>\r\n<blockquote>\r\n<p><em>Where does the newborn go from here? The net is vast and infinite.</em> -- Ghost in the Shell</p>\r\n</blockquote>\r\n<p>There are those among us who resist the steady march of science, who feel that the reductionist creed takes away the beauty of things, who would rather enjoy sacred mysteries instead of naturalistic explanations. I suspect not many of them are reading this site. But even among aspiring rationalists, there are probably many who still feel some sympathy to that line of thought, who cannot but feel a twinge of pain where something mysterious ends up explained.<br /><br />To them I reply thusly:<br /><br />Picture in your mind a vast, glowing network of things, at the center of which are you. I visualize it akin to a great, vast city at night, a city that never sleeps. For some reason, all the lights in the buildings are out, so all the light comes from the myriad cars, trains and buses bustling in the night. Greg Egan's phrase, \"<a href=\"http://gregegan.customer.netspace.net.au/DIASPORA/01/Orphanogenesis.html\">making the pulses race along the tracks like a quadrillion cars shuttling between the trillion junctions of a ten-thousand-tiered monorail</a>\" comes to mind. Though it is not the tracks or roads themselves that we are the most interested in, but the hubs where they meet and from which they emerge.</p>\r\n<p><br /><a id=\"more\"></a>The hubs in the network are many in kind: some are other people, some are books and items in which information is stored, some are past events and experiments once conducted. The brightest hubs are the ones closest to you, from which information is flowing to you directly: they are your glowing constellation of stars. Beyond them, the hubs glow less and less bright as they get more distant: finally they are but dim, barely visible dots in a sea of blackness, spread out like the rocks at the bottom of the seafloor. Further still, even those dots vanish, but you know it doesn't mean there aren't any: it simply means that you don't know what and where they are.<br /><br />Now overlay this network of lights on a landscape. Not just the physical landscape of the Earth and the universe, though the net covers that as well. To see things in full, you must also overlay the network on the maps showing the sum of all human knowledge: that which is known in maths, physics, biology, literature, and all the sciences and arts besides. By themselves, all these landscapes are dark and impenetrable: with the great web of knowledge overlaid, the hubs illuminate their surroundings, revealing the landscape's contents. You could say that the people and events illuminate the landscapes around them, but you could likewise also say that they are drawing their light from the landscapes, for every pulse of information that moves across the lines in the network has its origins in one of the landscapes.<br /><br />As you move and seek out new people, you forge new links and make previously dim hubs glow more brightly; where you lose contact with people and lose interest in things, connections fade away and hubs disappear back into the darkness. You yourself are glowing as well: as an infant, your glow was dim and weak, your parents and elder siblings the main things you were directly connected with. As you sucked in and absorbed their knowledge, your light grew brighter. Now, as you absorb more and more, you become better capable of understanding all you see: you learn to find understanding by simply looking at things, seeing in them much that you were previously blind to. Thus the radius of light surrounding you grows ever wider, in all the dimensions and fields you choose to pursue. <br /><br />For this network of things is mirrored by another network in your mind: the network of things you have learnt. As the other, this network is constantly changing, new nodes and connections appearing as you learn new things, vanishing as you forget them. When someone or something in the other web illuminates its surroundings, you can take what you see around him and store it in your mind, link it with the other previous things you already know. For as long as you do not forget this new thing, you will retain a connection to the source you learned it from. Thus the places you once saw will remain illuminated for as long as you remember, though as time passes you only retain the memory of what those places were once like, not necessarily the way they are now: and you may need to revisit them to find out that they have changed.<br /><br />As you come to know more and more related things, those things are bound tightly together in your mind's web. The places the knowledge was drawn from will likewise stay brightly and evenly lit, pushing back the darkness. As you expand your mental web of things more and more, entirely new fields of understanding will open themselves to you. Below the abstract fields of sciences and arts are the fields you mastered back as a child, the fields that your webs were once limited to and have now branched out from. Are you not for some reason mute, you will have learned the art of producing speech: are you not disabled, you will have learned to walk. But even if those fields wouldn't be known to you, the fact that you understand what I am writing means that you have learnt the basic fields of human understanding. In some way, you have learned to communicate, and you have learned to reason and to think. From this foundation, vast depths of knowledge have become open to you.<br /><br />Fear not that new understanding would render things cold and boring. If you avoid new understanding, you are limiting the reach of what you can learn. If you embrace and actively seek out new understanding, you will spread out across the entire universe.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qvnZG2gdBXZ5aWLqL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 6, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "1537", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["x4dG4GhpZH2hgz59x", "wyyfFfaRar2jEdeQK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-28T21:47:56.463Z", "modifiedAt": null, "url": null, "title": "Don't be Pathologically Mugged!", "slug": "don-t-be-pathologically-mugged", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:23.515Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Psy-Kosh", "createdAt": "2009-03-01T19:34:52.148Z", "isAdmin": false, "displayName": "Psy-Kosh"}, "userId": "CtHmuQzjA7Y7LnSss", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pbYRxTLAcxAQsGvJ7/don-t-be-pathologically-mugged", "pageUrlRelative": "/posts/pbYRxTLAcxAQsGvJ7/don-t-be-pathologically-mugged", "linkUrl": "https://www.lesswrong.com/posts/pbYRxTLAcxAQsGvJ7/don-t-be-pathologically-mugged", "postedAtFormatted": "Friday, August 28th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Don't%20be%20Pathologically%20Mugged!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADon't%20be%20Pathologically%20Mugged!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpbYRxTLAcxAQsGvJ7%2Fdon-t-be-pathologically-mugged%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Don't%20be%20Pathologically%20Mugged!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpbYRxTLAcxAQsGvJ7%2Fdon-t-be-pathologically-mugged", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpbYRxTLAcxAQsGvJ7%2Fdon-t-be-pathologically-mugged", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 422, "htmlBody": "<p>In a lot of discussion here, there's been talk about how decision algorithms would do for PD, Newcomb's Probmel, Parfit's Hitchhiker, and Counterfactual Mugging.</p>\n<p>There's a reasonable chain there, but especially for the last one, there's a bit of a concern I've had about the overal pattern. Specifically, while we're optimizing for extreme cases, we want to make sure we're not hurting our decision algorithm's ability to deal with less bizzare cases.</p>\n<p><a id=\"more\"></a></p>\n<p>Specifically, part of the reasoning for the last one could be stated as \"be/have the type of algorithm that would be expected to do well when a Counterfactual Mugger showed up. That is, would have a net positive expected utility, etc...\" This is reasonable, espectially given that there seems to be lines of reasoning (like Timeless Decision Theory) that _automatically_ get this right using the same rules that would get it to succeed with PD or any other such thing. But I worry about, well, actually it would be better for me to show an example:</p>\n<p>Consider the Pathological Decision Challenge.</p>\n<p>Omega shows up and presents a Decision Challenge, consisting of some assortment of your favorite decision theory puzzlers. (Newcomb, etc etc etc...)</p>\n<p>Unbeknownst to you, however, Omega also has a secret additional test: If the decisions you make are all something _OTHER_ than the normal rational ones, then Omega will pay you some huge superbonus of utilions, vastly dwarfing any cost to loosing all of the individual challenges...</p>\n<p>However, Omega also models you and if you would have willingly \"failed\" _HAD YOU KNOWN_ about the extra challenge above, (but not this extra extra criteria), then you get no bonus for failing everything.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>A decision algorithm that would tend to win in this contrived situation would tend to lose in regular situations, right? Again, yes, I can see the argument that being the type of algorithm that can be successfully counterfactually mugged can arise naturally from a simple rule that automatically gives the right answer for many other more reasonable situations. But I can't help but worry that as we construct more... extreme cases, we'll end up with this sort of thing, were optimizing our decision algorithm to win in the latest \"decision challenge\" stops it from doing as well in more, for lack of a better word, \"normal\" situations.</p>\n<p>Further, I'm not sure yet how to more precisely separate out pathalogical cases from more reasonable \"weird\" challenges. Just to clarify, this post isn't a complaint or direct objection to considering things like Newcomb's problem, just a concern I had about a possible way we might go wrong.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pbYRxTLAcxAQsGvJ7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 8, "extendedScore": null, "score": 5.182923324600288e-07, "legacy": true, "legacyId": "1538", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-29T02:08:10.855Z", "modifiedAt": null, "url": null, "title": "Some counterevidence for human sociobiology", "slug": "some-counterevidence-for-human-sociobiology", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:23.870Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EpLofhghaxDbhx7Wo/some-counterevidence-for-human-sociobiology", "pageUrlRelative": "/posts/EpLofhghaxDbhx7Wo/some-counterevidence-for-human-sociobiology", "linkUrl": "https://www.lesswrong.com/posts/EpLofhghaxDbhx7Wo/some-counterevidence-for-human-sociobiology", "postedAtFormatted": "Saturday, August 29th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Some%20counterevidence%20for%20human%20sociobiology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASome%20counterevidence%20for%20human%20sociobiology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEpLofhghaxDbhx7Wo%2Fsome-counterevidence-for-human-sociobiology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Some%20counterevidence%20for%20human%20sociobiology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEpLofhghaxDbhx7Wo%2Fsome-counterevidence-for-human-sociobiology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEpLofhghaxDbhx7Wo%2Fsome-counterevidence-for-human-sociobiology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 464, "htmlBody": "<p>I love seeing counter-evidence for everything. I estimate that while most of my beliefs are true (otherwise I wouldn't believe them in the first place), a small percentage is almost certainly completely false - and I don't really have any reliable way of telling the two apart.</p>\n<p>Indiscriminatingly looking for counter-evidence for all of them can be very rewarding - the ones that are true are much more likely to sustain the assault of it than the ones that aren't. Yes, I might ignore counter-evidence of something that's false, or accept it for something that's true, ending up worse off, but it seems plausible that on average it should improve quality of my beliefs.</p>\n<p>For example some of the standard beliefs about human sociobiology that seemed to be extremely widely held here are:</p>\n<ul>\n<li>Men have lower chances of having any kids than women</li>\n<li>Richer people, especially men, are more likely to have kids, and have more kids</li>\n</ul>\n<p><a href=\"http://fatherhood.hhs.gov/charting02/index.htm\">Charting Parenthood: Statistical Portrait of Fathers and Mothers in America</a> disagrees with them.</p>\n<ul>\n<li>It's true that young men are less likely to have children than young women, but it reverses at old age, and total chance of having children during lifetime is - for people over 45 - 84% for men, and 86% for women. As some of childless men might still have children between 45 and their death (quite a few according to data), but almost no woman will, the difference must get smaller by the time of death, or it might even reverse. This is pretty convincing evidence against a major gender difference in chance of having children, at least as far as modern America is concerned.</li>\n<li>The chance of having children is highest for people between 100% and 200% of poverty line (poverty line, not median income, these are all poorer than average people). For women going either lower or higher reduces chances of having children considerably. For men getting poorer reduces chance of having children considerably, while getting richer reduces it but only slightly. However - <a href=\"http://en.wikipedia.org/wiki/Poverty_in_the_United_States\">younger people are much more likely to fall below poverty line</a>, and men tend to reproduce later, so even that can easily be an artifact of age-income relationship. The data is fully compatible with both poverty and wealth being negatively correlated with chance of having children in both genders. </li>\n</ul>\n<p>These are not direct tests of sociobiological claims, so what we have is not exactly what we would like to, but I find them to be quite convincing counter-evidence. My belief in these sociobiological claims is definitely lower than before, at least as far as they concern modern world, even though I can imagine more focused studies changing my mind back.</p>\n<p>More counter-evidence for things we commonly believe here, sociobiological or otherwise, welcomed in comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EpLofhghaxDbhx7Wo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 2, "extendedScore": null, "score": 5.183350720047337e-07, "legacy": true, "legacyId": "1539", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-30T03:56:31.701Z", "modifiedAt": null, "url": null, "title": "Cookies vs Existential Risk", "slug": "cookies-vs-existential-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:38.864Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SkkhysBQgK79RvLbf/cookies-vs-existential-risk", "pageUrlRelative": "/posts/SkkhysBQgK79RvLbf/cookies-vs-existential-risk", "linkUrl": "https://www.lesswrong.com/posts/SkkhysBQgK79RvLbf/cookies-vs-existential-risk", "postedAtFormatted": "Sunday, August 30th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cookies%20vs%20Existential%20Risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACookies%20vs%20Existential%20Risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSkkhysBQgK79RvLbf%2Fcookies-vs-existential-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cookies%20vs%20Existential%20Risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSkkhysBQgK79RvLbf%2Fcookies-vs-existential-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSkkhysBQgK79RvLbf%2Fcookies-vs-existential-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1950, "htmlBody": "<p>I've been thinking for a while now about the possible trade-offs between present recreation and small reductions in existential risk, and I've finally gotten around to a (consequentialist) utilitarian analysis.</p>\n<p><strong>ETA: </strong>Most of the similar mathematical treatments I've seen assume a sort of duty to unrealized people, such as Bostrom's \"<a href=\"http://www.nickbostrom.com/astronomical/waste.html\" target=\"_blank\">Astronomical Waste</a>\" paper. In addition to avoiding that assumption, my aim was to provide a more general formula for&nbsp;someone to use, in which&nbsp;they can enter differing beliefs and hypotheses. Lastly I include 3 examples using widely varying ideas, and explore the results.</p>\n<p>Let's say that you've got a mind to make a batch of cookies. That action has a certain amount of utility, from the process itself and/or the delicious cookies. But it might lessen (or increase) the chances of you reducing existential risk, and hence affect the chance of existential disaster itself. Now if these cookies will help x-risk reduction efforts (networking!) and be enjoyable, the decision is an easy one. Same thing if they'll hurt your efforts and you hate making, eating, and giving away cookies. Any conflict arises when cookie making/eating is in opposition to x-risk reduction. If you were sufficiently egoist then risk of death would be comparable to existential disaster, and you should consider the two risks together. For readability I&rsquo;ll refer simply to existential risk.</p>\n<p>The question I'll attempt to answer is: what reduction in the probability of existential disaster makes refraining from an activity an equally good choice in terms of expected utility? If you think that by refraining and doing something else you would reduce the risk at least that much, then rationally you should pursue the alternative. If refraining would cut risk by less than this value, then head to the kitchen.<a id=\"more\"></a></p>\n<p>*<strong><em>ASSUMPTIONS</em></strong>: For simplicity I'll treats existential disaster as an abstract singular event, which we&rsquo;ll survive or not. If we do, it is assumed that we do so in a way such that there are no further x-risks. Further I'll assume the utility realized past that point is not dependent on the cookie-making decision in question, and that the utility realized before that point is not dependent on whether existential disaster will occur. <em>The utility calculation is also unbounded</em>, being easier to specify. It is hoped that those not seeking to approximate having such a utility function can modify the treatment to serve their needs. *</p>\n<p><em>E(U|cookies) = E(U|cookies, existential disaster) + U<sub>post-risk-future</sub> &bull; P(x-risk survival | cookies)</em></p>\n<p><em>E(U|alternative) = E(U|alternative, existential disaster) + U<sub>post-risk-future</sub> &bull; P(x-risk survival | alternative)</em></p>\n<p>Setting these two expected utilities to be equal we get:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>\n<p><em>E(U|cookies, existential disaster) - E(U|alternative, existential disaster) = U<sub>post-risk-future </sub>&bull; ( P(x-risk survival | alternative) - P(x-risk survival | cookies))</em></p>\n<p>or</p>\n<p><strong><em>&Delta;P(x-risk survival) = &Delta;E(U|existential disaster) / U<sub>post-risk-future</sub></em></strong></p>\n<p>Where&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <em>&Delta;P(x-risk survival) = P(x-risk survival | alternative) - P(x-risk survival | cookies)&nbsp; </em></p>\n<p>&nbsp;and&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <em>&Delta;E(U|existential disaster) = E(U|cookies, existential disaster) - E(U|alternative, existential disaster)</em></p>\n<p>*I&rsquo;m assuming both of these quantities are positive. Otherwise, there&rsquo;s no conflict.*</p>\n<p>Now to determine the utilities:</p>\n<p>&nbsp;<img src=\"http://images.lesswrong.com/t3_16g_0.png\" alt=\"\" width=\"666\" height=\"53\" /></p>\n<p><em>base value<sub>(utility/time)</sub></em> is a constant for normalizing to <em>&Delta;E(U|existential disaster)</em> and factors out of our ratio, but it can give us a scale of comparison. Obviously you should use the same time scale for the integral limits. <em>s<sub>i</sub>(t)</em> (range &ge; 0) is the multiplier for the change in subjective time due to faster cognition, <em>h<sub>i</sub>(t)</em> (range = all real numbers) is the multiplier for the change in the <em>base value<sub>(utility/time)</sub></em>, and <em>Di(t)</em> (0 &le; range &le; 1) is your discount function. All of these functions are with reference to each morally relevant entity <em>i</em>, assuming yourself as <em>i = 1</em>.</p>\n<p>There are of course a variety of ways to do this kind of calculation. I felt the multiplication of a discount function with increases in both subjective time quantity and quality, integrated over the time period of interest and summed across conscious entitites, was both general and intuitive.</p>\n<p>There're far too many variables here to summarize all possibilities with examples, but I'll do a few, with both pure egoist and agent-neutral utilitarian perspectives (equal consideration of yours and others' wellbeing). I'll assume the existential disaster would occur in 30 years, keeping in mind that the prior/common probability of disaster doesn't actually affect the calculation. I&rsquo;ll also set most of the functions to constants to keep it straightforward.</p>\n<p><strong>Static World</strong></p>\n<p>Here we assume that life span does not increase, nor does cognitive speed or quality of life. You're contemplating making cookies, which will take 1 hour. <em>base value<sub>(utility/time)</sub></em> of current life is 1 utility/hour, you expect to receive 2 extra utility by making cookies and will also obtain 1 utility/hour you live in a post-risk-future, which will be 175,200 hours over an assumed extra 20 years. For simplicity we'll assume no discounting, and start off with a pure egoist perspective. Then:</p>\n<p><em>&Delta;P(x-risk survival) = &Delta;E(U|existential disaster) / Upost-risk-future = 2/175,200 = 0.00114%</em>, which might be too much to expect from working for one hour instead.</p>\n<p>For an agent-neutral utilitarian, we'll assume there's another 2 utility that others gain from your cookies. We'll include only the &asymp;6.7 billion currently existing people, who have a <a href=\"http://en.wikipedia.org/wiki/Life_expectancy\" target=\"_blank\">current world life expectancy of 67 years</a> and <a href=\"https://www.cia.gov/library/publications/the-world-factbook/fields/2177.html\" target=\"_blank\">average age of 28.4</a>, which would give them each 75,336 utility over 8.6 years in a post-risk-future. Then:</p>\n<p><em>&Delta;P(x-risk survival) = &Delta;E(U|existential disaster) / Upost-risk-future = 4/(75,336 &bull; 6,700,000,000) =0.000000000000792%</em>. You can probably reduce existential risk this much with one hour of work, but then you&rsquo;re probably not a pure agent-neutral utilitarian with no time discounting. I&rsquo;m certainly not.</p>\n<p><strong>Conservative Transhuman World</strong></p>\n<p>In this world we&rsquo;ll assume that people live about a thousand years, a little over 10 times conventional expectancy. We&rsquo;ll also assume they think 10 times as fast and each subjective moment has 10 times higher utility. I&rsquo;m taking that kind of increase from the <a href=\"http://www.hedweb.com\" target=\"_blank\">hedonistic imperative</a> idea, but you&rsquo;d get the same effect by just thinking 100 times faster than we do now. Keeping it simple I&rsquo;ll treat these improvements as happening instantaneously upon entering a post-risk-future. On a conscious level I don&rsquo;t discount posthuman futures, but I&rsquo;ll set <em>D<sub>i</sub>(t) = e<sup>-t/20</sup></em> anyway. For those who want to check my math, the integral of that function from 30 to 1000 is 4.463.</p>\n<p>Though I phrased the equations in terms of baked goods, they of course apply to any decision of both greater existential risk and enjoyment. Let&rsquo;s assume you&rsquo;re able to forgo all pleasure now in terms of the greatest future pleasure, through existential risk reduction. In our calculation, this course of action is &ldquo;<em>alternative</em>&rdquo;, and living like a person unaware of existential risk is &ldquo;<em>cookie</em>&rdquo;. Our <em>base value<sub>(utility/time)</sub></em> is an expected 1 utility/year of &ldquo;normal&rdquo; life (a very different scale from the last example), and your total focus would realize a flat 0 utility for those first 30 years. For a pure egoist:</p>\n<p><em>&Delta;P(x-risk survival) = &Delta;E(U|existential disaster) / Upost-risk-future = 30/446.26 =6.72%</em>. This might be possible with 30 years of the total dedication we&rsquo;re considering, especially with so few people working on this, but maybe it wouldn&rsquo;t.</p>\n<p>For our agent-neutral calculation, we&rsquo;ll assume that your total focus on the large scale results in 5 fewer utility for those who won&rsquo;t end up having as much fun with the &ldquo;next person&rdquo; as with you, subtracted by the amount you might uniquely improve the lives of those you meet while trying to save the world.&nbsp; If they all realize utility similar to yours in a post-risk world, then:</p>\n<p><em>&Delta;P(x-risk survival) = &Delta;E(U|existential disaster) / Upost-risk-future = 35/(446.26*6,700,000,000) = 0.00000000117%</em>. With 30 years of dedicated work this seems extremely feasible.</p>\n<p>And if you hadn&rsquo;t used a discount rate in this example, the <em>&Delta;P(x-risk survival)</em> required to justify those short-term self-sacrifices would be over 217 times smaller.</p>\n<p><strong>Nick Bostrom&rsquo;s Utopia</strong></p>\n<p>Lastly I&rsquo;ll consider the world described in Bostrom&rsquo;s &ldquo;<a href=\"http://www.nickbostrom.com/utopia.html\" target=\"_blank\">Letter From Utopia</a>&rdquo;. We&rsquo;ll use the same <em>base value<sub>(utility/time)</sub></em> of 1 utility/year of &ldquo;normal&rdquo; life as the last example. Bostrom writes from the perspective of your future self: &ldquo;<em>And yet, what you had in your best moment is not close to what I have now &ndash; a beckoning scintilla at most.&nbsp; If the distance between base and apex for you is eight kilometers, then to reach my dwellings would take a million light-year ascent.</em>&rdquo; Taken literally this translates to <em>h<sub>i</sub>(t) = 1.183 &bull; 10<sup>18</sup></em>. I won&rsquo;t bother treating <em>s<sub>i</sub>(t)</em> as more than unity; though likely to be greater, that seems like overkill for this calculation. We&rsquo;ll assume people live till most stars burn out, approximately <em>10<sup>14</sup></em> years from now (if we find a way during that time to stop or meaningfully survive the entire heat death of the universe, it may be difficult to assign any finite bound to your utility). I&rsquo;ll start by assuming no discount rate.</p>\n<p>Assuming again that you&rsquo;re considering focusing entirely on preventing existential risk, then &Delta;P<em>(x-risk survival) = &Delta;E(U|existential disaster) / Upost-risk-future =30/(1.183 &bull; 1032) = 0.0000000000000000000000000000254%</em>. Even if you were almost completely paralyzed, able only to blink your eyes, you could pull this off. For an agent-neutral utilitarian, the change in existential risk could be about 7 billion times smaller and still justify such dedication. While I don&rsquo;t believe in any kind of obligation to create new people, if our civilization did seed the galaxy with eudaimonic lives, you might sacrifice unnecessary daily pleasures for a reduction in risk 1,000,000,000,000,000,000,000 smaller still. Even with the discount function specified in the last example, a pure egoist would still achieve the greatest expected utility or enjoyment from an extreme dedication that achieved an existential risk reduction of only <em>0.000000000000000568%</em>.</p>\n<p><strong>Summary</strong></p>\n<p>The above are meant only as illustrative examples. As long as we maintain our freedom to improve and change, and do so wisely, I put high probability on post-risk-futures gravitating in the direction of Bostrom&rsquo;s Utopia. But if you agree with or can tolerate my original assumptions, my intention is for you to play around, enter values you find plausible, and see whether or how much your beliefs justify short term enjoyment for its own sake.</p>\n<p>Lastly, keep in mind that maximizing your ability to reduce existential risk almost certainly does not include forgoing all enjoyment. For one thing, you'll have at least a <em>little</em> fun fighting existential risk. Secondly, we aren&rsquo;t (yet) robots and we generally need breaks, <em>some</em> time to relax and rejuvenate, and some friendship to keep our morale up (as well as stimulated or even sane). Over time, habit-formation and other self-optimizations can reduce some of those needs, and that will only be carried through if you don&rsquo;t treat short term enjoyment as much more than an element of reducing existential risk (assuming your analysis suggests you avoid doing so). But everyone requires &ldquo;balance&rdquo;, by definition, and raw application of willpower won&rsquo;t get you nearly far enough. <a href=\"http://www.sss.ias.edu/publications/papers/econpaper69.pdf\" target=\"_blank\">It&rsquo;s an exhaustible resource</a>, and while it can carry you through several hours or a few days, it&rsquo;s not going to carry you through several decades.</p>\n<p>The absolute <em>worst</em> thing you could do, assuming once again that your analysis justifies a given short term sacrifice for greater long term gain, is to <em>give up</em>. If your resolve is about to fail, or already has, just take a break to <em>really relax</em>, however long you honestly need (and you will <em>need</em> some time). Anticipating how effective you&rsquo;ll be in different motivational states (which can&rsquo;t be represented by a single number), and how to best balance motivation and direct application, is an incredibly complex problem which is difficult or impossible to quantify. Even the best solutions are approximations, people usually apply themselves too little and sometimes too much. But to do so and suffer burnout provides no rational basis for throwing up your hands in desperation and calling it quits, at least for longer than you need to. To an extent that we might not yet be able to imagine, someday billions or trillions of future persons, including yourself, may express gratitude that you didn&rsquo;t.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SkkhysBQgK79RvLbf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 18, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "1528", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I've been thinking for a while now about the possible trade-offs between present recreation and small reductions in existential risk, and I've finally gotten around to a (consequentialist) utilitarian analysis.</p>\n<p><strong>ETA: </strong>Most of the similar mathematical treatments I've seen assume a sort of duty to unrealized people, such as Bostrom's \"<a href=\"http://www.nickbostrom.com/astronomical/waste.html\" target=\"_blank\">Astronomical Waste</a>\" paper. In addition to avoiding that assumption, my aim was to provide a more general formula for&nbsp;someone to use, in which&nbsp;they can enter differing beliefs and hypotheses. Lastly I include 3 examples using widely varying ideas, and explore the results.</p>\n<p>Let's say that you've got a mind to make a batch of cookies. That action has a certain amount of utility, from the process itself and/or the delicious cookies. But it might lessen (or increase) the chances of you reducing existential risk, and hence affect the chance of existential disaster itself. Now if these cookies will help x-risk reduction efforts (networking!) and be enjoyable, the decision is an easy one. Same thing if they'll hurt your efforts and you hate making, eating, and giving away cookies. Any conflict arises when cookie making/eating is in opposition to x-risk reduction. If you were sufficiently egoist then risk of death would be comparable to existential disaster, and you should consider the two risks together. For readability I\u2019ll refer simply to existential risk.</p>\n<p>The question I'll attempt to answer is: what reduction in the probability of existential disaster makes refraining from an activity an equally good choice in terms of expected utility? If you think that by refraining and doing something else you would reduce the risk at least that much, then rationally you should pursue the alternative. If refraining would cut risk by less than this value, then head to the kitchen.<a id=\"more\"></a></p>\n<p>*<strong><em>ASSUMPTIONS</em></strong>: For simplicity I'll treats existential disaster as an abstract singular event, which we\u2019ll survive or not. If we do, it is assumed that we do so in a way such that there are no further x-risks. Further I'll assume the utility realized past that point is not dependent on the cookie-making decision in question, and that the utility realized before that point is not dependent on whether existential disaster will occur. <em>The utility calculation is also unbounded</em>, being easier to specify. It is hoped that those not seeking to approximate having such a utility function can modify the treatment to serve their needs. *</p>\n<p><em>E(U|cookies) = E(U|cookies, existential disaster) + U<sub>post-risk-future</sub> \u2022 P(x-risk survival | cookies)</em></p>\n<p><em>E(U|alternative) = E(U|alternative, existential disaster) + U<sub>post-risk-future</sub> \u2022 P(x-risk survival | alternative)</em></p>\n<p>Setting these two expected utilities to be equal we get:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>\n<p><em>E(U|cookies, existential disaster) - E(U|alternative, existential disaster) = U<sub>post-risk-future </sub>\u2022 ( P(x-risk survival | alternative) - P(x-risk survival | cookies))</em></p>\n<p>or</p>\n<p><strong id=\"_P_x_risk_survival_____E_U_existential_disaster____Upost_risk_future\"><em>\u0394P(x-risk survival) = \u0394E(U|existential disaster) / U<sub>post-risk-future</sub></em></strong></p>\n<p>Where&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <em>\u0394P(x-risk survival) = P(x-risk survival | alternative) - P(x-risk survival | cookies)&nbsp; </em></p>\n<p>&nbsp;and&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <em>\u0394E(U|existential disaster) = E(U|cookies, existential disaster) - E(U|alternative, existential disaster)</em></p>\n<p>*I\u2019m assuming both of these quantities are positive. Otherwise, there\u2019s no conflict.*</p>\n<p>Now to determine the utilities:</p>\n<p>&nbsp;<img src=\"http://images.lesswrong.com/t3_16g_0.png\" alt=\"\" width=\"666\" height=\"53\"></p>\n<p><em>base value<sub>(utility/time)</sub></em> is a constant for normalizing to <em>\u0394E(U|existential disaster)</em> and factors out of our ratio, but it can give us a scale of comparison. Obviously you should use the same time scale for the integral limits. <em>s<sub>i</sub>(t)</em> (range \u2265 0) is the multiplier for the change in subjective time due to faster cognition, <em>h<sub>i</sub>(t)</em> (range = all real numbers) is the multiplier for the change in the <em>base value<sub>(utility/time)</sub></em>, and <em>Di(t)</em> (0 \u2264 range \u2264 1) is your discount function. All of these functions are with reference to each morally relevant entity <em>i</em>, assuming yourself as <em>i = 1</em>.</p>\n<p>There are of course a variety of ways to do this kind of calculation. I felt the multiplication of a discount function with increases in both subjective time quantity and quality, integrated over the time period of interest and summed across conscious entitites, was both general and intuitive.</p>\n<p>There're far too many variables here to summarize all possibilities with examples, but I'll do a few, with both pure egoist and agent-neutral utilitarian perspectives (equal consideration of yours and others' wellbeing). I'll assume the existential disaster would occur in 30 years, keeping in mind that the prior/common probability of disaster doesn't actually affect the calculation. I\u2019ll also set most of the functions to constants to keep it straightforward.</p>\n<p><strong id=\"Static_World\">Static World</strong></p>\n<p>Here we assume that life span does not increase, nor does cognitive speed or quality of life. You're contemplating making cookies, which will take 1 hour. <em>base value<sub>(utility/time)</sub></em> of current life is 1 utility/hour, you expect to receive 2 extra utility by making cookies and will also obtain 1 utility/hour you live in a post-risk-future, which will be 175,200 hours over an assumed extra 20 years. For simplicity we'll assume no discounting, and start off with a pure egoist perspective. Then:</p>\n<p><em>\u0394P(x-risk survival) = \u0394E(U|existential disaster) / Upost-risk-future = 2/175,200 = 0.00114%</em>, which might be too much to expect from working for one hour instead.</p>\n<p>For an agent-neutral utilitarian, we'll assume there's another 2 utility that others gain from your cookies. We'll include only the \u22486.7 billion currently existing people, who have a <a href=\"http://en.wikipedia.org/wiki/Life_expectancy\" target=\"_blank\">current world life expectancy of 67 years</a> and <a href=\"https://www.cia.gov/library/publications/the-world-factbook/fields/2177.html\" target=\"_blank\">average age of 28.4</a>, which would give them each 75,336 utility over 8.6 years in a post-risk-future. Then:</p>\n<p><em>\u0394P(x-risk survival) = \u0394E(U|existential disaster) / Upost-risk-future = 4/(75,336 \u2022 6,700,000,000) =0.000000000000792%</em>. You can probably reduce existential risk this much with one hour of work, but then you\u2019re probably not a pure agent-neutral utilitarian with no time discounting. I\u2019m certainly not.</p>\n<p><strong id=\"Conservative_Transhuman_World\">Conservative Transhuman World</strong></p>\n<p>In this world we\u2019ll assume that people live about a thousand years, a little over 10 times conventional expectancy. We\u2019ll also assume they think 10 times as fast and each subjective moment has 10 times higher utility. I\u2019m taking that kind of increase from the <a href=\"http://www.hedweb.com\" target=\"_blank\">hedonistic imperative</a> idea, but you\u2019d get the same effect by just thinking 100 times faster than we do now. Keeping it simple I\u2019ll treat these improvements as happening instantaneously upon entering a post-risk-future. On a conscious level I don\u2019t discount posthuman futures, but I\u2019ll set <em>D<sub>i</sub>(t) = e<sup>-t/20</sup></em> anyway. For those who want to check my math, the integral of that function from 30 to 1000 is 4.463.</p>\n<p>Though I phrased the equations in terms of baked goods, they of course apply to any decision of both greater existential risk and enjoyment. Let\u2019s assume you\u2019re able to forgo all pleasure now in terms of the greatest future pleasure, through existential risk reduction. In our calculation, this course of action is \u201c<em>alternative</em>\u201d, and living like a person unaware of existential risk is \u201c<em>cookie</em>\u201d. Our <em>base value<sub>(utility/time)</sub></em> is an expected 1 utility/year of \u201cnormal\u201d life (a very different scale from the last example), and your total focus would realize a flat 0 utility for those first 30 years. For a pure egoist:</p>\n<p><em>\u0394P(x-risk survival) = \u0394E(U|existential disaster) / Upost-risk-future = 30/446.26 =6.72%</em>. This might be possible with 30 years of the total dedication we\u2019re considering, especially with so few people working on this, but maybe it wouldn\u2019t.</p>\n<p>For our agent-neutral calculation, we\u2019ll assume that your total focus on the large scale results in 5 fewer utility for those who won\u2019t end up having as much fun with the \u201cnext person\u201d as with you, subtracted by the amount you might uniquely improve the lives of those you meet while trying to save the world.&nbsp; If they all realize utility similar to yours in a post-risk world, then:</p>\n<p><em>\u0394P(x-risk survival) = \u0394E(U|existential disaster) / Upost-risk-future = 35/(446.26*6,700,000,000) = 0.00000000117%</em>. With 30 years of dedicated work this seems extremely feasible.</p>\n<p>And if you hadn\u2019t used a discount rate in this example, the <em>\u0394P(x-risk survival)</em> required to justify those short-term self-sacrifices would be over 217 times smaller.</p>\n<p><strong id=\"Nick_Bostrom_s_Utopia\">Nick Bostrom\u2019s Utopia</strong></p>\n<p>Lastly I\u2019ll consider the world described in Bostrom\u2019s \u201c<a href=\"http://www.nickbostrom.com/utopia.html\" target=\"_blank\">Letter From Utopia</a>\u201d. We\u2019ll use the same <em>base value<sub>(utility/time)</sub></em> of 1 utility/year of \u201cnormal\u201d life as the last example. Bostrom writes from the perspective of your future self: \u201c<em>And yet, what you had in your best moment is not close to what I have now \u2013 a beckoning scintilla at most.&nbsp; If the distance between base and apex for you is eight kilometers, then to reach my dwellings would take a million light-year ascent.</em>\u201d Taken literally this translates to <em>h<sub>i</sub>(t) = 1.183 \u2022 10<sup>18</sup></em>. I won\u2019t bother treating <em>s<sub>i</sub>(t)</em> as more than unity; though likely to be greater, that seems like overkill for this calculation. We\u2019ll assume people live till most stars burn out, approximately <em>10<sup>14</sup></em> years from now (if we find a way during that time to stop or meaningfully survive the entire heat death of the universe, it may be difficult to assign any finite bound to your utility). I\u2019ll start by assuming no discount rate.</p>\n<p>Assuming again that you\u2019re considering focusing entirely on preventing existential risk, then \u0394P<em>(x-risk survival) = \u0394E(U|existential disaster) / Upost-risk-future =30/(1.183 \u2022 1032) = 0.0000000000000000000000000000254%</em>. Even if you were almost completely paralyzed, able only to blink your eyes, you could pull this off. For an agent-neutral utilitarian, the change in existential risk could be about 7 billion times smaller and still justify such dedication. While I don\u2019t believe in any kind of obligation to create new people, if our civilization did seed the galaxy with eudaimonic lives, you might sacrifice unnecessary daily pleasures for a reduction in risk 1,000,000,000,000,000,000,000 smaller still. Even with the discount function specified in the last example, a pure egoist would still achieve the greatest expected utility or enjoyment from an extreme dedication that achieved an existential risk reduction of only <em>0.000000000000000568%</em>.</p>\n<p><strong id=\"Summary\">Summary</strong></p>\n<p>The above are meant only as illustrative examples. As long as we maintain our freedom to improve and change, and do so wisely, I put high probability on post-risk-futures gravitating in the direction of Bostrom\u2019s Utopia. But if you agree with or can tolerate my original assumptions, my intention is for you to play around, enter values you find plausible, and see whether or how much your beliefs justify short term enjoyment for its own sake.</p>\n<p>Lastly, keep in mind that maximizing your ability to reduce existential risk almost certainly does not include forgoing all enjoyment. For one thing, you'll have at least a <em>little</em> fun fighting existential risk. Secondly, we aren\u2019t (yet) robots and we generally need breaks, <em>some</em> time to relax and rejuvenate, and some friendship to keep our morale up (as well as stimulated or even sane). Over time, habit-formation and other self-optimizations can reduce some of those needs, and that will only be carried through if you don\u2019t treat short term enjoyment as much more than an element of reducing existential risk (assuming your analysis suggests you avoid doing so). But everyone requires \u201cbalance\u201d, by definition, and raw application of willpower won\u2019t get you nearly far enough. <a href=\"http://www.sss.ias.edu/publications/papers/econpaper69.pdf\" target=\"_blank\">It\u2019s an exhaustible resource</a>, and while it can carry you through several hours or a few days, it\u2019s not going to carry you through several decades.</p>\n<p>The absolute <em>worst</em> thing you could do, assuming once again that your analysis justifies a given short term sacrifice for greater long term gain, is to <em>give up</em>. If your resolve is about to fail, or already has, just take a break to <em>really relax</em>, however long you honestly need (and you will <em>need</em> some time). Anticipating how effective you\u2019ll be in different motivational states (which can\u2019t be represented by a single number), and how to best balance motivation and direct application, is an incredibly complex problem which is difficult or impossible to quantify. Even the best solutions are approximations, people usually apply themselves too little and sometimes too much. But to do so and suffer burnout provides no rational basis for throwing up your hands in desperation and calling it quits, at least for longer than you need to. To an extent that we might not yet be able to imagine, someday billions or trillions of future persons, including yourself, may express gratitude that you didn\u2019t.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "\u0394P(x-risk survival) = \u0394E(U|existential disaster) / Upost-risk-future", "anchor": "_P_x_risk_survival_____E_U_existential_disaster____Upost_risk_future", "level": 1}, {"title": "Static World", "anchor": "Static_World", "level": 1}, {"title": "Conservative Transhuman World", "anchor": "Conservative_Transhuman_World", "level": 1}, {"title": "Nick Bostrom\u2019s Utopia", "anchor": "Nick_Bostrom_s_Utopia", "level": 1}, {"title": "Summary", "anchor": "Summary", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "23 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-30T17:34:09.150Z", "modifiedAt": null, "url": null, "title": "Argument Maps Improve Critical Thinking", "slug": "argument-maps-improve-critical-thinking", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:04.285Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Johnicholas", "createdAt": "2009-02-27T15:01:52.708Z", "isAdmin": false, "displayName": "Johnicholas"}, "userId": "kBvTXutfPytNtzPyD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NJ8k2RTwy3ELmwYZT/argument-maps-improve-critical-thinking", "pageUrlRelative": "/posts/NJ8k2RTwy3ELmwYZT/argument-maps-improve-critical-thinking", "linkUrl": "https://www.lesswrong.com/posts/NJ8k2RTwy3ELmwYZT/argument-maps-improve-critical-thinking", "postedAtFormatted": "Sunday, August 30th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Argument%20Maps%20Improve%20Critical%20Thinking&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArgument%20Maps%20Improve%20Critical%20Thinking%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNJ8k2RTwy3ELmwYZT%2Fargument-maps-improve-critical-thinking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Argument%20Maps%20Improve%20Critical%20Thinking%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNJ8k2RTwy3ELmwYZT%2Fargument-maps-improve-critical-thinking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNJ8k2RTwy3ELmwYZT%2Fargument-maps-improve-critical-thinking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 400, "htmlBody": "<p><a href=\"http://sarbayes.org/ctwardy/\">Charles R. Twardy</a> provides <a href=\"http://cogprints.org/3008/\">evidence</a> that a course in argrument mapping, using a <a href=\"http://rationale.austhink.com/\">particular</a> software tool improves critical thinking. The improvement in critical thinking is measured by performance on a specific multiple choice test (<a href=\"http://www.insightassessment.com/test-cctst.html\">California Critical Thinking Skills Test</a>). This may not be the best way to measure rationality, but my point is that unlike almost everybody else, there was measurement and statistical improvement!</p>\n<p>Also, his paper is the best, methodologically, that I've seen in the field of \"individual rationality augmentation research\".</p>\n<p>To summarize my (clumsy) understanding of the activity of argument mapping:</p>\n<p>One takes a real argument in natural language. (<a href=\"http://en.wikipedia.org/wiki/Op-ed\">op-eds</a>&nbsp;are a good source of short arguments, philosophy is a source of long arguments). Then elaborate it into a tree structure, with the main conclusion at the root of the tree. The tree has two kinds of nodes (it is a <a href=\"http://en.wikipedia.org/wiki/Bipartite_graph\">bipartite graph</a>). The root conclusion is a \"claim\"&nbsp;node. Every claim node has approximately one sentence of english text associated. The children of a claim are \"reasons\", which do NOT have english text associated. The children of a reason are claims. Unless I am mistaken, the intended meaning of the connection from a claim's child (a reason) to the parent is implication, and the meaning of a reason is the conjunction of its children.</p>\n<p>In elaborating the argument, it's often necessary to insert implicit claims. This should be done abiding by the \"Principle of Charity\", that you should interpret the argument in such a way as to make it the strongest argument possible.&nbsp;</p>\n<p>There are two syntactic rules which can easily find flaws in argument maps:<a id=\"more\"></a></p>\n<p>The Rabbit Rule: Informally, \"You can't conclude something about rabbits if you haven't been talking about rabbits\". Formally, \"Every meaningful term in the conclusion must appear at least once in every reason.\"</p>\n<p>The Holding Hands Rule: Informally, \"We can't be connected unless we're holding hands\". Formally, \"Every meaningful term in one premise of a reason must appear at least once in another premise of that reason, or in the conclusion\".</p>\n<p>I have tried the Rationale tool, and it seems afflicted with creeping featurism. My guess is the open-source tool <a href=\"http://freemind.sourceforge.net/wiki/index.php/Main_Page\">Freemind</a>&nbsp;could support argument mapping as described in Twardy's article, if the user is disciplined about it.</p>\n<p>I'd love comments offering alternative rationality-improvement tools. I'd prefer tools intended for solo use (that is, prediction markets are awesome but not what I'm looking for) and downloadable rather than web services, but anything would be great.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb226": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NJ8k2RTwy3ELmwYZT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 26, "extendedScore": null, "score": 5.187239249299749e-07, "legacy": true, "legacyId": "1542", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-30T20:56:22.927Z", "modifiedAt": null, "url": null, "title": "Great post on Reddit about accepting atheism", "slug": "great-post-on-reddit-about-accepting-atheism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:36:07.675Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JC5AcJPfXZ5BhEeE8/great-post-on-reddit-about-accepting-atheism", "pageUrlRelative": "/posts/JC5AcJPfXZ5BhEeE8/great-post-on-reddit-about-accepting-atheism", "linkUrl": "https://www.lesswrong.com/posts/JC5AcJPfXZ5BhEeE8/great-post-on-reddit-about-accepting-atheism", "postedAtFormatted": "Sunday, August 30th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Great%20post%20on%20Reddit%20about%20accepting%20atheism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGreat%20post%20on%20Reddit%20about%20accepting%20atheism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJC5AcJPfXZ5BhEeE8%2Fgreat-post-on-reddit-about-accepting-atheism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Great%20post%20on%20Reddit%20about%20accepting%20atheism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJC5AcJPfXZ5BhEeE8%2Fgreat-post-on-reddit-about-accepting-atheism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJC5AcJPfXZ5BhEeE8%2Fgreat-post-on-reddit-about-accepting-atheism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 113, "htmlBody": "<p>Go <a href=\"http://www.reddit.com/r/atheism/comments/9fg1b/atheism_vs_theism_may_seem_like_a_battle_of_wits/\">see for yourself</a>.</p>\n<p><span style=\"font-family: verdana;\"> </span></p>\n<blockquote>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; padding: 0px;\">To reject heaven and accept atheism - is not merely about science, facts, beliefs, etc - it is about accepting the reality of all those who have died - being really dead. It is accepting the same reality about everyone you love NOW one day being - really dead. It is accepting the same reality about YOU one day.</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; padding: 0px;\">The older you are, the more dear loved ones have passed away, the harder it will be to reject the notions of religion. To reject religion requires the re-mourning of everyone who you love who has died.</p>\n</blockquote>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; padding: 0px;\">The complete post is quite long and just as good as this quote.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"E9ihK6bA9YKkmJs2f": 1, "NSMKfa8emSbGNXRKD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JC5AcJPfXZ5BhEeE8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 18, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "1543", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-08-31T15:52:53.822Z", "modifiedAt": null, "url": null, "title": "Optimal Strategies for Reducing Existential Risk", "slug": "optimal-strategies-for-reducing-existential-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:28.075Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9uAr5W7YBrsGHwB5c/optimal-strategies-for-reducing-existential-risk", "pageUrlRelative": "/posts/9uAr5W7YBrsGHwB5c/optimal-strategies-for-reducing-existential-risk", "linkUrl": "https://www.lesswrong.com/posts/9uAr5W7YBrsGHwB5c/optimal-strategies-for-reducing-existential-risk", "postedAtFormatted": "Monday, August 31st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Optimal%20Strategies%20for%20Reducing%20Existential%20Risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOptimal%20Strategies%20for%20Reducing%20Existential%20Risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9uAr5W7YBrsGHwB5c%2Foptimal-strategies-for-reducing-existential-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Optimal%20Strategies%20for%20Reducing%20Existential%20Risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9uAr5W7YBrsGHwB5c%2Foptimal-strategies-for-reducing-existential-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9uAr5W7YBrsGHwB5c%2Foptimal-strategies-for-reducing-existential-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 487, "htmlBody": "<p>I've been talking to a variety of people about this recently, and it was suggested that people (including myself) might benefit from a LessWrong discussion on the topic. I've been thinking about it on my own for a year, which took me through Neuroscience, Computer Science, and International Security Policy. I'm hoping and finding that through discussion, a much greater variety of options can be proposed and considered, and those with particular experience or observations can&nbsp;have others benefit from their knowledge. I've been very happy to find there are a&nbsp;number of&nbsp;people seriously working towards this already (still far fewer than we might need), and their deliberations and learning would be particularly valuable.</p>\r\n<p>This is primarily about careers and other long term focused efforts (academic research and writing on the side, etc), not smaller incremental tools such as motivation and akrasia discussions. Where you should be applying your efforts, now how (much). Unless there's a lot of interest, it might also be good to otherwise avoid discussions on self-improvement in general and how to best realize these long term concerns, bringing those up elsewhere or in a seperate post.</p>\r\n<p>A few&nbsp;initial thoughts:</p>\r\n<ul>\r\n<li>What's the marginal value of one's own contribution? How much less would the \"next person\" be able to accomplish, i.e. the person who would get that job if you didn't? </li>\r\n<li>What are efforts that provide compounding benefit over time, rather than a constant static contribution? Most things are likely to provide some compounding benefit, but where is this most significant?</li>\r\n<li>We may not need or desire to do a lot of discussion on this, but it's worth keeping in mind that as time goes on and we learn more, collectively and individually, charitable donations could become much more effective. This suggests that it may be best save money until better applications are discovered, to be balanced of course by the compounding effects earlier donations can have. See Alan Dawrst's <a href=\"http://www.utilitarian-essays.com/make-money.html\" target=\"_blank\">paper on the value of making money</a>, which besides being a worthwhile read has a point about this in the 3rd note. If others know of more thorough treatments of this, please mention them. </li>\r\n<li>What are the particular existential risks that have the largest expected marginal reduction for a given amount of effort? What dangers are particularly underserved, by society in general and by this community? Is there more marginal use in focusing on particular dangers or in efforts that reduce all of them at once (such as better general institutional treatment of them)? </li>\r\n<li>How likely is it&nbsp;to increase the dangers of certain risks by an increased effort to avoid risks? If considerably likely and significant, can it be avoided and how?</li>\r\n<li>How should we assess the risk of various efforts? Some efforts might provide small but very likely reductions in risk, and&nbsp;in others you may be likely to accomplish&nbsp;nothing, but a success could be immensely helpful. </li>\r\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9uAr5W7YBrsGHwB5c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 4, "extendedScore": null, "score": 5.189441794247579e-07, "legacy": true, "legacyId": "1544", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-01T10:54:03.786Z", "modifiedAt": null, "url": null, "title": "Open Thread: September 2009", "slug": "open-thread-september-2009", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:03.067Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AllanCrossman", "createdAt": "2009-03-13T20:51:34.435Z", "isAdmin": false, "displayName": "AllanCrossman"}, "userId": "uHa4S4jNkPPrEd8ha", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jyz7S4M8GhLmSoPtg/open-thread-september-2009", "pageUrlRelative": "/posts/jyz7S4M8GhLmSoPtg/open-thread-september-2009", "linkUrl": "https://www.lesswrong.com/posts/jyz7S4M8GhLmSoPtg/open-thread-september-2009", "postedAtFormatted": "Tuesday, September 1st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20September%202009&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20September%202009%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjyz7S4M8GhLmSoPtg%2Fopen-thread-september-2009%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20September%202009%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjyz7S4M8GhLmSoPtg%2Fopen-thread-september-2009", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjyz7S4M8GhLmSoPtg%2Fopen-thread-september-2009", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 19, "htmlBody": "<p>I declare this Open Thread open for discussion of Less Wrong topics that have not appeared in recent posts.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jyz7S4M8GhLmSoPtg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 4, "extendedScore": null, "score": 5.19132058350732e-07, "legacy": true, "legacyId": "1545", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 186, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-01T15:06:57.167Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes - September 2009", "slug": "rationality-quotes-september-2009", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:58.251Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "thomblake", "createdAt": "2009-02-27T15:35:08.282Z", "isAdmin": false, "displayName": "thomblake"}, "userId": "zCHE6bXWKB6kfJsJS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/neGQaayydS5gPYfpa/rationality-quotes-september-2009", "pageUrlRelative": "/posts/neGQaayydS5gPYfpa/rationality-quotes-september-2009", "linkUrl": "https://www.lesswrong.com/posts/neGQaayydS5gPYfpa/rationality-quotes-september-2009", "postedAtFormatted": "Tuesday, September 1st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20-%20September%202009&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20-%20September%202009%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FneGQaayydS5gPYfpa%2Frationality-quotes-september-2009%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20-%20September%202009%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FneGQaayydS5gPYfpa%2Frationality-quotes-september-2009", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FneGQaayydS5gPYfpa%2Frationality-quotes-september-2009", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 84, "htmlBody": "<p><span style=\"font-family: Arial; line-height: 19px;\"> </span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\">A monthly thread for posting any interesting rationality-related quotes you've seen recently on the Internet, or had stored in your quotesfile for ages.</p>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li>Please post all quotes separately (so that they can be voted up/down separately) unless they are strongly related/ordered.</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote comments/posts on LW/OB - there is a separate thread for it.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>\n<div><em>\"A witty saying proves nothing.\"</em> -- Voltaire</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "neGQaayydS5gPYfpa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 5.191737097945167e-07, "legacy": true, "legacyId": "1548", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 105, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-01T15:11:01.113Z", "modifiedAt": null, "url": null, "title": "LW/OB Quotes - Fall 2009", "slug": "lw-ob-quotes-fall-2009", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:37.243Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "thomblake", "createdAt": "2009-02-27T15:35:08.282Z", "isAdmin": false, "displayName": "thomblake"}, "userId": "zCHE6bXWKB6kfJsJS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xBhnsNzrr5JDQiCRL/lw-ob-quotes-fall-2009", "pageUrlRelative": "/posts/xBhnsNzrr5JDQiCRL/lw-ob-quotes-fall-2009", "linkUrl": "https://www.lesswrong.com/posts/xBhnsNzrr5JDQiCRL/lw-ob-quotes-fall-2009", "postedAtFormatted": "Tuesday, September 1st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LW%2FOB%20Quotes%20-%20Fall%202009&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALW%2FOB%20Quotes%20-%20Fall%202009%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxBhnsNzrr5JDQiCRL%2Flw-ob-quotes-fall-2009%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LW%2FOB%20Quotes%20-%20Fall%202009%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxBhnsNzrr5JDQiCRL%2Flw-ob-quotes-fall-2009", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxBhnsNzrr5JDQiCRL%2Flw-ob-quotes-fall-2009", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 79, "htmlBody": "<p><span style=\"font-family: Arial; line-height: 19px;\">This is a monthly thread for posting any interesting rationality-related quotes you've seen on LW/OB.<br /> </span></p>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li>Please post all quotes separately (so that they can be voted up/down separately) unless they are strongly related/ordered.</li>\n<li>Do not quote yourself.</li>\n<li>Do not post quotes that are&nbsp;<strong style=\"font-weight: bold;\">NOT</strong>&nbsp;comments/posts on LW/OB - there is a separate thread for this.</li>\n<li>No more than 5 quotes per person per thread, please.</li>\n</ul>\n<div>\"<em>this thread is insanely incestuous</em>\" - Z_M_Davis</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xBhnsNzrr5JDQiCRL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 5.191743794775995e-07, "legacy": true, "legacyId": "1549", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-02T05:24:11.734Z", "modifiedAt": null, "url": null, "title": "Knowing What You Know", "slug": "knowing-what-you-know", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:24.702Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "pwZ6qMgzoKr3JPqx4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2r7kp9QSNNkF2Lpd7/knowing-what-you-know", "pageUrlRelative": "/posts/2r7kp9QSNNkF2Lpd7/knowing-what-you-know", "linkUrl": "https://www.lesswrong.com/posts/2r7kp9QSNNkF2Lpd7/knowing-what-you-know", "postedAtFormatted": "Wednesday, September 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Knowing%20What%20You%20Know&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKnowing%20What%20You%20Know%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2r7kp9QSNNkF2Lpd7%2Fknowing-what-you-know%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Knowing%20What%20You%20Know%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2r7kp9QSNNkF2Lpd7%2Fknowing-what-you-know", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2r7kp9QSNNkF2Lpd7%2Fknowing-what-you-know", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1521, "htmlBody": "<p>From Kahneman and Tversky:</p>\n<blockquote>\n<div>\"A person is said to employ the availability heuristic whenever he estimates frequency or probability by the ease with which instances or associations could be brought to mind\"</div>\n</blockquote>\n<div>I doubt that's news to any LessWrong readers - the availability heuristic isn't exactly cutting edge psychology. But the degree to which our minds rely on mental availability goes far beyond estimating probabilities. In a sense, every conscious thought is determined by how available it is - by whether it pops into our heads or not. And it's not something we even have the illusion of control over - if we knew where we were going, we'd already be there. (If you spend time actually looking directly at how thoughts proceed through your head, the idea of free will becomes more and more unrealistic. But I digress). What does and doesn't come to mind has an enormous impact on our mental functioning - our higher brain functions can only process what enters our working memory.</div>\n<div>Whether it's called &nbsp;salience, availability, or vividness, marking certain things important relative to other things is key to the proper functioning of our brains. Schizophrenia (specifically psychosis) has been described as \"<a title=\"state of aberrant salience\" href=\"http://www.mindhacks.com/blog/2009/02/formerly_schizophren.html\" target=\"_blank\">a state of aberrant salience</a>\", where the brain incorrectly assigns importance to what it processes. And a quick perusal of the&nbsp; <a title=\"list of cognitive biases\" href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\" target=\"_blank\">list of cognitive biases</a>&nbsp; reveals a large number directly tied to mental availability - there's the obvious availability heuristic, but there's also the simulation heuristic (a special case of the availability heuristic), base-rate neglect (abstract probabilities aren't salient, so aren't taken into account), hyperbolic discounting (the present is more salient than the future), the conjunction fallacy (ornate, specific descriptions make something less likely but more salient), the primacy/recency bias, the false consensus effect, the halo effect, projection bias, etc etc etc. Even consciousness seems to be based on things hitting a certain availability threshold and entering working memory. And it's not particularly surprising that A) we tend to process only what we mark as important and B) our marking system is flawed. Our minds are efficient, not perfect - a \"good enough\" solution to the problem of finding food and avoiding lions.</div>\n<div><a id=\"more\"></a></div>\n<div>That doesn't mean that availability isn't a complex system. It doesn't seem to be a fixed number that gets assigned when a memory is written - it's highly dependent on the context of the situation. A perfect example of this is&nbsp; <a title=\"priming\" href=\"/lw/3b/never_leave_your_room/\" target=\"_blank\">priming</a>. Simply seeing a picture is enough to make certain things more available, and that small change in availability is all that's needed to change how you vote. In&nbsp; <a title=\"state-dependent memory\" href=\"http://en.wikipedia.org/wiki/State-dependent_learning\" target=\"_blank\">state-dependent memory</a>, information that's been absorbed while under a certain state can only be retrieved under that same state - the context of the situation is needed for activation. It's why students are told to study under the same conditions that the test will be taken, and why musicians are told not to always practice sitting in the same position, to avoid inadvertently setting up a context dependent state. And anecdotally, I notice that my mind tends to slide between memories that make me happy when I'm happy, and memories that make me upset when I'm angry (moods are thought to be important context cues). In general, the more available something is, the less context is needed to activate it, and the less available, the more context dependent it becomes. Frequency, prototypicality, and abstractness also contribute to availability. Some things are so available that they're activated in improper contexts - this is how figurative language is&nbsp; <a title=\"thought to work\" href=\"http://books.google.com/books?id=21wGBXE7FCEC&amp;dq=salience+figurative+language&amp;printsec=frontcover&amp;source=bl&amp;ots=Jtzkpdmamd&amp;sig=0hBnZitRSwQDQ4YfXoTnXtrKtVw&amp;hl=en&amp;ei=vP-dSuKTHMWGtgenrNXnAw&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=6#v=onepage&amp;q=&amp;f=false\" target=\"_blank\">thought to work</a>. But some context is always required, or our minds would be nothing a but a greatest hits of our most salient thoughts, on a continuous loop.</div>\n<div>The problems with this approach is that availability isn't always assigned the way we'd prefer it. If I'm at a bar and want to tell a hilarious story, I can't just think of \"funny stories\" and activate all my great bar stories - they have to be triggered by some memory. More perniciously, it's possible (and in my experience, all too likely) to have a thought or take an action without having access to the beliefs that produced it. If, for example, I'm playing a videogame, I find it almost impossible to tell someone a sequence of buttons for something unless I'm holding the controller in my hands. Or I might avoid seeing a movie because I think it's awful, but I won't be able to recall why I think it's awful. Or I'll get into an argument with someone because he disagrees with something I think is obvious, but I won't immediately be able to summon the reasons that generated that obviousness. And this lack of availability can go beyond simple bad memory.</div>\n<div>From&nbsp; <a title=\"block 2008\" href=\"http://www.google.com/url?sa=t&amp;source=web&amp;ct=res&amp;cd=2&amp;url=http%3A%2F%2Fwww.nyu.edu%2Fgsas%2Fdept%2Fphilo%2Ffaculty%2Fblock%2Fpapers%2F2008_Aristotsoc.pdf&amp;ei=eP6dSr7HH9Wvtgfb8ZHtAw&amp;usg=AFQjCNEaPN7YvC2jxCEu_jt7tkKUO1VamQ&amp;sig2=OQtH05ZOeZR1sXSUV9o8XA\" target=\"_blank\">Block 2008</a>:</div>\n<blockquote>\n<div>There is a type of brain injury which causes a syndrome known as 'visuo-spatial extinction' If the patient sees a single object on either side, the patient can identify it, but if there are objects on both sides, the patient can identify only the one on the right and claims not to see the one on the left. However as Geraint Rees has shown in two fMRI studies of one patient (known as 'GK'), when GK claims not to see a face on the left, his fusiform face area (on the right - which is fed by the left side of space) lights up almost as much as - and in overlapping areas involving the fusiform face area - when he reports seeing the face.</div>\n</blockquote>\n<div>The brain can detect a face without passing that information to our working memory. What's more, when subjects with visuo-spatial extinction are asked to compare objects on both sides - as either 'the same' or 'different' - they're more than 88% accurate, despite not being able to 'see' the object on the left. Judgements can be made based on something that we have no cognitive access to.</div>\n<div>In Landman et al. (2003), subjects were shown a circle of eight rectangles for a short period, then a blank screen, then the same circle where a line points to one of the rectangles, which may or may not have rotated 90 degrees. The number of correct answers suggest subjects could track about four different rectangles, in line with data suggesting that our working memory for visual perceptions is about four. Subjects were then given the same test, except the line pointing to the rectangle appeared on the blank screen, between when the first circle and the second circle of rectangles is shown. On this test, subjects were able to track between six and seven rectangles by keeping the first circle of rectangles in memory, and comparing the second circle to it (according to the subjects). They're able to do this despite the fact that they're unable to access the shape of each individual rectangle. The suggested reason for this is that our memory for visual perceptions exceeds what we're capable of fitting into working memory - that we process the information without it entering our conscious minds[1]. It seems perfectly possible for us to know something without knowing we know it, and to believe something without having access to why we believe it.</div>\n<div>This, of course, isn't good. If you're building complex interconnected structures of beliefs (and you are), you need to be sure the ground below you is sturdy. And there's a strong possibility that if you can't recall the reason behind something, your brain will&nbsp; <a title=\"invent one for you\" href=\"/lw/14q/why_youre_stuck_in_a_narrative/\" target=\"_blank\">invent one for you</a>. The good news is that memories don't seem to be deleted - we can lose access, but the memory itself doesn't fade[2]. The problem is one of keeping access open. One way is to simply keep your memory sharp - and the web is&nbsp; <a title=\"full of tips on that\" href=\"http://www.google.com/search?q=memory+tips&amp;ie=utf-8&amp;oe=utf-8&amp;aq=t&amp;rls=org.mozilla:en-US:official&amp;client=firefox-a\" target=\"_blank\">full of tips on that</a>. A better way might be to leverage your mind's propensity for habituation - force yourself to trace your chain of belief down to the base, and eventually it will start to become something you do automatically. This isn't perfect either - it's not something you can do for the fast pace of day-to-day life, and it in itself is probably subject ot a whole series of biases. It might even be worth it to write your beliefs down - this method has the dual benefits of creating a hard copy for you to reference later, and increasing the availability of each belief through the act of writing and recalling. There's no ideal solution - we're limited in what new mental structures we can create, and we're forced to rely on the same basic (and imperfect) set of cognitive tools. Since availability seems to be such an integral part of the brain, forcing availability on those things we want to come to mind might be our best bet.</div>\n<div><br /></div>\n<div>Notes</div>\n<div>1: If it's hard to understand this experiment, look at the linked Block paper - it provides diagrams.</div>\n<div>2: It can, however, be re-written. Memory seems to work like a save-as function, being saved (and distorted slightly) every time it's recalled.<br /></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2r7kp9QSNNkF2Lpd7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 21, "extendedScore": null, "score": 5.193149434588462e-07, "legacy": true, "legacyId": "1551", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZmQv4DFx6y4jFbhLy", "FSPKLFfMNbRGPFjmY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-02T09:23:34.936Z", "modifiedAt": null, "url": null, "title": "Decision theory: Why we need to reduce \u201ccould\u201d, \u201cwould\u201d, \u201cshould\u201d", "slug": "decision-theory-why-we-need-to-reduce-could-would-should", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:29.887Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gxxpK3eiSQ3XG3DW7/decision-theory-why-we-need-to-reduce-could-would-should", "pageUrlRelative": "/posts/gxxpK3eiSQ3XG3DW7/decision-theory-why-we-need-to-reduce-could-would-should", "linkUrl": "https://www.lesswrong.com/posts/gxxpK3eiSQ3XG3DW7/decision-theory-why-we-need-to-reduce-could-would-should", "postedAtFormatted": "Wednesday, September 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Decision%20theory%3A%20Why%20we%20need%20to%20reduce%20%E2%80%9Ccould%E2%80%9D%2C%20%E2%80%9Cwould%E2%80%9D%2C%20%E2%80%9Cshould%E2%80%9D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecision%20theory%3A%20Why%20we%20need%20to%20reduce%20%E2%80%9Ccould%E2%80%9D%2C%20%E2%80%9Cwould%E2%80%9D%2C%20%E2%80%9Cshould%E2%80%9D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgxxpK3eiSQ3XG3DW7%2Fdecision-theory-why-we-need-to-reduce-could-would-should%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Decision%20theory%3A%20Why%20we%20need%20to%20reduce%20%E2%80%9Ccould%E2%80%9D%2C%20%E2%80%9Cwould%E2%80%9D%2C%20%E2%80%9Cshould%E2%80%9D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgxxpK3eiSQ3XG3DW7%2Fdecision-theory-why-we-need-to-reduce-could-would-should", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgxxpK3eiSQ3XG3DW7%2Fdecision-theory-why-we-need-to-reduce-could-would-should", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1207, "htmlBody": "<p>(This is the second post in a planned <a href=\"/lw/16f/decision_theory_an_outline_of_some_upcoming_posts/\">sequence</a>.)<br />&nbsp;<br />Let&rsquo;s say you&rsquo;re building an artificial intelligence named Bob.&nbsp; You&rsquo;d like Bob to sally forth and win many utilons on your behalf.&nbsp; How should you build him?&nbsp; More specifically, should you build Bob to have a world-model in which there are many different actions he &ldquo;could&rdquo; take, each of which &ldquo;would&rdquo; give him particular expected results?&nbsp; (Note that e.g. evolution, rivers, and thermostats do not have explicit &ldquo;could&rdquo;/&ldquo;would&rdquo;/&ldquo;should&rdquo; models in this sense -- and while evolution, rivers, and thermostats are all varying degrees of stupid, they all still accomplish specific sorts of world-changes.&nbsp; One might imagine more powerful agents that also simply take useful actions, without claimed &ldquo;could&rdquo;s and &ldquo;woulds&rdquo;.)<br /><br />My aim in this post is simply to draw attention to &ldquo;could&rdquo;, &ldquo;would&rdquo;, and &ldquo;should&rdquo;, as concepts folk intuition fails to understand, but that seem nevertheless to do something important for real-world agents.&nbsp;&nbsp; If we want to build Bob, we may well need to figure out what the concepts &ldquo;could&rdquo; and &ldquo;would&rdquo; can do for him.*<a id=\"more\"></a></p>\n<p><strong>Introducing Could/Would/Should agents:</strong><br /><br />Let a Could/Would/Should Algorithm, or CSA for short, be any algorithm that chooses its actions by considering a list of alternatives, estimating the payoff it &ldquo;would&rdquo; get &ldquo;if&rdquo; it took each given action, and choosing the action from which it expects highest payoff.<br /><br />That is: let us say that to specify a CSA, we need to specify:</p>\n<ol>\n<li>A list of alternatives a_1, a_2, ..., a_n that are primitively labeled as actions it &ldquo;could&rdquo; take; </li>\n<li>For each alternative a_1 through a_n, an expected payoff U(a_i) that is labeled as what &ldquo;would&rdquo; happen if the CSA takes that alternative.</li>\n</ol>\n<p><br />To be a CSA, the algorithm must then search through the payoffs for each action, and must then trigger the agent to actually take the action a_i for which its labeled U(a_i) is maximal.</p>\n<p><img src=\"http://images.lesswrong.com/t3_174_0.png\" alt=\"\" width=\"660\" height=\"472\" /><br /><br />Note that we can, by this definition of &ldquo;CSA&rdquo;, create a CSA around <em>any</em> made-up list of &ldquo;alternative actions&rdquo; and of corresponding &ldquo;expected payoffs&rdquo;.</p>\n<p><br /><strong>The puzzle is that CSAs are common enough to suggest that they&rsquo;re useful -- but it isn&rsquo;t clear <em>why</em> CSAs are useful, or quite what kinds of CSAs are what <em>kind</em> of useful.</strong>&nbsp; To spell out the puzzle:<br /><br /><em>Puzzle piece 1:&nbsp; CSAs are common.</em>&nbsp; Humans, some (though far from all) other animals, and many human-created decision-making programs (game-playing programs, scheduling software, etc.), have CSA-like structure.&nbsp; That is, we consider &ldquo;alternatives&rdquo; and act out the alternative from which we &ldquo;expect&rdquo; the highest payoff (at least to a first approximation).&nbsp; The ubiquity of approximate CSAs suggests that CSAs are in some sense useful.<br /><br /><em>Puzzle piece 2:&nbsp; The na&iuml;ve realist model of CSAs&rsquo; nature and usefulness </em>doesn&rsquo;t<em> work as an explanation.</em></p>\n<p>That is:&nbsp; many people find CSAs&rsquo; usefulness unsurprising, because they imagine a Physically Irreducible Choice Point, where an agent faces Real Options; by thinking hard, and choosing the Option that looks best, na&iuml;ve realists figure that you can get the best-looking option (instead of one of those other options, that you Really Could have gotten).</p>\n<p>But CSAs, like other agents, are deterministic physical systems.&nbsp; Each CSA executes a single sequence of physical movements, some of which we consider &ldquo;examining alternatives&rdquo;, and some of which we consider &ldquo;taking an action&rdquo;.&nbsp; It isn&rsquo;t clear why or in what sense such systems do better than deterministic systems built in some other way.<em></em></p>\n<p><em>Puzzle piece 3:</em>&nbsp; Real CSAs are presumably not built from arbitrarily labeled &ldquo;coulds&rdquo; and &ldquo;woulds&rdquo; -- presumably, the &ldquo;woulds&rdquo; that humans and others use, when considering e.g. which chess move to make, have useful properties.&nbsp; <em>But it isn&rsquo;t clear what those properties are, or how to build an algorithm to compute &ldquo;woulds&rdquo; with the desired properties.</em><br /><br /><em>Puzzle piece 4:&nbsp; On their face, all calculations of counterfactual payoffs (&ldquo;woulds&rdquo;) involve asking questions about impossible worlds.</em>&nbsp; It is not clear how to interpret such questions.</p>\n<p>Determinism notwithstanding, it is tempting to interpret CSAs&rsquo; &ldquo;woulds&rdquo; -- our U(a_i)s above -- as calculating what &ldquo;really would&rdquo; happen, if they &ldquo;were&rdquo; somehow able to take each given action.&nbsp;</p>\n<p>But if agent X will (deterministically) choose action a_1, then when he asks what would happen &ldquo;if&rdquo; he takes alternative action a&shy;_2, he&rsquo;s asking what would happen if something impossible happens.</p>\n<p>If X is to calculate the payoff &ldquo;if he takes action a_2&rdquo; as part of a causal world-model, he&rsquo;ll need to choose some particular meaning of &ldquo;if he takes action a_2&rdquo; &ndash; some meaning that allows him to combine a model of himself taking action a_2 with the rest of his current picture of the world, without allowing predictions like &ldquo;if I take action a_2, then the laws of physics will have been broken&rdquo;. <br /><br />We are left with several questions:</p>\n<ul>\n<li>Just what are humans, and other common CSAs, calculating when we imagine what &ldquo;would&rdquo; happen &ldquo;if&rdquo; we took actions we won&rsquo;t take?</li>\n<li>In what sense, and in what environments, are such &ldquo;would&rdquo; calculations useful?&nbsp; Or, if &ldquo;would&rdquo; calculations are not useful in any reasonable sense, how did CSAs come to be so common?</li>\n<li>Is there more than one natural way to calculate these counterfactual &ldquo;would&rdquo;s?&nbsp; If so, what are the alternatives, and which alternative works best?</li>\n</ul>\n<p>&nbsp;</p>\n<hr />\n<p>*A draft-reader suggested to me that this question is poorly motivated: what other kinds of agents could there be, besides &ldquo;could&rdquo;/&ldquo;would&rdquo;/&ldquo;should&rdquo; agents?&nbsp; Also, how could modeling the world in terms of &ldquo;could&rdquo; and &ldquo;would&rdquo; <em>not</em> be useful to the agent?<br /> &nbsp;&nbsp;&nbsp; <br /> My impression is that there is a sort of gap in philosophical wariness here that is a bit difficult to bridge, but that one <em>must</em> bridge if one is to think well about AI design.&nbsp; I&rsquo;ll try an analogy.&nbsp; In my experience, beginning math students simply expect their nice-sounding procedures to work.&nbsp; For example, they expect to be able to add fractions straight across.&nbsp; When you tell them they can&rsquo;t, they demand to know <em>why</em> they can&rsquo;t, as though most nice-sounding theorems are true, and if you want to claim that one isn&rsquo;t, the burden of proof is on you.&nbsp; It is only after students gain considerable mathematical sophistication (or experience getting burned by expectations that don&rsquo;t pan out) that they place the burden of proofs on the theorems, assume theorems false or un-usable until proven true, and try to actively construct and prove their mathematical worlds.<br /> <br /> Reaching toward AI theory is similar.&nbsp; If you don&rsquo;t understand how to reduce a concept -- how to build circuits that compute that concept, and what exact positive results will follow from that concept and will be absent in agents which don&rsquo;t implement it -- you need to keep analyzing.&nbsp; You need to be suspicious of anything you can&rsquo;t derive for yourself, from scratch.&nbsp; Otherwise, even if there is something of the sort that is useful in the specific context of your head (e.g., some sort of &ldquo;could&rdquo;s and &ldquo;would&rdquo;s that do you good), your attempt to re-create something similar-looking in an AI may well lose the usefulness.&nbsp; You get <a href=\"http://en.wikipedia.org/wiki/Cargo_cult\">cargo cult</a> could/woulds.</p>\n<p>+ Thanks to <a href=\"/user/Z_M_Davis/\">Z M Davis</a> for the above gorgeous diagram.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gxxpK3eiSQ3XG3DW7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 31, "extendedScore": null, "score": 5.1e-05, "legacy": true, "legacyId": "1552", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sLxFqs8fdjsPdkpLC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-02T17:47:00.573Z", "modifiedAt": null, "url": null, "title": "The Featherless Biped", "slug": "the-featherless-biped", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:47.167Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Annoyance", "createdAt": "2009-02-28T04:06:40.600Z", "isAdmin": false, "displayName": "Annoyance"}, "userId": "yEm6LWatswJYGwBFq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zmmB5nu4odF8k6wAc/the-featherless-biped", "pageUrlRelative": "/posts/zmmB5nu4odF8k6wAc/the-featherless-biped", "linkUrl": "https://www.lesswrong.com/posts/zmmB5nu4odF8k6wAc/the-featherless-biped", "postedAtFormatted": "Wednesday, September 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Featherless%20Biped&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Featherless%20Biped%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzmmB5nu4odF8k6wAc%2Fthe-featherless-biped%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Featherless%20Biped%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzmmB5nu4odF8k6wAc%2Fthe-featherless-biped", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzmmB5nu4odF8k6wAc%2Fthe-featherless-biped", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 429, "htmlBody": "<p>The classical understanding of categories centers on necessary and sufficient properties.&nbsp; If a thing has X, Y, and Z, we say that it belongs to class A; if it lacks them, we say that it does not.&nbsp; This is the model of how humans construct and recognize categories that philosophers have held since the days of Aristotle.</p>\n<p>Cognitive scientists found that the reality isn't that simple.<a id=\"more\"></a></p>\n<p>Human categorization is not a neat and precise process.&nbsp; When asked to explain the necessary features of, say, a bird, people cannot.&nbsp; When confronted with collections of stimuli and asked to determine which represent examples of 'birds', people find it easy to accept or reject things that have all or none of the properties they associate with that concept; when shown entities that share some but not all of the critical properties, people spend much more time trying to decide, and their decisions are tentative.&nbsp;&nbsp; Their responses simply aren't compatible with binary models.</p>\n<p>Concepts are associational structures.&nbsp; They do not divide the world clearly into two parts.&nbsp; Not all of their features are logically necessary.&nbsp; The recognition of features produces an activation, the strength of which depends not only on the degree to which the feature is present but a weighting factor.&nbsp; When the sum of the activations crosses a threshold, the concept becomes active and the stimulus is said to belong to that category.&nbsp; The stronger the total activation, the more clearly the stimulus can be said to embody the concept.</p>\n<p>Does this sound familiar?&nbsp; It should for us - we have the benefit of hindsight.&nbsp; We can recognize that pattern - it's how neural networks function.&nbsp; Or to put it another way, it's how neurons work.</p>\n<p>But wait!&nbsp; There's more!</p>\n<p>Try applying that model to virtually every empirical fact we've acquired regarding how people produce their conclusions.&nbsp; For example, our beliefs about how seriously we should take a hypothetical problem scenario depend not on a rigorous statistical analysis, but a combination of how vividly we feel about the scenario and how frequently it appears in our memory.&nbsp; People are convinced not only by the logical structure of an argument but the traits of the entities presenting it and the specific way in which the arguments are made.&nbsp; And so on, and so forth.</p>\n<p>Most human behavior derives directly from the behavior of the associational structures in our minds.</p>\n<p>To put it another way:&nbsp; what we call 'thinking' doesn't involve rational thought.&nbsp; It's *feeling*.&nbsp; People ponder an issue, then respond in the way that they feel stands out the most from the sea of associations.</p>\n<p>Consider the implications for a while.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zmmB5nu4odF8k6wAc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 4, "extendedScore": null, "score": 7e-06, "legacy": true, "legacyId": "1547", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-03T00:53:55.237Z", "modifiedAt": "2022-04-24T12:10:57.373Z", "url": null, "title": "The Sword of Good", "slug": "the-sword-of-good", "viewCount": null, "lastCommentedAt": "2022-04-24T05:39:47.241Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XuLG6M7sHuenYWbfC/the-sword-of-good", "pageUrlRelative": "/posts/XuLG6M7sHuenYWbfC/the-sword-of-good", "linkUrl": "https://www.lesswrong.com/posts/XuLG6M7sHuenYWbfC/the-sword-of-good", "postedAtFormatted": "Thursday, September 3rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Sword%20of%20Good&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Sword%20of%20Good%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXuLG6M7sHuenYWbfC%2Fthe-sword-of-good%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Sword%20of%20Good%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXuLG6M7sHuenYWbfC%2Fthe-sword-of-good", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXuLG6M7sHuenYWbfC%2Fthe-sword-of-good", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 612, "htmlBody": "<p><em>..fragments of a book that would never be written...</em></p>\n<p style=\"padding-left: 30px;\">* &nbsp; &nbsp;&nbsp; *&nbsp; &nbsp; &nbsp; *</p>\n<p>Captain Selena, late of the pirate ship <em>Nemesis,</em> quietly extended the very tip of her blade around the corner, staring at the tiny reflection on the metal.&nbsp; At once, but still silently, she pulled back the sword; and with her other hand made a complex gesture.</p>\n<p>The translation spell told Hirou that the handsigns meant:&nbsp; \"Orcs.&nbsp; Seven.\"</p>\n<p>Dolf looked at Hirou.&nbsp; \"My Prince,\" the wizard signed, \"do not waste yourself against mundane opponents.&nbsp; Do not draw the Sword of Good as yet.&nbsp; Leave these to Selena.\"</p>\n<p>Hirou's mouth was very dry.&nbsp; He didn't know if the translation spell could understand the difference between wanting to talk and wanting to make gestures; and so Hirou simply nodded.</p>\n<p>Not for the first time, the thought occurred to Hirou that if he'd actually <em>known</em> he was going to be transported into a magical universe, informed he was the long-lost heir to the Throne of Bronze, handed the legendary Sword of Good, and told to fight evil, he would have spent less time reading fantasy novels.&nbsp; Joined the army, maybe.&nbsp; Taken fencing lessons, at least.&nbsp; If there was one thing that <em>didn't </em>prepare you for fantasy real life, it was sitting at home reading fantasy fiction.</p>\n<p>Dolf and Selena were looking at Hirou, as if waiting for something more.</p>\n<p><em>Oh.&nbsp; That's right.&nbsp; I'm the prince.</em></p>\n<p>Hirou raised a finger and pointed it around the corner, trying to indicate that they should go ahead -</p>\n<p>With a sudden burst of motion Selena plunged around the corner, Dolf following hard on her heels, and Hirou, startled and hardly thinking, moving after.</p>\n<p style=\"padding-left: 30px;\"><em>(This story ended up too long for a single LW post, so I put it <a href=\"http://yudkowsky.net/other/fiction/the-sword-of-good\">on yudkowsky.net</a>.<br />Do read <a href=\"http://yudkowsky.net/other/fiction/the-sword-of-good\">the rest of the story</a> there, before continuing to the Acknowledgments below.)</em></p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p style=\"padding-left: 30px;\"><strong>Acknowledgments:</strong></p>\n<p>I had the idea for this story during a conversation with Nick Bostrom and Robin Hanson about an awful little facet of human nature I call \"suspension of moral disbelief\".&nbsp; The archetypal case in my mind will always be the Passover Seder, watching my parents and family and sometimes friends reciting the Ten Plagues that God is supposed to have visited on Egypt.&nbsp; You take drops from the wine glass - or grape juice in my case - and drip them onto the plate, to symbolize your sadness at God slaughtering the first-born male children of the Egyptians.&nbsp; So the Seder actually <em>points out</em> the awfulness, and yet no one says:&nbsp; \"This is wrong; God should not have done that to innocent families in retaliation for the actions of an unelected Pharaoh.\"&nbsp; I forget when I first realized how horrible that was - the real horror being not the Plagues, of course, since they never happened; the real horror is watching your family <em>not notice </em>that they're swearing allegiance to an evil God in a happy wholesome family Cthulhu-worshiping ceremony.&nbsp; Arbitrarily hideous evils can be wholly concealed by a social atmosphere in which no one is expected to point them out and it would seem awkward and out-of-place to do so.</p>\n<p>In writing it's even simpler - the author gets to <em>create </em>the whole social universe, and the readers are immersed in the hero's own internal perspective.&nbsp; And so <em>anything </em>the heroes do, which no character notices as wrong, won't be noticed by the readers as unheroic.&nbsp; Genocide, mind-rape, eternal torture, <em>anything.</em></p>\n<p>Explicit inspiration was taken from <a href=\"http://xkcd.com/549/\">this XKCD</a> (warning: spoilers for <em>The Princess Bride</em>), <a href=\"http://www.boatcrime.com/2009/07/19/wizards-are-assholes/\">this Boat Crime</a>, and <a href=\"http://www.youtube.com/watch?v=5Xd_zkMEgkI\">this Monty Python</a>, not to mention <a href=\"http://dir.salon.com/story/ent/feature/2002/12/17/tolkien_brin/index.html\">that essay by David Brin</a> and <a href=\"http://goblins.keenspot.com/d/20051217.html\">the entire Goblins webcomic</a>.&nbsp; This <a href=\"http://lfgcomic.com/page/104\">Looking For Group</a> helped inspire the story's title, and everything else flowed downhill from there.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 15, "ouT6wKhACJRouGokM": 1, "DdgSyQoZXjj3KnF4N": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XuLG6M7sHuenYWbfC", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}], "voteCount": 112, "baseScore": 127, "extendedScore": null, "score": 0.000201, "legacy": true, "legacyId": "1521", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 129, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 304, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2009-09-03T00:53:55.237Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-03T23:04:27.130Z", "modifiedAt": null, "url": null, "title": "Torture vs. Dust vs. the Presumptuous Philosopher: Anthropic Reasoning in UDT", "slug": "torture-vs-dust-vs-the-presumptuous-philosopher-anthropic", "viewCount": null, "lastCommentedAt": "2018-06-22T08:12:22.927Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RcvyJjPQwimAeapNg/torture-vs-dust-vs-the-presumptuous-philosopher-anthropic", "pageUrlRelative": "/posts/RcvyJjPQwimAeapNg/torture-vs-dust-vs-the-presumptuous-philosopher-anthropic", "linkUrl": "https://www.lesswrong.com/posts/RcvyJjPQwimAeapNg/torture-vs-dust-vs-the-presumptuous-philosopher-anthropic", "postedAtFormatted": "Thursday, September 3rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Torture%20vs.%20Dust%20vs.%20the%20Presumptuous%20Philosopher%3A%20Anthropic%20Reasoning%20in%20UDT&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATorture%20vs.%20Dust%20vs.%20the%20Presumptuous%20Philosopher%3A%20Anthropic%20Reasoning%20in%20UDT%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRcvyJjPQwimAeapNg%2Ftorture-vs-dust-vs-the-presumptuous-philosopher-anthropic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Torture%20vs.%20Dust%20vs.%20the%20Presumptuous%20Philosopher%3A%20Anthropic%20Reasoning%20in%20UDT%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRcvyJjPQwimAeapNg%2Ftorture-vs-dust-vs-the-presumptuous-philosopher-anthropic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRcvyJjPQwimAeapNg%2Ftorture-vs-dust-vs-the-presumptuous-philosopher-anthropic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 672, "htmlBody": "<p>In this post, I'd like to examine whether <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">Updateless Decision Theory</a> can provide any insights into <a href=\"http://www.anthropic-principle.com/primer.html\">anthropic reasoning</a>. Puzzles/paradoxes in anthropic reasoning is what prompted me to consider UDT <a href=\"http://groups.google.com/group/everything-list/browse_thread/thread/8c25168e232a7efd/\">originally</a> and this post may be of interest to those who do not consider <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">Counterfactual Mugging</a> to provide <a href=\"/lw/16q/dont_be_pathologically_mugged/\">sufficient motivation</a> for UDT.</p>\n<p>The Presumptuous Philosopher is a thought experiment that Nick Bostrom used to argue against the <a href=\"http://en.wikipedia.org/wiki/Self-Indication_Assumption\">Self-Indication Assumption.</a> (SIA: Given the fact that you exist, you should (other things equal) favor hypotheses according to which many observers exist over hypotheses on which few observers exist.)</p>\n<p><a id=\"more\"></a></p>\n<blockquote>\n<p>It is the year 2100 and physicists have narrowed down the search for a theory of&nbsp; everything to only two remaining plausible candidate theories, T1 and T2 (using&nbsp; considerations from super-duper symmetry). According to T1 the world is very,&nbsp; very big but finite, and there are a total of a trillion trillion observers in&nbsp; the cosmos. According to T2, the world is very, very, very big but finite, and&nbsp; there are a trillion trillion trillion observers. The super-duper symmetry&nbsp; considerations seem to be roughly indifferent between these two theories. The&nbsp; physicists are planning on carrying out a simple experiment that will falsify&nbsp; one of the theories. Enter the presumptuous philosopher: \"Hey guys, it is&nbsp; completely unnecessary for you to do the experiment, because I can already show&nbsp; to you that T2 is about a trillion times more likely to be true than T1&nbsp; (whereupon the philosopher runs the God&rsquo;s Coin Toss thought experiment and&nbsp; explains Model 3)!\"<br /><br />One suspects the Nobel Prize committee to be a bit hesitant about awarding the presumptuous philosopher the big one for this contribution.</p>\n</blockquote>\n<p>To make this example clearer as a decision problem, let's say that the consequences of carrying out the \"simple experiment\" is a very small cost (one dollar). And the consequences of just assuming T2 is a disaster down the line if T1 turns out to be true (we create a power plant based on T2, and it blows up and kills someone).</p>\n<p>In UDT, no Bayesian updating occurs, and in particular, you don't update on the fact that you exist. Suppose in CDT you have a prior P(T1) = P(T2) = .5 before taking into account that you exist, then translated into UDT you have &Sigma; P(V<sub>i</sub>) = &Sigma; P(W<sub>i</sub>) = .5, where V<sub>i</sub> and W<sub>i</sub> are world programs where T1 and T2 respectively hold. Anthropic reasoning occurs as a result of considering the <em>consequences</em> of your decisions, which are a trillion times greater in T2 worlds than in T1 worlds, since your decision algorithm S is called about a trillion times more often in W<sub>i</sub> programs than in V<sub>i</sub> programs.</p>\n<p>Perhaps by now you've notice the parallel between this decision problem and Eliezer's <a href=\"/lw/kn/torture_vs_dust_specks/\">Torture vs. Dust Specks</a>. The very small cost of the simple physics experiment is akin to getting a dust speck in the eye, and the disaster of wrongly assuming T2 is akin to being tortured. By not doing the experiment, we can save one dollar for a trillion individuals in exchange for every individual we kill.</p>\n<p>In general, Updateless Decision Theory converts anthropic reasoning problems into ethical problems. I can see three approaches to taking advantage of this:</p>\n<ol>\n<li>If you have strong epistemic intuitions but weak moral intuitions, then you can adjust your morality to fit your epistemology.</li>\n<li>If you have strong moral intuitions but weak epistemic intuitions, then you can adjust your epistemology to fit your morality.</li>\n<li>You might argue that epistemology shouldn't be linked so intimately with morality, and therefore this whole approach is on the wrong track.</li>\n</ol>\n<p>Personally, I have vacillated between 1 and 2. I've <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/fpp\">argued</a>, based on 1, that we should discount the values of individuals by using a complexity-based measure. And I've also <a href=\"http://groups.google.com/group/everything-list/browse_thread/thread/fab13ff3968a6ae9\">argued</a>, based on 2, that perhaps the choice of an epistemic prior is more or less arbitrary (since objective morality seems unlikely to me). So I'm not sure what the right answer is, but this seems to be the right track to me.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 4, "5f5c37ee1b5cdee568cfb1dc": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RcvyJjPQwimAeapNg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 32, "extendedScore": null, "score": 5.2e-05, "legacy": true, "legacyId": "1553", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pbYRxTLAcxAQsGvJ7", "3wYTFWY3LKQCnAptN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-05T19:10:05.400Z", "modifiedAt": null, "url": null, "title": "Notes on utility function experiment", "slug": "notes-on-utility-function-experiment", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:33.393Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/A2XNfwqqqp8A5DZJn/notes-on-utility-function-experiment", "pageUrlRelative": "/posts/A2XNfwqqqp8A5DZJn/notes-on-utility-function-experiment", "linkUrl": "https://www.lesswrong.com/posts/A2XNfwqqqp8A5DZJn/notes-on-utility-function-experiment", "postedAtFormatted": "Saturday, September 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Notes%20on%20utility%20function%20experiment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANotes%20on%20utility%20function%20experiment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA2XNfwqqqp8A5DZJn%2Fnotes-on-utility-function-experiment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Notes%20on%20utility%20function%20experiment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA2XNfwqqqp8A5DZJn%2Fnotes-on-utility-function-experiment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA2XNfwqqqp8A5DZJn%2Fnotes-on-utility-function-experiment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 352, "htmlBody": "<p>I just finished a two-week experiment of trying to live by a point system. I attached a point value to various actions and events, and made some effort to maximize the score. I cannot say it was successful in making me achieve more than normally during the same period of time, but it made more clear some of the problems with my behaviour.</p>\n<p>Here's some notes from my experiment:</p>\n<ul>\n<li>Points are marginal utilities, they are for things you want to do more of, but don't due to akrasia. If you want to exercise more you'll assign very high value to half an hour of exercise, but that doesn't mean you want to spend 8 hours a day cross-training. As expected, I got most points for thing that weren't that important but I finally got myself to do more.</li>\n<li>Values can be put on terminal values (results), or instrumental values (effort, and partial results). Valuing only the former tends to be highly demoralizing, valuing the latter tends to be highly encouraging.</li>\n<li>It's a good idea to assign some points to cleanup of your system (decide against doing something that was previously on your list, get trivial thing off your list). It cleans your mind, even if it doesn't progress your big goals.</li>\n<li>Another good idea are points for sitting down and thinking, making mindmaps and so on.</li>\n<li>One big problem that the system didn't cover at all were distractions, like spending too much time on Wikipedia or TvTropes.</li>\n<li>Another big problem were times when I didn't have enough energy to do anything big, but wasn't sleepy enough to sleep. It's usually pure waste of time.</li>\n<li>During the first week I was more successful (in terms of points) almost every day, then it went far downhill, perhaps due to my enthusiasm running out. This made me decide against extending the experiment past the original two week schedule.</li>\n<li>In other words - there was some value to it, but akrasia mostly won again.</li>\n</ul>\n<p>Anyone else wants to share their anti-akrasia experiments?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "A2XNfwqqqp8A5DZJn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 18, "extendedScore": null, "score": 5.201641778194693e-07, "legacy": true, "legacyId": "1556", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-05T22:31:27.354Z", "modifiedAt": null, "url": null, "title": "Counterfactual Mugging and Logical Uncertainty", "slug": "counterfactual-mugging-and-logical-uncertainty", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:50.305Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rqt8RSKPvh4GzYoqE/counterfactual-mugging-and-logical-uncertainty", "pageUrlRelative": "/posts/rqt8RSKPvh4GzYoqE/counterfactual-mugging-and-logical-uncertainty", "linkUrl": "https://www.lesswrong.com/posts/rqt8RSKPvh4GzYoqE/counterfactual-mugging-and-logical-uncertainty", "postedAtFormatted": "Saturday, September 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Counterfactual%20Mugging%20and%20Logical%20Uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACounterfactual%20Mugging%20and%20Logical%20Uncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frqt8RSKPvh4GzYoqE%2Fcounterfactual-mugging-and-logical-uncertainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Counterfactual%20Mugging%20and%20Logical%20Uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frqt8RSKPvh4GzYoqE%2Fcounterfactual-mugging-and-logical-uncertainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frqt8RSKPvh4GzYoqE%2Fcounterfactual-mugging-and-logical-uncertainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 855, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/3l/counterfactual_mugging/\">Counterfactual Mugging</a>.</p>\n<p>Let's see what happens with <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">Counterfactual Mugging</a>, if we replace the uncertainty about an external fact of how a coin lands, with logical uncertainty, for example about what is the n-th place in the <a href=\"http://en.wikipedia.org/wiki/Computing_%CF%80\">decimal expansion of pi</a>.</p>\n<p>The original thought experiment is as follows:</p>\n<blockquote>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Omega\">Omega</a> appears and says that it has just tossed a fair coin, and given that the coin came up tails, it decided to ask you to give it $100. Whatever you do in this situation, nothing else will happen differently in reality as a result. Naturally you don't want to give up your $100. But Omega also tells you that if the coin came up heads instead of tails, it'd give you $10000, but only if you'd agree to give it $100 if the coin came up tails.</p>\n</blockquote>\n<p>Let's change \"coin came up tails\" to \"10000-th digit of pi is even\", and correspondingly for heads. This gives <em>Logical Counterfactual Mugging</em>:</p>\n<blockquote>\n<p>Omega appears and says that it has just found out what that 10000th decimal digit of pi is 8, and given that it is even, it decided to ask you to give it $100. Whatever you do in this situation, nothing else will happen differently in reality as a result. Naturally you don't want to give up your $100. But Omega also tells you that if the 10000th digit of pi turned out to be odd instead, it'd give you $10000, but only if you'd agree to give it $100 given that the 10000th digit is even.</p>\n</blockquote>\n<p>This form of Counterfactual Mugging may be instructive, as it slaughters the following false intuition, or equivalently conceptualization of \"could\": \"the coin <em>could</em> land either way, but a logical truth <em>couldn't</em> be either way\".<a id=\"more\"></a></p>\n<p>For the following, let's shift the perspective to Omega, and consider the problem about 10001th digit, which is 5 (odd). It's easy to imagine that given that the 10001th digit of pi is in fact 5, and you decided to only give away the $100 if the digit is odd, then Omega's prediction of your actions will still be that you'd give away $100 (because the digit is in fact odd). Direct prediction of your actions <em>can't</em> include the part where you observe that the digit is even, because the digit is odd.</p>\n<p>But Omega doesn't compute what you'll do in reality, it computes what you <em>would</em> do <em>if the 10001th digit of pi was even</em> (which it isn't). If you decline to give away the $100 if the digit is even, Omega's simulation <em>of counterfactual</em> where the digit is even <em>will</em> say that you wouldn't oblige, and so you won't get the $10000 in reality, where the digit is odd.</p>\n<p>Imagine it constructively this way: you have the code of a procedure, Pi(n), that computes the n-th digit of pi once it's run. If your strategy is</p>\n<blockquote>\n<p>if(Is_Odd(Pi(n))) then Give(\"$100\");</p>\n</blockquote>\n<p>then, given that n==10001, Pi(10001)==5, and Is_Odd(5)==<strong>true</strong>, the program outputs \"$100\". But Omega tests what's the output of the code on which it performed a surgery, replacing Is_Odd(Pi(n)) by <strong>false</strong> instead of <strong>true</strong> to which it normally evaluates. Thus it'll be testing the code</p>\n<blockquote>\n<p>if(<strong>false</strong>) then Give(\"$100\");</p>\n</blockquote>\n<p>This counterfactual case doesn't give away $100, and so Omega decides that you won't get the $10000.</p>\n<p>For the original problem, when you consider what would happen if the coin fell differently, you are basically performing the same surgery, replacing the knowledge about the state of the coin in the state of mind. If you use the (wrong) strategy</p>\n<blockquote>\n<p>if(Coin==\"heads\") then Give(\"$100\")</p>\n</blockquote>\n<p>and the coin comes up \"heads\", so that Omega is deciding whether to give you $10000, then Coin==\"heads\", but Omega is evaluating the modified algorithm where Coin is replaced by \"tails\":</p>\n<blockquote>\n<p>if(\"tails\"==\"heads\") then Give(\"$100\")</p>\n</blockquote>\n<p>Another way of intuitively thinking about Logical CM is to consider the index of the digit (here, 10000 or 10001) to be a random variable. Then, the choice of number n (value of the random variable) in Omega's question is a perfect analogy with the outcome of a coin toss.</p>\n<p>With a random index instead of \"direct\" mathematical uncertainty, the above evaluation of counterfactual uses (say) 10000 to replace n (so that Is_Odd(Pi(10000))==<strong>false</strong>), instead of directly using <strong>false</strong> to replace Is_Odd(P(n)) with <strong>false</strong>:</p>\n<blockquote>\n<p>if(Is_Odd(Pi(10000))) then Give(\"$100\");</p>\n</blockquote>\n<p>The difference is that with the coin or random digit number, the parameter is explicit and atomic (Coin and n, respectively), while with the oddness of n-th digit, the parameter Is_Odd(P(n)) isn't atomic. How can it be detected in the code (in the mind) &mdash; it could be written in obfuscated assembly, not even an explicit subexpression of the program? By the connection to the <a href=\"/lw/15n/sense_denotation_and_semantics/\">sense</a> of the problem statement itself: when you are talking about what you'll do if the n-th digit of pi is even or odd, or what Omega will do if you give or not give away $100 in each case, you are talking about exactly your Is_Odd(Pi(n)), or something from which this code will be constructed. The meaning of procedure Pi(n) is dependent on the meaning of the problem, and through this dependency counterfactual surgery can reach down and change the details of the algorithm to answer the counterfactual query posed by the problem.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YpHkTW27iMFR2Dkae": 1, "JHYaBGQuuKHdwnrAK": 1, "5f5c37ee1b5cdee568cfb1b6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rqt8RSKPvh4GzYoqE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 10, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "1557", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mg6jDEuQEjBGtibX7", "zFZicEFAxTfSEbK69"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-06T00:00:05.140Z", "modifiedAt": null, "url": null, "title": "Bay Area OBLW Meetup Sep 12", "slug": "bay-area-oblw-meetup-sep-12", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:26.735Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7PtAK5WaiwQpFrWxM/bay-area-oblw-meetup-sep-12", "pageUrlRelative": "/posts/7PtAK5WaiwQpFrWxM/bay-area-oblw-meetup-sep-12", "linkUrl": "https://www.lesswrong.com/posts/7PtAK5WaiwQpFrWxM/bay-area-oblw-meetup-sep-12", "postedAtFormatted": "Sunday, September 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bay%20Area%20OBLW%20Meetup%20Sep%2012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABay%20Area%20OBLW%20Meetup%20Sep%2012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7PtAK5WaiwQpFrWxM%2Fbay-area-oblw-meetup-sep-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bay%20Area%20OBLW%20Meetup%20Sep%2012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7PtAK5WaiwQpFrWxM%2Fbay-area-oblw-meetup-sep-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7PtAK5WaiwQpFrWxM%2Fbay-area-oblw-meetup-sep-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 25, "htmlBody": "<p><a href=\"http://www.meetup.com/Bay-Area-Overcoming-Bias-Meetup/calendar/11287547/\">Bay Area OB/LW meetup, 6pm September 12th 2009 at the SIAI House in Santa Clara.</a></p>\n<p>This will also serve as my 2nd annual 29th birthday party.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7PtAK5WaiwQpFrWxM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 5.202121093478688e-07, "legacy": true, "legacyId": "1558", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-06T06:10:20.344Z", "modifiedAt": null, "url": null, "title": "Decision theory: Why Pearl helps reduce \u201ccould\u201d and \u201cwould\u201d, but still leaves us with at least three alternatives", "slug": "decision-theory-why-pearl-helps-reduce-could-and-would-but", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:16.173Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/miwf7qQTh2HXNnSuq/decision-theory-why-pearl-helps-reduce-could-and-would-but", "pageUrlRelative": "/posts/miwf7qQTh2HXNnSuq/decision-theory-why-pearl-helps-reduce-could-and-would-but", "linkUrl": "https://www.lesswrong.com/posts/miwf7qQTh2HXNnSuq/decision-theory-why-pearl-helps-reduce-could-and-would-but", "postedAtFormatted": "Sunday, September 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Decision%20theory%3A%20Why%20Pearl%20helps%20reduce%20%E2%80%9Ccould%E2%80%9D%20and%20%E2%80%9Cwould%E2%80%9D%2C%20but%20still%20leaves%20us%20with%20at%20least%20three%20alternatives&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecision%20theory%3A%20Why%20Pearl%20helps%20reduce%20%E2%80%9Ccould%E2%80%9D%20and%20%E2%80%9Cwould%E2%80%9D%2C%20but%20still%20leaves%20us%20with%20at%20least%20three%20alternatives%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmiwf7qQTh2HXNnSuq%2Fdecision-theory-why-pearl-helps-reduce-could-and-would-but%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Decision%20theory%3A%20Why%20Pearl%20helps%20reduce%20%E2%80%9Ccould%E2%80%9D%20and%20%E2%80%9Cwould%E2%80%9D%2C%20but%20still%20leaves%20us%20with%20at%20least%20three%20alternatives%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmiwf7qQTh2HXNnSuq%2Fdecision-theory-why-pearl-helps-reduce-could-and-would-but", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmiwf7qQTh2HXNnSuq%2Fdecision-theory-why-pearl-helps-reduce-could-and-would-but", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1595, "htmlBody": "<p>(This is the third post in a planned <a href=\"/lw/16f/decision_theory_an_outline_of_some_upcoming_posts/\">sequence</a>.)<br /><br />My <a href=\"/lw/174/decision_theory_why_we_need_to_reduce_could_would/\">last post</a> left us with the questions:</p>\n<ul>\n<li>Just what are humans, and other common CSAs, calculating when we imagine what &ldquo;would&rdquo; happen &ldquo;if&rdquo; we took actions we won&rsquo;t take?&nbsp;</li>\n<li>Is there more than one natural way to calculate these counterfactual &ldquo;would&rdquo;s?&nbsp; If so, what are the alternatives, and which alternative works best?</li>\n</ul>\n<p>Today, I&rsquo;ll take an initial swing at these questions.&nbsp; I&rsquo;ll review Judea Pearl&rsquo;s causal Bayes nets; show how Bayes nets offer a general methodology for computing counterfactual &ldquo;would&rdquo;s; and note <em>three</em> plausible alternatives for how to use Pearl&rsquo;s Bayes nets to set up a CSA.&nbsp; One of these alternatives will be the &ldquo;timeless&rdquo; counterfactuals of Eliezer&rsquo;s <a href=\"/lw/15z/ingredients_of_timeless_decision_theory/\">Timeless Decision Theory</a>.<a id=\"more\"></a></p>\n<p><strong>The problem of counterfactuals</strong> is the problem what we do and should mean when we we discuss what &ldquo;would&rdquo; have happened, &ldquo;if&rdquo; something impossible had happened.&nbsp; In its general form, this problem has proved to be quite gnarly.&nbsp; It has been bothering philosophers of science for at least 57 years, since the publication of Nelson Goodman&rsquo;s book &ldquo;Fact, Fiction, and Forecast&rdquo; in 1952:</p>\n<blockquote>\n<p>Let us confine ourselves for the moment to [counterfactual conditionals] in which the antecedent and consequent are inalterably false--as, for example, when I say of a piece of butter that was eaten yesterday, and that had never been heated,</p>\n<p style=\"padding-left: 30px;\"><em>`If that piece of butter had been heated to 150&deg;F, it would have melted.'</em></p>\n<p>Considered as truth-functional compounds, all counterfactuals are of course true, since their antecedents are false.&nbsp; Hence</p>\n<p style=\"padding-left: 30px;\"><em>`If that piece of butter had been heated to 150&deg;F, it would not have melted.'</em></p>\n<p>would also hold.&nbsp; Obviously something different is intended, and the problem is to define the circumstances under which a given counterfactual holds while the opposing conditional with the contradictory consequent fails to hold.</p>\n</blockquote>\n<p>Recall that we seem to need counterfactuals in order to build agents that do useful decision theory -- we need to build agents that can think about the consequences of each of their &ldquo;possible actions&rdquo;, and can choose the action with best expected-consequences.&nbsp; So we need to know how to compute those counterfactuals.&nbsp; As Goodman puts it, &ldquo;[t]he analysis of counterfactual conditionals is no fussy little grammatical exercise.&rdquo;</p>\n<p><strong>Judea Pearl&rsquo;s Bayes nets offer a method for computing counterfactuals.</strong>&nbsp; As noted, it is hard to reduce human counterfactuals in general: it is hard to build an algorithm that explains what (humans will say) really &ldquo;would&rdquo; have happened, &ldquo;if&rdquo; an impossible event had occurred.&nbsp; But it is easier to construct specific formalisms within which counterfactuals have well-specified meanings.&nbsp; Judea Pearl&rsquo;s causal Bayes nets offer perhaps the best such formalism.<br /><br />Pearl&rsquo;s idea is to model the world as based on some set of causal variables, which may be observed or unobserved.&nbsp; In Pearl&rsquo;s model, each variable is determined by a conditional probability distribution on the state of its parents (or by a simple probability distribution, if it has no parents).&nbsp; For example, in the following Bayes net, the beach&rsquo;s probability of being &ldquo;Sunny&rdquo; depends only on the &ldquo;Season&rdquo;, and the probability that there is each particular &ldquo;Number of beach-goers&rdquo; depends only on the &ldquo;Day of the week&rdquo; and on the &ldquo;Sunniness&rdquo;.&nbsp; Since the &ldquo;Season&rdquo; and the &ldquo;Day of the week&rdquo; have no parents, they simply have fixed probability distributions.</p>\n<p><img src=\"http://images.lesswrong.com/t3_17b_0.png\" alt=\"\" width=\"329\" height=\"295\" /></p>\n<p>Once we have a Bayes net set up to model a given domain, computing counterfactuals is easy*.&nbsp; We just:</p>\n<ol>\n<li>Take the usual conditional and unconditional probability distributions, that come with the Bayes net;</li>\n<li>Do &ldquo;surgery&rdquo; on the Bayes net to plug in the variable values that define the counterfactual situation we&rsquo;re concerned with, while ignoring the parents of surgically set nodes, and leaving other probability distributions unchanged;</li>\n<li>Compute the resulting probability distribution over outcomes.</li>\n</ol>\n<p>For example, suppose I want to evaluate the truth of: &ldquo;If last Wednesday had been sunny, there would have been more beach-goers&rdquo;.&nbsp; I leave the &ldquo;Day of the week&rdquo; node at Wednesday&ldquo;, set the &rdquo;Sunny?&ldquo; node to &rdquo;Sunny&ldquo;, ignore the &ldquo;Season&rdquo; node, since it is the parent of a surgically set node, and compute the probability distribution on beach-goers.<br /><br />*Okay, not quite easy: I&rsquo;m sweeping under the carpet the conversion from the English counterfactual to the list of variables to surgically alter, in step 2.&nbsp; Still, Pearl&rsquo;s Bayes nets do much of the work.</p>\n<p><strong>But, even if we decide to use Pearl&rsquo;s method, we are left with the choice of how to represent the agent's \"possible choices\" using a Bayes net.</strong>&nbsp; More specifically, we are left with the choice of what surgeries to execute, when we represent the alternative actions the agent &ldquo;could&rdquo; take.<br /><br />There are at least three plausible alternatives:<br /><br /><em>Alternative One:&nbsp; &ldquo;Actions CSAs&rdquo;:</em><br /><br />Here, we model the outside world however we like, but have the agent&rsquo;s own &ldquo;action&rdquo; -- its choice of a_1, a_2, or ... , a_n -- be the critical &ldquo;choice&rdquo; node in the causal graph.&nbsp; For example, we might show <a href=\"/lw/16i/confusion_about_newcomb_is_confusion_about/\">Newcomb&rsquo;s problem</a> as follows:</p>\n<p><img src=\"http://images.lesswrong.com/t3_17b_1.png?v=65d092540ae410bbefb972bf430c1c21\" alt=\"\" width=\"293\" height=\"280\" /></p>\n<p>The assumption built into this set-up is that the agent&rsquo;s action is uncorrelated with the other nodes in the network.&nbsp; For example, if we want to program an understanding of Newcomb&rsquo;s problem into an Actions CSA, we are forced to choose a probability distribution over Omega&rsquo;s prediction that is independent of the agent&rsquo;s actual choice.<br /><br /><em>How Actions CSAs reckon their coulds and woulds:</em></p>\n<ul>\n<li>Each &ldquo;could&rdquo; is an alternative state of the &ldquo;My action&rdquo; node.&nbsp; Actions CSAs search over each state of the &ldquo;action&rdquo; node before determining their action.</li>\n<li>Each &ldquo;would&rdquo; is then computed in the usual Bayes net fashion: the &ldquo;action&rdquo; node is surgically set, the probability distribution for other nodes is left unchanged, and a probability distribution over outcomes is computed.</li>\n</ul>\n<p><br />So, if causal decision theory is what I think it is, an &ldquo;actions CSA&rdquo; is simply a causal decision theorist.&nbsp; Also, Actions CSAs will two-box on Newcomb&rsquo;s problem, since, in their network, the contents of box B is independent of their choice to take box A.</p>\n<p><em>Alternative Two:&nbsp; &ldquo;Innards CSAs&rdquo;:</em><br /><br />Here, we again model the outside world however we like, but we this time have the agent&rsquo;s own &ldquo;innards&rdquo; -- the physical circuitry that interposes between the agent&rsquo;s sense-inputs and its action-outputs -- be the critical &ldquo;choice&rdquo; node in the causal graph.&nbsp; For example, we might show Newcomb&rsquo;s problem as follows:</p>\n<p><img src=\"http://images.lesswrong.com/t3_17b_2.png?v=0f218c30e6050faa51266e13259707f9\" alt=\"\" width=\"321\" height=\"358\" /></p>\n<p>Here, the agent&rsquo;s innards are allowed to cause both the agent&rsquo;s actions and outside events -- to, for example, we can represent Omega&rsquo;s prediction as correlated with the agent&rsquo;s action.<br /><br /><em>How Innards CSAs reckon their coulds and woulds:</em></p>\n<ul>\n<li>Each &ldquo;could&rdquo; is an alternative state of the &ldquo;My innards&rdquo; node.&nbsp; Innards CSAs search over each state of the &ldquo;innards&rdquo; node before determining their optimal innards, from which their action follows.</li>\n<li>Each &ldquo;would&rdquo; is then computed in the usual Bayes net fashion: the &ldquo;innards&rdquo; node is surgically set, the probability distribution for other nodes is left unchanged, and a probability distribution over outcomes is computed.</li>\n</ul>\n<p>Innards CSAs will one-box on Newcomb&rsquo;s problem, because they reason that if their innards were such as to make them one-box, those same innards would cause Omega, after scanning their brain, to put the $1M in box B.&nbsp; And so they &ldquo;choose&rdquo; innards of a sort that one-boxes on Newcomb&rsquo;s problem, and they one-box accordingly.</p>\n<p><em>Alternative Three:&nbsp; &ldquo;Timeless&rdquo; or &ldquo;Algorithm-Output&rdquo; CSAs:</em><br /><br />In this alternative, as Eliezer suggested in Ingredients of Timeless Decision Theory, we have a &ldquo;Platonic mathematical computation&rdquo; as one of the nodes in our causal graph, which gives rise at once to our agent&rsquo;s decision, to the beliefs of accurate predictors about our agent&rsquo;s decision, and to the decision of similar agents in similar circumstances.&nbsp; It is the output to this <em>mathematical</em> function that our CSA uses as the critical &ldquo;choice&rdquo; node in its causal graph.&nbsp; For example:</p>\n<p><img src=\"http://images.lesswrong.com/t3_17b_3.png?v=9e0c222878fbf1f5b1b5ac26d01dbb62\" alt=\"\" width=\"369\" height=\"395\" /></p>\n<p><em>How Timeless CSAs reckon their coulds and woulds:</em></p>\n<ul>\n<li>Each &ldquo;could&rdquo; is an alternative state of the &ldquo;Output of the Platonic math algorithm that I&rsquo;m an instance of&rdquo; node.&nbsp; Timeless CSAs search over each state of the &ldquo;algorithm-output&rdquo; node before determining the optimal output of this algorithm, from which their action follows.</li>\n<li>Each &ldquo;would&rdquo; is then computed in the usual Bayes net fashion: the &ldquo;algorithm-output&rdquo; node is surgically set, the probability distribution for other nodes is left unchanged, and a probability distribution over outcomes is computed.</li>\n</ul>\n<p>Like innards CSAs, algorithm-output CSAs will one-box on Newcomb&rsquo;s problem, because they reason that if the output of their algorithm was such as to make them one-box, that same algorithm-output would also cause Omega, simulating them, to believe they will one-box and so to put $1M in box B.&nbsp; They therefore &ldquo;choose&rdquo; to have their algorithm output &ldquo;one-box on Newcomb&rsquo;s problem!&rdquo;, and they one-box accordingly.<br /><br />Unlike innards CSAs, algorithm-output CSAs will also Cooperate in single-shot prisoner&rsquo;s dilemmas against Clippy -- in cases where they think it sufficiently likely that Clippy&rsquo;s actions are output by an instantiation of &ldquo;their same algorithm&rdquo; -- even in cases where Clippy cannot at all scan their brain, and where their innards play no physically causal role in Clippy&rsquo;s decision.&nbsp; (An Innards CSA, by contrast, will Cooperate if having Cooperating-type innards will physically cause Clippy to cooperate, and not otherwise.)<br /><br />Coming up: considerations as to the circumstances under which each of the above types of agents will be useful, under different senses of &ldquo;useful&rdquo;.</p>\n<p>\n<hr />\nThanks again to <a href=\"/user/Z_M_Davis/\">Z M Davis</a> for the diagrams.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "miwf7qQTh2HXNnSuq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 43, "extendedScore": null, "score": 0.000121, "legacy": true, "legacyId": "1559", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sLxFqs8fdjsPdkpLC", "gxxpK3eiSQ3XG3DW7", "szfxvS8nsxTgJLBHs", "B7bMmhvaufdtxBtLW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-07T19:02:52.990Z", "modifiedAt": null, "url": null, "title": "Forcing Anthropics: Boltzmann Brains", "slug": "forcing-anthropics-boltzmann-brains", "viewCount": null, "lastCommentedAt": "2020-01-01T11:32:44.848Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LubwxZHKKvCivYGzx/forcing-anthropics-boltzmann-brains", "pageUrlRelative": "/posts/LubwxZHKKvCivYGzx/forcing-anthropics-boltzmann-brains", "linkUrl": "https://www.lesswrong.com/posts/LubwxZHKKvCivYGzx/forcing-anthropics-boltzmann-brains", "postedAtFormatted": "Monday, September 7th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Forcing%20Anthropics%3A%20Boltzmann%20Brains&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AForcing%20Anthropics%3A%20Boltzmann%20Brains%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLubwxZHKKvCivYGzx%2Fforcing-anthropics-boltzmann-brains%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Forcing%20Anthropics%3A%20Boltzmann%20Brains%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLubwxZHKKvCivYGzx%2Fforcing-anthropics-boltzmann-brains", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLubwxZHKKvCivYGzx%2Fforcing-anthropics-boltzmann-brains", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1489, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/175/torture_vs_dust_vs_the_presumptuous_philosopher/\">Anthropic Reasoning in UDT</a> by Wei Dai</p>\n<p>Suppose that I flip a logical coin - e.g. look at some binary digit of pi unknown to either of us - and depending on the result, either create a billion of you in green rooms and one of you in a red room if the coin came up 1; or, if the coin came up 0, create one of you in a green room and a billion of you in red rooms.&nbsp; You go to sleep at the start of the experiment, and wake up in a red room.</p>\n<p>Do you reason that the coin very probably came up 0?&nbsp; Thinking, perhaps:&nbsp; \"If the coin came up 1, there'd be a billion of me in green rooms and only one of me in a red room, and in that case, it'd be very <em>surprising</em> that I found myself in a red room.\"</p>\n<p>What is your degree of subjective credence - your posterior probability - that the logical coin came up 1?</p>\n<p>There are only two answers I can see that might in principle be coherent, and they are \"50%\" and \"a billion to one against\".</p>\n<p>Tomorrow I'll talk about what sort of trouble you run into if you reply \"a billion to one\".</p>\n<p>But for today, suppose you reply \"50%\".&nbsp; Thinking, perhaps:&nbsp; \"I don't understand this whole <em>consciousness</em> rigamarole, I wouldn't try to program a computer to update on it, and I'm not going to update on it myself.\"</p>\n<p>In that case, why don't you believe you're a Boltzmann brain?<a id=\"more\"></a></p>\n<p>Back when the laws of thermodynamics were being worked out, there was first asked the question:&nbsp; \"Why did the universe seem to start from a condition of low entropy?\"&nbsp; Boltzmann suggested that the larger universe <em>was</em> in a state of high entropy, but that, given a <em>long enough </em>time, regions of low entropy would spontaneously occur - wait long enough, and the egg will unscramble itself - and that our own universe was such a region.</p>\n<p>The problem with this explanation is now known as the \"Boltzmann brain\" problem; namely, while Hubble-region-sized low-entropy fluctuations will <em>occasionally</em> occur, it would be far more likely - though still not likely in any absolute sense - for a handful of particles to come together in a configuration performing a computation that lasted just long enough to think a single conscious thought (whatever that means) before dissolving back into chaos.&nbsp; A random reverse-entropy fluctuation is exponentially vastly more likely to take place in a small region than a large one.</p>\n<p>So on Boltzmann's attempt to explain the low-entropy initial condition of the universe as a random statistical fluctuation, it's far more likely that we are a little blob of chaos temporarily hallucinating the <em>rest</em> of the universe, than that a multi-billion-light-year region spontaneously ordered itself.&nbsp; And most such little blobs of chaos will dissolve in the next moment.</p>\n<p>\"Well,\" you say, \"that may be an <em>unpleasant </em>prediction, but that's no license to <em>reject </em>it.\"&nbsp; But wait, it gets worse:&nbsp; The vast majority of Boltzmann brains have experiences <em>much less ordered</em> than what you're seeing right now.&nbsp; Even if a blob of chaos coughs up a visual cortex (or equivalent), that visual cortex is unlikely to see a highly ordered visual field - the vast majority of possible visual fields more closely resemble \"static on a television screen\" than \"words on a computer screen\".&nbsp; So on the Boltzmann hypothesis, <em>highly ordered </em>experiences like the ones we are having now, constitute an exponentially infinitesimal fraction of all experiences.</p>\n<p>In contrast, suppose one more simple law of physics not presently understood, which forces the initial condition of the universe to be low-entropy.&nbsp; Then the exponentially vast majority of brains occur as the result of ordered processes in ordered regions, and it's not at all surprising that we find ourselves having ordered experiences.</p>\n<p>But wait!&nbsp; This is <em>just the same sort of logic </em>(is it?) that one would use to say, \"Well, if the logical coin came up heads, then it's very surprising to find myself in a red room, since the vast majority of people-like-me are in green rooms; but if the logical coin came up tails, then most of me are in red rooms, and it's not surprising that I'm in a red room.\"</p>\n<p>If you reject that reasoning, saying, \"There's only <em>one </em>me, and that person seeing a red room does exist, even if the logical coin came up heads\" then you should have no trouble saying, \"There's only one me, having a highly ordered experience, and that person exists even if all experiences are generated at random by a Boltzmann-brain process or something similar to it.\"&nbsp; And furthermore, the Boltzmann-brain process is a much <em>simpler</em> process - it could occur with only the barest sort of causal structure, no need to postulate the full complexity of our own hallucinated universe.&nbsp; So if you're not updating on the apparent conditional <em>rarity</em> of having a highly ordered experience of gravity, then you should just believe the very simple hypothesis of a high-volume random experience generator, which would necessarily create your current experiences - albeit with extreme relative infrequency, but you don't care about that.</p>\n<p>Now, doesn't the Boltzmann-brain hypothesis also predict that reality will dissolve into chaos in the next moment?&nbsp; Well, it predicts that the <em>vast majority</em> of blobs who experience this moment, cease to exist after; and that among the few who <em>don't</em> dissolve, the vast majority of <em>those</em> experience chaotic successors.&nbsp; But there would be an infinitesimal fraction of a fraction of successors, who experience ordered successor-states as well.&nbsp; And you're not alarmed by the rarity of those successors, just as you're not alarmed by the rarity of waking up in a red room if the logical coin came up 1 - right?</p>\n<p>So even though your friend is standing right next to you, saying, \"I predict the sky will <em>not</em> turn into green pumpkins and explode - oh, look, I was successful again!\", you are not disturbed by their unbroken string of successes.&nbsp; You just keep on saying, \"Well, it was necessarily true that <em>someone</em> would have an ordered successor experience, on the Boltzmann-brain hypothesis, and that just <em>happens</em> to be us, but in the <em>next</em> instant I will sprout wings and fly away.\"</p>\n<p>Now this is not quite a <em>logical contradiction</em>.&nbsp; But the total rejection of all science, induction, and inference in favor of an unrelinquishable faith that the next moment will dissolve into pure chaos, is sufficiently unpalatable that even I decline to bite that bullet.</p>\n<p>And so I still can't seem to dispense with anthropic reasoning - I can't seem to dispense with trying to think about <em>how many</em> of me or <em>how much</em> of me there are, which in turn requires that I think about what sort of process constitutes a <em>me</em>.&nbsp; Even though I confess myself to be sorely confused, about what could possibly make a certain computation \"real\" or \"not real\", or how some universes and experiences could be quantitatively realer than others (possess more reality-fluid, as 'twere), and I still don't know what exactly makes a causal process count as something I might have been for purposes of being surprised to find myself as me, or for that matter, what exactly is a causal process.</p>\n<p>Indeed this is all greatly and terribly confusing unto me, and I would be less confused if I could go through life while only answering questions like \"Given the Peano axioms, what is SS0 + SS0?\"</p>\n<p>But then I have no defense against the one who says to me, \"Why don't you think you're a Boltzmann brain?&nbsp; Why don't you think you're the result of an all-possible-experiences generator?&nbsp; Why don't you think that gravity is a matter of branching worlds in which all objects accelerate in all directions and in some worlds all the observed objects happen to be accelerating downward?&nbsp; It <em>explains</em> all your observations, in the sense of logically necessitating them.\"</p>\n<p>I want to reply, \"But then <em>most people</em> don't have experiences <em>this ordered,</em> so <em>finding myself </em>with an ordered experience is, on your hypothesis, very <em>surprising.</em>&nbsp; Even if there are <em>some versions</em> of me that <em>exist</em> in regions or universes where they arose by chaotic chance, I <em>anticipate</em>, for purposes of predicting <em>my future experiences</em>, that <em>most of my existence</em> is <em>encoded </em>in regions and universes where I am the product of ordered processes.\"</p>\n<p>And I currently know of no way to reply thusly, that does not make use of poorly defined concepts like \"number of real processes\" or \"amount of real processes\"; and \"people\", and \"me\", and \"anticipate\" and \"future experience\".</p>\n<p>Of course confusion exists in the mind, not in reality, and it would not be the least bit surprising if a <em>resolution</em> of this problem were to dispense with such notions as \"real\" and \"people\" and \"my future\".&nbsp; But I do not presently have that resolution.</p>\n<p>(Tomorrow I will argue that anthropic updates must be illegal and that the correct answer to the original problem must be \"50%\".)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LubwxZHKKvCivYGzx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 26, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "1561", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RcvyJjPQwimAeapNg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-07T20:15:56.595Z", "modifiedAt": null, "url": null, "title": "Why I'm Staying On Bloggingheads.tv", "slug": "why-i-m-staying-on-bloggingheads-tv", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:38.633Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wCecHd3z8EEjNYWYg/why-i-m-staying-on-bloggingheads-tv", "pageUrlRelative": "/posts/wCecHd3z8EEjNYWYg/why-i-m-staying-on-bloggingheads-tv", "linkUrl": "https://www.lesswrong.com/posts/wCecHd3z8EEjNYWYg/why-i-m-staying-on-bloggingheads-tv", "postedAtFormatted": "Monday, September 7th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20I'm%20Staying%20On%20Bloggingheads.tv&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20I'm%20Staying%20On%20Bloggingheads.tv%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwCecHd3z8EEjNYWYg%2Fwhy-i-m-staying-on-bloggingheads-tv%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20I'm%20Staying%20On%20Bloggingheads.tv%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwCecHd3z8EEjNYWYg%2Fwhy-i-m-staying-on-bloggingheads-tv", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwCecHd3z8EEjNYWYg%2Fwhy-i-m-staying-on-bloggingheads-tv", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 570, "htmlBody": "<p>Recently, <a href=\"http://blogs.discovermagazine.com/cosmicvariance/2009/08/31/bye-to-bloggingheads/\">Sean Carroll</a>, <a href=\"http://blogs.discovermagazine.com/loom/2009/09/01/bloggingheads-and-the-old-challenges-of-new-tools\">Carl Zimmer</a>, and <a href=\"http://blogs.discovermagazine.com/badastronomy/2009/09/04/bloggingheads-capo-non-grata/\">Phil Plait</a> have all decided to stop appearing on BloggingHeads.TV (BHTV), and <a href=\"http://scienceblogs.com/pharyngula/2009/09/phil_plait_ditches_blogginghea.php\">PZ Myers</a> announced he would not appear on it in the future, after a disastrous decision to have creationist Michael Behe interviewed by the linguist and non-biologist John McWhorter, who failed to call Behe on his standard BS.</p>\n<p>I'm hereby publicly announcing that I intend to <em>stay</em> on BloggingHeads.TV.</p>\n<p>Why?&nbsp; Two main reasons:</p>\n<p>1)&nbsp; Robert Wright publicly said that this was foolish, apologized for the poor editorial oversight that led to it, and says they're going to try never to do this again.&nbsp; This looks sincere to me, and given that it's sincere, people really ought to be allowed more chance than this to recover from their mistakes.</p>\n<p>2)&nbsp; Bloggingheads.TV has given me a forum to debate <em>accomodationist atheists who are insufficiently condemning of religion</em> - for example <a href=\"/lw/4i/bhtv_yudkowsky_adam_frank_on_religious_experience/\">my diavlog with Adam Frank</a>, author of \"The Constant Fire\".&nbsp; Adam Frank argues that, while of course we now know that God doesn't exist, nonetheless scientific wonder at the universe and its mysteries has a lot in common with the roots of religion.&nbsp; And I said this was wishful thinking, historically ignorant of how religions really arose and propagated themselves, and a continuation of such <a href=\"http://wiki.lesswrong.com/wiki/Anti-epistemology\">theistic bad habits</a> as thinking that things of which we are <a href=\"/lw/iu/mysterious_answers_to_mysterious_questions/\">temporarily ignorant</a> are \"<a href=\"/lw/57/the_sacred_mundane/\">sacred mysteries</a>\".&nbsp; And no one at BHTV complained that I was being too confrontational, or too anti-religious, or that it was unfair to have the diavlog be between two atheists.<a id=\"more\"></a></p>\n<p>If BHTV is willing to let me come on and (politely) kick hell out of atheists who aren't atheistic enough to suit me, then I don't believe that their unfortunate failure to have Behe interviewed by someone who could call his BS, represents any deep hidden agenda in favor of religion and against science.</p>\n<p>Rather, I think it represents a commitment to having interesting discussions by people who intelligently disagree with each other and have something courteous to say about it<em> - even if </em>that discussion wanders into the fearsome death zones where science does (\"does not!\") clash with religion - and this commitment managed to go wrong on one or two occasions.</p>\n<p>My friends and fellow <a href=\"/tag/antitheism\">antitheists</a>, this is an <em>important </em>commitment while most of the world is continuing to <a href=\"/lw/i8/religions_claim_to_be_nondisprovable/\">pretend that there is no conflict between science and religion</a>.&nbsp; It's not surprising if that commitment goes wrong now and then.&nbsp; It is not reasonable to expect that a commitment to repeatedly discuss a scary controversy will never go wrong.&nbsp; It may well go wrong <em>again</em> despite Robert Wright's best intentions.&nbsp; But unless it starts to go wrong <em>systematically</em>, I'm going to stay on BHTV, arguing that science and religion are not compatible.</p>\n<p>Of course, if most other non-accomodationists jump ship from BHTV as a result of the Behe affair, then it will become a hangout for accomodationists only.&nbsp; \"<a href=\"/lw/lr/evaporative_cooling_of_group_beliefs/\">Evaporative Cooling of Group Beliefs</a>\" is another reason why you should put forth at least a little effort to \"<a href=\"/lw/42/tolerate_tolerance/\">Tolerate Tolerance</a>\" - to <em>not </em>insist that all your potential trade-partners punish the same people you've labeled defectors, exactly the way you want them punished, before you cooperate.&nbsp; Yes, Behe is an enemy of science, but Wright is not; and Wright may also dislike Behe, yet not wish to implement <em>exactly </em>the same punishment-policy toward Behe that you advocate; and that needs to be all right, if we're all going to end up cooperating.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NSMKfa8emSbGNXRKD": 1, "MXcpQvaPGtXpB6vkM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wCecHd3z8EEjNYWYg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 30, "extendedScore": null, "score": 5.3e-05, "legacy": true, "legacyId": "1562", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 101, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xDroHJ3AzWwJ45ufJ", "6i3zToomS86oj9bS6", "Fwt4sDDacko8Sh5iR", "fAuWLS7RKWD2npBFR", "ZQG9cwKbct2LtmL3p", "JKxxFseBWz8SHkTgt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-08T09:52:05.118Z", "modifiedAt": null, "url": null, "title": "An idea: Sticking Point Learning", "slug": "an-idea-sticking-point-learning", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:26.432Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9RG2ubLz8hWvDMWFE/an-idea-sticking-point-learning", "pageUrlRelative": "/posts/9RG2ubLz8hWvDMWFE/an-idea-sticking-point-learning", "linkUrl": "https://www.lesswrong.com/posts/9RG2ubLz8hWvDMWFE/an-idea-sticking-point-learning", "postedAtFormatted": "Tuesday, September 8th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20idea%3A%20Sticking%20Point%20Learning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20idea%3A%20Sticking%20Point%20Learning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9RG2ubLz8hWvDMWFE%2Fan-idea-sticking-point-learning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20idea%3A%20Sticking%20Point%20Learning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9RG2ubLz8hWvDMWFE%2Fan-idea-sticking-point-learning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9RG2ubLz8hWvDMWFE%2Fan-idea-sticking-point-learning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 249, "htmlBody": "<p>When trying to learn technical topics from online expositions, I imagine that most people hit snags&nbsp;at some moment - passages that they can't seem to grasp right away and that&nbsp;impede further progress. Moreover, I imagine that different people often get stuck in the same places, and that a few fortunate words of explanation can often help overcome the hump. (For example, \"integral is the area under the curve\" or \"entropy is the expected number of bits\".) And finally, perhaps unintuitively, I also imagine that someone who just overcame a sticking point is more likely to say the right magic words about it than someone who has understood the topic for years.</p>\n<p>Hence my suggestion: let's try to identify and resolve such sticking points together, maybe as part of our&nbsp;<a href=\"/lw/l7/the_simple_math_of_everything/\">Simple</a> <a href=\"/lw/13f/creating_the_simple_math_of_everything/\">Math</a> of <a href=\"http://wiki.lesswrong.com/wiki/Simple_math_of_everything\">Everything</a>. This&nbsp;idea might be more appropriate for <a href=\"http://news.ycombinator.com/\">Hacker News</a>, but I'm submitting it here because it sounds like a not-for-profit rather than a business, and seems nicely aligned with the goals of our community.</p>\n<p>The required software certainly exists: our wiki would do fine. One of us posts a copy of a technical text. Others try to parse it, hit the difficult points, resolve them by intellectual force and insert (as a mid-article comment) the magic words or hyperlinks that helped them in that particular case. I really wonder what the result would look like; hopefully, something comfortably readable by people with modest math-reading skillz.</p>\n<p>Any number of technical topics suggest themselves immediately - now what would you like to see?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9RG2ubLz8hWvDMWFE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 12, "extendedScore": null, "score": 5.207865813969311e-07, "legacy": true, "legacyId": "1566", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HnPEpu5eQWkbyAJCT", "rXbgabdnmxd3jdY7w"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-08T18:18:43.157Z", "modifiedAt": null, "url": null, "title": "FHI postdoc at Oxford", "slug": "fhi-postdoc-at-oxford", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:26.195Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FSqfpaBnz2RCJR5RE/fhi-postdoc-at-oxford", "pageUrlRelative": "/posts/FSqfpaBnz2RCJR5RE/fhi-postdoc-at-oxford", "linkUrl": "https://www.lesswrong.com/posts/FSqfpaBnz2RCJR5RE/fhi-postdoc-at-oxford", "postedAtFormatted": "Tuesday, September 8th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20FHI%20postdoc%20at%20Oxford&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFHI%20postdoc%20at%20Oxford%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFSqfpaBnz2RCJR5RE%2Ffhi-postdoc-at-oxford%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=FHI%20postdoc%20at%20Oxford%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFSqfpaBnz2RCJR5RE%2Ffhi-postdoc-at-oxford", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFSqfpaBnz2RCJR5RE%2Ffhi-postdoc-at-oxford", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 52, "htmlBody": "<p><a href=\"http://www.fhi.ox.ac.uk/get_involved/jmrf_vacancy/James_Martin_Research_Fellow\">The Future of Humanity Institute at the University of Oxford is looking to fill a postdoctoral research fellowship</a> in interdisciplinary science or philosophy.&nbsp; Current work areas include global catastrophic risks, probabilistic methodology and applied epistemology and rationality, impacts of future technologies, and ethical issues related to human enhancement.&nbsp; Apply by Oct 14.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FSqfpaBnz2RCJR5RE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 5.208705021656776e-07, "legacy": true, "legacyId": "1567", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-08T18:31:49.270Z", "modifiedAt": null, "url": null, "title": "Outlawing Anthropics: An Updateless Dilemma", "slug": "outlawing-anthropics-an-updateless-dilemma", "viewCount": null, "lastCommentedAt": "2021-09-22T00:41:37.388Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZTEkZNLrmycNuCNYq/outlawing-anthropics-an-updateless-dilemma", "pageUrlRelative": "/posts/ZTEkZNLrmycNuCNYq/outlawing-anthropics-an-updateless-dilemma", "linkUrl": "https://www.lesswrong.com/posts/ZTEkZNLrmycNuCNYq/outlawing-anthropics-an-updateless-dilemma", "postedAtFormatted": "Tuesday, September 8th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Outlawing%20Anthropics%3A%20An%20Updateless%20Dilemma&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOutlawing%20Anthropics%3A%20An%20Updateless%20Dilemma%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZTEkZNLrmycNuCNYq%2Foutlawing-anthropics-an-updateless-dilemma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Outlawing%20Anthropics%3A%20An%20Updateless%20Dilemma%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZTEkZNLrmycNuCNYq%2Foutlawing-anthropics-an-updateless-dilemma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZTEkZNLrmycNuCNYq%2Foutlawing-anthropics-an-updateless-dilemma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1090, "htmlBody": "<p>Let us start with a (non-quantum) logical coinflip - say, look at the heretofore-unknown-to-us-personally 256th binary digit of pi, where the choice of binary digit is itself intended not to be random.</p>\n<p>If the result of this logical coinflip is 1 (aka \"heads\"), we'll create 18 of you in green rooms and 2 of you in red rooms, and if the result is \"tails\" (0), we'll create 2 of you in green rooms and 18 of you in red rooms.</p>\n<p>After going to sleep at the start of the experiment, you wake up in a green room.</p>\n<p>With what degree of credence do you believe - what is your posterior probability - that the logical coin came up \"heads\"?</p>\n<p>There are exactly two tenable answers that I can see, \"50%\" and \"90%\".</p>\n<p>Suppose you reply 90%.</p>\n<p>And suppose you also happen to be \"altruistic\" enough to care about what happens to all the copies of yourself.&nbsp; (If your current system cares about yourself and your future, but doesn't care about very similar xerox-siblings, then you will tend to self-modify to have <em>future</em> copies of yourself care about each other, as this maximizes your <em>expectation </em>of pleasant experience over <em>future</em> selves.)</p>\n<p>Then I attempt to force a reflective inconsistency in your decision system, as follows:</p>\n<p>I inform you that, after I look at the unknown binary digit of pi, I will ask all the copies of you in green rooms whether to pay $1 to every version of you in a green room and steal $3 from every version of you in a red room.&nbsp; If they all reply \"Yes\", I will do so.<a id=\"more\"></a></p>\n<p>(It will be understood, of course, that $1 represents 1 utilon, with actual monetary amounts rescaled as necessary to make this happen.&nbsp; Very little rescaling should be necessary.)</p>\n<p>(<a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">Timeless decision agents</a> reply as if controlling all similar decision processes, including all copies of themselves.&nbsp; Classical causal decision agents, to reply \"Yes\" as a group, will need to somehow work out that other copies of themselves reply \"Yes\", and then reply \"Yes\" themselves.&nbsp; We can try to help out the causal decision agents on their coordination problem by supplying rules such as \"If conflicting answers are delivered, everyone loses $50\".&nbsp; If causal decision agents can win on the problem \"If everyone says 'Yes' you all get $10, if everyone says 'No' you all lose $5, if there are conflicting answers you all lose $50\" then they can presumably handle this.&nbsp; If not, then ultimately, I decline to be responsible for <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">the stupidity of causal decision agents</a>.)</p>\n<p>Suppose that you wake up in a green room.&nbsp; You reason, \"With 90% probability, there are 18 of me in green rooms and 2 of me in red rooms; with 10% probability, there are 2 of me in green rooms and 18 of me in red rooms.&nbsp; Since I'm altruistic enough to at least care about my xerox-siblings, I calculate the expected utility of replying 'Yes' as (90% * ((18 * +$1) + (2 * -$3))) + (10% * ((18 * -$3) + (2 * +$1))) = +$5.60.\"&nbsp; You reply yes.</p>\n<p>However, before the experiment, you calculate the general utility of the conditional strategy \"Reply 'Yes' to the question if you wake up in a green room\" as (50% * ((18 * +$1) + (2 * -$3))) + (50% * ((18 * -$3) + (2 * +$1))) = -$20.&nbsp; You want your future selves to reply 'No' under these conditions.</p>\n<p>This is a dynamic inconsistency - different answers at different times - which argues that decision systems which update on anthropic evidence will self-modify <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">not to update probabilities</a> on anthropic evidence.</p>\n<p>I originally thought, on first formulating this problem, that it had to do with <em>double-counting </em>the <em>utilons </em>gained by your variable numbers of green friends, and the <em>probability </em>of being one of your green friends.</p>\n<p>However, the problem also works if we care about paperclips.&nbsp; No selfishness, no altruism, just paperclips.</p>\n<p>Let the dilemma be, \"I will ask all people who wake up in green rooms if they are willing to take the bet 'Create 1 paperclip if the logical coinflip came up heads, destroy 3 paperclips if the logical coinflip came up tails'.&nbsp; (Should they disagree on their answers, I will destroy 5 paperclips.)\"&nbsp; Then a paperclip maximizer, before the experiment, wants the paperclip maximizers who wake up in green rooms to refuse the bet.&nbsp; But a conscious paperclip maximizer who updates on anthropic evidence, who wakes up in a green room, will want to take the bet, with expected utility ((90% * +1 paperclip) + (10% * -3 paperclips)) = +0.6 paperclips.</p>\n<p>This argues that, in general, decision systems - whether they start out selfish, or start out caring about paperclips - will not want their future versions to update on anthropic \"evidence\".</p>\n<p>Well, that's not too disturbing, is it?&nbsp; I mean, the whole anthropic thing seemed very confused to begin with - full of notions about \"consciousness\" and \"reality\" and \"identity\" and \"reference classes\" and other poorly defined terms.&nbsp; Just throw out anthropic reasoning, and you won't have to bother.</p>\n<p>When I explained this problem to Marcello, he said, \"Well, <a href=\"/lw/x7/cant_unbirth_a_child/\">we don't want to build conscious AIs</a>, so of course we don't want them to use anthropic reasoning\", which is a fascinating sort of reply.&nbsp; And I responded, \"But when you have a problem this confusing, and you find yourself wanting to build an AI that just doesn't use anthropic reasoning to begin with, maybe that implies that the correct resolution involves <em>us</em> not using anthropic reasoning either.\"</p>\n<p>So we can just throw out anthropic reasoning, and relax, and <a href=\"/lw/17d/forcing_anthropics_boltzmann_brains/\">conclude that we are Boltzmann brains</a>.&nbsp; QED.</p>\n<p>In general, I find the sort of argument given here - that a certain type of decision system is not reflectively consistent - to be pretty damned compelling.&nbsp; But I also find the Boltzmann conclusion to be, ahem, more than ordinarily unpalatable.</p>\n<p>In personal conversation, Nick Bostrom suggested that a division-of-responsibility principle might cancel out the anthropic update - i.e., the paperclip maximizer would have to reason, \"If the logical coin came up heads then I am 1/18th responsible for adding +1 paperclip, if the logical coin came up tails then I am 1/2 responsible for destroying 3 paperclips.\"&nbsp; I confess that my initial reaction to this suggestion was \"Ewwww\", but I'm not exactly comfortable concluding I'm a Boltzmann brain, either.</p>\n<p>EDIT:&nbsp; On further reflection, I also wouldn't want to build an AI that concluded it was a Boltzmann brain!&nbsp; Is there a form of inference which rejects this conclusion without relying on any reasoning about subjectivity?</p>\n<p>EDIT2:&nbsp; Psy-Kosh has <a href=\"/lw/17c/outlawing_anthropics_an_updateless_dilemma/13e1\">converted this into a non-anthropic problem</a>!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZTEkZNLrmycNuCNYq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 33, "extendedScore": null, "score": 5.2e-05, "legacy": true, "legacyId": "1560", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 209, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gb6zWstjmkYHLrbrg", "LubwxZHKKvCivYGzx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-09T18:15:40.967Z", "modifiedAt": null, "url": null, "title": "Let Them Debate College Students", "slug": "let-them-debate-college-students", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:08.739Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yN38rRLzyuvNnhqr3/let-them-debate-college-students", "pageUrlRelative": "/posts/yN38rRLzyuvNnhqr3/let-them-debate-college-students", "linkUrl": "https://www.lesswrong.com/posts/yN38rRLzyuvNnhqr3/let-them-debate-college-students", "postedAtFormatted": "Wednesday, September 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Let%20Them%20Debate%20College%20Students&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALet%20Them%20Debate%20College%20Students%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyN38rRLzyuvNnhqr3%2Flet-them-debate-college-students%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Let%20Them%20Debate%20College%20Students%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyN38rRLzyuvNnhqr3%2Flet-them-debate-college-students", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyN38rRLzyuvNnhqr3%2Flet-them-debate-college-students", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 745, "htmlBody": "<p>(EDIT:&nbsp; Woozle has an <a href=\"/lw/17f/let_them_debate_college_students/13ot\">even better idea</a>, which would apply to many debates in general if the true goal were seeking resolution and truth.)</p>\n<p>Friends, Romans, non-Romans, lend me your ears.&nbsp; I have for you a modest proposal, in this question of whether we should publicly debate creationists, or freeze them out as unworthy of debate.</p>\n<p>My fellow humans, I have two misgivings about this notion that there should not be a debate.&nbsp; My first misgiving is that - even though on <em>this particular</em> occasion scientific society is absolutely positively not wrong to dismiss creationism - this business of <em>not having debates</em> sounds like dangerous business to me.&nbsp; Science <em>is </em>sometimes wrong, you know, even if it is not wrong <em>this</em> time, and debating is part of the recovery process.</p>\n<p>And my second misgiving is that, like it or not, the creationists <em>are</em> on the radio, in the town halls, and of course on the Web, and they are <em>already </em>talking to large audiences; and the idea that there is not going to be a debate about this, may be slightly naive.</p>\n<p>\"But,\" you cry, \"when prestigious scientists lower themselves so far as to debate creationists, afterward the creationists smugly advertise that prestigious scientists are debating them!\"</p>\n<p>Ah, but who says that <em>prestigious scientists</em> are required to debate creationists?<a id=\"more\"></a></p>\n<p>Find some bright ambitious young college student working toward a biology degree, someone who's read <em>Pharyngula</em> and the talk.origins FAQ.&nbsp; Maybe have P. Z. Myers or someone run a test debate on them, to make sure they know how to answer all the standard lies and are generally good at debating and explaining.&nbsp; Then have the college student debate the creationists - if the creationists are still up for it.&nbsp; If not, of course, we can all make a big ruckus about how Michael Behe is afraid to debate a mere college student, and have the college student reply to all requests to debate Richard Dawkins or supply a scientific authority for the TV networks.&nbsp; And if Michael Behe manages to defeat the college student, <em>then</em> he can go on to debate a PhD, and if that doesn't work, Behe gets to talk to P. Z. Myers, and in the unlikely event Behe manages not to get his butt handed to him by P. Z. Myers, he would have earned the right to debate Richard Dawkins.</p>\n<p>If we're dealing with young-earth creationists, then we add a bright 12-year-old at the start of the chain.</p>\n<p>That way, anyone who wants to know the state of the debate and the status of the arguments, is welcome to watch creationists being beaten up by some college kid - armed with real science, mind!</p>\n<p>But there will still be a debate.&nbsp; And if the scientific community, at some point in the future, manages to go astray on some issue where the opposing side seems \"<a href=\"http://www.overcomingbias.com/2008/04/arbitrary-silli.html\">silly</a>\", then we can hope - if public debate is any use at all - that the challenger will gently defeat the 12-year-old, unravel the college student, score points against the PhD, and hold their own against senior scientists.&nbsp; There would still be a path to victory for <em>worthy</em> new ideas, and not a general license for a community to shut down all debate it thinks unworthy.</p>\n<p>It's this notion of <em>shutting down debate </em>that I fear as dangerous; and it seems to me that you can get just the same strategic conservation of prestige, by endorsing the principle of debate, but sending out some bright college students to present the standard position.&nbsp; If the \"controversy\" as shown on CNN consists of some ID-er with a sober-looking business suit and an impressive-sounding title, versus a TA in jeans to represent the scientific community - but with accurate science, mind! - then I think this would viscerally answer what the scientific community thinks of creationism, and <em>not</em> create the false impression of an ongoing debate, while still giving airtime to the standard scientific replies.&nbsp; If CNN isn't interested in showing <em>that</em> \"controversy\" - well then, that tells us what CNN really wanted, doesn't it.</p>\n<p>If an idea is so completely ridiculous as to be unworthy even of debate - then send out some bright un-titled college students to debate it!&nbsp; Do vet them for knowledge of standard replies, explanation ability, and debating ability against evil opponents, to make sure standard science is not needlessly embarrassed.&nbsp; But there should be plenty of ambitious young bright college students who can pass that filter and who would enjoy some TV exposure.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NSMKfa8emSbGNXRKD": 1, "wzgcQCrwKfETcBpR9": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yN38rRLzyuvNnhqr3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 56, "baseScore": 50, "extendedScore": null, "score": 0.00010820870984540109, "legacy": true, "legacyId": "1563", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 46, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 142, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-10T03:06:21.614Z", "modifiedAt": null, "url": null, "title": "Pittsburgh Meetup: Saturday 9/12, 6:30PM, CMU", "slug": "pittsburgh-meetup-saturday-9-12-6-30pm-cmu", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:26.921Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nick_Tarleton", "createdAt": "2009-03-05T18:07:15.687Z", "isAdmin": false, "displayName": "Nick_Tarleton"}, "userId": "nwrGYcfC4sPPn73Aw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dD4Ls86msCjZCEMda/pittsburgh-meetup-saturday-9-12-6-30pm-cmu", "pageUrlRelative": "/posts/dD4Ls86msCjZCEMda/pittsburgh-meetup-saturday-9-12-6-30pm-cmu", "linkUrl": "https://www.lesswrong.com/posts/dD4Ls86msCjZCEMda/pittsburgh-meetup-saturday-9-12-6-30pm-cmu", "postedAtFormatted": "Thursday, September 10th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pittsburgh%20Meetup%3A%20Saturday%209%2F12%2C%206%3A30PM%2C%20CMU&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APittsburgh%20Meetup%3A%20Saturday%209%2F12%2C%206%3A30PM%2C%20CMU%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdD4Ls86msCjZCEMda%2Fpittsburgh-meetup-saturday-9-12-6-30pm-cmu%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pittsburgh%20Meetup%3A%20Saturday%209%2F12%2C%206%3A30PM%2C%20CMU%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdD4Ls86msCjZCEMda%2Fpittsburgh-meetup-saturday-9-12-6-30pm-cmu", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdD4Ls86msCjZCEMda%2Fpittsburgh-meetup-saturday-9-12-6-30pm-cmu", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<p>The <a href=\"/lw/16n/pittsburgh_meetup_survey_of_interest/\">aforementioned</a>&nbsp;Pittsburgh, PA meetup will happen this Saturday at 6:30.&nbsp;Location is Baker Hall 231A <em>(subject to change)</em>&nbsp;at Carnegie Mellon. (<a href=\"http://www.cmu.edu/about/visit/getting-here.shtml\">Directions</a>, <a href=\"http://www.cmu.edu/about/visit/campus-map.shtml\">campus map</a>; 231A is on the second floor of Baker, near the end nearer the quad.) Please RSVP if you're coming.</p>\n<p>If necessary, I can be reached at <a href=\"mailto:nickptar@gmail.com\">nickptar@gmail.com</a> or (919) 302-1147.</p>\n<p>Hope to see you there!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dD4Ls86msCjZCEMda", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 5.211966566608673e-07, "legacy": true, "legacyId": "1573", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AYKL87S4QRDrsrsvY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-10T18:45:24.123Z", "modifiedAt": null, "url": null, "title": "The Lifespan Dilemma", "slug": "the-lifespan-dilemma", "viewCount": null, "lastCommentedAt": "2020-10-04T05:02:40.315Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9RCoE7jmmvGd5Zsh2/the-lifespan-dilemma", "pageUrlRelative": "/posts/9RCoE7jmmvGd5Zsh2/the-lifespan-dilemma", "linkUrl": "https://www.lesswrong.com/posts/9RCoE7jmmvGd5Zsh2/the-lifespan-dilemma", "postedAtFormatted": "Thursday, September 10th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Lifespan%20Dilemma&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Lifespan%20Dilemma%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9RCoE7jmmvGd5Zsh2%2Fthe-lifespan-dilemma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Lifespan%20Dilemma%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9RCoE7jmmvGd5Zsh2%2Fthe-lifespan-dilemma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9RCoE7jmmvGd5Zsh2%2Fthe-lifespan-dilemma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2082, "htmlBody": "<p>One of our most controversial posts ever was \"<a href=\"/lw/kn/torture_vs_dust_specks/\">Torture vs. Dust Specks</a>\".&nbsp; Though I can't seem to find the reference, one of the more interesting uses of this dilemma was by a professor whose student said \"I'm a utilitarian consequentialist\", and the professor said \"No you're not\" and told them about SPECKS vs. TORTURE, and then the student - to the professor's surprise - chose TORTURE.&nbsp; (Yay student!)</p>\n<p>In the spirit of always making these things worse, let me offer a dilemma that might have been more likely to unconvince the student - at least, as a consequentialist, I find the inevitable conclusion much harder to swallow.<a id=\"more\"></a></p>\n<p>I'll start by briefly introducing Parfit's Repugnant Conclusion, sort of a little brother to the main dilemma.&nbsp; Parfit starts with a world full of a million happy people - people with plenty of resources apiece.&nbsp; Next, Parfit says, let's introduce one more person who leads a life barely worth living - but since their life <em>is </em>worth living, adding this person must be a good thing.&nbsp; Now we redistribute the world's resources, making it fairer, which is also a good thing.&nbsp; Then we introduce another person, and another, until finally we've gone to a billion people whose lives are barely at subsistence level.&nbsp; And since (Parfit says) it's obviously better to have a million happy people than a billion people at subsistence level, we've gone in a circle and revealed inconsistent preferences.</p>\n<p>My own analysis of the Repugnant Conclusion is that its apparent force comes from equivocating between senses of <em>barely worth living.</em>&nbsp; In order to <em>voluntarily create </em>a new person, what we need is a life that is <em>worth celebrating</em> or <em>worth birthing,</em> one that contains more good than ill and more happiness than sorrow - otherwise we should reject the step where we choose to birth that person.&nbsp; Once someone is alive, on the other hand, we're obliged to take care of them in a way that <a href=\"/lw/ws/for_the_people_who_are_still_alive/\">we wouldn't be obliged to create them in the first place</a> - and they may choose not to commit suicide, even if their life contains more sorrow than happiness.&nbsp; If we would be saddened to <em>hear the news </em>that such a person existed, we shouldn't <em>kill</em> them, but we should <em>not</em> voluntarily create such a person in an otherwise happy world.&nbsp; So each time we <em>voluntarily</em> add another person to Parfit's world, we have a little celebration and say with honest joy \"Whoopee!\", not, \"Damn, now it's <a href=\"/lw/x7/cant_unbirth_a_child/\">too late to uncreate them</a>.\"</p>\n<p>And then the rest of the Repugnant Conclusion - that it's better to have a billion lives slightly worth celebrating, than a million lives very worth celebrating - is just \"repugnant\" because of standard <a href=\"/lw/hw/scope_insensitivity/\">scope insensitivity</a>.&nbsp; The brain fails to multiply a billion small birth celebrations to end up with a larger total celebration of life than a million big celebrations.&nbsp; Alternatively, average utilitarians - <a href=\"/lw/ws/for_the_people_who_are_still_alive/\">I suspect I am one</a> - may just reject the very first step, in which the average quality of life goes down.</p>\n<p>But now we introduce the Repugnant Conclusion's big sister, the Lifespan Dilemma, which - at least in my own opinion - seems much worse.</p>\n<p>To start with, suppose you have a 20% chance of dying in an hour, and an 80% chance of living for 10<sup>10,000,000,000</sup> years -</p>\n<p>Now I know what you're thinking, of course.&nbsp; You're thinking, \"Well, 10^(10^10) years may <em>sound</em> like a long time, unimaginably vaster than the 10^15 years the universe has lasted so far, but it isn't much, really.&nbsp; I mean, most finite numbers are very much larger than that.&nbsp; The realms of math are infinite, the realms of novelty and knowledge are infinite, and <a href=\"http://wiki.lesswrong.com/wiki/Fun_theory\">Fun Theory</a> argues that we'll never run out of fun.&nbsp; If I live for 10<sup>10,000,000,000</sup> years and then die, then when I draw my last metaphorical breath - not that I'd still have anything like a human body after that amount of time, of course - I'll go out raging against the night, for a life so short compared to all the experiences I wish I could have had.&nbsp; You can't compare that to real immortality.&nbsp; As Greg Egan put it, immortality isn't living for a very long time and then dying.&nbsp; Immortality is just not dying, ever.\"</p>\n<p>Well, I can't offer you <em>real</em> immortality - not in <em>this </em>dilemma, anyway.&nbsp; However, on behalf of my patron, Omega, who I believe is sometimes also known as Nyarlathotep, I'd like to make you a little offer.</p>\n<p>If you pay me just one penny, I'll replace your 80% chance of living for 10^(10^10) years, with a 79.99992% chance of living 10^(10^(10^10)) years.&nbsp; That's 99.9999% of 80%, so I'm just shaving a tiny fraction 10<sup>-6</sup> off your probability of survival, and in exchange, if you do survive, you'll survive - not ten times as long, my friend, but <em>ten to the power of </em>as long.&nbsp; And it goes without saying that you won't run out of memory (RAM) or other physical resources during that time.&nbsp; If you feel that the notion of \"years\" is ambiguous, let's just measure your lifespan in computing operations instead of years.&nbsp; Really there's not much of a difference when you're dealing with numbers like 10^(10<sup>10,000,000,000</sup>).</p>\n<p>My friend - can I call you friend? - let me take a few moments to dwell on what a wonderful bargain I'm offering you.&nbsp; Exponentiation is a rare thing in gambles.&nbsp; Usually, you put $1,000 at risk for a chance at making $1,500, or some multiplicative factor like that.&nbsp; But when you exponentiate, you pay linearly and buy whole factors of 10 - buy them in wholesale quantities, my friend!&nbsp; We're talking here about 10<sup>10,000,000,000</sup> factors of 10!&nbsp; If you could use $1,000 to buy a 99.9999% chance of making $10,000 - gaining a single factor of ten - why, that would be the greatest investment bargain in history, too good to be true, but the deal that Omega is offering you is far beyond that!&nbsp; If you started with $1, it takes a mere <em>eight</em> factors of ten to increase your wealth to $100,000,000.&nbsp; Three more factors of ten and you'd be the wealthiest person on Earth.&nbsp; Five more factors of ten beyond that and you'd own the Earth outright.&nbsp; How old is the universe?&nbsp; Ten factors-of-ten years.&nbsp; Just ten!&nbsp; How many quarks in the whole visible universe?&nbsp; Around eighty factors of ten, as far as anyone knows.&nbsp; And we're offering you here - why, not even ten billion factors of ten.&nbsp; Ten billion factors of ten is just what you started with!&nbsp; No, this is <em>ten to the ten billionth power</em> factors of ten.</p>\n<p>Now, you may say that your utility isn't linear in lifespan, just like it isn't linear in money.&nbsp; But even if your utility is <em>logarithmic </em>in lifespan - a pessimistic assumption, surely; doesn't money decrease in value faster than life? - why, just the <em>logarithm</em> goes from 10,000,000,000 to 10<sup>10,000,000,000</sup>.</p>\n<p>From a <a href=\"http://wiki.lesswrong.com/wiki/Fun_theory\">fun-theoretic</a> standpoint, exponentiating seems like something that really should let you have Significantly More Fun.&nbsp; If you can afford to simulate a mind a quadrillion bits large, then you merely need 2^(1,000,000,000,000,000) times as much computing power - a quadrillion factors of 2 - to simulate <em>all possible</em> minds with a quadrillion binary degrees of freedom so defined.&nbsp; Exponentiation lets you <em>completely</em> explore the <em>whole space </em>of which you were previously a single point - and that's just if you use it for brute force.&nbsp; So going from a lifespan of 10^(10^10) to 10^(10^(10^10)) seems like it ought to be a significant improvement, from a fun-theoretic standpoint.</p>\n<p>And Omega is offering you this special deal, not for a dollar, not for a dime, but one penny!&nbsp; That's right!&nbsp; Act now!&nbsp; Pay a penny and go from a 20% probability of dying in an hour and an 80% probability of living 10<sup>10,000,000,000</sup> years, to a 20.00008% probability of dying in an hour and a 79.99992% probability of living 10^(10<sup>10,000,000,000</sup>) years!&nbsp; That's far more <em>factors of ten</em> in your lifespan than the number of quarks in the visible universe raised to the millionth power!</p>\n<p>Is that a penny, friend?&nbsp; - thank you, thank you.&nbsp; But wait!&nbsp; There's another special offer, and you won't even have to pay a penny for this one - this one is <em>free!</em>&nbsp; That's right, I'm offering to exponentiate your lifespan <em>again,</em> to 10^(10^(10<sup>10,000,000,000</sup>)) years!&nbsp; Now, I'll have to multiply your probability of survival by 99.9999% again, but really, what's that compared to the nigh-incomprehensible increase in your expected lifespan?</p>\n<p>Is that an avaricious light I see in your eyes?&nbsp; Then go for it!&nbsp; Take the deal!&nbsp; It's free!</p>\n<p><em>(Some time later.)</em></p>\n<p>My friend, I really don't understand your grumbles.&nbsp; At every step of the way, you seemed eager to take the deal.&nbsp; It's hardly my fault that you've ended up with... let's see... a probability of 1/10<sup>1000</sup> of living 10^^(2,302,360,800) years, and otherwise dying in an hour.&nbsp; Oh, the ^^?&nbsp; That's just a compact way of expressing tetration, or repeated exponentiation - it's really supposed to be Knuth up-arrows, &uarr;&uarr;, but I prefer to just write ^^.&nbsp; So 10^^(2,302,360,800) means 10^(10^(10^...^10)) where the exponential tower of tens is 2,302,360,800 layers high.</p>\n<p>But, tell you what - these deals <em>are</em> intended to be permanent, you know, but if you pay me another penny, I'll trade you your current gamble for an 80% probability of living 10<sup>10,000,000,000</sup> years.</p>\n<p>Why, thanks!&nbsp; I'm glad you've given me your two cents on the subject.</p>\n<p>Hey, don't make that face!&nbsp; You've learned something about your own preferences, and that's the most valuable sort of information there is!</p>\n<p>Anyway, I've just received telepathic word from Omega that I'm to offer you another bargain - hey!&nbsp; Don't run away until you've at least heard me out!</p>\n<p>Okay, I know you're feeling sore.&nbsp; How's this to make up for it?&nbsp; Right now you've got an 80% probability of living 10<sup>10,000,000,000</sup> years.&nbsp; But right now - for free - I'll replace that with an 80% probability (that's right, 80%) of living 10^^10 years, that's 10^10^10^10^10^10^10^10<sup>10,000,000,000</sup> years.</p>\n<p>See?&nbsp; I thought that'd wipe the frown from your face.</p>\n<p>So right now you've got an 80% probability of living 10^^10 years.&nbsp; But if you give me a penny, I'll <em>tetrate</em> that sucker!&nbsp; That's right - your lifespan will go to 10^^(10^^10) years!&nbsp; That's an exponential tower (10^^10) tens high!&nbsp; You could write that as 10^^^3, by the way, if you're interested.&nbsp; Oh, and I'm afraid I'll have to multiply your survival probability by 99.99999999%.</p>\n<p><em>What?</em>&nbsp; What do you mean, <em>no?</em>&nbsp; The benefit here is vastly larger than the mere 10^^(2,302,360,800) years you bought previously, and you merely have to send your probability to 79.999999992% instead of 10<sup>-1000</sup> to purchase it!&nbsp; Well, that and the penny, of course.&nbsp; If you turn down <em>this</em> offer, what does it say about that whole road you went down before?&nbsp; Think of how silly you'd look in retrospect!&nbsp; Come now, pettiness aside, this is the real world, wouldn't you rather have a 79.999999992% probability of living 10^^(10^^10) years than an 80% probability of living 10^^10 years?&nbsp; Those arrows suppress a lot of detail, as the saying goes!&nbsp; If you can't have Significantly More Fun with tetration, how can you possibly hope to have fun at all?</p>\n<p>Hm?&nbsp; Why yes, that's right, I <em>am</em> going to offer to tetrate the lifespan and fraction the probability yet again... I was thinking of taking you down to a survival probability of 1/(10^^^20), or something like that... oh, don't make that face at me, if you want to refuse the whole garden path you've got to refuse some particular step along the way.</p>\n<p>Wait!&nbsp; Come back!&nbsp; I have even faster-growing functions to show you!&nbsp; And I'll take even smaller slices off the probability each time!&nbsp; Come back!</p>\n<p>...ahem.</p>\n<p>While I feel that the Repugnant Conclusion has an obvious answer, and that <a href=\"/lw/kn/torture_vs_dust_specks/\">SPECKS vs. TORTURE</a> has an obvious answer, the Lifespan Dilemma actually confuses me - the more I demand answers of my mind, the stranger my intuitive responses get.&nbsp; How are yours?</p>\n<p>Based on <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/1107\">an argument by Wei Dai</a>.&nbsp; Dai proposed a <em>reductio </em>of unbounded utility functions by (correctly) pointing out that an unbounded utility on lifespan implies willingness to trade an 80% probability of living some large number of years for a 1/(3^^^3) probability of living some <em>sufficiently longer </em>lifespan.&nbsp; I looked at this and realized that there existed an obvious garden path, which meant that denying the conclusion would create a preference reversal.&nbsp; Note also the relation to the <a href=\"http://en.wikipedia.org/wiki/St._Petersburg_paradox\">St. Petersburg Paradox</a>, although the Lifespan Dilemma requires only a finite number of steps to get us in trouble.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"L3NcKBNTvQaFXwv9u": 1, "HAFdXkW4YW4KRe2Gx": 1, "ZTRNmvQGgoYiymYnq": 1, "HNJiR8Jzafsv8cHrC": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9RCoE7jmmvGd5Zsh2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": 55, "extendedScore": null, "score": 8.8e-05, "legacy": true, "legacyId": "1565", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 220, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3wYTFWY3LKQCnAptN", "cfZ8zveqrTZbQrjeD", "gb6zWstjmkYHLrbrg", "2ftJ38y9SRBCBsCzy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-10T20:16:01.304Z", "modifiedAt": null, "url": null, "title": "Formalizing informal logic", "slug": "formalizing-informal-logic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:32.965Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Johnicholas", "createdAt": "2009-02-27T15:01:52.708Z", "isAdmin": false, "displayName": "Johnicholas"}, "userId": "kBvTXutfPytNtzPyD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/abDxxhhtexErMints/formalizing-informal-logic", "pageUrlRelative": "/posts/abDxxhhtexErMints/formalizing-informal-logic", "linkUrl": "https://www.lesswrong.com/posts/abDxxhhtexErMints/formalizing-informal-logic", "postedAtFormatted": "Thursday, September 10th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Formalizing%20informal%20logic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFormalizing%20informal%20logic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FabDxxhhtexErMints%2Fformalizing-informal-logic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Formalizing%20informal%20logic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FabDxxhhtexErMints%2Fformalizing-informal-logic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FabDxxhhtexErMints%2Fformalizing-informal-logic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 852, "htmlBody": "<p>As an exercise, I take a scrap of argumentation, expand it into a tree diagram (using <a href=\"http://freemind.sourceforge.net/wiki/index.php/Main_Page\">FreeMind</a>), and then formalize the argument (in <a href=\"http://www.cs.ru.nl/~freek/aut/\">Automath</a>). This towards the goal of creating&nbsp; \"rationality augmentation\" software. In the short term, my suspicion is that such software would look like a group of existing tools glued together with human practices.</p>\n<p>About my choice of tools: I investigated <a href=\"http://www.computing.dundee.ac.uk/staff/creed/araucaria/\">Araucaria</a>, <a href=\"http://austhink.com/\">Rationale</a>, <a href=\"http://sourceforge.net/projects/argumentative/\">Argumentative</a>, and <a href=\"http://carneades.berlios.de/\">Carneades</a>. With the exception of Rationale, they're not as polished graphically as FreeMind, and the rigid argumentation-theory structure was annoying in the early stages of analysis. Using a general-purpose mapping/outlining tool may not be ideal, but it's easy to obtain. The primary reason I used Automath to formalize the argument was because I'm somewhat familiar with it. Another reason is that it's easy to obtain and build (at least, on GNU/Linux).</p>\n<p>Automath is an ancient and awesomely flexible proof checker. (Of course, other more modern <a href=\"http://www.cs.ru.nl/~freek/comparison/index.html\">proof-checkers</a> are often just as flexible, maybe more flexible, and may be more useable.) The amount of \"proof checking\" done in this example is trivial - roughly, what the checker is checking is: \"after assuming all of these bits and pieces of opaque human reasoning, do they form some sort of tree?\" - but cutting down a powerful tool leaves a nice upgrade path, in case people start <a href=\"http://en.wikipedia.org/wiki/Deontic_logic\">using</a> <a href=\"http://plato.stanford.edu/entries/logic-inductive/\">exotic</a> <a href=\"http://en.wikipedia.org/wiki/Possible_world\">forms</a> <a href=\"http://plato.stanford.edu/entries/logic-relevance/\">of</a> <a href=\"http://plato.stanford.edu/entries/logic-belief-revision/\">logic</a>.&nbsp; However, the argument checkers built into the various argumentation-theory tools do not have such upgrade paths, and so are not really credible as candidates to formalize the <a href=\"/lw/17c/outlawing_anthropics_an_updateless_dilemma/\">arguments</a> on this site.</p>\n<p style=\"padding-left: 30px;\"><a id=\"more\"></a></p>\n<p>Here's a piece of argumentation, abstracted from something that I was really thinking about at work:</p>\n<blockquote style=\"padding-left: 30px;\">\n<p style=\"padding-left: 30px; \">There aren't any memory leaks in this method, but how would I argue it? If I had tested it with a tool like <a href=\"http://valgrind.org/\">Valgrind</a> or <a href=\"http://en.wikipedia.org/wiki/Mtrace\">mtrace</a>, I would have some justification - but I didn't. By eye, it doesn't look like it does any allocations from the heap. Of course, if a programmer violated coding standards, they could conceal an allocation from casual inspection. However, the author of the code wouldn't violate the coding standard that badly. Why do I believe that they wouldn't violate the coding standard? Well, I can't think of a reason for them to violate the coding standard deliberately, and they're competent enough to avoid making such an egregious mistake by accident.</p>\n</blockquote>\n<p>In order to apply the argument diagramming technique, try to form a tree structure, with <em>claims</em> supported by independent <em>reasons.</em>&nbsp;Reasons are small combinations of claims, which do not support the conclusion alone, but do support it together. This is what I came up with (<a href=\"http://www.johnicholas.com/ThereAreNoLeaks.png\">png</a>, <a href=\"http://www.johnicholas.com/ThereAreNoLeaks.svg\">svg</a>, FreeMind's native <a href=\"http://www.johnicholas.com/ThereAreNoLeaks.mm\">mm</a> format).</p>\n<p>Some flaws in this analysis:</p>\n<p>1. The cursory inspection might be an independent reason for believing there are no allocations.</p>\n<p>2. There isn't any mention in the analysis of the egregiousness of the mistake.</p>\n<p>3. The treatment of benevolence and competence is asymmetrical.</p>\n<p>(My understanding with argument diagramming so far is that bouncing between \"try to diagram the argument as best you can\" and \"try to explain in what ways the diagram is still missing aspects of the original text\" is helpful.)</p>\n<p>In order to translate this informal argument into Automath, I used a very simple logical framework. Propositions are a kind of thing, and each justification is of some proposition. There are no general-purpose <a href=\"http://en.wikipedia.org/wiki/List_of_rules_of_inference\">inference rules</a> like <a href=\"http://en.wikipedia.org/wiki/Modus_ponens\">modus ponens</a> or \"From truth of a proposition, conclude necessity of that proposition\". This means that every reason in the argument needs to assume its own special-case inference rule, warranting that step.</p>\n<blockquote style=\"padding-left: 30px;\">\n<pre style=\"padding-left: 30px;\"># A proposition is a kind of thing, by assumption.<br />* proposition<br />&nbsp; : TYPE<br />&nbsp; = PRIM<br /><br /># a justification of a proposition is a kind of thing, by assumption<br />* [p : proposition]<br />&nbsp; justification<br />&nbsp; : TYPE<br />&nbsp; = PRIM</pre>\n</blockquote>\n<p>To translate a claim in this framework, we create (by assuming) a the proposition, with a chunk like this:</p>\n<blockquote style=\"padding-left: 30px;\">\n<pre style=\"padding-left: 30px;\">* claim_foo<br /> : proposition<br /> = PRIM<br /></pre>\n</blockquote>\n<p>To translate a reason for a claim \"baz\" from predecessor claims \"foo\" and \"bar\", first we create (by assuming) the inference rule, with a chunk like this:</p>\n<blockquote style=\"padding-left: 30px;\">\n<pre style=\"padding-left: 30px;\">* [p:justification(claim_foo)]<br /> [q:justification(claim_bar)]<br /> reason_baz<br /> : justification(claim_baz)<br /> = PRIM</pre>\n</blockquote>\n<p>Secondly, we actually use the inference rule (exactly once), with a chunk like this:</p>\n<blockquote>\n<pre style=\"padding-left: 60px;\">* justification_baz<br /> : justification(claim_baz)<br /> = reason_baz(justification_foo, justification_bar)</pre>\n</blockquote>\n<p>My formalization of the above scrap of reasoning is <a href=\"http://www.johnicholas.com/there_are_no_leaks.aut\">here</a>. Running \"aut\" is really easy. Do something like:</p>\n<blockquote>\n<pre style=\"padding-left: 30px;\">aut there_are_no_leaks.aut<br /></pre>\n</blockquote>\n<p>Where to go next:</p>\n<ul>\n<li>Formalize more arguments, and bigger arguments.</li>\n<li>Investigate more tools (a more modern proof checker?).</li>\n<li>The translation from Freemind diagram to Automath is probably automatable via FreeMind's XSLT export.</li>\n<li>How do we incorporate other tools into the workflow? For example <a href=\"http://www.google.com\">Google</a>, <a href=\"http://genie.sis.pitt.edu/\">Genie&amp;Smile</a>, or some sort of <a href=\"http://www.newyorker.com/reporting/2007/12/10/071210fa_fact_gawande\">checklist</a> tool?</li>\n<li>What argument schemes (reflective inconsistency? don't be continually surprised?) do we actually use?</li>\n<li>Try to get some experimental evidence that formalization actually helps rationality somehow.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "abDxxhhtexErMints", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 16, "extendedScore": null, "score": 5.213674765201614e-07, "legacy": true, "legacyId": "1574", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZTEkZNLrmycNuCNYq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-11T02:37:01.745Z", "modifiedAt": null, "url": null, "title": "Timeless Identity Crisis", "slug": "timeless-identity-crisis", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:30.258Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Psy-Kosh", "createdAt": "2009-03-01T19:34:52.148Z", "isAdmin": false, "displayName": "Psy-Kosh"}, "userId": "CtHmuQzjA7Y7LnSss", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CwBPX8f59rp2LK4kL/timeless-identity-crisis", "pageUrlRelative": "/posts/CwBPX8f59rp2LK4kL/timeless-identity-crisis", "linkUrl": "https://www.lesswrong.com/posts/CwBPX8f59rp2LK4kL/timeless-identity-crisis", "postedAtFormatted": "Friday, September 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Timeless%20Identity%20Crisis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATimeless%20Identity%20Crisis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCwBPX8f59rp2LK4kL%2Ftimeless-identity-crisis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Timeless%20Identity%20Crisis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCwBPX8f59rp2LK4kL%2Ftimeless-identity-crisis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCwBPX8f59rp2LK4kL%2Ftimeless-identity-crisis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 673, "htmlBody": "<p>Followup/summary/extension to <a href=\"/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/135t\">this conversation with SilasBarta</a></p>\n<p>So, you're going along, cheerfully deciding things, doing counterfactual surgery on the output of decision algorithm A1 to calculate the results of your decisions, but it turns out that a dark secret is undermining your efforts...</p>\n<p>You are not running/being decision algorithm A1, but instead decision algorithm A2, an algorithm that happens to have the property of believing (erroneously) that it actually is A1.</p>\n<p>Ruh-roh.</p>\n<p>Now, it is _NOT_ my intent here to try to solve the problem of \"how can you know which one you really are?\", but instead to deal with the problem of \"how can TDT take into account this possibility?\"<a id=\"more\"></a></p>\n<p>Well, first, let me suggest a slightly more concrete way in which this might come up:</p>\n<p>Physical computation errors. For instance, a stray cosmic ray hits your processor and flips a bit in such a way that a certain conditional that would have otherwise gone down one branch instead goes down the other, so instead of computing the output of your usual algorithm in this circumstance, you're computing the output of the version that, at that specific step, behaves in that slightly different way. (Yes, this sort of thing can be mitigated with error correction/etc. The problem that is being addressed here is that, (to me at least) it seems that basic TDT doesn't have a natural way to even represent this possibility).</p>\n<p>Consider a slightly modified causal net with in which the innards of an agent are more more of an \"initial state\", and that there's a selector node/process (ie, the resulting computation) that selects which abstract algorithm's output is the one that's the actual output. ie, this process determines which algorithm you, well, are.</p>\n<p>Similarly, another being that might base its actions on a model of your behavior will be represented as having a model of your innards and the model itself having a selector, analogous to the above.</p>\n<p><img src=\"http://images.lesswrong.com/t3_17s_0.png\" alt=\"TDT with self ambiguity\" width=\"642\" height=\"538\" /></p>\n<p>To actually compute consequences of decisions and do all the relevant counterfactual surgery, ideally (ignoring \"minor\" issues like computability), one iterates over all possible algorithms one might be. That is, one first goes \"if the actual results of the combination of my innards and all the messy details of reality and so on is to do computation A1, then...\" and subiterate over all possible decisions. The second thing, of course, being done via the usual counterfactual surgery.</p>\n<p>Then, weigh all of those by the probability that one actually _is_ algorithm A1, and then go \"if I actually was algorithm A2...\" etc etc... ie, and one does the same counterfactual surgery.</p>\n<p>In the above diagram, that lets one consider the possibility of ones own choice being decoupled from what the model of their choice would predict, given that the initial model is correct, but while they are actually considering the decision, a hardware error or whatever causes the agent to be/implement A2 while the model of them is instead properly implementing A1.</p>\n<p>&nbsp;</p>\n<p>I am far from convinced that this is the best way to deal with this issue, but I haven't seen anyone else bringing it up, and the usual form of TDT that we've been describing didn't seem to have any obvious way to even represent this issue. So, if anyone has any better ideas for how to clean up this solution, or otherwise alternate ideas for dealing with this problem, go ahead.</p>\n<p>I just think it is important that it be dealt with _somehow_... That is, that the decision theory have some way of representing errors or other things that could cause ambiguity as to which algorithm it is actually implementing in the first place.</p>\n<p>&nbsp;</p>\n<p>EDIT: sorry, to clarify: one determines the utility for a possible choice by summing over the results of all the possible algorithms making that particular choice. (ie, \"I don't know if my decision corresponds to deciding the outcome of algorithm A1 or A2 or...\") so sum over those for each choice, weighing by the probability of that being the actual algorithm in quesiton)</p>\n<p>EDIT2: SilasBarta came up with a <a href=\"http://4.bp.blogspot.com/_SL1MFVbilH8/Sqh-2bUk53I/AAAAAAAAACw/uDxbdpyxjno/s1600-h/Psy_Kosh_causal_net2.PNG\">different causal graph</a>&nbsp;during our discussion to represent this issue.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CwBPX8f59rp2LK4kL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "1576", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-12T14:42:44.684Z", "modifiedAt": null, "url": null, "title": "The New Nostradamus", "slug": "the-new-nostradamus", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:31.418Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XHjAxJvdpiqck8Da2/the-new-nostradamus", "pageUrlRelative": "/posts/XHjAxJvdpiqck8Da2/the-new-nostradamus", "linkUrl": "https://www.lesswrong.com/posts/XHjAxJvdpiqck8Da2/the-new-nostradamus", "postedAtFormatted": "Saturday, September 12th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20New%20Nostradamus&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20New%20Nostradamus%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXHjAxJvdpiqck8Da2%2Fthe-new-nostradamus%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20New%20Nostradamus%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXHjAxJvdpiqck8Da2%2Fthe-new-nostradamus", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXHjAxJvdpiqck8Da2%2Fthe-new-nostradamus", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1344, "htmlBody": "<p>I stumbled upon an article called <a href=\"http://www.good.is/post/the-new-nostradamus/\">The New Nostradamus</a>, reporting of a game-theoretic model which predicts political outcomes with startling effectiveness. The results are very impressive. However, the site hosting the article is unfamiliar to me, so I'm not certain of the article's verity, but <a href=\"http://www.google.com/search?q=Bueno+de+Mesquita\">a quick Google</a> seems to support the claims, at least on a superficial skimming. <a href=\"http://www.ted.com/talks/bruce_bueno_de_mesquita_predicts_iran_s_future.html\">Here's his TED talk</a>. The model seems almost too good to be true, though. Anybody know more?</p>\r\n<p>Some choice bits from the article:</p>\r\n<p><strong>The claim:</strong></p>\r\n<blockquote>\r\n<p>In fact, the professor says that a computer model he built and has perfected over the last 25 years can predict the outcome of virtually any international conflict, provided the basic input is accurate. What&rsquo;s more, his predictions are alarmingly specific. His fans include at least one current presidential hopeful, a gaggle of Fortune 500 companies, the CIA, and the Department of Defense.</p>\r\n</blockquote>\r\n<p><strong>The results:</strong></p>\r\n<blockquote>\r\n<p>The criticism rankles him, because, to his mind, the proof is right there on the page. &ldquo;I&rsquo;ve published a lot of forecasting papers over the years,&rdquo; he says. &ldquo;Papers that are about things that had not yet happened when the paper was published but would happen within some reasonable amount of time. There&rsquo;s a track record that I can point to.&rdquo; And indeed there is. Bueno de Mesquita has made a slew of uncannily accurate predictions&mdash;more than 2,000, on subjects ranging from the terrorist threat to America to the peace process in Northern Ireland&mdash;that would seem to prove him right.</p>\r\n<p>[...]</p>\r\n<p>To verify the accuracy of his model, the CIA set up a kind of forecasting face-off that pit predictions from his model against those of Langley&rsquo;s more traditional in-house intelligence analysts and area specialists. &ldquo;We tested Bueno de Mesquita&rsquo;s model on scores of issues that were conducted in real time&mdash;that is, the forecasts were made before the events actually happened,&rdquo; says Stanley Feder, a former high-level CIA analyst. &ldquo;We found the model to be accurate 90 percent of the time,&rdquo; he wrote. Another study evaluating Bueno de Mesquita&rsquo;s real-time forecasts of 21 policy decisions in the European community concluded that &ldquo;the probability that the predicted outcome was what indeed occurred was an astounding 97 percent.&rdquo; What&rsquo;s more, Bueno de Mesquita&rsquo;s forecasts were much more detailed than those of the more traditional analysts. &ldquo;The real issue is the specificity of the accuracy,&rdquo; says Feder. &ldquo;We found that DI (Directorate of National Intelligence) analyses, even when they were right, were vague compared to the model&rsquo;s forecasts. To use an archery metaphor, if you hit the target, that&rsquo;s great. But if you hit the bull&rsquo;s eye&mdash;that&rsquo;s amazing.\"</p>\r\n</blockquote>\r\n<p><a id=\"more\"></a></p>\r\n<p><strong>Gets good money for it:</strong></p>\r\n<blockquote>\r\n<p>Though controversial in the academic world, Bueno de Mesquita and his model have proven quite popular in the private sector. In addition to his teaching responsibilities and consulting for the government, he also runs a successful private business, Mesquita &amp; Roundell, with offices in Rockefeller Center. Advising some of the top companies in the country, he earns a tidy sum: Mesquita &amp; Roundell&rsquo;s minimum fee is $50,000 for a project that includes two issues. Most projects involve multiple issues. &ldquo;I&rsquo;m not selling my wisdom,&rdquo; he says. &ldquo;I&rsquo;m selling a tool that can help them get better results. That tool is the model.&rdquo;<br /><strong><br /></strong>&ldquo;In the private sector, we deal with three areas: litigation, mergers and acquisitions, and regulation,&rdquo; he says. &ldquo;On average in litigation, we produce a settlement that is 40 percent better than what the attorneys think is the best that can be achieved.&rdquo; While Bueno de Mesquita&rsquo;s present client list is confidential, past clients include Union Carbide, which needed a little help in structuring its defense after its 1984 chemical-plant disaster in Bhopal, India, claimed the lives of an estimated 22,000 people; the giant accounting firm Arthur Andersen; and British Aerospace during its merger with GEC-Marconi.<strong><br /></strong></p>\r\n</blockquote>\r\n<p><strong>The method</strong> should be of special interest to the OB/LW audience, as it brings to mind discussions about self-deception and evolutionary vs. acknowledged goals and behavior:</p>\r\n<blockquote>\r\n<p>Which illustrates the next incontrovertible fact about game theory: In the foreboding world view of rational choice, everyone is a raging dirtbag. Bueno de Mesquita points to dictatorships to prove his point: &ldquo;If you liberate people from the constraint of having to satisfy other people in order to advance themselves, people don&rsquo;t do good things.&rdquo; When analyzing a problem in international relations, Bueno de Mesquita doesn&rsquo;t give a whit about the local culture, history, economy, or any of the other considerations that more traditional political scientists weigh. In fact, rational choicers like Bueno de Mesquita tend to view such traditional approaches with a condescension bordering on disdain. &ldquo;One is the study of politics as an expression of personal opinion as opposed to political science,&rdquo; he says dryly. His only concern is with what the political actors want, what they say they want (often two very different things), and how each of their various options will affect their career advancement. He feeds this data into his computer model and out pop the answers.</p>\r\n<p>[...]</p>\r\n<p>In his continuing work for the CIA and the Defense Department, one of his most recent assignments has been North Korea and its nuclear program. His analysis starts from the premise that what Kim Jong Il cares most about is his political survival. As Bueno de Mesquita sees it, the principal reason for his nuclear program is to deter the United States from taking him out, by raising the costs of doing so. &ldquo;The solution, then, lies in a mechanism that guarantees us that he not use these weapons and guarantees him that we not interfere with his political survival,&rdquo; he says.</p>\r\n<p>[...]</p>\r\n<p>Recently, he&rsquo;s applied his science to come up with some novel ideas on how to resolve the Israeli-Palestinian conflict. &ldquo;In my view, it is a mistake to look for strategies that build mutual trust because it ain&rsquo;t going to happen. Neither side has any reason to trust the other, for good reason,&rdquo; he says. &ldquo;Land for peace is an inherently flawed concept because it has a fundamental commitment problem. If I give you land on your promise of peace in the future, after you have the land, as the Israelis well know, it is very costly to take it back if you renege. You have an incentive to say, &lsquo;You made a good step, it&rsquo;s a gesture in the right direction, but I thought you were giving me more than this. I can&rsquo;t give you peace just for this, it&rsquo;s not enough.&rsquo; Conversely, if we have peace for land&mdash;you disarm, put down your weapons, and get rid of the threats to me and I will then give you the land&mdash;the reverse is true: I have no commitment to follow through. Once you&rsquo;ve laid down your weapons, you have no threat.&rdquo;<br /><br />Bueno de Mesquita&rsquo;s answer to this dilemma, which he discussed with the former Israeli prime minister and recently elected Labor leader Ehud Barak, is a formula that guarantees mutual incentives to cooperate. &ldquo;In a peaceful world, what do the Palestinians anticipate will be their main source of economic viability? Tourism. This is what their own documents say. And, of course, the Israelis make a lot of money from tourism, and that revenue is very easy to track. As a starting point requiring no trust, no mutual cooperation, I would suggest that all tourist revenue be [divided by] a fixed formula based on the current population of the region, which is roughly 40 percent Palestinian, 60 percent Israeli. The money would go automatically to each side. Now, when there is violence, tourists don&rsquo;t come. So the tourist revenue is automatically responsive to the level of violence on either side for both sides. You have an accounting firm that both sides agree to, you let the U.N. do it, whatever. It&rsquo;s completely self-enforcing, it requires no cooperation except the initial agreement by the Israelis that they are going to turn this part of the revenue over, on a fixed formula based on population, to some international agency, and that&rsquo;s that.&rdquo;</p>\r\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nGLvnaZCH5mx8h8Mh": 1, "8daMDi9NEShyLqxth": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XHjAxJvdpiqck8Da2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 21, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "1577", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I stumbled upon an article called <a href=\"http://www.good.is/post/the-new-nostradamus/\">The New Nostradamus</a>, reporting of a game-theoretic model which predicts political outcomes with startling effectiveness. The results are very impressive. However, the site hosting the article is unfamiliar to me, so I'm not certain of the article's verity, but <a href=\"http://www.google.com/search?q=Bueno+de+Mesquita\">a quick Google</a> seems to support the claims, at least on a superficial skimming. <a href=\"http://www.ted.com/talks/bruce_bueno_de_mesquita_predicts_iran_s_future.html\">Here's his TED talk</a>. The model seems almost too good to be true, though. Anybody know more?</p>\n<p>Some choice bits from the article:</p>\n<p><strong id=\"The_claim_\">The claim:</strong></p>\n<blockquote>\n<p>In fact, the professor says that a computer model he built and has perfected over the last 25 years can predict the outcome of virtually any international conflict, provided the basic input is accurate. What\u2019s more, his predictions are alarmingly specific. His fans include at least one current presidential hopeful, a gaggle of Fortune 500 companies, the CIA, and the Department of Defense.</p>\n</blockquote>\n<p><strong id=\"The_results_\">The results:</strong></p>\n<blockquote>\n<p>The criticism rankles him, because, to his mind, the proof is right there on the page. \u201cI\u2019ve published a lot of forecasting papers over the years,\u201d he says. \u201cPapers that are about things that had not yet happened when the paper was published but would happen within some reasonable amount of time. There\u2019s a track record that I can point to.\u201d And indeed there is. Bueno de Mesquita has made a slew of uncannily accurate predictions\u2014more than 2,000, on subjects ranging from the terrorist threat to America to the peace process in Northern Ireland\u2014that would seem to prove him right.</p>\n<p>[...]</p>\n<p>To verify the accuracy of his model, the CIA set up a kind of forecasting face-off that pit predictions from his model against those of Langley\u2019s more traditional in-house intelligence analysts and area specialists. \u201cWe tested Bueno de Mesquita\u2019s model on scores of issues that were conducted in real time\u2014that is, the forecasts were made before the events actually happened,\u201d says Stanley Feder, a former high-level CIA analyst. \u201cWe found the model to be accurate 90 percent of the time,\u201d he wrote. Another study evaluating Bueno de Mesquita\u2019s real-time forecasts of 21 policy decisions in the European community concluded that \u201cthe probability that the predicted outcome was what indeed occurred was an astounding 97 percent.\u201d What\u2019s more, Bueno de Mesquita\u2019s forecasts were much more detailed than those of the more traditional analysts. \u201cThe real issue is the specificity of the accuracy,\u201d says Feder. \u201cWe found that DI (Directorate of National Intelligence) analyses, even when they were right, were vague compared to the model\u2019s forecasts. To use an archery metaphor, if you hit the target, that\u2019s great. But if you hit the bull\u2019s eye\u2014that\u2019s amazing.\"</p>\n</blockquote>\n<p><a id=\"more\"></a></p>\n<p><strong id=\"Gets_good_money_for_it_\">Gets good money for it:</strong></p>\n<blockquote>\n<p>Though controversial in the academic world, Bueno de Mesquita and his model have proven quite popular in the private sector. In addition to his teaching responsibilities and consulting for the government, he also runs a successful private business, Mesquita &amp; Roundell, with offices in Rockefeller Center. Advising some of the top companies in the country, he earns a tidy sum: Mesquita &amp; Roundell\u2019s minimum fee is $50,000 for a project that includes two issues. Most projects involve multiple issues. \u201cI\u2019m not selling my wisdom,\u201d he says. \u201cI\u2019m selling a tool that can help them get better results. That tool is the model.\u201d<br><strong><br></strong>\u201cIn the private sector, we deal with three areas: litigation, mergers and acquisitions, and regulation,\u201d he says. \u201cOn average in litigation, we produce a settlement that is 40 percent better than what the attorneys think is the best that can be achieved.\u201d While Bueno de Mesquita\u2019s present client list is confidential, past clients include Union Carbide, which needed a little help in structuring its defense after its 1984 chemical-plant disaster in Bhopal, India, claimed the lives of an estimated 22,000 people; the giant accounting firm Arthur Andersen; and British Aerospace during its merger with GEC-Marconi.<strong><br></strong></p>\n</blockquote>\n<p><strong>The method</strong> should be of special interest to the OB/LW audience, as it brings to mind discussions about self-deception and evolutionary vs. acknowledged goals and behavior:</p>\n<blockquote>\n<p>Which illustrates the next incontrovertible fact about game theory: In the foreboding world view of rational choice, everyone is a raging dirtbag. Bueno de Mesquita points to dictatorships to prove his point: \u201cIf you liberate people from the constraint of having to satisfy other people in order to advance themselves, people don\u2019t do good things.\u201d When analyzing a problem in international relations, Bueno de Mesquita doesn\u2019t give a whit about the local culture, history, economy, or any of the other considerations that more traditional political scientists weigh. In fact, rational choicers like Bueno de Mesquita tend to view such traditional approaches with a condescension bordering on disdain. \u201cOne is the study of politics as an expression of personal opinion as opposed to political science,\u201d he says dryly. His only concern is with what the political actors want, what they say they want (often two very different things), and how each of their various options will affect their career advancement. He feeds this data into his computer model and out pop the answers.</p>\n<p>[...]</p>\n<p>In his continuing work for the CIA and the Defense Department, one of his most recent assignments has been North Korea and its nuclear program. His analysis starts from the premise that what Kim Jong Il cares most about is his political survival. As Bueno de Mesquita sees it, the principal reason for his nuclear program is to deter the United States from taking him out, by raising the costs of doing so. \u201cThe solution, then, lies in a mechanism that guarantees us that he not use these weapons and guarantees him that we not interfere with his political survival,\u201d he says.</p>\n<p>[...]</p>\n<p>Recently, he\u2019s applied his science to come up with some novel ideas on how to resolve the Israeli-Palestinian conflict. \u201cIn my view, it is a mistake to look for strategies that build mutual trust because it ain\u2019t going to happen. Neither side has any reason to trust the other, for good reason,\u201d he says. \u201cLand for peace is an inherently flawed concept because it has a fundamental commitment problem. If I give you land on your promise of peace in the future, after you have the land, as the Israelis well know, it is very costly to take it back if you renege. You have an incentive to say, \u2018You made a good step, it\u2019s a gesture in the right direction, but I thought you were giving me more than this. I can\u2019t give you peace just for this, it\u2019s not enough.\u2019 Conversely, if we have peace for land\u2014you disarm, put down your weapons, and get rid of the threats to me and I will then give you the land\u2014the reverse is true: I have no commitment to follow through. Once you\u2019ve laid down your weapons, you have no threat.\u201d<br><br>Bueno de Mesquita\u2019s answer to this dilemma, which he discussed with the former Israeli prime minister and recently elected Labor leader Ehud Barak, is a formula that guarantees mutual incentives to cooperate. \u201cIn a peaceful world, what do the Palestinians anticipate will be their main source of economic viability? Tourism. This is what their own documents say. And, of course, the Israelis make a lot of money from tourism, and that revenue is very easy to track. As a starting point requiring no trust, no mutual cooperation, I would suggest that all tourist revenue be [divided by] a fixed formula based on the current population of the region, which is roughly 40 percent Palestinian, 60 percent Israeli. The money would go automatically to each side. Now, when there is violence, tourists don\u2019t come. So the tourist revenue is automatically responsive to the level of violence on either side for both sides. You have an accounting firm that both sides agree to, you let the U.N. do it, whatever. It\u2019s completely self-enforcing, it requires no cooperation except the initial agreement by the Israelis that they are going to turn this part of the revenue over, on a fixed formula based on population, to some international agency, and that\u2019s that.\u201d</p>\n</blockquote>", "sections": [{"title": "The claim:", "anchor": "The_claim_", "level": 1}, {"title": "The results:", "anchor": "The_results_", "level": 1}, {"title": "Gets good money for it:", "anchor": "Gets_good_money_for_it_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "27 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-13T04:23:04.076Z", "modifiedAt": null, "url": null, "title": "Formalizing reflective inconsistency", "slug": "formalizing-reflective-inconsistency", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:27.948Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Johnicholas", "createdAt": "2009-02-27T15:01:52.708Z", "isAdmin": false, "displayName": "Johnicholas"}, "userId": "kBvTXutfPytNtzPyD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rDLr86BRG2dHxxPHx/formalizing-reflective-inconsistency", "pageUrlRelative": "/posts/rDLr86BRG2dHxxPHx/formalizing-reflective-inconsistency", "linkUrl": "https://www.lesswrong.com/posts/rDLr86BRG2dHxxPHx/formalizing-reflective-inconsistency", "postedAtFormatted": "Sunday, September 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Formalizing%20reflective%20inconsistency&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFormalizing%20reflective%20inconsistency%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrDLr86BRG2dHxxPHx%2Fformalizing-reflective-inconsistency%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Formalizing%20reflective%20inconsistency%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrDLr86BRG2dHxxPHx%2Fformalizing-reflective-inconsistency", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrDLr86BRG2dHxxPHx%2Fformalizing-reflective-inconsistency", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 705, "htmlBody": "<p>In the post <a href=\"/lw/17c/outlawing_anthropics_an_updateless_dilemma/\">Outlawing Anthropics</a>, there was a brief and intriguing scrap of reasoning, which used the principle of reflective inconsistency, which so far as I know is unique to this community:</p>\n<blockquote>\n<p>If your current system cares about yourself and your future, but doesn't care about very similar xerox-siblings, then you will tend to self-modify to have future copies of yourself care about each other, as this maximizes your expectation of pleasant experience over future selves.</p>\n</blockquote>\n<p>This post expands upon and attempts to formalize that reasoning, in hopes of developing a logical framework for reasoning about reflective inconsistency.<a id=\"more\"></a></p>\n<p>In diagramming and analyzing this, I encountered a difficulty. There are probably many ways to resolve it, but in resolving it, I basically changed the argument. You might have reasonably chosen a different resolution. Anyway, I'll explain the difficulty and where I ended up.</p>\n<p>The difficulty: The text \"...maximizes your expectation of pleasant experience over future selves.\". How would you compute <a href=\"http://en.wikipedia.org/wiki/Expected_value\">expectation</a> of pleasant experience? It ought to depend intensely on the situation. For example, a flat future, with no opportunity to influence my experience or that of my sibs for better or worse, would argue that caring for sibs has exactly the same expectation as not-caring. Alternatively, if a mad <a href=\"http://www.aynrand.org\">Randian</a> was experimenting on me, rewarding selfishness, not-caring for my sibs might well have more pleasant experiences than caring. Also, I don't know how to compute with experiences - Total Utility, Average Utility, <a href=\"http://en.wikipedia.org/wiki/John_Rawls\">Rawlsian</a> Minimum Utility, some sort of <a href=\"http://en.wikipedia.org/wiki/Multiobjective_optimization\">multiobjective optimization</a>? Finally, I don't know how to compute with future selves. For example, imagine some sort of <a href=\"http://en.wikipedia.org/wiki/Bicameralism_(psychology)\">bicameral</a> cognitive architecture, where two individuals have exactly the same percepts (and therefore choose exactly the same actions). Should I count that as one future self or two?</p>\n<p>To resolve this, I replace <a href=\"http://yudkowsky.net/\">EY</a>'s reason with an argument from analogy, like so:</p>\n<blockquote>\n<p>If your current system cares about yourself and your future, but doesn't care about very similar xerox-siblings, then you will tend to self-modify to have future copies of yourself care about each other, for the same reasons that the process of evolution created <a href=\"http://en.wikipedia.org/wiki/Kin_selection\">kin altruism</a>.</p>\n</blockquote>\n<p>Here is the same argument again, \"expanded\". Remember, the primary reason to expand it is not readability - the expanded version is certainly less readable - it is as a step towards a generally applicable scheme for reasoning using the principle of reflective inconsistency.</p>\n<p>At first glance, the mechanism of natural selection seems to explain selfish, but not unselfish behavior. However, the structure of the <a href=\"http://www.anth.ucsb.edu/projects/human/epfaq/eea.html\">EEA</a> seems to have offered sufficient opportunities for kin to recognize kin with low-enough uncertainty and assist (with small-enough price to the helper and large-enough benefit to the helped) that unselfish entities do outcompete purely selfish ones. Note that the policy of selfishness is sufficiently simple that it was almost certainly tried many times. We believe that unselfishness is still a winning strategy in the present environment, and will continue to be a winning strategy in the future.</p>\n<p>The two policies, caring about sibs or not-caring, do in fact behave differently in the EEA, and so they are incompatible - we cannot behave according to both policies at once. Also, since caring about sibs outcompetes not-caring in the EEA, if a not-caring agent, X, were selecting a proxy (or \"future self\") to compete in an EEA-tournament to for utilons (or <a href=\"http://wiki.lesswrong.com/wiki/Paper_clip_maximizer\">paperclips</a>), X would pick a caring agent as proxy. The policy of not-caring would choose to delegate to an incompatible policy. This is what \"reflectively inconsistent\" means. Given a particular situation S1, one can always construct another situation S2 where the choices available in S2 correspond to policies to send as proxies into S1. One might understand the new situation as having an extra \"<a href=\"http://www.earlham.edu/~peters/writing/selfmod.htm\">self-modification</a>\" or \"<a href=\"http://en.wikipedia.org/wiki/Precommitment\">precommitment</a>\" choice point at the beginning. If a policy chooses an incompatible policy as its proxy, then that policy is \"reflectively inconsistent\" on that situation. Therefore, not-caring is reflectively inconsistent on the EEA.</p>\n<p>The last step to the conclusion is less interesting than the part about reflective inconsistency. The conclusion is something like: \"Other things being equal, prefer caring about sibs to not-caring\".</p>\n<p>Enough <a href=\"http://press.princeton.edu/titles/7929.html\">handwaving</a> - to the <a href=\"http://www.johnicholas.com/reflective_inconsistency_example.aut\">code</a>! My (crude) formalization is written in <a href=\"http://www.cs.ru.nl/~freek/aut/\">Automath</a>, and to check my proof, the command (on GNU/Linux) is something like:</p>\n<blockquote>\n<p>aut reflective_inconsistency_example.aut</p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rDLr86BRG2dHxxPHx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 5.219267584606939e-07, "legacy": true, "legacyId": "1579", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZTEkZNLrmycNuCNYq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-13T11:28:05.581Z", "modifiedAt": null, "url": null, "title": "Beware of WEIRD psychological samples", "slug": "beware-of-weird-psychological-samples", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:25.292Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/33YYcoWwtmqzAq9QR/beware-of-weird-psychological-samples", "pageUrlRelative": "/posts/33YYcoWwtmqzAq9QR/beware-of-weird-psychological-samples", "linkUrl": "https://www.lesswrong.com/posts/33YYcoWwtmqzAq9QR/beware-of-weird-psychological-samples", "postedAtFormatted": "Sunday, September 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Beware%20of%20WEIRD%20psychological%20samples&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABeware%20of%20WEIRD%20psychological%20samples%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F33YYcoWwtmqzAq9QR%2Fbeware-of-weird-psychological-samples%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Beware%20of%20WEIRD%20psychological%20samples%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F33YYcoWwtmqzAq9QR%2Fbeware-of-weird-psychological-samples", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F33YYcoWwtmqzAq9QR%2Fbeware-of-weird-psychological-samples", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 224, "htmlBody": "<p>Most of the research on cognitive biases and other psychological phenomena that we draw on here is based on samples of students at US universities.&nbsp; To what extent are we uncovering human universals, and to what extent facts about these WEIRD (Western, Educated, Industrialized, Rich, and Democratic) sample sources? A paper in press in <em>Behavioural and Brain Sciences</em> the evidence from studies that reach outside this group and highlights the many instances in which US students are outliers for many crucial studies in behavioural economics.</p>\n<p>Epiphenom: <a href=\"http://bhascience.blogspot.com/2009/09/how-normal-is-weird.html\">How normal is WEIRD?</a></p>\n<p>Henrich, J., Heine, S. J., &amp; Norenzayan, A. (in press)<strong>.</strong> <a href=\"http://www.psych.ubc.ca/%7Eara/Manuscripts/Weird_People_BBS_Henrichetal.pdf\">The Weirdest people in the world</a>? (PDF) <em>Behavioral and Brain Sciences</em>.</p>\n<p style=\"padding-left: 30px;\">Broad claims about human psychology and behavior based on narrow samples from Western&nbsp; societies are regularly published in leading journals. Are such species-generalizing claims justified? This review suggests not only that substantial variability in experimental results emerges across populations in basic domains, but that standard subjects are in fact rather unusual compared with the rest of the species - frequent outliers. The domains reviewed include visual perception, fairness, categorization, spatial cognition, memory, moral reasoning and self\u2010concepts. This review (1) indicates caution in addressing questions of human nature based on this thin slice of humanity, and (2) suggests that understanding human psychology will require tapping broader subject pools. We close by proposing ways to address these challenges.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dBPou4ihoQNY4cquv": 1, "vg4LDxjdwHLotCm8w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "33YYcoWwtmqzAq9QR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 45, "extendedScore": null, "score": 7.8e-05, "legacy": true, "legacyId": "1581", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 45, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-16T00:51:45.730Z", "modifiedAt": null, "url": null, "title": "The Absent-Minded Driver", "slug": "the-absent-minded-driver", "viewCount": null, "lastCommentedAt": "2018-06-22T13:50:19.761Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GfHdNfqxe3cSCfpHL/the-absent-minded-driver", "pageUrlRelative": "/posts/GfHdNfqxe3cSCfpHL/the-absent-minded-driver", "linkUrl": "https://www.lesswrong.com/posts/GfHdNfqxe3cSCfpHL/the-absent-minded-driver", "postedAtFormatted": "Wednesday, September 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Absent-Minded%20Driver&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Absent-Minded%20Driver%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGfHdNfqxe3cSCfpHL%2Fthe-absent-minded-driver%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Absent-Minded%20Driver%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGfHdNfqxe3cSCfpHL%2Fthe-absent-minded-driver", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGfHdNfqxe3cSCfpHL%2Fthe-absent-minded-driver", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 965, "htmlBody": "<p>This post examines an attempt by professional decision theorists to treat an example of time inconsistency, and asks why they failed to reach the solution (i.e., <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">TDT</a>/<a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">UDT</a>) that this community has more or less converged upon. (Another aim is to introduce this example, which some of us may not be familiar with.) Before I begin, I should note that I don't think \"<a href=\"/lw/17c/outlawing_anthropics_an_updateless_dilemma/13g0\">people are crazy, the world is mad</a>\" (as Eliezer puts it) is a good explanation. Maybe people are crazy, but unless we can understand how and why people are crazy (or to put it more diplomatically, \"make mistakes\"), how can we know that we're not being crazy in the same way or making the same kind of mistakes?</p>\n<p>The problem of the &lsquo;&lsquo;absent-minded driver&rsquo;&rsquo; was introduced by Michele Piccione and Ariel Rubinstein in their 1997 paper \"<a href=\"http://scholar.google.com/scholar?hl=en&amp;q=&quot;On+the+Interpretation+of+Decision+Problems+with+Imperfect+Recall&quot;\">On the Interpretation of Decision Problems with Imperfect Recall</a>\". But I'm going to use \"<a href=\"http://scholar.google.com/scholar?cluster=7923865138992199097&amp;hl=en\">The Absent-Minded Driver</a>\" by Robert J. Aumann, Sergiu Hart, and Motty Perry instead, since it's shorter and more straightforward. (Notice that the authors of this paper worked for a place called <a href=\"http://www.ratio.huji.ac.il/index.php\">Center for the Study of Rationality</a>, and <a href=\"http://en.wikipedia.org/wiki/Robert_Aumann\">one of them</a> won a Nobel Prize in Economics for his work on game theory. I really don't think we want to call <em>these</em> people \"crazy\".)</p>\n<p>Here's the problem description:</p>\n<blockquote>\n<p>An absent-minded driver starts driving at START in Figure 1. At X he<br />can either EXIT and get to A (for a payoff of 0) or CONTINUE to Y. At Y he<br />can either EXIT and get to B (payoff 4), or CONTINUE to C (payoff 1). The<br />essential assumption is that he cannot distinguish between intersections X<br />and Y, and cannot remember whether he has already gone through one of<br />them.</p>\n<p><img src=\"file:///C:/DOCUME~1/WEIDAI~1/LOCALS~1/Temp/moz-screenshot.png\" alt=\"\" /></p>\n<p><img src=\"http://images.lesswrong.com/t3_182_0.png?v=db84ecec1c02be41286059128fc71cc7\" alt=\"graphic description of the problem\" width=\"255\" height=\"392\" /></p>\n</blockquote>\n<p><a id=\"more\"></a></p>\n<p>At START, the problem seems very simple. If p is the probability of choosing CONTINUE at each intersection, then the expected payoff is p<sup>2</sup>+4(1-p)p, which is maximized at <a href=\"http://www.wolframalpha.com/input/?i=max+{p^2%2B4+%281-p%29+p}\">p = 2/3</a>. Aumann et al. call this the <em>planning-optimal </em>decision.</p>\n<p>The puzzle, as Piccione and Rubinstein saw it, is that once you are at an intersection, you should think that you have some probability&nbsp;&alpha; of being at X, and 1-&alpha; of being at Y. Your payoff for choosing CONTINUE with probability p becomes &alpha;[p<sup>2</sup>+4(1-p)p] + (1-&alpha;)[p+4(1-p)], which doesn't equal p<sup>2</sup>+4(1-p)p unless &alpha; = 1. So, once you get to an intersection, you'd choose a p that's different from the p you thought optimal at START.</p>\n<p>Aumann et al. reject this reasoning and instead suggest a notion of <em>action-optimality</em>, which they argue should govern decision making at the intersections. I'm going to skip explaining its definition and how it works (read section 4 of the paper if you want to find out), and go straight to listing some of its relevant properties:</p>\n<ol>\n<li>It still involves a notion of \"probability of being at X\".</li>\n<li>It's conceptually more complicated than planning-optimality.</li>\n<li>Mathematically, it has the same first-order necessary conditions as planning-optimality, but different sufficient conditions. </li>\n<li>If mixed strategies are allowed, any choice that is planning-optimal is also action-optimal.</li>\n<li>A choice that is action-optimal isn't necessarily planning-optimal. (In other words, there can be several action-optimal choices, only one of which is planning-optimal.) </li>\n<li>If we are restricted to pure strategies (i.e., p has to be either 0 or 1) then the set of action-optimal choices in this example is empty, even though there is still a planning-optimal one (namely p=1).</li>\n</ol>\n<p>In problems like this one, UDT is essentially equivalent to planning-optimality. So why did the authors propose and argue for action-optimality despite its downsides (see 2, 5, and 6 above), instead of the alternative solution of simply remembering or recomputing the planning-optimal decision at each intersection and carrying it out?</p>\n<p>Well, the authors don't say (they never bothered to argue against it), but I'm going to venture some guesses:</p>\n<ul>\n<li>That solution is too simple and obvious, and you can't publish a paper arguing for it.</li>\n<li>It disregards \"the probability of being at X\", which intuitively ought to play a role.</li>\n<li>The authors were trying to figure out what is rational for human beings, and that solution seems too alien for us to accept and/or put into practice.</li>\n<li>The authors were <em>not</em> thinking in terms of an AI, which can modify itself to use whatever decision theory it wants to.</li>\n<li>Aumann is known for his work in game theory. The action-optimality solution looks particularly game-theory like, and perhaps appeared more natural than it really is because of his specialized knowledge base.</li>\n<li>The authors were trying to solve one particular case of time inconsistency. They didn't have all known instances of time/dynamic/reflective inconsistencies/paradoxes/puzzles laid out in front of them, to be solved in one fell swoop.</li>\n</ul>\n<p>Taken together, these guesses perhaps suffice to explain the behavior of these professional rationalists, without needing to hypothesize that they are \"crazy\". Indeed, many of us are probably still not fully convinced by UDT for one or more of the above reasons.</p>\n<p><strong>EDIT:</strong> Here's the solution to this problem in <a href=\"/lw/15m/towards_a_new_decision_theory/\">UDT1</a>. We start by representing the scenario using a world program:</p>\n<p style=\"padding-left: 30px;\">def P(i, j):<br />&nbsp;&nbsp;&nbsp; if S(i) == \"EXIT\":<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; payoff = 0<br />&nbsp;&nbsp;&nbsp; elif S(j) == \"EXIT\":<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; payoff = 4<br />&nbsp;&nbsp;&nbsp; else:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; payoff = 1</p>\n<p>(Here we assumed that mixed strategies are allowed, so S gets a random string as input. Get rid of i and j if we want to model a situation where only pure strategies are allowed.) Then S computes that payoff at the end of P, averaged over all possible i and j, is maximized by returning \"EXIT\" for 1/3 of its possible inputs, and does that.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1, "PbShukhzpLsWpGXkM": 2, "5f5c37ee1b5cdee568cfb1dc": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GfHdNfqxe3cSCfpHL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 43, "extendedScore": null, "score": 7.2e-05, "legacy": true, "legacyId": "1586", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 150, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["de3xjFaACCAk6imzv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-16T07:18:06.675Z", "modifiedAt": null, "url": null, "title": "What is the Singularity Summit?", "slug": "what-is-the-singularity-summit", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:28.767Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Liron", "createdAt": "2009-02-27T04:43:11.294Z", "isAdmin": false, "displayName": "Liron"}, "userId": "AyzRrs8hNm54QptLi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ntJKwW3noSzefi8p7/what-is-the-singularity-summit", "pageUrlRelative": "/posts/ntJKwW3noSzefi8p7/what-is-the-singularity-summit", "linkUrl": "https://www.lesswrong.com/posts/ntJKwW3noSzefi8p7/what-is-the-singularity-summit", "postedAtFormatted": "Wednesday, September 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20the%20Singularity%20Summit%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20the%20Singularity%20Summit%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FntJKwW3noSzefi8p7%2Fwhat-is-the-singularity-summit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20the%20Singularity%20Summit%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FntJKwW3noSzefi8p7%2Fwhat-is-the-singularity-summit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FntJKwW3noSzefi8p7%2Fwhat-is-the-singularity-summit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 537, "htmlBody": "<p>As you know, the <a href=\"http://singularitysummit.com\">Singularity Summit 2009</a> is on the weekend of Oct 3 - Oct 4. What is it, you ask? I'll start from the beginning...</p>\n<hr />\n<p><sub>&nbsp;</sub></p>\n<p>An interesting collection of molecules occupied a certain tide pool 3.5 to 4.5 billion years ago, interesting because the molecule collection built copies of itself out of surrounding molecules, and the resulting molecule collections also replicated while accumulating beneficial mutations. Those molecule collections satisfied a high-level functional criterion called \"genetic fitness\", and it happened by pure chance.</p>\n<p>If you think about all the possible arrangements of atoms that can occupy a 1-millimeter by 1-millimeter by 1-millimeter cube of space, most of them are going to suck at causing the future universe to contain copies of themselves. Genetic fitness is a vanishingly small target in configuration-space.<br /><br />And if you studied the universe 5 billion years ago, you would not see a process capable of hitting such a small target. No physical process could create low-entropy collections of atoms satisfying high-level functional criteria. The second law of thermodynamics thus ensured that mice, as well as mousetraps, were physically impossible.</p>\n<p><a id=\"more\"></a></p>\n<p>Then a mutating replicator randomly emerged, and suddenly Earth was home to something special: the process of Natural Selection. Natural Selection optimizes for genetic fitness. It squeezes the space of possible futures into a tiny subspace -- the space of universes that contain self-replicators which are very good at self-replicating. And it remained a flickering candle of optimization in a dark, random universe for three billion years.<br /><br />An interesting product of Natural Selection occupied a certain region of savannah 100 thousand to 2 million years ago, interesting because it could form internal representations of the world around it and predict the consequences of its own actions. By pure chance, Natural Selection had created its successor.<br /><br />Thought is a more powerful process than Natural Selection. Thought can optimize atom configurations much faster than Natural Selection can. It takes much less time to think of a big design improvement for an organism, than to breed it for as many generations as it takes for a specimen to manifest one.<br /><br />Now remember, Natural Selection emerged by coincidence -- not by Natural Selection. Processes that optimize for genetic fitness were previously not to be found in the universe. And remember, Thought was evolved by coincidence -- not by Thought. Organs that represent the world around them and make predictions were previously not to be found among the optimized organisms of Earth.<br /><br />It is still early in the age of optimization processes. Brains are not very good equipment for doing optimization -- Natural Selection just hacked them together out of cells. Yet, Thought is much more powerful than Natural Selection. So what happens when Thought designs an optimization process more powerful than Thought?<br /><br />What happens when that optimization process designs an optimization process that is more powerful still?<br /><br />The Singularity Summit is about the critical transition into the third era of optimization processes, the successor to human Thought. To say we need to be careful about initial conditions is to make the understatement of our own entire era.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ntJKwW3noSzefi8p7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 14, "extendedScore": null, "score": 5.226750622520945e-07, "legacy": true, "legacyId": "1587", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-16T10:00:06.816Z", "modifiedAt": null, "url": null, "title": "Sociosexual Orientation Inventory, or failing to perform basic sanity checks", "slug": "sociosexual-orientation-inventory-or-failing-to-perform", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:06.482Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TCCwcmQKQvb53JWba/sociosexual-orientation-inventory-or-failing-to-perform", "pageUrlRelative": "/posts/TCCwcmQKQvb53JWba/sociosexual-orientation-inventory-or-failing-to-perform", "linkUrl": "https://www.lesswrong.com/posts/TCCwcmQKQvb53JWba/sociosexual-orientation-inventory-or-failing-to-perform", "postedAtFormatted": "Wednesday, September 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sociosexual%20Orientation%20Inventory%2C%20or%20failing%20to%20perform%20basic%20sanity%20checks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASociosexual%20Orientation%20Inventory%2C%20or%20failing%20to%20perform%20basic%20sanity%20checks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTCCwcmQKQvb53JWba%2Fsociosexual-orientation-inventory-or-failing-to-perform%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sociosexual%20Orientation%20Inventory%2C%20or%20failing%20to%20perform%20basic%20sanity%20checks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTCCwcmQKQvb53JWba%2Fsociosexual-orientation-inventory-or-failing-to-perform", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTCCwcmQKQvb53JWba%2Fsociosexual-orientation-inventory-or-failing-to-perform", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 531, "htmlBody": "<p>I just did some reading about \"<a href=\"http://en.wikipedia.org/wiki/Sociosexual_orientation\">Sociosexual Orientation Inventory</a>\", a simple 7-item test designed to measure one's openness to sex without love and long term commitment.</p>\n<p>Here are the questions. How long will it take you to spot the huge problem ahead...</p>\n<ol>\n<li>With how many different partners have you had sex (sexual intercourse) within the last year.</li>\n<li>How many different partners do you foresee yourself having sex with during the next five years? (Please give a specific, realistic estimate)</li>\n<li>With how many different partners have you had sex on one and only one occasion?</li>\n<li>How often do (did) you fantasize about having sex with someone other than your current (most recent) dating partner? (1 never ... 8 at least once a day)</li>\n<li>\"Sex without love is OK\" (1 strongly disagree ... 9 stronly agree)</li>\n<li>\"I can imagine myself being comfortable and enjoying `casual' sex with different partners (1 strongly disagree ... 9 stronly agree)</li>\n<li>\"I would have to be closely attached to someone (both emotionally and psychologially) before I could feel comfortable and fully enjoy having sex with him or her\" (1 strongly disagree ... 9 stronly agree)</li>\n</ol>\n<p>Score is: 5 x item1 + 1 x item2 (capped at 30) + 5 x item3 + 4 x item4 + 2 x (mean of item5, item6, and reversed item7)</p>\n<p>Do you see the problem already?</p>\n<p>Quite predictably, researchers report that men have much higher SOI scores than women in all countries. But the first three questions (ignoring non-1:1 gender ratios, differently biased sampling for different genders, different rates of homosexuality between genders, different behaviour of homosexuals of different genders, 30 partners cap on the second item, differently biased forecasts of the second item and other small details that won't affect the score much) - simply have to be identical for men and women, so the entire difference would have to  be explained by items 4 to 7, which have relatively low weights!</p>\n<p>The differences between men and women can be really extreme for some countries, Ukraine has 50.79&plusmn;28.92 (mean&plusmn;sd) for men, and 17.36&plusmn;8.65 for women, which means that either Ukrainian men, or Ukrainian women, or both, are notoriously lying when asked about past and future sex partners. In most countries the differences are more moderate, with total 48-country sample's scores being 46.67&plusmn;29.68 for men, and 27.34&plusmn;19.55 for women. Latvia leads the way with smallest difference, and so most likely greatest honesty, with 49.42&plusmn;23.61 for men, and 41.68&plusmn;26.68 for women, what can be plausibly explained by just differences in attitude. (fake lie detector experiments have shown it's almost exclusively women who are lying when answering questions like that)</p>\n<p>SOI seems to be considered quite useful by psychologists, it correlates with many nice things, not only other questionnaires, but country SOI averages correlate with various demographic, economic, and health scores in quite systematic way. Still, I cannot read papers about it without asking myself - why didn't they bother to perform this basic sanity check - which would detect huge number of outright lies in answers. And more importantly - what proportion of \"serious science\" suffers from problems like that?</p>\n<p>References: <a href=\"http://psych.mcmaster.ca/dalywilson/commentary_schmitt.pdf\">The 48 country SOI study</a>, <a href=\"http://researchnews.osu.edu/archive/sexsurv.htm\">fake lie detectors shows which gender lies more</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TCCwcmQKQvb53JWba", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 5, "extendedScore": null, "score": 5.227020667156891e-07, "legacy": true, "legacyId": "1588", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-18T08:49:43.865Z", "modifiedAt": null, "url": null, "title": "Quantum Russian Roulette", "slug": "quantum-russian-roulette", "viewCount": null, "lastCommentedAt": "2021-10-01T18:40:50.765Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Christian_Szegedy", "createdAt": "2009-08-10T06:05:52.698Z", "isAdmin": false, "displayName": "Christian_Szegedy"}, "userId": "D9uj4b3s5ntgDrQnK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XH9ZN8bLidtcqMxY2/quantum-russian-roulette", "pageUrlRelative": "/posts/XH9ZN8bLidtcqMxY2/quantum-russian-roulette", "linkUrl": "https://www.lesswrong.com/posts/XH9ZN8bLidtcqMxY2/quantum-russian-roulette", "postedAtFormatted": "Friday, September 18th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Quantum%20Russian%20Roulette&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuantum%20Russian%20Roulette%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXH9ZN8bLidtcqMxY2%2Fquantum-russian-roulette%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Quantum%20Russian%20Roulette%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXH9ZN8bLidtcqMxY2%2Fquantum-russian-roulette", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXH9ZN8bLidtcqMxY2%2Fquantum-russian-roulette", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 429, "htmlBody": "<p>The quantum Russian roulette is a game where 16 people participate. Each of them gets a unique four digit binary code assigned and deposits $50000. They are put to deep sleep using some drug. The organizer flips a quantum coin four times. Unlike in Russian roulette, here only the participant survives whose code was flipped. The others are executed in a completely painless manner. The survivor takes all the money.<br /><br />Let us assume that none of them have families or very good friends. Then the only result of the game is that the guy who wins will enjoy a much better quality of life. The others die in his Everett branch, but they live on in others. So everybody's only subjective experience will be that he went into a room and woke up $750000 richer.<br /><br />Being extremely spooky to our human intuition, there are hardly any trivial objective reasons to oppose this game under the following assumptions:</p>\n<ol>\n<li><a href=\"/lw/91/average_utilitarianism_must_be_correct/\">Average utilitarianism</a></li>\n<li>Near 100% confidence in the Multiple World nature of our universe</li>\n<li>It is possible to kill someone without invoking any negative experiences.<a id=\"more\"></a></li>\n</ol>\n<p>The natural question arises whether it could be somehow checked that the method really works, especially that the Multiple World Hypothesis is correct. At first sight, it looks impossible to convince anybody besides the participant who survived the game.<br /><br />However there is a way to convince a lot of people in a <em>few</em> Everett branches: You make a <strong>one-time</strong> big announcement in the Internet, TV etc. and say that there is a well tested quantum coin-flipper, examined by a community consisting of the most honest and trusted members of the society. You take some random 20 bit number and say that you will flip the equipment 20 times and if the outcome is the same as the predetermined number, then you will take it as a one to million evidence that the Multiple World theory works as expected. Of course, only people in the right branch will be convinced. Nevertheless, they could be convinced enough to make serious thoughts about the viability of quantum Russian roulette type games.</p>\n<p>My question is: What are the possible moral or logical reasons not to play such games? Both from individual or societal standpoints.</p>\n<p>[EDIT] A Simpler version (single player version of the experiment): The single player generates lottery numbers by flipping quantum coins. Sets up an equipment that kills him in sleep if the generated numbers don't coincide with his. In this way, he can guarantee waking up as a lottery millionaire.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XH9ZN8bLidtcqMxY2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 7, "extendedScore": null, "score": 5.231707999128123e-07, "legacy": true, "legacyId": "1592", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["px4nYEy3rDqeegJw3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-18T16:45:47.741Z", "modifiedAt": null, "url": null, "title": "MWI, weird quantum experiments and future-directed continuity of conscious experience", "slug": "mwi-weird-quantum-experiments-and-future-directed-continuity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:17.223Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SforSingularity", "createdAt": "2009-08-10T19:17:46.580Z", "isAdmin": false, "displayName": "SforSingularity"}, "userId": "cS3uqQdsTzLnHwtKa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MoFqnLnXDDG8WXMjB/mwi-weird-quantum-experiments-and-future-directed-continuity", "pageUrlRelative": "/posts/MoFqnLnXDDG8WXMjB/mwi-weird-quantum-experiments-and-future-directed-continuity", "linkUrl": "https://www.lesswrong.com/posts/MoFqnLnXDDG8WXMjB/mwi-weird-quantum-experiments-and-future-directed-continuity", "postedAtFormatted": "Friday, September 18th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20MWI%2C%20weird%20quantum%20experiments%20and%20future-directed%20continuity%20of%20conscious%20experience&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMWI%2C%20weird%20quantum%20experiments%20and%20future-directed%20continuity%20of%20conscious%20experience%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMoFqnLnXDDG8WXMjB%2Fmwi-weird-quantum-experiments-and-future-directed-continuity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=MWI%2C%20weird%20quantum%20experiments%20and%20future-directed%20continuity%20of%20conscious%20experience%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMoFqnLnXDDG8WXMjB%2Fmwi-weird-quantum-experiments-and-future-directed-continuity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMoFqnLnXDDG8WXMjB%2Fmwi-weird-quantum-experiments-and-future-directed-continuity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1142, "htmlBody": "<p>Response to:&nbsp;<a href=\"/lw/188/quantum_russian_roulette/\">Quantum Russian Roulette</a></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \">Related:&nbsp;<a style=\"text-decoration: none;\" href=\"/lw/174/decision_theory_why_we_need_to_reduce_could_would/\">Decision theory: Why we need to reduce &ldquo;could&rdquo;, &ldquo;would&rdquo;, &ldquo;should&rdquo;</a></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \">In&nbsp;<a href=\"/lw/188/quantum_russian_roulette/\">Quantum Russian Roulette</a>, Christian_Szegedy tells of a game which uses a \"quantum source of randomness\" to somehow make a game which consists in terminating the lives of 15 rich people to create one very rich person sound like an attractive proposition. To quote the key deduction:</p>\n<blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \"><em>Then the only result of the game is that the guy who wins will enjoy a much better quality of life. The others die in his Everett branch, but </em><strong><em>they live on</em></strong><em> in others. So </em><strong><em>everybody's only subjective experience</em></strong><em> will be that he went into a room and woke up $750000 richer.</em></p>\n</blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \">I think that&nbsp;Christian_Szegedy is mistaken, but in an interesting way.&nbsp;I think that the intuition at steak here is something about continuity of conscious experience. The intuition that Christian might have, if I may anticipate him, is that&nbsp;<em style=\"font-style: italic; \">everyone</em>&nbsp;in the experiment will&nbsp;<em style=\"font-style: italic; \">actually experience</em>&nbsp;getting $750,000, because somehow the word-line of their conscious experience will continue only in the worlds where they do not die. To formalize this, we imagine an arbitrary decision problem as a tree with nodes corresponding to decision points that create duplicate persons, and time increasing from left to right:</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_189_0.png\" alt=\"\" width=\"395\" height=\"246\" /></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><a id=\"more\"></a>The skull and crossbones symbols indicate that the person created in the previous decision point is killed. We might even consider putting probabilities on the arcs coming out of a given node to indicate how likely a given outcome is. When we try to assess whether a given decision was a good one, we might want to look at the utilities on the leaves of the tree are. But what if there is more than one leaf, and the person concerned is me, i.e. the root of the tree corresponds to \"me, now\" and the leaves correspond to \"possible me's in 10 days' time\"? I find myself querying for \"what will I <em>really experience</em>\" when trying to decide which way to steer reality. So I tend to want to mark some nodes in the decision tree as \"really me\" and others as \"zombie-like copies of me that <em>I</em> <em>will not</em> experience being\", resulting in a generic decision tree that looks like this:</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><img src=\"http://images.lesswrong.com/t3_189_2.png\" alt=\"\" width=\"560\" height=\"354\" />&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \">I decorated the tree with normal faces and zombie faces consistent with the following rules:</p>\n<ol>\n<li>At a decision node,&nbsp;if the parent is a zombie&nbsp;then child nodes have to be zombies, and&nbsp;</li>\n<li>If a node is a normal face then exactly one of its children must also be a normal face.&nbsp;</li>\n</ol> \n<ul>\n</ul>\n<p>Let me call these the \"forward continuity of consciousness\" rules. These rules guarantee that there will be an unbroken line of normal faces from the root to a unique leaf. Some faces are happier than others, representing, for exmaple, financial loss or gain, though zombies can never be smiling, since that would be out of character. In the case of a simplified version of&nbsp;<a href=\"/lw/188/quantum_russian_roulette/\">Quantum Russian Roulette</a>, where I am the only player and Omega pays the reward iff the quantum die comes up \"6\", we might draw a decision tree like this:&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \"><img src=\"http://images.lesswrong.com/t3_189_1.png\" alt=\"\" width=\"464\" height=\"302\" /></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \">The game looks attractive, since the only way of decorating it that is consistent with&nbsp;the \"forward continuity of consciousness\" rules places the worldline of my conscious experience such that I will <em>experience </em>getting the reward, and the zombie-me's will lose the money, and then get killed. It is a shame that they will die, but it isn't that bad, because they are not me, I do not experience being them; killing a collection of beings who had a breif existence and that are a lot like me is not so great, but <em>dying myself </em>is much worse.&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \">Our intuitions about forward continuity of our own conscious experience, in particular that at each stage there must be a unique answer to the question \"what will I be experiencing at that point in time?\" are important to us, but I think that they are fundamentally mistaken; in the end, the word <em>\"I\"</em> comes with a semantics that is incompatible with what we know about physics, namely that the process in our brains that generates \"<em>I</em>-ness\" is capable of being duplicated with no difference between the copies. Of course a lot of ink has been spilled over the issue. The MWI of quantum mechanics dictates that I am being copied at a frightening rate, as the quantum system that I label as \"me\" interacts with other systems around it, such as incoming photons. The notion of quantum immortality comes from pushing the \"unique unbroken line of conscious experience\" to its logical conclusion: you will never experience your own death, rather you will experience a string of increasingly unlikley events that seem to be contrived just to keep you alive.&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \">In the comments for the&nbsp;<a href=\"/lw/188/quantum_russian_roulette/\">Quantum Russian Roulette</a>&nbsp;article, Vladimir Nesov says:&nbsp;</p>\n<blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \"><em>MWI is morally uninteresting, unless you do nontrivial quantum computation. ...&nbsp;when you are saying \"everyone survives in one of the worlds\", this statement gets intuitive approval (as opposed to doing the experiment in a deterministic world where all participants but one \"die completely\"), but there is no term in the expected utility calculation that corresponds to the sentiment everyone survives in one of the worlds\"</em></p>\n</blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \">The sentiment \"I will survive in one of the worlds\" corresponds to my intuition that my own subjective experience continuing, or not continuing, is of the upmost importance. Combine this with the intuition that the&nbsp;\"forward continuity of consciousness\" rules are correct and we get the intuition that in a copying scenario, killing all but one of the copies simply shifts the route that my worldline of conscious experience takes from one copy to another, so that the following tree represents the situation if only two copies of me will be killed:</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \"><img src=\"http://images.lesswrong.com/t3_189_3.png?v=8c2d6e13af978345ab87d379b7f4fd09\" alt=\"\" width=\"464\" height=\"302\" /></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \">The survival of some extra zombies seems to be of no benefit to me, because I wouldn't have experienced being them anyway. The reason that quantum mechanics and the MWI plays a role despite the fact that decision-theoretically the situation looks exactly the same as it would in a classical world - the utility calculations are the same - is that if we draw a tree where only one line of possibility is realized, we might encounter a situation where the&nbsp;\"forward continuity of consciousness\" rules have to be broken - actual death:</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \"><img src=\"http://images.lesswrong.com/t3_189_4.png?v=3bad8db4ca6b2c9e5509e4e7de9a42c3\" alt=\"\" width=\"508\" height=\"160\" /></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \">The interesting question is: why do I have a strong intuition that the \"forward continuity of consciousness\" rules are correct? Why does my existence feel smooth, unlike the topology of a branch point in a graph?&nbsp;</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \"><strong>ADDED:</strong></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \">The problem of how this all relates to sleep, anasthesia or cryopreservation has come up. When I was anesthetized, there appeared to be a sharp but instantaneous jump from the anasthetic room to the recovery room, indicating that our intuition about continuity of conscious experience treats \"go to sleep, wake up some time later\" as being rather like ordinary survival. This is puzzling, since a peroid of sleep or anaesthesia or even cryopreservation can be arbitrarily long.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MoFqnLnXDDG8WXMjB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 6, "extendedScore": null, "score": 5.232502119940669e-07, "legacy": true, "legacyId": "1593", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 92, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XH9ZN8bLidtcqMxY2", "gxxpK3eiSQ3XG3DW7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-18T18:52:50.278Z", "modifiedAt": null, "url": null, "title": "Minneapolis Meetup: Survey of interest", "slug": "minneapolis-meetup-survey-of-interest", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:30.461Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JustinShovelain", "createdAt": "2009-06-10T00:56:47.112Z", "isAdmin": false, "displayName": "JustinShovelain"}, "userId": "LEeresErqn3BpWrwG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NbDFqQ887mgKQACS7/minneapolis-meetup-survey-of-interest", "pageUrlRelative": "/posts/NbDFqQ887mgKQACS7/minneapolis-meetup-survey-of-interest", "linkUrl": "https://www.lesswrong.com/posts/NbDFqQ887mgKQACS7/minneapolis-meetup-survey-of-interest", "postedAtFormatted": "Friday, September 18th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Minneapolis%20Meetup%3A%20Survey%20of%20interest&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMinneapolis%20Meetup%3A%20Survey%20of%20interest%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNbDFqQ887mgKQACS7%2Fminneapolis-meetup-survey-of-interest%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Minneapolis%20Meetup%3A%20Survey%20of%20interest%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNbDFqQ887mgKQACS7%2Fminneapolis-meetup-survey-of-interest", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNbDFqQ887mgKQACS7%2Fminneapolis-meetup-survey-of-interest", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 91, "htmlBody": "<p>Frank Adamek and I are going to host a Less Wrong/Overcoming Bias meetup tentatively on Saturday September 26 at 3pm in Coffman Memorial Union at the University of Minnesota (there is a coffee shop and a food court there). Frank is the president of the University of Minnesota transhumanist group and some of them may be attending also. We'd like to gauge the level of interest so please comment if you'd be likely to attend.</p>\n<p>(ps. If you have any time conflicts or would like to suggest a better venue please comment)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NbDFqQ887mgKQACS7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 5.232715142047471e-07, "legacy": true, "legacyId": "1594", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-19T06:28:06.637Z", "modifiedAt": null, "url": null, "title": "Hypothetical Paradoxes", "slug": "hypothetical-paradoxes", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:33.617Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Psychohistorian", "createdAt": "2009-04-05T18:10:22.976Z", "isAdmin": false, "displayName": "Psychohistorian"}, "userId": "mAQgf4N8jv2jTMK5B", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8rhJ8EKQ46HYqECSh/hypothetical-paradoxes", "pageUrlRelative": "/posts/8rhJ8EKQ46HYqECSh/hypothetical-paradoxes", "linkUrl": "https://www.lesswrong.com/posts/8rhJ8EKQ46HYqECSh/hypothetical-paradoxes", "postedAtFormatted": "Saturday, September 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hypothetical%20Paradoxes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHypothetical%20Paradoxes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8rhJ8EKQ46HYqECSh%2Fhypothetical-paradoxes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hypothetical%20Paradoxes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8rhJ8EKQ46HYqECSh%2Fhypothetical-paradoxes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8rhJ8EKQ46HYqECSh%2Fhypothetical-paradoxes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1013, "htmlBody": "<div>\n<p>When we form hypotheticals, they must use entirely consistent and clear language, and avoid hiding complicated operations behind simple assumptions. In particular, with respect to decision theory, hypotheticals must employ a clear and consistent concept of free will, and they must make all information available to the theorizer available to the decider in the question. Failure to do either of these can make a hypothetical meaningless or self-contradictory if properly understood.</p>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Newcomb%27s_problem\">Newcomb's problem</a> and the the <a href=\"/lw/164/timeless_decision_theory_and_metacircular\">Smoking Lesion</a> fail to do both. I will argue that hidden assumptions in both problems imply internally contradictory concepts of free will, and thus both hypotheticals are incomprehensible and irrelevant when used to contradict decision theories.</p>\n<p>And I'll do it without math or programming! Metatheory is fun.</p>\n<p><a id=\"more\"></a></p>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Newcomb%27s_problem\">Newcomb's problem</a>, insofar as it is used as a refutation of causal decision theory, relies on convenient ignorance and a paradoxical concept of free will, though it takes some thinking to see why, because the concept of naive free will is such an innate part of human thought. In order for Newcomb's to work, there must exist some thing or set of things (\"A\") that very closely (even perfectly) link \"Omega predicts Y-boxing\" with \"Decider takes Y boxes.\" If there is no A, Omega cannot predict your behaviour. The existence of A is a fact necessary for the hypothetical and the decision maker should be aware of it, even if he doesn't know anything about how A generates a prediction.</p>\n<p>Newcomb's problem assumes two contradictory things about A. It assumes that, for the purpose of Causal Decision Theory, A is irrelevant and completely separated from your actual decision process; it assumes you have some kind of <a href=\"http://wiki.lesswrong.com/wiki/Free_will\">free will</a> such that you can decide to two-box without this decision having been reflected in A. It also assumes that, for purposes of the actual outcome, A is quite relevant; if you decided to two-box, your decision will have been reflected in A. This contradiction is the reason the problem seems complicated. If CDT were allowed to consider A, as it should be, it would realize:</p>\n<p style=\"padding-left: 30px;\">(B), \"I might not understand <em>how</em> it works, but my decision is somehow bound to the prediction in such a way that however I decide will have been predicted. Therefore, for all intents and purposes, even though my decision feels free, it is not, and, insofar as it feels free, deciding to one-box will cause that box to be filled, even if I can't begin to comprehend *how*.\"</p>\n<p>\"I should one-box\" follows rather clearly from this. If B is false, and your decision is *not* bound to the prediction, then you should two-box. To let the theorizer know that B is true, but to forbid the decider from using such knowledge is what makes Newcomb's being a \"problem.\" Newcomb's assumes that CDT operates with naive free will. It also assumes that naive free will is false and that Omega accurately employs purely deterministic free will. It is this paradox of simultaneously assuming naive free will *and* deterministic will that makes Necomb's problem a problem. CDT does not appear to be bound to assume naive free will, and therefore it seems capable of treating your \"free\" decision as causal, which it seems that it functionally must be.</p>\n<p>The Smoking Lesion problem relies on the same trick in reverse. There is, by necessary assumption, some C such that C causes smoking and cancer, but smoking does not actually cause cancer. The decider is utterly forbidden from thinking about what C is and how C might influence the decision under consideration. The *decision to smoke* very, very strongly predicts *being a smoker.*<sup>1</sup>Indeed, given that there is no question of being able to afford or find cigarettes, the outcome of the decision to smoke is <em>precisely what C predicts.</em> The desire to smoke is essential to the decision to smoke - under the hypothetical, if there were no desire, the decider would always decide not to smoke; if there is a desire and a low enough risk of cancer, the decider will always decide to smoke. Thus, the desire appears to correspond significantly (perhaps perfectly) with C, but Evidential Decision Theory is arbitrarily prevented from taking this into account. This is despite the fact that C is so well understood that we can say with absolute certainty that the correlation between smoking and cancer is completely explained by it.</p>\n<p>The problem forces EDT to assume that C operates deterministically on the decision, and that the decision is naively free. It requires that the decision to smoke both is and is not correlated with the desire to smoke - if it were correlated, EDT would consider this and significantly adjust the odds of getting cancer conditional on deciding to smoke *given* that there is a desire to smoke. Forcing the decider to assume a paradox proves nothing, so TSL fails to refute a evidential decision theory that actually uses all of the evidence given to it.</p>\n<p>Both TSL and Newcomb's exploit our intuitive understanding of free will to assume paradoxes, then uses these unrecognized paradoxes to undermine a decision strategy. As these problems force the decider to secretly assume a paradox, it is little surprise that they generate convoluted and problematic outputs. This suggests that the problem lies not in these decision theories, but in the challenge of fully and accurately translating our language to our decision maker's decision theory.</p>\n<p>Newcomb's, TSL, <a href=\"/lesswrong.com/lw/3l/counterfactual_mugging\">Counterfactual Mugging</a>, and the <a href=\"/lw/182/the_absentminded_driver\">Absent-Minded Driver</a> all have another larger, simpler problem, but it is practical rather than conceptual, so I'll address it in a subsequent post.</p>\n<p>1 - In the TSL version EY used in the link I provided, C is assumed to be \"a gene that causes a taste for cigarettes.\" Since the decider already *knows* they have a taste for cigarettes, Evidential Decision Theory should take this into account. If it does, it should assume that C is present (or present with high probability), and then the decision to smoke is obvious. Thus, the hypothetical I'm addressing is a more general version of TSL where C is not specified, only the existence of an acausal correlation is assumed.</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8rhJ8EKQ46HYqECSh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 13, "extendedScore": null, "score": 5.233876620577365e-07, "legacy": true, "legacyId": "1597", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fQv85Rd3pw789MHaX", "GfHdNfqxe3cSCfpHL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}