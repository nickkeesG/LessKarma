{"results": [{"createdAt": null, "postedAt": "2012-07-11T09:55:11.169Z", "modifiedAt": null, "url": null, "title": "Kurzweil's predictions: good accuracy, poor self-calibration", "slug": "kurzweil-s-predictions-good-accuracy-poor-self-calibration", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:09.022Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kK5rabDsKWMkup7gw/kurzweil-s-predictions-good-accuracy-poor-self-calibration", "pageUrlRelative": "/posts/kK5rabDsKWMkup7gw/kurzweil-s-predictions-good-accuracy-poor-self-calibration", "linkUrl": "https://www.lesswrong.com/posts/kK5rabDsKWMkup7gw/kurzweil-s-predictions-good-accuracy-poor-self-calibration", "postedAtFormatted": "Wednesday, July 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Kurzweil's%20predictions%3A%20good%20accuracy%2C%20poor%20self-calibration&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKurzweil's%20predictions%3A%20good%20accuracy%2C%20poor%20self-calibration%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkK5rabDsKWMkup7gw%2Fkurzweil-s-predictions-good-accuracy-poor-self-calibration%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Kurzweil's%20predictions%3A%20good%20accuracy%2C%20poor%20self-calibration%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkK5rabDsKWMkup7gw%2Fkurzweil-s-predictions-good-accuracy-poor-self-calibration", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkK5rabDsKWMkup7gw%2Fkurzweil-s-predictions-good-accuracy-poor-self-calibration", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2632, "htmlBody": "<p>Predictions of the future rely, to a much greater extent than in most fields, on the personal judgement of the expert making them. Just one problem - personal expert judgement generally&nbsp;<a href=\"http://press.princeton.edu/titles/7959.html\">sucks</a>, especially when the experts don't receive immediate feedback on their hits and misses. Formal models perform better than experts, but when talking about unprecedented future events such as nanotechnology or AI, the choice of the model is also dependent on expert judgement.</p>\n<p>Ray Kurzweil has a model of technological intelligence development where, broadly speaking, evolution, pre-computer technological development, post-computer technological development and future AIs all fit into the <a href=\"http://en.wikipedia.org/wiki/Accelerating_change\">same exponential increase</a>. When assessing the validity of that model, we could look at Kurzweil's credentials, and maybe compare them with those of his critics - but Kurzweil has given us something even better than credentials, and that's a track record. In various books, he's made predictions about what would happen in 2009, and we're now in a position to judge their accuracy. I haven't been satisfied by the <a href=\"http://spectrum.ieee.org/computing/software/ray-kurzweils-slippery-futurism\">various</a>&nbsp;<a href=\"http://www.markdice.com/index.php?option=com_content&amp;view=article&amp;id=132:an-analysis-of-ray-kurzweils-predictions&amp;catid=66:articles-by-mark-dice&amp;Itemid=89\">accuracy</a>&nbsp;<a href=\"http://singularityhub.com/2011/01/04/kurzweil-defends-his-predictions-again-was-he-86-correct/\">ratings</a> I've <a href=\"http://www.forbes.com/sites/alexknapp/2012/03/21/ray-kurzweil-defends-his-2009-predictions/\">found</a> <a href=\"http://www.techi.com/2011/01/ray-kurzweils-tech-predictions-have-been-eerily-accurate/\">online</a>, so I decided to do my own.</p>\n<p><a href=\"http://spectrum.ieee.org/computing/software/ray-kurzweils-slippery-futurism\">Some</a> have argued that we should penalise predictions that \"lack originality\" or were \"anticipated by many sources\". But <a href=\"http://en.wikipedia.org/wiki/Hindsight_bias\">hindsight bias</a> means that we certainly judge many profoundly revolutionary past ideas as \"unoriginal\", simply because they are obvious today. And saying that other sources anticipated the ideas is worthless unless we can&nbsp;quantify&nbsp;how mainstream and believable those sources were.&nbsp;For these reasons, I'll focus only on the accuracy of the predictions, and make no judgement as to their ease or difficulty (unless they say things that were already true when the prediction was made).</p>\n<p>Conversely, I won't be giving any credit for \"near misses\": this has the hindsight problem in the other direction, where we fit potentially ambiguous predictions to what we know happened.&nbsp;I'll be strict about the meaning of the prediction, as written. A prediction in a published book is a form of communication, so if Kurzweil actually meant something different to what was written, then the fault is entirely his for not spelling it out unambiguously.</p>\n<p>One exception to that strictness: I'll be tolerant on the timeline, as I feel that a lot of the predictions were forced into a \"ten years from 1999\" format. So I'll estimate the prediction accurate if it happened at any point up to the end of 2011, if data is available.&nbsp;</p>\n<p>The number of predictions actually made seem to vary from source to source; I used my copy of \"<a href=\"http://en.wikipedia.org/wiki/The_Age_of_Spiritual_Machines\">The Age of Spiritual Machines</a>\", which seems to be the original 1999 edition. In the chapter \"2009\", I counted 63 prediction paragraphs. I then chose ten numbers at random between 1 and 63, and analysed those ten predictions for correctness (those wanting to skip directly to the final score can scroll down). Seeing Kurzweil's nationality and location, I will assume all prediction refer only to technologically advanced nations, and specifically to the United States if there is any doubt. Please feel free to comment on my judgements below; we may be able to build a Less Wrong consensus verdict. It would be best if you tried to reach your own conclusions before reading my verdict or anyone else's. Hence I present the ten predictions,&nbsp;initially&nbsp;without commentary:<a id=\"more\"></a></p>\n<ul>\n<li><strong>Prediction 5</strong>: Cables are disappearing. Communication between components, such as pointing devices, microphones, displays, printers and the occasional keyboard, uses short-distance wireless technology.</li>\n<li><strong>Prediction 7</strong>: The majority of text is created using&nbsp;continuous&nbsp;speech recognition (CSR) dictation software, but keyboards are still used. CSR is very accurate, far more so than the human transcriptionists who were used up until a few years ago. </li>\n<li><strong>Prediction 8</strong>: Also&nbsp;ubiquitous&nbsp;are language user interfaces (LUIs) which combine CSR and natural language recognition. For routine matters, such as simple business transactions and information inquiries, LUIs are quite responsive and precise. They tend to be narrowly focused, however, on specific types of tasks. LUIs are frequently combined with animated personalities. Interacting with an animated personality to conduct a purchase or make a reservation is like talking to a person using video conferencing, except the person is simulated. </li>\n<li><strong>Prediction 18</strong>: In the twentieth century, computers in schools were mostly on the trailing edge, with most effective learning from computers taking place in the home. Now in 2009, while schools are still not on the cutting edge, the profound importance of the computer as a knowledge tool is widely recognised. Computers play a central role in all facets of education, as they do in other spheres of life. </li>\n<li><strong>Prediction 20</strong>: Students of all ages typically have a computer of their own, which is a thin tabletlike device weighing under a pound with a very high resolution display suitable for&nbsp;reading. Students interact with their computers primarily by voice and by pointing with a device that looks like a pencil. Keyboards still exist, but most textual language is created by speaking. Learning materials are accessed through wireless communication. </li>\n<li><strong>Prediction 26</strong>: Print-to-speech reading devices for the blind are now very small, inexpensive, palm-sized devices that can read books (those that still exist in paper form) and other printed documents, and other real-world text such as signs and displays. These reading systems are equally adept at reading the trillions of electronic documents that are instantly available from the&nbsp;ubiquitous&nbsp;wireless worldwide network. </li>\n<li><strong>Prediction 29</strong>: Computer-controlled orthotic devices have been introduced. These \"walking machines\" enable paraplegics to walk and climb stairs. The prosthetic devices are not yet usable by all paraplegic persons, as many physically disabled persons have dysfunctional joints from years of disuse. However, the advent of orthotic walking systems is providing more motivations to have these joints replaced. </li>\n<li><strong>Prediction 44</strong>: Intelligent roads are in use, mainly for long-distance travel. Once your car's guidance system locks into the control sensors on one these highways, you can sit back and relax. Local roads, though, are still predominantly conventional. </li>\n<li><strong>Prediction 48</strong>: There is continuing concern with an underclass that the skill ladder has left far behind. The size of the underclass appears to be stable, however. Although not politically popular, the underclass is&nbsp;politically neutralised through public assistance and the generally high level of affluence. </li>\n<li><strong>Prediction 53</strong>: Beyond musical recordings, images, and movie videos, the most popular type of digital entertainment object is virtual experience software. These interactive virtual&nbsp;environments allow you to go whitewater rafting on virtual rivers, to hang-glide in a virtual Grand Canyon, or to engage in intimate encounters with your favourite movie star. Users also experience fantasy environments with no counterpart in the physical world. The visual and auditory experience of virtual reality is compelling, but tactile interaction is still limited. </li>\n</ul>\n<p>&nbsp;</p>\n<h2>Verdict</h2>\n<ul>\n</ul>\n<p>My scale for judging the predictions is: true, weakly true, weakly false, false.</p>\n<p><strong>Prediction 5</strong>: My office and the computer I'm typing on seem pretty full of cables. Nevertheless, it is true there has been a rise in wireless technology, and wireless computer components, even if they're not&nbsp;ubiquitous. I'll grade this as a <strong>weakly true</strong>.</p>\n<p><strong>Prediction 7</strong>: I have failed to find proper data for the first prediction.&nbsp;<a href=\"http://blog.oregonlive.com/commuting/2012/01/columnist_finds_that_hands-fre.html\">Anecdotally</a>, it certainly seems false - keyboards are still in&nbsp;ubiquitous&nbsp;use, and I've never personally seen anyone use voice recognition to write documents of any length or even to send texts (a few personal experiments with Siri notwithstanding). The second claim in false: according to an <a href=\"http://www.itl.nist.gov/iad/mig/publications/ASRhistory/index.html\">assessment</a> by the National Institute of Standards and Technology, the accuracy of CSR is still nowhere near surpassing human transcription. This leads extra credence to the first claim being false as well: without the diminished error rate, it's very hard to see CSR being used for the majority of text creation. <strong>False</strong>.</p>\n<p><strong>Prediction 8</strong>: Apart from the belief that the animated personality would be visual, this is a near-perfect description of Siri and similar assistants. The term \"ubiquitous\" is tricky, but if we interpret it to mean \"to be found everywhere\" (rather than \"everyone has one\"), then the prediction is&nbsp;<strong>weakly</strong>&nbsp;<strong>t</strong><strong>rue</strong>&nbsp;(knocked down from true because of the uncertainty about ubiquity).</p>\n<p><strong>Prediction 18</strong>: Without needing to do the research, I think we can take this claim as evidently <strong>true</strong>.</p>\n<p><strong>Prediction 20</strong>: All the stuff about voice recognition is false. The only device that fits that description today is the smartphone, which has <a href=\"http://articles.cnn.com/2010-12-17/tech/youth.cellphones.gahran_1_prepaid-phone-plans-teen-texting-mobile-users?_s=PM:TECH\">not</a> <a href=\"http://news.consumerreports.org/electronics/2011/07/teenage-smart-phone-use-triples-in-two-years.html\">achieved</a> penetration of more than 50% among teenagers in 2011 (teenagers are the median \"students of all ages\"; adding in university students <em>as well as</em> pre-teens should lower the proportion, not raise it). \"Learning materials are accessed through wireless communication\" is hard to interpret, as it doesn't give any estimate to what proportion of learning material we are talking about. So though we can give Kurzweil kudos for imagining something like the smartphone, the prediction is <strong>weakly&nbsp;</strong><strong>false</strong>.</p>\n<p><strong>Prediction 26</strong>: One can quibble about inexpensive, as the products seem to be in the <a href=\"http://careinnovations.hostedbyamazon.com/Intel-GE-Innovations-Portable-Capture-Station/dp/B0074MTKX0\">$600 range</a>, but those products certainly exist for book and magazine reading (though not for most signs and displays, as far as I can tell - certainly not in a form the blind can use). The second sentence is true for some <a href=\"http://en.wikipedia.org/wiki/Screen_reader\">screen readers</a>, making the prediction essentially <strong>true</strong>.</p>\n<p><strong>Prediction 29</strong>: 2009 timeline wrong, but <strong>true</strong> in&nbsp;<a href=\"http://www.slashgear.com/rewalk-exo-skeleton-gets-paraplegics-walking-again-03225775/\">later</a> <a href=\"http://www.nytimes.com/2012/05/15/health/losing-more-to-gain-more-amputees-once-unthinkable-choice.html?pagewanted=all\">years</a>.</p>\n<p><strong>Prediction 44</strong>: The relative quantifier in the last sentence (\"though, are still predominantly conventional\") makes it clear that we should expect intelligent highways to be common among long-distance highways - this isn't a few experimental roads we're talking about. Though we have a few self-driving cars, we have nothing like the intelligent roads implied in this prediction, which specifically implies that most cars on those roads will be self-driven. <strong>False</strong>.</p>\n<p><strong>Prediction 48</strong>: The first part of the prediction is true. The second sentence seems false, whether one measures the underclass through relative income (where inequality has been <a href=\"http://en.wikipedia.org/wiki/Income_inequality_in_the_United_States\">increasing</a>) or through an absolute standard of educational attainment (where the various graduating rates have gone up, implying the underclass is&nbsp;<a href=\"http://en.wikipedia.org/wiki/Educational_attainment_in_the_United_States\">decreasing</a>). There are other ways one could measure the underclass, giving different results. Since one could read the underclass as increasing or decreasing, should we take Kurzweil's claim that it is stable as the correct mean? No. All that means is that had he spelt out his claim in more detail at the time, it would likely have ended up false. Ambiguity does not make a false statement true. The last sentence is virtually impossible to confirm or infirm, so the whole prediction is <strong>weakly true and weakly false</strong>.</p>\n<p><strong>Prediction 53</strong>: This is a tricky one. The <a href=\"http://en.wikipedia.org/wiki/Wii\">Wii</a> and similar game consoles seem to fit the bill to some extent. However the tone suggests he is talking about a virtual reality experience, which is not what we currently have.&nbsp;So, does he mean virtual reality, or does he mean \"games like what they had in 1999, except with much better graphics and features\"?&nbsp;How would someone at the time have read the prediction? Again, ambiguity cannot be used to make a false statement true.&nbsp;I'm going to work on the assumption that had he merely meant \"graphics and features of video games will improve a lot\", he would have said so (certainly his prediction seems to promise much more than that). So the prediction is false.</p>\n<p>But what if he was talking about modern games? For a start, his initial sentence gets the relative size of the industries wrong (though that can be read as a throw-away statement rather than a prediction). He also doesn't consider things like Facebook games, which make up a large part of the games industry, and are certainly not interactive virtual environments.&nbsp;What about \"these virtual environments allow...\"? Well, the statement is possibly an utter triviality, claiming that games exist which feature rafting, hang-gliding or erotic situations (that was already true in 1999). Or it claims that&nbsp;features like these are a major component of the most&nbsp;<a href=\"http://www.imdb.com/search/title?sort=moviemeter,asc&amp;title_type=game&amp;year=2011,2011\">most popular games</a>&nbsp;today, which is false (now, if he'd said \"blowing things up with a&nbsp;marvellous&nbsp;amount of weapons...\"). Fantasy&nbsp;environment&nbsp;is a much more common feature, so, I'm taking that as correct. Under this interpretation, the prediction is weakly true and weakly false for games. In total, reading the statement either way, I'll classify it as (contentiously) <strong>weakly false</strong>.</p>\n<p><strong>Note</strong>: I did read Kurzweil's <a href=\"http://www.kurzweilai.net/images/How-My-Predictions-Are-Faring.pdf\">assessment</a> of his own predictions, <em>after</em> I had conducted my own analysis. In that assessment, nearly every ambiguous clause is interpreted in Kurzweil's favour. This could be Kurzweil twisting the predictions in his direction; it could be a blatant example of hindsight bias; or it could be that what Kurzweil meant to say was different from what he wrote. Unfortunately, there is no way for us to tell, so we must make do with what was written and interpret it as best <em>we</em> can.</p>\n<p>&nbsp;</p>\n<h2>Analysis</h2>\n<ul>\n</ul>\n<p>So, out of the ten predictions, five are to some extent true, four are to some extent false, and one is unclassifiable (reading through the rest of the predictions,&nbsp;completely&nbsp;informally, these proportions seem roughly correct).</p>\n<p>Now imagine Kurzweil as a predictor who gives predictions, each with independent probability p of bring true (alternately, assume that a fixed proportion p of the 63 predictions are true, and pretend 63 is high enough that we can treat p as&nbsp;continuous&nbsp;without much loss). If we start with a uniform prior on p between 0 and 1, then we can update given this data. Model prediction 48 as true or false with equal probability. Then the posterior must be proportional to (1-p)<sup>5</sup>p<sup>5</sup> + (1-p)<sup>4</sup>p<sup>6</sup>:</p>\n<p><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_diz_1.png?v=1694f552e007d00fc69e83712cebe003\" alt=\"\" width=\"50%\" height=\"50%\" /></p>\n<p>This has a mean above 54%, which I'd say is excellent. A prediction record over 50% for a decade that included huge increases in computer power, September 11th and the great recession is intuitively a very good one. Alas there is no central repository of prediction records from various futurists, but in the absence of that, his track record certainly feels impressive. Don't let the hindsight bias blind you to how hard this was, and don't simply think of every prediction as binary: generally, there are far more ways for a prediction to be false than there are for them to be true.</p>\n<p>On the other hand, if we look at Kurzweil's own ranking of the predictions he gave in the \"Age of Spiritual Machines\", he&nbsp;grades himself as having either <a href=\"http://www.acceleratingfuture.com/michael/blog/2010/01/kurzweils-2009-predictions/\">102 out of 108</a> or <a href=\"http://www.forbes.com/sites/alexknapp/2012/03/21/ray-kurzweil-defends-his-2009-predictions/\">127 out of 147</a> correct (with caveats that \"even the predictions that were considered 'wrong' in this report were not all wrong\"). I've plotted the lower 127/147&asymp;0.86&nbsp;accuracy on the above graph; that is very far from being a mean estimate (it's in the 99<sup>th</sup> percentile of the probability distribution). But let's give Kurzweil all we can: we'll reclassify the arguable <strong>prediction 53</strong> as being true (posterior proportional to&nbsp;(1-p)<sup>4</sup>p<sup>6</sup>&nbsp;+ (1-p)<sup>3</sup>p<sup>7</sup>):</p>\n<p><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_diz_0.png?v=123388d532afa21652e533843e218edc\" alt=\"\" width=\"50%\" height=\"50%\" /></p>\n<p>That is still not enough to make his accuracy estimate reasonable: his estimate is in the 96<sup>th</sup> percentile of the probability distribution. Let's be even more generous: let's reclassify the intermediate <strong>prediction 48</strong> as also being true (posterior proportional to&nbsp;(1-p)<sup>3</sup>p<sup>7</sup>):</p>\n<p><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_diz_2.png?v=81c3bfd9c369804f0e64132cf37ed345\" alt=\"\" width=\"50%\" height=\"50%\" /></p>\n<p>Those were very generous adjustments; changing two results is a lot from a sample of ten. But even with the most generous adjustments and taking Kurzweil's lowest estimate of his own accuracy, he is still extraordinarily overconfident: his estimate is in the 94<sup>th</sup> percentile of the probability distribution. For fun, I&nbsp;flipped&nbsp;another prediction from false to true: even then, his estimate is in the 81<sup>th</sup> percentile of the probability distribution (and recall that if we were rigorous about the timeline that Kurzweil claimed, at least one of the true prediction would be false).</p>\n<p>So what can this tell us about Kurzweil as a futurist, and about the predictions he makes? Essentially two points stand out:</p>\n<ol>\n<li>He's most likely good at predicting.</li>\n<li>He's most likely overconfident, reluctant to admit his misses, and hence unlikely to update on his failures.</li>\n</ol>\n<p>So I feel we should take Kurzweil's predictions as a good baseline, with much wider error bars and caveats, paying relatively less attention to those areas where we feel that being a good Bayesian updater becomes important. We should thus probably pay more attention to his models than to his interpretation of his models.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8daMDi9NEShyLqxth": 1, "8hPTCJbwJnLBmfpCX": 1, "33BrBRSrRQS4jEHdk": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kK5rabDsKWMkup7gw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 50, "extendedScore": null, "score": 0.000126, "legacy": true, "legacyId": "17531", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Predictions of the future rely, to a much greater extent than in most fields, on the personal judgement of the expert making them. Just one problem - personal expert judgement generally&nbsp;<a href=\"http://press.princeton.edu/titles/7959.html\">sucks</a>, especially when the experts don't receive immediate feedback on their hits and misses. Formal models perform better than experts, but when talking about unprecedented future events such as nanotechnology or AI, the choice of the model is also dependent on expert judgement.</p>\n<p>Ray Kurzweil has a model of technological intelligence development where, broadly speaking, evolution, pre-computer technological development, post-computer technological development and future AIs all fit into the <a href=\"http://en.wikipedia.org/wiki/Accelerating_change\">same exponential increase</a>. When assessing the validity of that model, we could look at Kurzweil's credentials, and maybe compare them with those of his critics - but Kurzweil has given us something even better than credentials, and that's a track record. In various books, he's made predictions about what would happen in 2009, and we're now in a position to judge their accuracy. I haven't been satisfied by the <a href=\"http://spectrum.ieee.org/computing/software/ray-kurzweils-slippery-futurism\">various</a>&nbsp;<a href=\"http://www.markdice.com/index.php?option=com_content&amp;view=article&amp;id=132:an-analysis-of-ray-kurzweils-predictions&amp;catid=66:articles-by-mark-dice&amp;Itemid=89\">accuracy</a>&nbsp;<a href=\"http://singularityhub.com/2011/01/04/kurzweil-defends-his-predictions-again-was-he-86-correct/\">ratings</a> I've <a href=\"http://www.forbes.com/sites/alexknapp/2012/03/21/ray-kurzweil-defends-his-2009-predictions/\">found</a> <a href=\"http://www.techi.com/2011/01/ray-kurzweils-tech-predictions-have-been-eerily-accurate/\">online</a>, so I decided to do my own.</p>\n<p><a href=\"http://spectrum.ieee.org/computing/software/ray-kurzweils-slippery-futurism\">Some</a> have argued that we should penalise predictions that \"lack originality\" or were \"anticipated by many sources\". But <a href=\"http://en.wikipedia.org/wiki/Hindsight_bias\">hindsight bias</a> means that we certainly judge many profoundly revolutionary past ideas as \"unoriginal\", simply because they are obvious today. And saying that other sources anticipated the ideas is worthless unless we can&nbsp;quantify&nbsp;how mainstream and believable those sources were.&nbsp;For these reasons, I'll focus only on the accuracy of the predictions, and make no judgement as to their ease or difficulty (unless they say things that were already true when the prediction was made).</p>\n<p>Conversely, I won't be giving any credit for \"near misses\": this has the hindsight problem in the other direction, where we fit potentially ambiguous predictions to what we know happened.&nbsp;I'll be strict about the meaning of the prediction, as written. A prediction in a published book is a form of communication, so if Kurzweil actually meant something different to what was written, then the fault is entirely his for not spelling it out unambiguously.</p>\n<p>One exception to that strictness: I'll be tolerant on the timeline, as I feel that a lot of the predictions were forced into a \"ten years from 1999\" format. So I'll estimate the prediction accurate if it happened at any point up to the end of 2011, if data is available.&nbsp;</p>\n<p>The number of predictions actually made seem to vary from source to source; I used my copy of \"<a href=\"http://en.wikipedia.org/wiki/The_Age_of_Spiritual_Machines\">The Age of Spiritual Machines</a>\", which seems to be the original 1999 edition. In the chapter \"2009\", I counted 63 prediction paragraphs. I then chose ten numbers at random between 1 and 63, and analysed those ten predictions for correctness (those wanting to skip directly to the final score can scroll down). Seeing Kurzweil's nationality and location, I will assume all prediction refer only to technologically advanced nations, and specifically to the United States if there is any doubt. Please feel free to comment on my judgements below; we may be able to build a Less Wrong consensus verdict. It would be best if you tried to reach your own conclusions before reading my verdict or anyone else's. Hence I present the ten predictions,&nbsp;initially&nbsp;without commentary:<a id=\"more\"></a></p>\n<ul>\n<li><strong>Prediction 5</strong>: Cables are disappearing. Communication between components, such as pointing devices, microphones, displays, printers and the occasional keyboard, uses short-distance wireless technology.</li>\n<li><strong>Prediction 7</strong>: The majority of text is created using&nbsp;continuous&nbsp;speech recognition (CSR) dictation software, but keyboards are still used. CSR is very accurate, far more so than the human transcriptionists who were used up until a few years ago. </li>\n<li><strong>Prediction 8</strong>: Also&nbsp;ubiquitous&nbsp;are language user interfaces (LUIs) which combine CSR and natural language recognition. For routine matters, such as simple business transactions and information inquiries, LUIs are quite responsive and precise. They tend to be narrowly focused, however, on specific types of tasks. LUIs are frequently combined with animated personalities. Interacting with an animated personality to conduct a purchase or make a reservation is like talking to a person using video conferencing, except the person is simulated. </li>\n<li><strong>Prediction 18</strong>: In the twentieth century, computers in schools were mostly on the trailing edge, with most effective learning from computers taking place in the home. Now in 2009, while schools are still not on the cutting edge, the profound importance of the computer as a knowledge tool is widely recognised. Computers play a central role in all facets of education, as they do in other spheres of life. </li>\n<li><strong>Prediction 20</strong>: Students of all ages typically have a computer of their own, which is a thin tabletlike device weighing under a pound with a very high resolution display suitable for&nbsp;reading. Students interact with their computers primarily by voice and by pointing with a device that looks like a pencil. Keyboards still exist, but most textual language is created by speaking. Learning materials are accessed through wireless communication. </li>\n<li><strong>Prediction 26</strong>: Print-to-speech reading devices for the blind are now very small, inexpensive, palm-sized devices that can read books (those that still exist in paper form) and other printed documents, and other real-world text such as signs and displays. These reading systems are equally adept at reading the trillions of electronic documents that are instantly available from the&nbsp;ubiquitous&nbsp;wireless worldwide network. </li>\n<li><strong>Prediction 29</strong>: Computer-controlled orthotic devices have been introduced. These \"walking machines\" enable paraplegics to walk and climb stairs. The prosthetic devices are not yet usable by all paraplegic persons, as many physically disabled persons have dysfunctional joints from years of disuse. However, the advent of orthotic walking systems is providing more motivations to have these joints replaced. </li>\n<li><strong>Prediction 44</strong>: Intelligent roads are in use, mainly for long-distance travel. Once your car's guidance system locks into the control sensors on one these highways, you can sit back and relax. Local roads, though, are still predominantly conventional. </li>\n<li><strong>Prediction 48</strong>: There is continuing concern with an underclass that the skill ladder has left far behind. The size of the underclass appears to be stable, however. Although not politically popular, the underclass is&nbsp;politically neutralised through public assistance and the generally high level of affluence. </li>\n<li><strong>Prediction 53</strong>: Beyond musical recordings, images, and movie videos, the most popular type of digital entertainment object is virtual experience software. These interactive virtual&nbsp;environments allow you to go whitewater rafting on virtual rivers, to hang-glide in a virtual Grand Canyon, or to engage in intimate encounters with your favourite movie star. Users also experience fantasy environments with no counterpart in the physical world. The visual and auditory experience of virtual reality is compelling, but tactile interaction is still limited. </li>\n</ul>\n<p>&nbsp;</p>\n<h2 id=\"Verdict\">Verdict</h2>\n<ul>\n</ul>\n<p>My scale for judging the predictions is: true, weakly true, weakly false, false.</p>\n<p><strong>Prediction 5</strong>: My office and the computer I'm typing on seem pretty full of cables. Nevertheless, it is true there has been a rise in wireless technology, and wireless computer components, even if they're not&nbsp;ubiquitous. I'll grade this as a <strong>weakly true</strong>.</p>\n<p><strong>Prediction 7</strong>: I have failed to find proper data for the first prediction.&nbsp;<a href=\"http://blog.oregonlive.com/commuting/2012/01/columnist_finds_that_hands-fre.html\">Anecdotally</a>, it certainly seems false - keyboards are still in&nbsp;ubiquitous&nbsp;use, and I've never personally seen anyone use voice recognition to write documents of any length or even to send texts (a few personal experiments with Siri notwithstanding). The second claim in false: according to an <a href=\"http://www.itl.nist.gov/iad/mig/publications/ASRhistory/index.html\">assessment</a> by the National Institute of Standards and Technology, the accuracy of CSR is still nowhere near surpassing human transcription. This leads extra credence to the first claim being false as well: without the diminished error rate, it's very hard to see CSR being used for the majority of text creation. <strong>False</strong>.</p>\n<p><strong>Prediction 8</strong>: Apart from the belief that the animated personality would be visual, this is a near-perfect description of Siri and similar assistants. The term \"ubiquitous\" is tricky, but if we interpret it to mean \"to be found everywhere\" (rather than \"everyone has one\"), then the prediction is&nbsp;<strong>weakly</strong>&nbsp;<strong>t</strong><strong>rue</strong>&nbsp;(knocked down from true because of the uncertainty about ubiquity).</p>\n<p><strong>Prediction 18</strong>: Without needing to do the research, I think we can take this claim as evidently <strong>true</strong>.</p>\n<p><strong>Prediction 20</strong>: All the stuff about voice recognition is false. The only device that fits that description today is the smartphone, which has <a href=\"http://articles.cnn.com/2010-12-17/tech/youth.cellphones.gahran_1_prepaid-phone-plans-teen-texting-mobile-users?_s=PM:TECH\">not</a> <a href=\"http://news.consumerreports.org/electronics/2011/07/teenage-smart-phone-use-triples-in-two-years.html\">achieved</a> penetration of more than 50% among teenagers in 2011 (teenagers are the median \"students of all ages\"; adding in university students <em>as well as</em> pre-teens should lower the proportion, not raise it). \"Learning materials are accessed through wireless communication\" is hard to interpret, as it doesn't give any estimate to what proportion of learning material we are talking about. So though we can give Kurzweil kudos for imagining something like the smartphone, the prediction is <strong>weakly&nbsp;</strong><strong>false</strong>.</p>\n<p><strong>Prediction 26</strong>: One can quibble about inexpensive, as the products seem to be in the <a href=\"http://careinnovations.hostedbyamazon.com/Intel-GE-Innovations-Portable-Capture-Station/dp/B0074MTKX0\">$600 range</a>, but those products certainly exist for book and magazine reading (though not for most signs and displays, as far as I can tell - certainly not in a form the blind can use). The second sentence is true for some <a href=\"http://en.wikipedia.org/wiki/Screen_reader\">screen readers</a>, making the prediction essentially <strong>true</strong>.</p>\n<p><strong>Prediction 29</strong>: 2009 timeline wrong, but <strong>true</strong> in&nbsp;<a href=\"http://www.slashgear.com/rewalk-exo-skeleton-gets-paraplegics-walking-again-03225775/\">later</a> <a href=\"http://www.nytimes.com/2012/05/15/health/losing-more-to-gain-more-amputees-once-unthinkable-choice.html?pagewanted=all\">years</a>.</p>\n<p><strong>Prediction 44</strong>: The relative quantifier in the last sentence (\"though, are still predominantly conventional\") makes it clear that we should expect intelligent highways to be common among long-distance highways - this isn't a few experimental roads we're talking about. Though we have a few self-driving cars, we have nothing like the intelligent roads implied in this prediction, which specifically implies that most cars on those roads will be self-driven. <strong>False</strong>.</p>\n<p><strong>Prediction 48</strong>: The first part of the prediction is true. The second sentence seems false, whether one measures the underclass through relative income (where inequality has been <a href=\"http://en.wikipedia.org/wiki/Income_inequality_in_the_United_States\">increasing</a>) or through an absolute standard of educational attainment (where the various graduating rates have gone up, implying the underclass is&nbsp;<a href=\"http://en.wikipedia.org/wiki/Educational_attainment_in_the_United_States\">decreasing</a>). There are other ways one could measure the underclass, giving different results. Since one could read the underclass as increasing or decreasing, should we take Kurzweil's claim that it is stable as the correct mean? No. All that means is that had he spelt out his claim in more detail at the time, it would likely have ended up false. Ambiguity does not make a false statement true. The last sentence is virtually impossible to confirm or infirm, so the whole prediction is <strong>weakly true and weakly false</strong>.</p>\n<p><strong>Prediction 53</strong>: This is a tricky one. The <a href=\"http://en.wikipedia.org/wiki/Wii\">Wii</a> and similar game consoles seem to fit the bill to some extent. However the tone suggests he is talking about a virtual reality experience, which is not what we currently have.&nbsp;So, does he mean virtual reality, or does he mean \"games like what they had in 1999, except with much better graphics and features\"?&nbsp;How would someone at the time have read the prediction? Again, ambiguity cannot be used to make a false statement true.&nbsp;I'm going to work on the assumption that had he merely meant \"graphics and features of video games will improve a lot\", he would have said so (certainly his prediction seems to promise much more than that). So the prediction is false.</p>\n<p>But what if he was talking about modern games? For a start, his initial sentence gets the relative size of the industries wrong (though that can be read as a throw-away statement rather than a prediction). He also doesn't consider things like Facebook games, which make up a large part of the games industry, and are certainly not interactive virtual environments.&nbsp;What about \"these virtual environments allow...\"? Well, the statement is possibly an utter triviality, claiming that games exist which feature rafting, hang-gliding or erotic situations (that was already true in 1999). Or it claims that&nbsp;features like these are a major component of the most&nbsp;<a href=\"http://www.imdb.com/search/title?sort=moviemeter,asc&amp;title_type=game&amp;year=2011,2011\">most popular games</a>&nbsp;today, which is false (now, if he'd said \"blowing things up with a&nbsp;marvellous&nbsp;amount of weapons...\"). Fantasy&nbsp;environment&nbsp;is a much more common feature, so, I'm taking that as correct. Under this interpretation, the prediction is weakly true and weakly false for games. In total, reading the statement either way, I'll classify it as (contentiously) <strong>weakly false</strong>.</p>\n<p><strong>Note</strong>: I did read Kurzweil's <a href=\"http://www.kurzweilai.net/images/How-My-Predictions-Are-Faring.pdf\">assessment</a> of his own predictions, <em>after</em> I had conducted my own analysis. In that assessment, nearly every ambiguous clause is interpreted in Kurzweil's favour. This could be Kurzweil twisting the predictions in his direction; it could be a blatant example of hindsight bias; or it could be that what Kurzweil meant to say was different from what he wrote. Unfortunately, there is no way for us to tell, so we must make do with what was written and interpret it as best <em>we</em> can.</p>\n<p>&nbsp;</p>\n<h2 id=\"Analysis\">Analysis</h2>\n<ul>\n</ul>\n<p>So, out of the ten predictions, five are to some extent true, four are to some extent false, and one is unclassifiable (reading through the rest of the predictions,&nbsp;completely&nbsp;informally, these proportions seem roughly correct).</p>\n<p>Now imagine Kurzweil as a predictor who gives predictions, each with independent probability p of bring true (alternately, assume that a fixed proportion p of the 63 predictions are true, and pretend 63 is high enough that we can treat p as&nbsp;continuous&nbsp;without much loss). If we start with a uniform prior on p between 0 and 1, then we can update given this data. Model prediction 48 as true or false with equal probability. Then the posterior must be proportional to (1-p)<sup>5</sup>p<sup>5</sup> + (1-p)<sup>4</sup>p<sup>6</sup>:</p>\n<p><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_diz_1.png?v=1694f552e007d00fc69e83712cebe003\" alt=\"\" width=\"50%\" height=\"50%\"></p>\n<p>This has a mean above 54%, which I'd say is excellent. A prediction record over 50% for a decade that included huge increases in computer power, September 11th and the great recession is intuitively a very good one. Alas there is no central repository of prediction records from various futurists, but in the absence of that, his track record certainly feels impressive. Don't let the hindsight bias blind you to how hard this was, and don't simply think of every prediction as binary: generally, there are far more ways for a prediction to be false than there are for them to be true.</p>\n<p>On the other hand, if we look at Kurzweil's own ranking of the predictions he gave in the \"Age of Spiritual Machines\", he&nbsp;grades himself as having either <a href=\"http://www.acceleratingfuture.com/michael/blog/2010/01/kurzweils-2009-predictions/\">102 out of 108</a> or <a href=\"http://www.forbes.com/sites/alexknapp/2012/03/21/ray-kurzweil-defends-his-2009-predictions/\">127 out of 147</a> correct (with caveats that \"even the predictions that were considered 'wrong' in this report were not all wrong\"). I've plotted the lower 127/147\u22480.86&nbsp;accuracy on the above graph; that is very far from being a mean estimate (it's in the 99<sup>th</sup> percentile of the probability distribution). But let's give Kurzweil all we can: we'll reclassify the arguable <strong>prediction 53</strong> as being true (posterior proportional to&nbsp;(1-p)<sup>4</sup>p<sup>6</sup>&nbsp;+ (1-p)<sup>3</sup>p<sup>7</sup>):</p>\n<p><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_diz_0.png?v=123388d532afa21652e533843e218edc\" alt=\"\" width=\"50%\" height=\"50%\"></p>\n<p>That is still not enough to make his accuracy estimate reasonable: his estimate is in the 96<sup>th</sup> percentile of the probability distribution. Let's be even more generous: let's reclassify the intermediate <strong>prediction 48</strong> as also being true (posterior proportional to&nbsp;(1-p)<sup>3</sup>p<sup>7</sup>):</p>\n<p><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_diz_2.png?v=81c3bfd9c369804f0e64132cf37ed345\" alt=\"\" width=\"50%\" height=\"50%\"></p>\n<p>Those were very generous adjustments; changing two results is a lot from a sample of ten. But even with the most generous adjustments and taking Kurzweil's lowest estimate of his own accuracy, he is still extraordinarily overconfident: his estimate is in the 94<sup>th</sup> percentile of the probability distribution. For fun, I&nbsp;flipped&nbsp;another prediction from false to true: even then, his estimate is in the 81<sup>th</sup> percentile of the probability distribution (and recall that if we were rigorous about the timeline that Kurzweil claimed, at least one of the true prediction would be false).</p>\n<p>So what can this tell us about Kurzweil as a futurist, and about the predictions he makes? Essentially two points stand out:</p>\n<ol>\n<li>He's most likely good at predicting.</li>\n<li>He's most likely overconfident, reluctant to admit his misses, and hence unlikely to update on his failures.</li>\n</ol>\n<p>So I feel we should take Kurzweil's predictions as a good baseline, with much wider error bars and caveats, paying relatively less attention to those areas where we feel that being a good Bayesian updater becomes important. We should thus probably pay more attention to his models than to his interpretation of his models.</p>", "sections": [{"title": "Verdict", "anchor": "Verdict", "level": 1}, {"title": "Analysis", "anchor": "Analysis", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "39 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-11T11:56:57.489Z", "modifiedAt": null, "url": null, "title": "Useful maxims", "slug": "useful-maxims", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:51.475Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kygDQHagScPj87KFP/useful-maxims", "pageUrlRelative": "/posts/kygDQHagScPj87KFP/useful-maxims", "linkUrl": "https://www.lesswrong.com/posts/kygDQHagScPj87KFP/useful-maxims", "postedAtFormatted": "Wednesday, July 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Useful%20maxims&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUseful%20maxims%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkygDQHagScPj87KFP%2Fuseful-maxims%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Useful%20maxims%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkygDQHagScPj87KFP%2Fuseful-maxims", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkygDQHagScPj87KFP%2Fuseful-maxims", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 156, "htmlBody": "<p>In a New York shop, I once got pressure-sold something expensive I didn't really want; when I said it cost too much, I was asked what I might be prepared to pay, and we ended up haggling. &nbsp;Since then, I've had a rule:</p>\n<p>\n<ul>\n<li>If it's a non trivial price, never decide to buy while you're in the shop</li>\n</ul>\n</p>\n<p>and I have been very glad of it on many occasions. &nbsp;I can go for a short walk to decide, and if I don't want it, I simply don't return to the shop. &nbsp;This means I'm deciding in calm surroundings, based on what I want rather than on embarrassment.</p>\n<p>Are there other maxims I could adopt that would serve me equally well?</p>\n<p>(Personal note: I'm in the Bay Area for a week after minicamp, Sunday July 29th to Sunday August 5th. Let's hang out, go to things together, help make my visit cooler! Mail me: paul at ciphergoth.org. Thanks!)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vcvfjGJwRmFbMMS3d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kygDQHagScPj87KFP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 38, "extendedScore": null, "score": 9.404788952293845e-07, "legacy": true, "legacyId": "17554", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 111, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-11T17:41:46.777Z", "modifiedAt": null, "url": null, "title": "Moderate alcohol consumption inversely correlated with all-cause mortality", "slug": "moderate-alcohol-consumption-inversely-correlated-with-all", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:57.920Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "bfq5YorFxpih9j6nL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fucMGWg5K7swfHHYY/moderate-alcohol-consumption-inversely-correlated-with-all", "pageUrlRelative": "/posts/fucMGWg5K7swfHHYY/moderate-alcohol-consumption-inversely-correlated-with-all", "linkUrl": "https://www.lesswrong.com/posts/fucMGWg5K7swfHHYY/moderate-alcohol-consumption-inversely-correlated-with-all", "postedAtFormatted": "Wednesday, July 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Moderate%20alcohol%20consumption%20inversely%20correlated%20with%20all-cause%20mortality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AModerate%20alcohol%20consumption%20inversely%20correlated%20with%20all-cause%20mortality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfucMGWg5K7swfHHYY%2Fmoderate-alcohol-consumption-inversely-correlated-with-all%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Moderate%20alcohol%20consumption%20inversely%20correlated%20with%20all-cause%20mortality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfucMGWg5K7swfHHYY%2Fmoderate-alcohol-consumption-inversely-correlated-with-all", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfucMGWg5K7swfHHYY%2Fmoderate-alcohol-consumption-inversely-correlated-with-all", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 98, "htmlBody": "<p>My roommate recently sent me a review article that LW might find interesting:</p>\n<blockquote>\n<p><strong style=\"margin: 0px; padding: 0px; border: 0px; font: inherit; vertical-align: baseline;\">Conclusions:</strong><span style=\"line-height: 1.5; color: #333333; font-family: Helvetica, Arial, Verdana, sans-serif;\">&nbsp;&nbsp;</span><span style=\"margin: 0px; padding: 0px; border: 0px; font-size: 13px; font: inherit; vertical-align: baseline;\">Low levels of alcohol intake (1-2 drinks per day for women and 2-4 drinks per day for men) are inversely associated with total mortality in both men and women. Our findings, while confirming the hazards of excess drinking, indicate potential windows of alcohol intake that may confer a net beneficial effect of moderate drinking, at least in terms of survival.</span></p>\n</blockquote>\n<p>Personal observation says that LWers tend not to drink very much or often. Perhaps that should change, to the degree suggested by the article?</p>\n<p>Full article <a href=\"http://archinte.jamanetwork.com/article.aspx?articleid=769554\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fucMGWg5K7swfHHYY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 4, "extendedScore": null, "score": 9.406414187966105e-07, "legacy": true, "legacyId": "17559", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 78, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-11T22:26:40.730Z", "modifiedAt": null, "url": null, "title": "Rational Ethics", "slug": "rational-ethics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:38.311Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OrphanWilde", "createdAt": "2012-06-21T18:24:36.749Z", "isAdmin": false, "displayName": "OrphanWilde"}, "userId": "cdW87AS6fdK2sywL2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KYY7bz6uPsJzyQ5ED/rational-ethics", "pageUrlRelative": "/posts/KYY7bz6uPsJzyQ5ED/rational-ethics", "linkUrl": "https://www.lesswrong.com/posts/KYY7bz6uPsJzyQ5ED/rational-ethics", "postedAtFormatted": "Wednesday, July 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20Ethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20Ethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKYY7bz6uPsJzyQ5ED%2Frational-ethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20Ethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKYY7bz6uPsJzyQ5ED%2Frational-ethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKYY7bz6uPsJzyQ5ED%2Frational-ethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1119, "htmlBody": "<p>[Looking for feedback, particularly on links to related posts; I'd like to finish this out as a post on the main, provided there aren't too many wrinkles for it to be salvaged.]</p>\n<p><a href=\"/lw/sw/morality_as_fixed_computation/\">Morality as Fixed Computation</a>,&nbsp;<a href=\"/lw/t0/abstracted_idealized_dynamics/\">Abstracted Idealized Dynamics</a>, as part of the <a href=\"http://wiki.lesswrong.com/wiki/Metaethics_sequence\">Metaethics Sequence</a>,&nbsp;discuss ethics as computation. &nbsp;This is a post primarily a response to these two posts, which discuss computation, and the impossibility of computing the full ethical ramifications of an action. &nbsp;Note that I treat morality as objective, which means, loosely speaking, that two people who share the same ethical values should arrive, provided neither makes logical errors, at approximately the same ethical system.</p>\n<p>On to the subject matter of this post - are Bayesian utilitarian ethics utilitarian? &nbsp;For you? &nbsp;For most people?</p>\n<p>And, more specifically, is a rational ethics system more rational than a heuristics and culturally based one?</p>\n<p>I would argue that the answer is, for most people, \"No.\"</p>\n<p>The summary explanation of why: Because cultural ethics are functioning ethics. &nbsp;They have been tested, and work. &nbsp;They may not be ideal, but most of the \"ideal\" ethics systems that have been proposed in the past haven't worked. &nbsp;In terms of Eliezer's post, cultural ethics are the answers that other people have already agreed upon; they are ethical computations which have already been computed, and while there may be errors, most of the potential errors an ethicist might arrive upon have already been weeded out.</p>\n<p>The longer explanation of why:</p>\n<p>First and foremost, rationality, which I will use from here instead of the word \"computation,\" is -expensive-. &nbsp;\"A witch did it\", or the equivalent \"Magic!\", while not in fact conceptually simple, is in fact logically simple; the complexity is encoded in the concept, not the logic. &nbsp;The rational explanation for, say, static electricity, requires far more information about the universe, which for an individual who aspires to be a farmer because he likes growing things, may never be useful, and whose internalization may never pay for itself. &nbsp;It can be fully consistent with a rational attitude to accept irrational explanations, when you have no reasonable expectation that the rational explanation will provide any kind of benefit, or more exactly when the cost of the rational explanation exceeds its expected benefit.</p>\n<p>Or, to phrase it another way, it's not always rational to be rational.</p>\n<p><a href=\"/lw/l4/terminal_values_and_instrumental_values/\">Terminal Values versus Instrumental Values</a> discusses some of the computational expenses involved in ethics. &nbsp;It's a nontrivial problem.</p>\n<p>Rationality is a -means-, not an ends. &nbsp;A \"rational ethics system\" is merely an ethical system based on logic, on reason. &nbsp;But if you don't have a rational reason to adopt a rational ethics system, you're failing before you begin; logic is a formalized process, but it's still just a process. &nbsp;The reason for adopting a rational ethics system is the starting point, the beginning, of that process. &nbsp;If you don't have a beginning, what do you have? &nbsp;An ends? &nbsp;That's not rationality, that's rationalization.</p>\n<p>So the very first step in adopting a rational ethics system is determining -why- you want to adopt a rational ethics system. &nbsp;\"I want to be more rational\" is irrational.</p>\n<p>\"I want to know the truth\" is a better reason for wanting to be rational.</p>\n<p>But the question in turn must, of course, be \"Why?\"</p>\n<p>\"Truth has inherent value\" isn't an answer, because value isn't inherent, and certainly not to truth. &nbsp;There is a blue pillow in a cardboard box to my left. &nbsp;This is a true statement. &nbsp;You have truth. &nbsp;Are you more valuable now? &nbsp;Has this truth enriched your life? &nbsp;There are some circumstances in which this information might be useful to you, but you aren't in those circumstances, nor in any feasible universe will you be. &nbsp;It doesn't matter if I lied about the blue pillow. &nbsp;If truth has inherent value, then every true statement must, in turn, inherit that inherent value. &nbsp;Not all truth matters.</p>\n<p>A rational ethics system must have its axioms. &nbsp;\"Rationality,\" I hope I have established, is not a useful axiom, nor is \"Truth.\" &nbsp;It is the values that your ethics system seeks to maximize which are its most important axioms.</p>\n<p>The truths that matter are the truths which directly relate to your moral values, to your ethical axioms. &nbsp;A rational ethics system is a means of maximizing those values - nothing more.</p>\n<p>If you have a relatively simple set of axioms, a rational ethics system is relatively simple, if still potentially expensive to compute. &nbsp;Strict Randian Objectivism, for example, attempts to use human life as its sole primary axiom, which makes it a relatively simple ethical system. &nbsp;(I'm a less strict Objectivist, and use a different axiom, personal happiness, but this rarely leads to conflict with Randian Objectivism, which uses it as a secondary axiom.)</p>\n<p>If, on the other hand, you, like most people, have a wide variety of personal values which you are attempting to maximize, attempting to assess each action on its ethical merits becomes computationally prohibitive.</p>\n<p>Which is where heuristics, and inherited ethics, start to become pretty attractive, particularly when you share (and most people do, to more extent than they don't) your culture's ethical values.</p>\n<p>If you share at least some of your culture's ethical values, normative ethics can provide immense value to you, by eliminating most of the work necessary in evaluating ethical scenarios. &nbsp;You don't need to start from the bottom up, and prove to yourself that murder is wrong. &nbsp;You don't need to weigh the pros and cons of alcoholism. &nbsp;You don't need to prove that charity is a worthwhile thing to engage in.</p>\n<p>\"We all engage in ethics, though; it's not like a farmer with static electricity, don't we have a responsibility to understand ethics?\"</p>\n<p>My flippant response to this question is, should every driver know how to rebuild their car's transmission?</p>\n<p>You don't need to be a rationalist in order to reevaluate your ethics. &nbsp;An expert can rebuild your transmission - an expert can also pose arguments to change your mind. &nbsp;This has, indeed, happened before on mass scales; racism is no longer broadly acceptable in our society. &nbsp;It took too long, yes, -but-, a long-established ethics system, being well-tested, should require extraordinary efforts to change. &nbsp;If it were easily mutable, it would lose much of its value, for it would largely be composed of poorly-tested ideas.</p>\n<p>All of which is not to say that rational ethics are inherently irrational - only that one should have a rational reason for engaging in them to begin with. &nbsp;If you find that societal norms frequently conflict with your own ethical values, that is a good reason to engage in rational ethics. &nbsp;But if you don't, perhaps you shouldn't. &nbsp;And if you do, you should be cautious of pushing a rational ethics system on somebody for whom existing ethical systems do well, if your goal is to improve their well-being.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KYY7bz6uPsJzyQ5ED", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -5, "extendedScore": null, "score": -1.1e-05, "legacy": true, "legacyId": "17557", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FnJPa8E9ZG5xiLLp5", "9KacKm5yBv27rxWnJ", "n5ucT5ZbPdhfGNLtP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-12T00:17:00.016Z", "modifiedAt": null, "url": null, "title": "What Is Optimal Philanthropy?", "slug": "what-is-optimal-philanthropy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:56.438Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alyssavance", "createdAt": "2009-10-07T20:08:31.887Z", "isAdmin": false, "displayName": "alyssavance"}, "userId": "zQSAWAS5tnqtzp55N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mTeWmzrgtkZo7Ynt2/what-is-optimal-philanthropy", "pageUrlRelative": "/posts/mTeWmzrgtkZo7Ynt2/what-is-optimal-philanthropy", "linkUrl": "https://www.lesswrong.com/posts/mTeWmzrgtkZo7Ynt2/what-is-optimal-philanthropy", "postedAtFormatted": "Thursday, July 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20Is%20Optimal%20Philanthropy%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20Is%20Optimal%20Philanthropy%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmTeWmzrgtkZo7Ynt2%2Fwhat-is-optimal-philanthropy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20Is%20Optimal%20Philanthropy%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmTeWmzrgtkZo7Ynt2%2Fwhat-is-optimal-philanthropy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmTeWmzrgtkZo7Ynt2%2Fwhat-is-optimal-philanthropy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1610, "htmlBody": "<p>Much has been written about the idea of <a href=\"/lw/3gj/efficient_charity_do_unto_others/\"><em>optimal philanthropy</em></a>. Yet, it seems like optimal philanthropy isn't a single claim. Instead, it's a collection of related, but quite distinct, claims that have all been bundled together, much like the <a href=\"http://yudkowsky.net/singularity/schools\">Singularity</a>.</p>\n<p><a href=\"http://www.givewell.org/\">Here's</a> the website of GiveWell, and <a href=\"http://vimeo.com/panchenko/80000hours\">here's</a> the main video introduction for <a href=\"http://80000hours.org/\">80,000 Hours</a>, two of the major optimal philanthropy sites. I'll try to break them down into their component claims (written in <strong>bold</strong>), and also give my views on each of the claims. Some of them are explicitly stated, but others are more implicit, so I definitely welcome feedback if optimal philanthropists feel they disagree with some of the claims as stated.</p>\n<p><strong>1. We should evaluate charities according to how efficient they are, along some common metric - for example, number of lives saved per dollar, or existential risk reduction per dollar. We should then encourage charities to be more efficient, and selectively donate to (or otherwise help) the most efficient ones.</strong></p>\n<p>This one I support wholeheartedly. It's the main message of <a href=\"http://www.givewell.org/\">GiveWell</a>, and though I have disagreements with their methodology, the basic idea (of marginal utility evaluation) is one that must happen more often. People are <em>way</em> too prone, by default, to donate to the <a href=\"/lw/6z\">Society for Curing Rare Diseases in Cute Puppies</a>. Much has been written about this in <a href=\"/lw/6z\">Purchase Fuzzies And Utilons Separately</a>, and other Less Wrong posts.</p>\n<p><strong>2. In order to do the most good for our fellow humans, we should start/work for/donate to/otherwise become involved in charitable organizations.</strong></p>\n<p>Of course, this is widely believed outside the optimal philanthropy movement. But I think this belief is inherent in many optimal philanthropy claims, and it ought to be examined more critically. It's plausible, but if one looks at the total good done over the last thousand years, the vast majority comes from science and various businesses, not popular causes like (back then) \"tithe to the church\" or (now, from the 80,000 Hours video) \"campaigning against climate change\". (Examples: electricity, air travel, enough food, trains, air conditioning... ) However, it's also true that much more total effort has been put into for-profit organizations than non-profits. Which one is more efficient <em>per dollar</em>, I don't know, but it's a question worth examining, rather than just ignoring it by default.</p>\n<p><strong>3. We should design careers around being able to donate the largest possible amount.</strong></p>\n<p>This one I see as highly damaging. Human psychology is such that, in order for a movement to get long-term, voluntary participation by highly capable people, stuff needs to be <em>fun</em>. Less Wrong <a href=\"/lw/c1/wellkept_gardens_die_by_pacifism/\">itself</a>, and the New York Less Wrong <a href=\"/lw/5c0/epistle_to_the_new_york_less_wrongians/\">meetup group</a>, are two obvious examples. \"The more fun we have, the more people will want to join us.\"</p>\n<p>Of course, the <em>primary</em> purpose of a community or activity doesn't have to be fun. Eg., Google doesn't exist for its employees to have fun. But working for Google still <em>is</em> fun, and if it weren't, Google would soon start losing people, become less productive, and ultimately go bankrupt. (Disclosure: I am a former Google intern.)</p>\n<p>Writing a donation check can be very useful, but it isn't <em>fun</em> - it violates all the <a href=\"/lw/y0/31_laws_of_fun/\">principles of Fun Theory</a>. To go through the list, it isn't <em>novel</em>, doesn't involve <em>tackling new challenges</em>, doesn't <em>engage the senses</em>, doesn't <em>get better over time</em> (if we assume things work well, the marginal utility of dollars donated should go down, not up), doesn't involve <em>long-term personal consequences</em>, doesn't involve <em>freedom of action</em>, doesn't involve <em>personal control over politics</em> (assuming that one isn't personally involved in the charity, which is generally assumed), etc. etc. etc. (I'm referring to the actual act of writing the check here - for earning the money in the first place, see the next claim.)</p>\n<p>Not everything in life is fun, nor can it be, at least pre-Singularity. Taking out the garbage isn't fun, but I do it anyway. However, trying to <em>design lives around things that are inherently un-fun</em> will probably lead to bad outcomes.</p>\n<p><strong>4. People can donate the largest amount through a traditional \"high-earning career\", like investment banking. </strong></p>\n<p>This one involves, to some extent, the classic American confusion between<em> social class</em> and <em>income</em>. One might think of \"lawyer\" as a high-earning career, since it's an upper middle class career; you need a graduate degree and dress up in suits. However, lawyer is actually a terrible career from a money-making perspective, and a law degree usually leaves people worse off financially (details <a href=\"http://www.nytimes.com/2011/01/09/business/09law.html?_r=1&amp;pagewanted=all\">here</a>). Investment bankers themselves don't make that much money, except at the top levels (details <a href=\"http://www.mergersandinquisitions.com/investment-banking-salary/\">here</a>, and see <a href=\"/lw/43m/optimal_employment/\">here</a> for general analysis of why gross pay isn't money in the bank).</p>\n<p>In fact, all things being equal, one would expect a <em>negative</em> correlation between how prestigious a career is and how much money it makes. Prestige is, to some extent, a substitute for money - a musician might happily play for nothing, because being a musician is cool. \"Where there's muck, there's brass\" - for info on people who made millions doing boring stuff, see the excellent books <a href=\"http://www.amazon.com/The-Millionaire-Next-Door-Surprising/dp/1589795474/ref=sr_1_1?ie=UTF8&amp;qid=1342048778&amp;sr=8-1&amp;keywords=the+millionaire+next+door\">The Millionaire Next Door</a> and <a href=\"http://www.amazon.com/How-Get-Rich-Greatest-Entrepreneurs/dp/1591842719/ref=sr_1_1?ie=UTF8&amp;qid=1342048821&amp;sr=8-1&amp;keywords=how+to+get+rich\">How To Get Rich</a>.</p>\n<p>But, even supposing that a \"high-earning career\" actually pays a lot (eg., partner at a Big Law firm), standard \"career tracks\" have serious disadvantages, like working insane hours doing unpleasant stuff. They sap what I call human capital and social capital - human capital is your skills, capabilities, and the value you can provide to an organization, while social capital is your network of friends and people who want to work with you. Human capital and social capital are the two critical things one needs to do <em>anything</em>, including world saving; they shouldn't be spent lightly.</p>\n<p><strong>5. People are morally responsible for the opportunity costs of their actions.</strong></p>\n<p>This is somewhat tricky/ambiguous, so I've deliberately made the wording vague, but the best example I've found is Peter Singer's argument (analyzed <a href=\"https://www2.bc.edu/~mathiepa/classmag/schaefer.htm\">here</a>). Singer compares philanthropy to a Trolley Problem. There's a set of train tracks, which a child is lying on, and a train is fast approaching. You're driving a luxury car, and if you drive the car on the tracks, the train will run into the car and save the child. What should you do?</p>\n<p>In standard morality, the right thing to do is save the child, even if it means destroying your really expensive car. Indeed, we might socially shame someone who didn't. According to Singer, this means that we should be willing to donate any amount of money less than the price of an expensive car to charity, if it meant saving a life. Not donating to charity would be the same as letting the train run over the kid - murder through inaction.</p>\n<p>I haven't figured out in detail what the real moral framework should be, but this argument doesn't work. For one thing, it produces <em>atrocious</em> incentives. Suppose you have a nice, cushy software job, and donate 10% of your income to charity, even though you could easily afford 20%. You work really hard, and a year later, you get another job for twice as much money. If not donating surplus money is morally equivalent to causing whatever bad outcome the donation would prevent, you are now <em>twice as guilty</em>, since the amount you aren't donating (20% vs. 10%) is twice as large. This is despite the fact that the <em>total amount of good done</em> is also twice as large. Why punish an improvement?</p>\n<p>Another huge problem is the creation of <em>unbounded obligations</em>. I suspect a lot of thinking is inherently binary - you've either graduated college or you haven't, either paid back the loan or you haven't, either obeyed the rules or you haven't. With this line of argument, there's literally <em>no point</em> at which one can sit back and say, \"I've fulfilled my duty to charity - there's nothing more to do\". There's (short of FAI) always another child to save. One can never say, \"I've met the goal\", or even \"I've gotten a third of the way to the goal\", since the goal of solving all the world's problems is so huge. But if all states of the world - whether they be donating 0%, 10%, or 20% of income - result in 0% total goal fulfillment, then they're all equivalent, at least in some sense. A moral framework should make the good outcome and bad outcome as distinct as possible, not the same.</p>\n<p><strong>6. More people interested in doing good should become professional philanthropists. </strong></p>\n<p>This one I totally agree with, which might seem odd, given how closely related it is to #3 and #4. However, I think there are two important differences. A professional philanthropist is, typically, someone whose full-time job it is to figure out how to give away their money. But almost always, it's someone who <em>already has</em> lots of money. Historically, there isn't much precedent for people taking high-paying jobs and donating most of their salary... but there's lots of precedent for getting rich <em>first</em>, in whatever field, and then working full-time on donating.</p>\n<p>The other difference is that professional philanthropists don't <em>optimize for</em> donating the maximum amount. They see donating as good, but they also see it as a good to be traded off against other goods, like having lots of nice stuff and social respect. Optimizing for more than one thing allows one to have a lot more Fun, as I suspect Bill Gates and Warren Buffett do.</p>\n<p>This really does seem to be better than conventional routes of do-gooding. When I was in college, a huge number of people did stuff like fly to Africa to dig wells. This isn't just inefficient - it actually does net harm, since the cost of utilizing unskilled labor <a href=\"http://www.cracked.com/article_19899_5-popular-forms-charity-that-arent-helping_p2.html\">usually</a> <a href=\"/lw/uk/beyond_the_reach_of_god/tca\">outweighs</a> the benefits of such labor. Surely we can do better.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qAvbtzdG2A2RBn7in": 1, "4kQXps8dYsKJgaayN": 1, "TG8zMvjnhydE7Mcue": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mTeWmzrgtkZo7Ynt2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 40, "extendedScore": null, "score": 8.4e-05, "legacy": true, "legacyId": "17212", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Much has been written about the idea of <a href=\"/lw/3gj/efficient_charity_do_unto_others/\"><em>optimal philanthropy</em></a>. Yet, it seems like optimal philanthropy isn't a single claim. Instead, it's a collection of related, but quite distinct, claims that have all been bundled together, much like the <a href=\"http://yudkowsky.net/singularity/schools\">Singularity</a>.</p>\n<p><a href=\"http://www.givewell.org/\">Here's</a> the website of GiveWell, and <a href=\"http://vimeo.com/panchenko/80000hours\">here's</a> the main video introduction for <a href=\"http://80000hours.org/\">80,000 Hours</a>, two of the major optimal philanthropy sites. I'll try to break them down into their component claims (written in <strong>bold</strong>), and also give my views on each of the claims. Some of them are explicitly stated, but others are more implicit, so I definitely welcome feedback if optimal philanthropists feel they disagree with some of the claims as stated.</p>\n<p><strong id=\"1__We_should_evaluate_charities_according_to_how_efficient_they_are__along_some_common_metric___for_example__number_of_lives_saved_per_dollar__or_existential_risk_reduction_per_dollar__We_should_then_encourage_charities_to_be_more_efficient__and_selectively_donate_to__or_otherwise_help__the_most_efficient_ones_\">1. We should evaluate charities according to how efficient they are, along some common metric - for example, number of lives saved per dollar, or existential risk reduction per dollar. We should then encourage charities to be more efficient, and selectively donate to (or otherwise help) the most efficient ones.</strong></p>\n<p>This one I support wholeheartedly. It's the main message of <a href=\"http://www.givewell.org/\">GiveWell</a>, and though I have disagreements with their methodology, the basic idea (of marginal utility evaluation) is one that must happen more often. People are <em>way</em> too prone, by default, to donate to the <a href=\"/lw/6z\">Society for Curing Rare Diseases in Cute Puppies</a>. Much has been written about this in <a href=\"/lw/6z\">Purchase Fuzzies And Utilons Separately</a>, and other Less Wrong posts.</p>\n<p><strong id=\"2__In_order_to_do_the_most_good_for_our_fellow_humans__we_should_start_work_for_donate_to_otherwise_become_involved_in_charitable_organizations_\">2. In order to do the most good for our fellow humans, we should start/work for/donate to/otherwise become involved in charitable organizations.</strong></p>\n<p>Of course, this is widely believed outside the optimal philanthropy movement. But I think this belief is inherent in many optimal philanthropy claims, and it ought to be examined more critically. It's plausible, but if one looks at the total good done over the last thousand years, the vast majority comes from science and various businesses, not popular causes like (back then) \"tithe to the church\" or (now, from the 80,000 Hours video) \"campaigning against climate change\". (Examples: electricity, air travel, enough food, trains, air conditioning... ) However, it's also true that much more total effort has been put into for-profit organizations than non-profits. Which one is more efficient <em>per dollar</em>, I don't know, but it's a question worth examining, rather than just ignoring it by default.</p>\n<p><strong id=\"3__We_should_design_careers_around_being_able_to_donate_the_largest_possible_amount_\">3. We should design careers around being able to donate the largest possible amount.</strong></p>\n<p>This one I see as highly damaging. Human psychology is such that, in order for a movement to get long-term, voluntary participation by highly capable people, stuff needs to be <em>fun</em>. Less Wrong <a href=\"/lw/c1/wellkept_gardens_die_by_pacifism/\">itself</a>, and the New York Less Wrong <a href=\"/lw/5c0/epistle_to_the_new_york_less_wrongians/\">meetup group</a>, are two obvious examples. \"The more fun we have, the more people will want to join us.\"</p>\n<p>Of course, the <em>primary</em> purpose of a community or activity doesn't have to be fun. Eg., Google doesn't exist for its employees to have fun. But working for Google still <em>is</em> fun, and if it weren't, Google would soon start losing people, become less productive, and ultimately go bankrupt. (Disclosure: I am a former Google intern.)</p>\n<p>Writing a donation check can be very useful, but it isn't <em>fun</em> - it violates all the <a href=\"/lw/y0/31_laws_of_fun/\">principles of Fun Theory</a>. To go through the list, it isn't <em>novel</em>, doesn't involve <em>tackling new challenges</em>, doesn't <em>engage the senses</em>, doesn't <em>get better over time</em> (if we assume things work well, the marginal utility of dollars donated should go down, not up), doesn't involve <em>long-term personal consequences</em>, doesn't involve <em>freedom of action</em>, doesn't involve <em>personal control over politics</em> (assuming that one isn't personally involved in the charity, which is generally assumed), etc. etc. etc. (I'm referring to the actual act of writing the check here - for earning the money in the first place, see the next claim.)</p>\n<p>Not everything in life is fun, nor can it be, at least pre-Singularity. Taking out the garbage isn't fun, but I do it anyway. However, trying to <em>design lives around things that are inherently un-fun</em> will probably lead to bad outcomes.</p>\n<p><strong id=\"4__People_can_donate_the_largest_amount_through_a_traditional__high_earning_career___like_investment_banking__\">4. People can donate the largest amount through a traditional \"high-earning career\", like investment banking. </strong></p>\n<p>This one involves, to some extent, the classic American confusion between<em> social class</em> and <em>income</em>. One might think of \"lawyer\" as a high-earning career, since it's an upper middle class career; you need a graduate degree and dress up in suits. However, lawyer is actually a terrible career from a money-making perspective, and a law degree usually leaves people worse off financially (details <a href=\"http://www.nytimes.com/2011/01/09/business/09law.html?_r=1&amp;pagewanted=all\">here</a>). Investment bankers themselves don't make that much money, except at the top levels (details <a href=\"http://www.mergersandinquisitions.com/investment-banking-salary/\">here</a>, and see <a href=\"/lw/43m/optimal_employment/\">here</a> for general analysis of why gross pay isn't money in the bank).</p>\n<p>In fact, all things being equal, one would expect a <em>negative</em> correlation between how prestigious a career is and how much money it makes. Prestige is, to some extent, a substitute for money - a musician might happily play for nothing, because being a musician is cool. \"Where there's muck, there's brass\" - for info on people who made millions doing boring stuff, see the excellent books <a href=\"http://www.amazon.com/The-Millionaire-Next-Door-Surprising/dp/1589795474/ref=sr_1_1?ie=UTF8&amp;qid=1342048778&amp;sr=8-1&amp;keywords=the+millionaire+next+door\">The Millionaire Next Door</a> and <a href=\"http://www.amazon.com/How-Get-Rich-Greatest-Entrepreneurs/dp/1591842719/ref=sr_1_1?ie=UTF8&amp;qid=1342048821&amp;sr=8-1&amp;keywords=how+to+get+rich\">How To Get Rich</a>.</p>\n<p>But, even supposing that a \"high-earning career\" actually pays a lot (eg., partner at a Big Law firm), standard \"career tracks\" have serious disadvantages, like working insane hours doing unpleasant stuff. They sap what I call human capital and social capital - human capital is your skills, capabilities, and the value you can provide to an organization, while social capital is your network of friends and people who want to work with you. Human capital and social capital are the two critical things one needs to do <em>anything</em>, including world saving; they shouldn't be spent lightly.</p>\n<p><strong id=\"5__People_are_morally_responsible_for_the_opportunity_costs_of_their_actions_\">5. People are morally responsible for the opportunity costs of their actions.</strong></p>\n<p>This is somewhat tricky/ambiguous, so I've deliberately made the wording vague, but the best example I've found is Peter Singer's argument (analyzed <a href=\"https://www2.bc.edu/~mathiepa/classmag/schaefer.htm\">here</a>). Singer compares philanthropy to a Trolley Problem. There's a set of train tracks, which a child is lying on, and a train is fast approaching. You're driving a luxury car, and if you drive the car on the tracks, the train will run into the car and save the child. What should you do?</p>\n<p>In standard morality, the right thing to do is save the child, even if it means destroying your really expensive car. Indeed, we might socially shame someone who didn't. According to Singer, this means that we should be willing to donate any amount of money less than the price of an expensive car to charity, if it meant saving a life. Not donating to charity would be the same as letting the train run over the kid - murder through inaction.</p>\n<p>I haven't figured out in detail what the real moral framework should be, but this argument doesn't work. For one thing, it produces <em>atrocious</em> incentives. Suppose you have a nice, cushy software job, and donate 10% of your income to charity, even though you could easily afford 20%. You work really hard, and a year later, you get another job for twice as much money. If not donating surplus money is morally equivalent to causing whatever bad outcome the donation would prevent, you are now <em>twice as guilty</em>, since the amount you aren't donating (20% vs. 10%) is twice as large. This is despite the fact that the <em>total amount of good done</em> is also twice as large. Why punish an improvement?</p>\n<p>Another huge problem is the creation of <em>unbounded obligations</em>. I suspect a lot of thinking is inherently binary - you've either graduated college or you haven't, either paid back the loan or you haven't, either obeyed the rules or you haven't. With this line of argument, there's literally <em>no point</em> at which one can sit back and say, \"I've fulfilled my duty to charity - there's nothing more to do\". There's (short of FAI) always another child to save. One can never say, \"I've met the goal\", or even \"I've gotten a third of the way to the goal\", since the goal of solving all the world's problems is so huge. But if all states of the world - whether they be donating 0%, 10%, or 20% of income - result in 0% total goal fulfillment, then they're all equivalent, at least in some sense. A moral framework should make the good outcome and bad outcome as distinct as possible, not the same.</p>\n<p><strong id=\"6__More_people_interested_in_doing_good_should_become_professional_philanthropists__\">6. More people interested in doing good should become professional philanthropists. </strong></p>\n<p>This one I totally agree with, which might seem odd, given how closely related it is to #3 and #4. However, I think there are two important differences. A professional philanthropist is, typically, someone whose full-time job it is to figure out how to give away their money. But almost always, it's someone who <em>already has</em> lots of money. Historically, there isn't much precedent for people taking high-paying jobs and donating most of their salary... but there's lots of precedent for getting rich <em>first</em>, in whatever field, and then working full-time on donating.</p>\n<p>The other difference is that professional philanthropists don't <em>optimize for</em> donating the maximum amount. They see donating as good, but they also see it as a good to be traded off against other goods, like having lots of nice stuff and social respect. Optimizing for more than one thing allows one to have a lot more Fun, as I suspect Bill Gates and Warren Buffett do.</p>\n<p>This really does seem to be better than conventional routes of do-gooding. When I was in college, a huge number of people did stuff like fly to Africa to dig wells. This isn't just inefficient - it actually does net harm, since the cost of utilizing unskilled labor <a href=\"http://www.cracked.com/article_19899_5-popular-forms-charity-that-arent-helping_p2.html\">usually</a> <a href=\"/lw/uk/beyond_the_reach_of_god/tca\">outweighs</a> the benefits of such labor. Surely we can do better.</p>\n<p>&nbsp;</p>", "sections": [{"title": "1. We should evaluate charities according to how efficient they are, along some common metric - for example, number of lives saved per dollar, or existential risk reduction per dollar. We should then encourage charities to be more efficient, and selectively donate to (or otherwise help) the most efficient ones.", "anchor": "1__We_should_evaluate_charities_according_to_how_efficient_they_are__along_some_common_metric___for_example__number_of_lives_saved_per_dollar__or_existential_risk_reduction_per_dollar__We_should_then_encourage_charities_to_be_more_efficient__and_selectively_donate_to__or_otherwise_help__the_most_efficient_ones_", "level": 1}, {"title": "2. In order to do the most good for our fellow humans, we should start/work for/donate to/otherwise become involved in charitable organizations.", "anchor": "2__In_order_to_do_the_most_good_for_our_fellow_humans__we_should_start_work_for_donate_to_otherwise_become_involved_in_charitable_organizations_", "level": 1}, {"title": "3. We should design careers around being able to donate the largest possible amount.", "anchor": "3__We_should_design_careers_around_being_able_to_donate_the_largest_possible_amount_", "level": 1}, {"title": "4. People can donate the largest amount through a traditional \"high-earning career\", like investment banking. ", "anchor": "4__People_can_donate_the_largest_amount_through_a_traditional__high_earning_career___like_investment_banking__", "level": 1}, {"title": "5. People are morally responsible for the opportunity costs of their actions.", "anchor": "5__People_are_morally_responsible_for_the_opportunity_costs_of_their_actions_", "level": 1}, {"title": "6. More people interested in doing good should become professional philanthropists. ", "anchor": "6__More_people_interested_in_doing_good_should_become_professional_philanthropists__", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "35 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pC47ZTsPNAkjavkXs", "3p3CYauiX8oLjmwRF", "tscc3e5eujrsEeFN4", "jP583FwKepjiWbeoQ", "qZJBighPrnv9bSqTZ", "jtedBLdducritm8y6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-12T02:07:43.998Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Math is Subjunctively Objective", "slug": "seq-rerun-math-is-subjunctively-objective", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:03.842Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RD4v4FWd4XEEA8gr8/seq-rerun-math-is-subjunctively-objective", "pageUrlRelative": "/posts/RD4v4FWd4XEEA8gr8/seq-rerun-math-is-subjunctively-objective", "linkUrl": "https://www.lesswrong.com/posts/RD4v4FWd4XEEA8gr8/seq-rerun-math-is-subjunctively-objective", "postedAtFormatted": "Thursday, July 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Math%20is%20Subjunctively%20Objective&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Math%20is%20Subjunctively%20Objective%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRD4v4FWd4XEEA8gr8%2Fseq-rerun-math-is-subjunctively-objective%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Math%20is%20Subjunctively%20Objective%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRD4v4FWd4XEEA8gr8%2Fseq-rerun-math-is-subjunctively-objective", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRD4v4FWd4XEEA8gr8%2Fseq-rerun-math-is-subjunctively-objective", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 190, "htmlBody": "<p>Today's post, <a href=\"/lw/si/math_is_subjunctively_objective/\">Math is Subjunctively Objective</a> was originally published on 25 July 2008. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Math_is_Subjunctively_Objective\">LW wiki</a>):</p>\n<blockquote>It really does seem like \"2+3=5\" is true. Things get confusing if you ask what you mean when you say \"2+3=5 is true\". But because the simple rules of addition function so well to predict observations, it really does seem like it really must be true.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/lw/djg/seq_rerun_can_counterfactuals_be_true/\">Can Counterfactuals Be True?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RD4v4FWd4XEEA8gr8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.408799782697184e-07, "legacy": true, "legacyId": "17563", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WAQ3qMD4vdXheQmui", "gKnTic56BLYu3Y7hF", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-12T17:43:56.045Z", "modifiedAt": "2022-04-24T02:25:43.574Z", "url": null, "title": "What Is Signaling, Really?", "slug": "what-is-signaling-really", "viewCount": null, "lastCommentedAt": "2022-04-24T02:24:05.073Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KheBaeW8Pi7LwewoF/what-is-signaling-really", "pageUrlRelative": "/posts/KheBaeW8Pi7LwewoF/what-is-signaling-really", "linkUrl": "https://www.lesswrong.com/posts/KheBaeW8Pi7LwewoF/what-is-signaling-really", "postedAtFormatted": "Thursday, July 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20Is%20Signaling%2C%20Really%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20Is%20Signaling%2C%20Really%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKheBaeW8Pi7LwewoF%2Fwhat-is-signaling-really%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20Is%20Signaling%2C%20Really%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKheBaeW8Pi7LwewoF%2Fwhat-is-signaling-really", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKheBaeW8Pi7LwewoF%2Fwhat-is-signaling-really", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1852, "htmlBody": "<p>The most commonly used introduction to signaling, promoted both <a href=\"http://www.overcomingbias.com/2007/01/excess_signalin.html\">by Robin Hanson</a> and in <a href=\"http://www.amazon.com/The-Art-Strategy-Theorists-Business/dp/0393062430\"><em>The Art of Strategy</em></a>, starts with college degrees. Suppose, there are two kinds of people, smart people and stupid people; and suppose, with wild starry-eyed optimism, that the populace is split 50-50 between them. Smart people would add enough value to a company to be worth a $100,000 salary each year, but stupid people would only be worth $40,000. And employers, no matter how hard they try to come up with silly lateral-thinking interview questions like &ldquo;How many ping-pong balls could fit in the Sistine Chapel?&rdquo;, can't tell the difference between them.<br /><br />Now suppose a certain college course, which costs $50,000, passes all smart people but flunks half the stupid people. A strategic employer might declare a policy of hiring (for a one year job; let's keep this model simple) graduates at $100,000 and non-graduates at $40,000.<br /><br />Why? Consider the thought process of a smart person when deciding whether or not to take the course. She thinks &ldquo;I am smart, so if I take the course, I will certainly pass. Then I will make an extra $60,000 at this job. So my costs are $50,000, and my benefits are $60,000. Sounds like a good deal.&rdquo;<br /><br />The stupid person, on the other hand, thinks: &ldquo;As a stupid person, if I take the course, I have a 50% chance of passing and making $60,000 extra, and a 50% chance of failing and making $0 extra. My expected benefit is $30,000, but my expected cost is $50,000. I'll stay out of school and take the $40,000 salary for non-graduates.&rdquo;<br /><br />...assuming that stupid people all know they're stupid, and that they're all perfectly rational experts at game theory, to name two of several dubious premises here. Yet despite its flaws, this model does give some interesting results. For example, it suggests that rational employers will base decisions upon - and rational employees enroll in - college courses, even if those courses teach nothing of any value. So an investment bank might reject someone who had no college education, even while hiring someone who studied Art History, not known for its relevance to derivative trading.<br /><br />We'll return to the specific example of education later, but for now it is more important to focus on the general definition that X signals Y if X is more likely to be true when Y is true than when Y is false. Amoral self-interested agents after the $60,000 salary bonus for intelligence, whether they are smart or stupid, will always say &ldquo;Yes, I'm smart&rdquo; if you ask them. So saying &ldquo;I am smart&rdquo; is not a signal of intelligence. Having a college degree is a signal of intelligence, because a smart person is more likely to get one than a stupid person.<br /><br />Life frequently throws us into situations where we want to convince other people of something. If we are employees, we want to convince bosses we are skillful, honest, and hard-working. If we run the company, we want to convince customers we have superior products. If we are on the dating scene, we want to show potential mates that we are charming, funny, wealthy, interesting, you name it.<br /><br />In some of these cases, mere assertion goes a long way. If I tell my employer at a job interview that I speak fluent Spanish, I'll probably get asked to talk to a Spanish-speaker at my job, will either succeed or fail, and if I fail will have a lot of questions to answer and probably get fired - or at the very least be in more trouble than if I'd just admitted I didn't speak Spanish to begin with. Here society and its system of reputational penalties help turn mere assertion into a credible signal: asserting I speak Spanish is costlier if I don't speak Spanish than if I do, and so is believable.<br /><br />In other cases, mere assertion doesn't work. If I'm at a seedy bar looking for a one-night stand, I can tell a girl I'm totally a multimillionaire and feel relatively sure I won't be found out until after that one night - and so in this she would be naive to believe me, unless I did something only a real multimillionaire could, like give her an expensive diamond necklace.<br /><br />How expensive a diamond necklace, exactly?&nbsp; To absolutely prove I am a millionaire, only a million dollars worth of diamonds will do; $10,000 worth of diamonds could in theory come from anyone with at least $10,000. But in practice, people only care so much about impressing a girl at a seedy bar; if everyone cares about the same amount, the amount they'll spend on the signal depends mostly on their marginal utility of money, which in turn depends mostly on how much they have. Both a millionaire and a tenthousandaire can afford to buy $10,000 worth of diamonds, but only the millionaire can afford to buy $10,000 worth of diamonds on a whim. If in general people are only willing to spend 1% of their money on an impulse gift, then $10,000 is sufficient evidence that I am a millionaire.<br /><br />But when the stakes are high, signals can get prohibitively costly. If a dozen millionaires are wooing Helen of Troy, the most beautiful woman in the world, and willing to spend arbitrarily much money on her - and if they all believe Helen will choose the richest among them - then if I only spend $10,000 on her I'll be outshone by a millionaire who spends the full million. Thus, if I want any chance with her at all, then even if I am genuinely the richest man around I might have to squander my entire fortune on diamonds.<br /><br />This raises an important point: <em>signaling can be really horrible</em>. What if none of us are entirely sure how much Helen's other suitors have? It might be rational for all of us to spend everything we have on diamonds for her. Then twelve millionaires lose their fortunes, eleven of them for nothing. And this isn't some kind of wealth transfer - for all we know, Helen might not even like diamonds; maybe she locks them in her jewelry box after the wedding and never thinks about them again. It's about as economically productive as digging a big hole and throwing money into it.<br /><br />If all twelve millionaires could get together beforehand and compare their wealth, and agree that only the wealthiest one would woo Helen, then they could all save their fortunes and the result would be exactly the same: Helen marries the wealthiest. If all twelve millionaires are remarkably trustworthy, maybe they can pull it off. But if any of them believe the others might lie about their wealth, or that one of the poorer men might covertly break their pact and woo Helen with gifts, then they've got to go through with the whole awful &ldquo;everyone wastes everything they have on shiny rocks&rdquo; ordeal.<br /><br />Examples of destructive signaling are not limited to hypotheticals. Even if one does not believe Jared Diamond's hypothesis that Easter Island civilization collapsed after <a href=\"http://www.skeptically.org/env/id12.html\">chieftains expended all of their resources trying to out-signal each other</a> by building larger and larger stone heads, one can look at Nikolai Roussanov's study on how <a href=\"http://knowledge.wharton.upenn.edu/article.cfm?articleid=1963\">the dynamics of signaling games in US minority communities</a> encourage conspicuous consumption and prevent members of those communities from investing in education and other important goods. <br /><br /><em>The Art of Strategy</em> even advances the surprising hypothesis that corporate advertising can be a form of signaling. When a company advertises during the Super Bowl or some other high-visibility event, it costs a lot of money. To be able to afford the commercial, the company must be pretty wealthy; which in turn means it probably sells popular products and isn't going to collapse and leave its customers in the lurch. And to want to afford the commercial, the company must be pretty confident in its product: advertising that you should shop at Wal-Mart is more profitable if you shop at Wal-Mart, love it, and keep coming back than if you're likely to go to Wal-Mart, hate it, and leave without buying anything. This signaling, too, can become destructive: if every other company in your industry is buying Super Bowl commercials, then none of them have a comparative advantage and they're in exactly the same relative position as if none of them bought Super Bowl commercials - throwing money away just as in the diamond example.<br /><br />Most of us cannot afford a Super Bowl commercial or a diamond necklace, and less people may build giant stone heads than during Easter Island's golden age, but a surprising amount of everyday life can be explained by signaling. For example, why did about 50% of readers get a mental flinch and an overpowering urge to correct me when I used &ldquo;less&rdquo; instead of &ldquo;fewer&rdquo; in the sentence above? According to Paul Fussell's &ldquo;Guide Through The American Class System&rdquo; (ht SIAI mailing list), nitpicky attention to good grammar, even when a sentence is perfectly clear without it, can be a way to signal education, and hence intelligence and probably social class. I would not dare to summarize Fussell's guide here, but it shattered my illusion that I mostly avoid thinking about class signals, and instead convinced me that pretty much everything I do from waking up in the morning to going to bed at night is a class signal. On flowers:<br /><br /><em></em></p>\n<blockquote>\n<p><em>Anyone imagining that just any sort of flowers can be presented in the front of a house without status jeopardy would be wrong. Upper-middle-class flowers are rhododendrons, tiger lilies, amaryllis, columbine, clematis, and roses, except for bright-red ones. One way to learn which flowers are vulgar is to notice the varieties favored on Sunday-morning TV religious programs like Rex Humbard's or Robert Schuller's. There you will see primarily geraniums (red are lower than pink), poinsettias, and chrysanthemums, and you will know instantly, without even attending to the quality of the discourse, that you are looking at a high-prole setup. Other prole flowers include anything too vividly red, like red tulips. Declassed also are phlox, zinnias, salvia, gladioli, begonias, dahlias, fuchsias, and petunias. Members of the middle class will sometimes hope to mitigate the vulgarity of bright-red flowers by planting them in a rotting wheelbarrow or rowboat displayed on the front lawn, but seldom with success. </em></p>\n</blockquote>\n<p>Seriously, <a href=\"http://www.phenomenologycenter.org/course/status.htm\">read the essay</a>.<br /><br />In conclusion, a signal is a method of conveying information among not-necessarily-trustworthy parties by performing an action which is more likely or less costly if the information is true than if it is not true. Because signals are often costly, they can sometimes lead to a depressing waste of resources, but in other cases they may be the only way to believably convey important information.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q6P8jLn8hH7kbuXRr": 12, "b8FHrKqyXuYGWc6vn": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KheBaeW8Pi7LwewoF", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 106, "baseScore": 129, "extendedScore": null, "score": 0.000272, "legacy": true, "legacyId": "17509", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 129, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 175, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 14, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-07-12T17:43:56.045Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-12T20:40:45.798Z", "modifiedAt": null, "url": null, "title": "Valuable Things to Know While Discussing Moral Philosophy", "slug": "valuable-things-to-know-while-discussing-moral-philosophy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:37.856Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mwengler", "createdAt": "2010-04-29T14:43:20.667Z", "isAdmin": false, "displayName": "mwengler"}, "userId": "iNn4oZpoPFvwnqbpL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ogf6AEFG4Gq3bhszH/valuable-things-to-know-while-discussing-moral-philosophy", "pageUrlRelative": "/posts/ogf6AEFG4Gq3bhszH/valuable-things-to-know-while-discussing-moral-philosophy", "linkUrl": "https://www.lesswrong.com/posts/ogf6AEFG4Gq3bhszH/valuable-things-to-know-while-discussing-moral-philosophy", "postedAtFormatted": "Thursday, July 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Valuable%20Things%20to%20Know%20While%20Discussing%20Moral%20Philosophy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AValuable%20Things%20to%20Know%20While%20Discussing%20Moral%20Philosophy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fogf6AEFG4Gq3bhszH%2Fvaluable-things-to-know-while-discussing-moral-philosophy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Valuable%20Things%20to%20Know%20While%20Discussing%20Moral%20Philosophy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fogf6AEFG4Gq3bhszH%2Fvaluable-things-to-know-while-discussing-moral-philosophy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fogf6AEFG4Gq3bhszH%2Fvaluable-things-to-know-while-discussing-moral-philosophy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3, "htmlBody": "<h2><br /></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ogf6AEFG4Gq3bhszH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -7, "extendedScore": null, "score": -5e-06, "legacy": true, "legacyId": "17558", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-12T21:17:10.435Z", "modifiedAt": null, "url": null, "title": "New book on atheism, transhumanism, and x-risk", "slug": "new-book-on-atheism-transhumanism-and-x-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:38.026Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RbWqa6wa5K6YLv4We/new-book-on-atheism-transhumanism-and-x-risk", "pageUrlRelative": "/posts/RbWqa6wa5K6YLv4We/new-book-on-atheism-transhumanism-and-x-risk", "linkUrl": "https://www.lesswrong.com/posts/RbWqa6wa5K6YLv4We/new-book-on-atheism-transhumanism-and-x-risk", "postedAtFormatted": "Thursday, July 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20book%20on%20atheism%2C%20transhumanism%2C%20and%20x-risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20book%20on%20atheism%2C%20transhumanism%2C%20and%20x-risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRbWqa6wa5K6YLv4We%2Fnew-book-on-atheism-transhumanism-and-x-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20book%20on%20atheism%2C%20transhumanism%2C%20and%20x-risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRbWqa6wa5K6YLv4We%2Fnew-book-on-atheism-transhumanism-and-x-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRbWqa6wa5K6YLv4We%2Fnew-book-on-atheism-transhumanism-and-x-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 92, "htmlBody": "<p>Phil Torres is the creative force behind the highly enjoyable folk music of <a href=\"http://baobabmusic.bandcamp.com/\">Baobab</a>, and he also writes <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9973.2011.01715.x/abstract\">philosophy</a> <a href=\"http://www.springerlink.com/content/05k84q3322q2k337/\">papers</a> (under the name \"Philippe Verdoux\").</p>\n<p>His forthcoming book may be of interest to LWers:&nbsp;<em><a href=\"http://www.acrisisoffaiththebook.com/\">A Crisis of Faith: Atheism, Emerging Technologies, and the Future of Humanity</a></em>. Mostly it's a beginner's book about atheism, but chapter 20 discusses cognitive enhancement and mind uploading, and chapter 21 discusses existential risks as one of the most important things for humans to address once they've stopped fooling around with religion. There's also an appendix on the simulation argument.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RbWqa6wa5K6YLv4We", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 10, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "17561", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-13T03:39:21.180Z", "modifiedAt": null, "url": null, "title": "WorldviewNaturalism.com: A \"landing page\" for scientific naturalism", "slug": "worldviewnaturalism-com-a-landing-page-for-scientific", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:57.553Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yoc7qG8x4i6xgAYPf/worldviewnaturalism-com-a-landing-page-for-scientific", "pageUrlRelative": "/posts/yoc7qG8x4i6xgAYPf/worldviewnaturalism-com-a-landing-page-for-scientific", "linkUrl": "https://www.lesswrong.com/posts/yoc7qG8x4i6xgAYPf/worldviewnaturalism-com-a-landing-page-for-scientific", "postedAtFormatted": "Friday, July 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20WorldviewNaturalism.com%3A%20A%20%22landing%20page%22%20for%20scientific%20naturalism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWorldviewNaturalism.com%3A%20A%20%22landing%20page%22%20for%20scientific%20naturalism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyoc7qG8x4i6xgAYPf%2Fworldviewnaturalism-com-a-landing-page-for-scientific%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=WorldviewNaturalism.com%3A%20A%20%22landing%20page%22%20for%20scientific%20naturalism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyoc7qG8x4i6xgAYPf%2Fworldviewnaturalism-com-a-landing-page-for-scientific", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyoc7qG8x4i6xgAYPf%2Fworldviewnaturalism-com-a-landing-page-for-scientific", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p>I've just <a href=\"http://worldviewnaturalism.com/2012/07/10/welcome/\">launched</a> <a href=\"http://worldviewnaturalism.com/\">WorldviewNaturalism.com</a>, which is intended as a simple \"landing page\" to be used for introducing your friends to scientific naturalism. Many of the recommended readings linked there are written by LWers. Enjoy.</p>\n<p>(This is a very old personal project on which I've spent a few hours per month, and it is not at all associated with the Singularity Institute or the Center for Applied Rationality.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yoc7qG8x4i6xgAYPf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "17602", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-13T05:23:06.664Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Does Your Morality Care What You Think?", "slug": "seq-rerun-does-your-morality-care-what-you-think", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4NtqWK3NJA6kzySWR/seq-rerun-does-your-morality-care-what-you-think", "pageUrlRelative": "/posts/4NtqWK3NJA6kzySWR/seq-rerun-does-your-morality-care-what-you-think", "linkUrl": "https://www.lesswrong.com/posts/4NtqWK3NJA6kzySWR/seq-rerun-does-your-morality-care-what-you-think", "postedAtFormatted": "Friday, July 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Does%20Your%20Morality%20Care%20What%20You%20Think%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Does%20Your%20Morality%20Care%20What%20You%20Think%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4NtqWK3NJA6kzySWR%2Fseq-rerun-does-your-morality-care-what-you-think%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Does%20Your%20Morality%20Care%20What%20You%20Think%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4NtqWK3NJA6kzySWR%2Fseq-rerun-does-your-morality-care-what-you-think", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4NtqWK3NJA6kzySWR%2Fseq-rerun-does-your-morality-care-what-you-think", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 195, "htmlBody": "<p>Today's post, <a href=\"/lw/sj/does_your_morality_care_what_you_think/\">Does Your Morality Care What You Think?</a> was originally published on 26 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Does_Your_Morality_Care_What_You_Think.3F\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If, for whatever reason, evolution or education had convinced you to believe that it was moral to do something that you now believe is immoral, you would go around saying \"This is moral to do no matter what anyone else thinks of it.\" How much does this matter?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/djv/seq_rerun_math_is_subjunctively_objective/\">Math is Subjunctively Objective</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4NtqWK3NJA6kzySWR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.41651798470435e-07, "legacy": true, "legacyId": "17603", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GAR8gT3d9uCtr4kv8", "RD4v4FWd4XEEA8gr8", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-13T08:43:11.468Z", "modifiedAt": null, "url": null, "title": "Two books by Celia Green", "slug": "two-books-by-celia-green", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:39.287Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kWYF2Y4PhSKsHiBqS/two-books-by-celia-green", "pageUrlRelative": "/posts/kWYF2Y4PhSKsHiBqS/two-books-by-celia-green", "linkUrl": "https://www.lesswrong.com/posts/kWYF2Y4PhSKsHiBqS/two-books-by-celia-green", "postedAtFormatted": "Friday, July 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Two%20books%20by%20Celia%20Green&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATwo%20books%20by%20Celia%20Green%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkWYF2Y4PhSKsHiBqS%2Ftwo-books-by-celia-green%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Two%20books%20by%20Celia%20Green%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkWYF2Y4PhSKsHiBqS%2Ftwo-books-by-celia-green", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkWYF2Y4PhSKsHiBqS%2Ftwo-books-by-celia-green", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 758, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/Celia_Green\">Celia Green</a> is a figure who should interest some LW readers. If you can imagine Eliezer, not as an A.I. futurist in 2000s America, but as a parapsychologist in 1960s Britain - she must have been a little like that. She founded her own research institute in her mid-20s, invented psychological theories meant to explain why the human race was walking around resigned to mortality and ignorance, felt that her peers (who got all the research money) were doing everything wrong... I would say that her two outstanding books are <a href=\"http://www.naturalthinker.net/trl/texts/Green,Celia/HumanEvasion.html\"><em>The Human Evasion</em></a> and <a href=\"http://www.scribd.com/doc/48901608/Advice-to-Clever-Children-Celia-Green\"><em>Advice to Clever Children</em></a>. The first book, while still very obscure, has slowly acquired a fanbase online; but the second book remains thoroughly unknown.</p>\n<p>For a synopsis of what the books are about, I think something I wrote in <a href=\"http://web.archive.org/web/19970706121851/http://www.deoxy.org/green.htm\">1993</a> (I've been promoting her work on the Internet for years) remains reasonable. They contain an analysis of the alleged deficiencies and hidden motivations of normal human psychology, description of an alternative outlook, and an examination of various topics from that new perspective. There is some similarity to the rationalist ideal developed in the Sequences here, in that her alternative involves existential urgency, deep respect for uncertainty, and superhuman aspiration.</p>\n<p>There are also prominent differences. Green's starting point is not Bayesian calculation, it's Humean skepticism. Green would agree that one should aspire to \"think like reality\", but for her this would mean, above all, being mindful of \"total uncertainty\". It's a fact that I don't know what comes next, that I don't know the true nature of reality, that I don't know what's possible if I try; I may have habitual opinions about these matters, but a moment's honest reflection shows that none of these opinions are knowledge in any genuine sense; even if they are correct, I don't know them to be correct. So if I am interested in thinking like reality, I can begin by acknowledging the radical uncertainty of my situation. I exist, I don't know why, I don't know what I am, I don't know what the world is or what it has planned for me. I may have my ideas, but I should be able to see them as ideas and hold them apart from the unknown reality.</p>\n<p>If you are like me, you will enjoy the outlook of open-ended striving that Green develops in this intellectual context, but you will be jarred by her account of ordinary, non-striving psychology. Her answer to the question, why does the human race have such petty interests and limited ambitions, is that it is sunk in an orgy of mutual hatred, mostly disguised, and resulting from an attempt to evade the psychology of striving. More precisely, to be a finite human being is to be in a desperate and frustrating situation; and people attempt to solve this problem, not by overcoming their limitations, but by suppressing their reactions to the situation. Other people are central to the resulting psychological maneuvers. They are a way for you to distract yourself from your own situation, and they are a safe target if the existential frustration and desperation reassert themselves.</p>\n<p>Celia Green's psychological ideas are the product of her personal confrontation with the mysterious existential situation, and also her confrontation with an uncomprehending society. I've thought for some time that her portrayal of universal human depravity results from overestimating the potential of the average human being; that in effect she has asked herself, if I were that person, how could I possibly lead the life I see them living, and say the things I hear them saying, unless I were that twisted up inside? Nonetheless, I do think she has described an aspect of human psychology which is real and largely unexamined, and also that her advice on how to avoid the resentful turning-away from reality, and live in the uncertainty, is quite profound. One reason I'm promoting these books is in the hope that some small part of the culture at large is finally ready to digest their contents and critically assess them. People ought to be doing PhDs on the thought of Celia Green, but she's unknown in that world.</p>\n<p>As for Celia Green herself, she's still alive and still going. She has a <a href=\"http://celiagreen.blogspot.com/\">blog</a> and a <a href=\"http://www.celiagreen.com/\">personal website</a> and an <a href=\"http://en.wikipedia.org/wiki/Oxford_Forum\">organization</a> based near Oxford. She's an \"academic exile\", but true to her philosophy, she hasn't compromised one iota and hopes to start her own private university. She may especially be of interest to the metaphysically inclined faction of LW readers, identified by Yvain in a <a href=\"http://squid314.livejournal.com/312453.html\">recent blog post</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kWYF2Y4PhSKsHiBqS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": -10, "extendedScore": null, "score": 9.4174630358415e-07, "legacy": true, "legacyId": "17610", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-13T11:36:12.375Z", "modifiedAt": null, "url": null, "title": "Confused about Solomonoff induction", "slug": "confused-about-solomonoff-induction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:38.581Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nebulous", "createdAt": "2011-11-23T06:28:39.231Z", "isAdmin": false, "displayName": "nebulous"}, "userId": "nqrJWFKPS8dLDDjmi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5LARSHmMMqp72YmBz/confused-about-solomonoff-induction", "pageUrlRelative": "/posts/5LARSHmMMqp72YmBz/confused-about-solomonoff-induction", "linkUrl": "https://www.lesswrong.com/posts/5LARSHmMMqp72YmBz/confused-about-solomonoff-induction", "postedAtFormatted": "Friday, July 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Confused%20about%20Solomonoff%20induction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConfused%20about%20Solomonoff%20induction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5LARSHmMMqp72YmBz%2Fconfused-about-solomonoff-induction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Confused%20about%20Solomonoff%20induction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5LARSHmMMqp72YmBz%2Fconfused-about-solomonoff-induction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5LARSHmMMqp72YmBz%2Fconfused-about-solomonoff-induction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 22, "htmlBody": "<p>Why wouldn't the probability of two algorithms of different lengths appearing approach the same value as longer strings of bits are searched?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5LARSHmMMqp72YmBz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": -3, "extendedScore": null, "score": 9.418280385354316e-07, "legacy": true, "legacyId": "17612", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-13T16:25:25.066Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Berkeley, Hyderabad, Madison, New Jersey, Salt Lake City, Tucson, Washington DC", "slug": "weekly-lw-meetups-berkeley-hyderabad-madison-new-jersey-salt", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ANyffFJ6CtjnpgupG/weekly-lw-meetups-berkeley-hyderabad-madison-new-jersey-salt", "pageUrlRelative": "/posts/ANyffFJ6CtjnpgupG/weekly-lw-meetups-berkeley-hyderabad-madison-new-jersey-salt", "linkUrl": "https://www.lesswrong.com/posts/ANyffFJ6CtjnpgupG/weekly-lw-meetups-berkeley-hyderabad-madison-new-jersey-salt", "postedAtFormatted": "Friday, July 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Berkeley%2C%20Hyderabad%2C%20Madison%2C%20New%20Jersey%2C%20Salt%20Lake%20City%2C%20Tucson%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Berkeley%2C%20Hyderabad%2C%20Madison%2C%20New%20Jersey%2C%20Salt%20Lake%20City%2C%20Tucson%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FANyffFJ6CtjnpgupG%2Fweekly-lw-meetups-berkeley-hyderabad-madison-new-jersey-salt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Berkeley%2C%20Hyderabad%2C%20Madison%2C%20New%20Jersey%2C%20Salt%20Lake%20City%2C%20Tucson%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FANyffFJ6CtjnpgupG%2Fweekly-lw-meetups-berkeley-hyderabad-madison-new-jersey-salt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FANyffFJ6CtjnpgupG%2Fweekly-lw-meetups-berkeley-hyderabad-madison-new-jersey-salt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 448, "htmlBody": "<p><strong>This summary was posted to LW main on July 6th, and has now been moved to discussion.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/b9\">Hyderabad Meetup:&nbsp;<span class=\"date\">08 July 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/bk\">Washington DC Meetup-Post-Minicamp edition:&nbsp;<span class=\"date\">08 July 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/bi\">Tucson: Fundamental Questions:&nbsp;<span class=\"date\">09 July 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/bn\">First New Jersey Meetup:&nbsp;<span class=\"date\">12 July 2012 01:30AM</span></a></li>\n<li><a href=\"/meetups/bf\">Brussels meetup:&nbsp;<span class=\"date\">14 July 2012 12:00PM</span></a></li>\n<li><a href=\"/meetups/bm\">Less Wrong Sydney 16th July Event for Less Wrong:&nbsp;<span class=\"date\">16 July 2012 06:35PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bl\">Salt Lake City: The Really Getting Bayes Game:&nbsp;<span class=\"date\">08 July 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/bo\">Madison: Probability Calibration:&nbsp;<span class=\"date\">08 July 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/bp\">Berkeley meta-meetup:&nbsp;<span class=\"date\">11 July 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/bh\">(NYC) A Game of Nomic:&nbsp;<span class=\"date\">21 July 2012 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong>&nbsp;</strong><strong></strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ANyffFJ6CtjnpgupG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.419646944837674e-07, "legacy": true, "legacyId": "17453", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-13T19:24:10.336Z", "modifiedAt": null, "url": null, "title": "Adding up to normality", "slug": "adding-up-to-normality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:39.428Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "n5dZTufewLK86nWwb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aiamAi5vAyQY9aGkh/adding-up-to-normality", "pageUrlRelative": "/posts/aiamAi5vAyQY9aGkh/adding-up-to-normality", "linkUrl": "https://www.lesswrong.com/posts/aiamAi5vAyQY9aGkh/adding-up-to-normality", "postedAtFormatted": "Friday, July 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Adding%20up%20to%20normality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdding%20up%20to%20normality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaiamAi5vAyQY9aGkh%2Fadding-up-to-normality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Adding%20up%20to%20normality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaiamAi5vAyQY9aGkh%2Fadding-up-to-normality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaiamAi5vAyQY9aGkh%2Fadding-up-to-normality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 546, "htmlBody": "<p>I think that the idea of &lsquo;adding up to normality&rsquo; is incoherent, but maybe I don&rsquo;t understand it. There is a rule of thumb that, in general, a theory or explanation should &lsquo;save the phenomena&rsquo; as much as possible. But Egan&rsquo;s law is presented in the sequences as something more strict than an exceptionable rule of thumb. I&rsquo;m going to try to explain and formalize Egan&rsquo;s law as I understand it so that once it&rsquo;s been made clear, we can talk about how we would argue for it.</p>\n<p>If a theory adds up to normality in the strict sense, then there are no true sentences in normal language which do not have true counterparts in a theory. Thus,&nbsp;<em>if</em>&nbsp;it is true to say that the apple is green, a theory which adds up to normality will contain a sentence which describes the same phenomenon as the normal language sentence, and is true (and false if the normal language sentence is false). For example: if an apple is green, then light of such and such wavelength is predominantly reflected from its surface while other visible wavelengths are predominantly absorbed. Let&rsquo;s call this the Egan property of a theory. A theory would fail to add up to normality either if it denied the truth of true sentences in normal language (e.g. &lsquo;the apple isn&rsquo;t really green&rsquo;) or if it could make nothing of the phenomenon of normal language at all (e.g. nothing really has color).</p>\n<p>t has the property E = for all a in n, there is an &alpha; in t such that a if and only if &alpha;</p>\n<p>t is a theoretical language and &lsquo;<span lang=\"EL\">&alpha;</span>&nbsp;&lsquo;is a sentence within it, n is the normal language and &lsquo;a&rsquo; is a sentence within it. E is the Egan property. Now that we&rsquo;ve defined the Egan property of a theory, we can move on to Egan&rsquo;s law.&nbsp;</p>\n<p>The way Egan&rsquo;s law is articulated in the sequences, it seems to be an a priori necessary but insufficient condition on the truth of a theory. So it is necessary that, if a theory is true, it has the Egan property.</p>\n<p>If&nbsp;<span lang=\"EL\">&alpha;<sub>1</sub>, &alpha;<sub>2</sub>, &alpha;<sub>3</sub>...,&nbsp;</span>then Et.</p>\n<p>Or alternatively: If t is true, then Et.</p>\n<p>That&rsquo;s Egan&rsquo;s law, so far as I understand it. Now, how do we argue for it?&nbsp; There&rsquo;s an inviting, but I think troublesome Tarskian way to argue for Egan&rsquo;s law. Tarski&rsquo;s semantic definition of truth is such that some sentence &beta; is true in language L if and only if&nbsp;<span lang=\"EL\">b</span>, where b is a sentence is a metalanguage. Following this, we could say that for any theory t to be true, all its sentences&nbsp;<span lang=\"EL\">&alpha;</span>&nbsp;must be true, and what it means for any &alpha; to be true is that a, where a is a sentence in the metalanguage we call normal language. But this would mean that a and &alpha; are strictly translations of one another in two different languages. If a theory is going to be explanitory of phenomena, then sentences like &ldquo;light of such and such wavelength is predominantly reflected from the apple&rsquo;s surface while other visible wavelengths are predominantly absorbed&rdquo; have to have&nbsp;<em>more</em>&nbsp;content than &ldquo;the apple is green&rdquo;. If they mean the same thing, as sentences in Tarski&rsquo;s definition of truth must, then theories can&rsquo;t do any explaining.</p>\n<p>So how else can we argue for Egan&rsquo;s law?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aiamAi5vAyQY9aGkh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": -5, "extendedScore": null, "score": 9.42049175697464e-07, "legacy": true, "legacyId": "17615", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-13T20:51:23.463Z", "modifiedAt": null, "url": null, "title": "Ruthless Extrapolation", "slug": "ruthless-extrapolation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:39.079Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "quanticle", "createdAt": "2009-12-02T01:39:50.714Z", "isAdmin": false, "displayName": "quanticle"}, "userId": "usztQFrTvM67pdcCq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/54d8frZxqCFAho7uw/ruthless-extrapolation", "pageUrlRelative": "/posts/54d8frZxqCFAho7uw/ruthless-extrapolation", "linkUrl": "https://www.lesswrong.com/posts/54d8frZxqCFAho7uw/ruthless-extrapolation", "postedAtFormatted": "Friday, July 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ruthless%20Extrapolation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARuthless%20Extrapolation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F54d8frZxqCFAho7uw%2Fruthless-extrapolation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ruthless%20Extrapolation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F54d8frZxqCFAho7uw%2Fruthless-extrapolation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F54d8frZxqCFAho7uw%2Fruthless-extrapolation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 213, "htmlBody": "<p><a href=\"http://www.theoildrum.com/node/9336\" target=\"_blank\">Ruthless Extrapolation</a></p>\n<p><strong>Article Summary</strong>: One of the key adaptations of humanity is the ability to see trends, which allows us to anticipate and preemptively adapt to future conditions. However, this ability has its limits. We're very good at seeing first derivatives, but terrible at seeing higher level trends. This leaves us vulnerable to situations where those first derivative trends unexpectedly change. The example used is with energy resources, where our adaptation to continually increasing energy usage leaves us vulnerable to a situation where we no longer have access to ever increasing energy resources.</p>\n<p>I have two questions regarding the linked article. First, is there a name for this cognitive bias? The author uses \"Ruthless Extrapolation\", which I find quite fetching, but I think this is well known enough to have a name already. Secondly, what assumptions do we make that could be described as ruthless extrapolation? It seems to me that many in the Singularity Studies community simply assume that CPU transistor densities will continue to increase indefinitely, which certainly seems to be a case of ruthless extrapolation. What would happen to whole-brain emulation if we woke up tomorrow and found out that the most powerful CPU possible would have a transistor density only two or four times higher than an Ivy Bridge Core i7?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "54d8frZxqCFAho7uw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 1, "extendedScore": null, "score": 9.42090401045042e-07, "legacy": true, "legacyId": "17617", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-14T03:57:00.120Z", "modifiedAt": null, "url": null, "title": "Where did mathematics begin to disagree between frequentist and Bayesian statistics, and why?", "slug": "where-did-mathematics-begin-to-disagree-between-frequentist", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:52.096Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Arkanj3l", "createdAt": "2011-04-23T03:48:47.569Z", "isAdmin": false, "displayName": "Arkanj3l"}, "userId": "nQmA4dnBdX99WyCt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gQhhMpRpBrxojXDWj/where-did-mathematics-begin-to-disagree-between-frequentist", "pageUrlRelative": "/posts/gQhhMpRpBrxojXDWj/where-did-mathematics-begin-to-disagree-between-frequentist", "linkUrl": "https://www.lesswrong.com/posts/gQhhMpRpBrxojXDWj/where-did-mathematics-begin-to-disagree-between-frequentist", "postedAtFormatted": "Saturday, July 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Where%20did%20mathematics%20begin%20to%20disagree%20between%20frequentist%20and%20Bayesian%20statistics%2C%20and%20why%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhere%20did%20mathematics%20begin%20to%20disagree%20between%20frequentist%20and%20Bayesian%20statistics%2C%20and%20why%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgQhhMpRpBrxojXDWj%2Fwhere-did-mathematics-begin-to-disagree-between-frequentist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Where%20did%20mathematics%20begin%20to%20disagree%20between%20frequentist%20and%20Bayesian%20statistics%2C%20and%20why%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgQhhMpRpBrxojXDWj%2Fwhere-did-mathematics-begin-to-disagree-between-frequentist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgQhhMpRpBrxojXDWj%2Fwhere-did-mathematics-begin-to-disagree-between-frequentist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p>I still can't see the relevance of Bayesian Statistics over Frequentist Statistics, and I take Less Wrong as evidence that this is a cause for clarification.</p>\n<p>I'm looking for a historical narrative of the development of mathematics that tells me what mistake lead to frequentism over Bayesianism, which is supposedly the correct view. Alternatively, you can just say \"Read PT:TLOS!\" if it's that silly of a question.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gQhhMpRpBrxojXDWj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 0, "extendedScore": null, "score": 9.42291618520238e-07, "legacy": true, "legacyId": "17630", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-14T05:40:17.988Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Changing Your Metaethics", "slug": "seq-rerun-changing-your-metaethics", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CBu8DvBdwQcDd33Ta/seq-rerun-changing-your-metaethics", "pageUrlRelative": "/posts/CBu8DvBdwQcDd33Ta/seq-rerun-changing-your-metaethics", "linkUrl": "https://www.lesswrong.com/posts/CBu8DvBdwQcDd33Ta/seq-rerun-changing-your-metaethics", "postedAtFormatted": "Saturday, July 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Changing%20Your%20Metaethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Changing%20Your%20Metaethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCBu8DvBdwQcDd33Ta%2Fseq-rerun-changing-your-metaethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Changing%20Your%20Metaethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCBu8DvBdwQcDd33Ta%2Fseq-rerun-changing-your-metaethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCBu8DvBdwQcDd33Ta%2Fseq-rerun-changing-your-metaethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<p>Today's post, <a href=\"/lw/sk/changing_your_metaethics/\">Changing Your Metaethics</a> was originally published on 27 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Changing_Your_Metaethics\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Discusses the various lines of retreat that have been set up in the discussion on metaethics.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/dkz/seq_rerun_does_your_morality_care_what_you_think/\">Does Your Morality Care What You Think?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CBu8DvBdwQcDd33Ta", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.423404664015526e-07, "legacy": true, "legacyId": "17635", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LhP2zGBWR5AdssrdJ", "4NtqWK3NJA6kzySWR", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-14T18:13:51.915Z", "modifiedAt": null, "url": null, "title": "PZ Myers on the Infeasibility of Whole Brain Emulation", "slug": "pz-myers-on-the-infeasibility-of-whole-brain-emulation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:52.081Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4Y5hjr32xfTKkRtrS/pz-myers-on-the-infeasibility-of-whole-brain-emulation", "pageUrlRelative": "/posts/4Y5hjr32xfTKkRtrS/pz-myers-on-the-infeasibility-of-whole-brain-emulation", "linkUrl": "https://www.lesswrong.com/posts/4Y5hjr32xfTKkRtrS/pz-myers-on-the-infeasibility-of-whole-brain-emulation", "postedAtFormatted": "Saturday, July 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20PZ%20Myers%20on%20the%20Infeasibility%20of%20Whole%20Brain%20Emulation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APZ%20Myers%20on%20the%20Infeasibility%20of%20Whole%20Brain%20Emulation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4Y5hjr32xfTKkRtrS%2Fpz-myers-on-the-infeasibility-of-whole-brain-emulation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=PZ%20Myers%20on%20the%20Infeasibility%20of%20Whole%20Brain%20Emulation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4Y5hjr32xfTKkRtrS%2Fpz-myers-on-the-infeasibility-of-whole-brain-emulation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4Y5hjr32xfTKkRtrS%2Fpz-myers-on-the-infeasibility-of-whole-brain-emulation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 488, "htmlBody": "<p>From: <strong><a href=\"http://freethoughtblogs.com/pharyngula/2012/07/14/and-everyone-gets-a-robot-pony/\">http://freethoughtblogs.com/pharyngula/2012/07/14/and-everyone-gets-a-robot-pony/</a></strong></p>\n<blockquote>\n<p style=\"border: 0px none; font-family: Georgia, 'Times New Roman', Times, serif; font-size: 14px; margin: 0px 0px 10px; padding: 0px; vertical-align: baseline; line-height: 1.75; text-align: left; color: #222222; background-color: #fafafa;\">I&rsquo;ve worked with tiny little zebrafish brains, things a few hundred microns long on one axis, and I&rsquo;ve done lots of EM work on them. You can&rsquo;t fix them into a state resembling life very accurately: even with chemical perfusion with strong aldehyedes of small tissue specimens that takes hundreds of milliseconds, you get degenerative changes. There&rsquo;s a technique where you&nbsp;<a style=\"border: 0px none; font-family: inherit; font-style: inherit; margin: 0px; padding: 0px; vertical-align: baseline; color: #3366cc; text-decoration: none;\" href=\"http://pcp.oxfordjournals.org/content/42/9/885.full\">slam the specimen into a block cooled to liquid helium temperatures</a>&nbsp;&mdash; even there you get variation in preservation, it still takes 0.1ms to cryofix the tissue, and what they&rsquo;re interested in preserving is cell states in a single cell layer, not whole multi-layered tissues. With the most elaborate and careful procedures, they report excellent fixation within 5 microns of the surface, and disruption of the tissue by ice crystal formation within 20 microns. So even with the best techniques available now, we could possibly preserve the thinnest, outermost, single cell layer of your brain&hellip;but all the fine axons and dendrites that penetrate deeper? Forget those.</p>\n<p style=\"border: 0px none; font-family: Georgia, 'Times New Roman', Times, serif; font-size: 14px; margin: 0px 0px 10px; padding: 0px; vertical-align: baseline; line-height: 1.75; text-align: left; color: #222222; background-color: #fafafa;\">[...]</p>\n<p><span style=\"background-color: #fafafa; color: #222222; font-family: Georgia, 'Times New Roman', Times, serif; font-size: 14px; line-height: 1.75; text-align: left;\">And that&rsquo;s another thing: what the heck is going to be recorded? You need to measure the epigenetic state of every nucleus, the distribution of highly specific, low copy number molecules in every dendritic spine, the state of molecules in flux along transport pathways, and the precise concentration of all ions in every single compartment. Does anyone have a fixation method that preserves the chemical state of the tissue? All the ones I know of involve chemically</span><span style=\"background-color: #fafafa; color: #222222; font-family: Georgia, 'Times New Roman', Times, serif; font-size: 14px; line-height: 1.75; text-align: left;\">&nbsp;</span><em style=\"color: #222222; font-family: Georgia, 'Times New Roman', Times, serif; font-size: 14px; line-height: 1.75; text-align: left;\">modifying</em><span style=\"background-color: #fafafa; color: #222222; font-family: Georgia, 'Times New Roman', Times, serif; font-size: 14px; line-height: 1.75; text-align: left;\">&nbsp;</span><span style=\"background-color: #fafafa; color: #222222; font-family: Georgia, 'Times New Roman', Times, serif; font-size: 14px; line-height: 1.75; text-align: left;\">the cells and proteins and fluid environment. Does anyone have a scanning technique that records a complete chemical breakdown of every complex component present?</span></p>\n<p><span style=\"color: #222222; font-family: Georgia, 'Times New Roman', Times, serif; font-size: 14px; line-height: 24px; text-align: left; background-color: #fafafa;\">I think they&rsquo;re grossly underestimating the magnitude of the problem. We can&rsquo;t even record the complete state of a single cell; we can&rsquo;t model a nematode with a grand total of 959 cells. We can&rsquo;t even&nbsp;</span><em style=\"color: #222222; font-family: Georgia, 'Times New Roman', Times, serif; font-size: 14px; line-height: 24px; text-align: left; background-color: #fafafa;\">start</em><span style=\"color: #222222; font-family: Georgia, 'Times New Roman', Times, serif; font-size: 14px; line-height: 24px; text-align: left; background-color: #fafafa;\">&nbsp;on this problem, and here are philosophers and computer scientists blithely turning an immense and physically intractable problem into an&nbsp;</span><em style=\"color: #222222; font-family: Georgia, 'Times New Roman', Times, serif; font-size: 14px; line-height: 24px; text-align: left; background-color: #fafafa;\">assumption</em><span style=\"color: #222222; font-family: Georgia, 'Times New Roman', Times, serif; font-size: 14px; line-height: 24px; text-align: left; background-color: #fafafa;\">.</span></p>\n<p><span style=\"color: #222222; font-family: Georgia, 'Times New Roman', Times, serif; font-size: 14px; line-height: 24px; text-align: left; background-color: #fafafa;\">[...]</span></p>\n<p><span style=\"color: #222222; font-family: Georgia, 'Times New Roman', Times, serif; font-size: 14px; line-height: 24px; text-align: left; background-color: #fafafa;\">You&rsquo;re just going to increase the speed of the computations &mdash; how are you going to do that without disrupting the interactions between all of the subunits? You&rsquo;ve assumed you&rsquo;ve got this gigantic database of every cell and synapse in the brain, and you&rsquo;re going to just tweak the clock speed&hellip;how? You&rsquo;ve got varying length constants in different axons, different kinds of processing, different kinds of synaptic outputs and receptor responses, and you&rsquo;re just going to wave your hand and say, &ldquo;Make them go faster!&rdquo;&nbsp;</span></p>\n<p><span style=\"color: #222222; font-family: Georgia, 'Times New Roman', Times, serif; font-size: 14px; line-height: 24px; text-align: left; background-color: #fafafa;\">[...]</span></p>\n<p><span style=\"color: #222222; font-family: Georgia, 'Times New Roman', Times, serif; font-size: 14px; line-height: 24px; text-align: left; background-color: #fafafa;\">I&rsquo;m not anti-AI; I think we are going to make great advances in the future, and we&rsquo;re going to learn all kinds of interesting things. But reverse-engineering something that is the product of almost 4 billion years of evolution, that has been tweaked and finessed in complex and incomprehensible ways, and that is dependent on activity at a sub-cellular level, by hacking it apart and taking pictures of it? Total bollocks.</span></p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4Y5hjr32xfTKkRtrS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 17, "extendedScore": null, "score": 4e-05, "legacy": true, "legacyId": "17643", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-14T19:02:56.108Z", "modifiedAt": null, "url": null, "title": "Berkeley visit report", "slug": "berkeley-visit-report", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:39.601Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xQoMYN7ZKoKTA4NqP/berkeley-visit-report", "pageUrlRelative": "/posts/xQoMYN7ZKoKTA4NqP/berkeley-visit-report", "linkUrl": "https://www.lesswrong.com/posts/xQoMYN7ZKoKTA4NqP/berkeley-visit-report", "postedAtFormatted": "Saturday, July 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Berkeley%20visit%20report&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABerkeley%20visit%20report%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxQoMYN7ZKoKTA4NqP%2Fberkeley-visit-report%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Berkeley%20visit%20report%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxQoMYN7ZKoKTA4NqP%2Fberkeley-visit-report", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxQoMYN7ZKoKTA4NqP%2Fberkeley-visit-report", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 721, "htmlBody": "<p>A few weeks ago, <a href=\"/user/ShannonFriedman/\">ShannonFriedman</a>, an IFS counselor living in Berkeley,&nbsp;posted&nbsp;<a href=\"/lw/d6s/female_compatriots_stay_for_a_week_in_berkeley/\">Female compatriots stay for a week in Berkeley</a>&nbsp;an offer for rationalist women who are agenty to stay in her spare&nbsp;room for a few days. I replied, as I have a goal of getting to know the&nbsp;people associated with Less Wrong, the Singularity Institute and the Center&nbsp;for Applied Rationality (CFAR).&nbsp;</p>\n<p>She and I had a get to know you skype video interview, to talk about the&nbsp;trip. Shannon is a master connector, who genuinely enjoys introducing&nbsp;people who share common interests and complementary skills. As I had a goal&nbsp;of meeting interesting people, the planning was around helping me accomplish&nbsp;that.</p>\n<p>I traveled with a friend, <a href=\"/user/copt/\">copt</a>, who I met through the Less Wrong group I&nbsp;organize in Fort Collins Colorado. Shannon, fortunately, had two spare rooms for us.&nbsp;Her house, which she shares with several other people is in a residential neighborhood not too far from the secondary&nbsp;shopping district on San Pedro. It is on the main floor of a house, with a&nbsp;big back yard.</p>\n<p>Berkeley is beautiful. The yards are full of flowers. There was an&nbsp;excellent coffee shop, Caffee Trieste (the sad coffee), within walking&nbsp;distance of the house, and a good breakfast restaurant also within walking&nbsp;distance. Downtown Berkeley is only a mile away, with good public transit&nbsp;connections to San Francisco, The mornings were foggy and cool (50F), the&nbsp;afternoons sunny and warm (75F). &nbsp;</p>\n<p>Over the 4 days of our visit, we met:</p>\n<p>Julia Galef, mathy and elegant, formerly of New York, who is in&nbsp;Berkeley working with CFAR. &nbsp;She's known for her organizing work within&nbsp;the skeptical community.&nbsp;&nbsp;I had enjoyed watching her presentation to&nbsp;Skepticon this year, on <em><a href=\"/lw/8ko/communicating_rationality_to_the_public_julia/\">The Straw Vulcan</a></em>. She and Shannon are working on improving marketing for CFAR.</p>\n<p>Luke Muehlhauser (lukeprog), tall, handsome, with dark hair, the Executive&nbsp;Director of the Singularity Institute, met briefly with us to discuss&nbsp;copt's work in finance.</p>\n<p>Nisan, who organizes the Berkeley Less Wrong meetup, is a PhD student&nbsp;mathematician. He generously shared his Paleo groceries with us, and&nbsp;helped us navigate the BART on our trip into the city for a housewarming&nbsp;party. He explained operant conditioning as a technique for personal&nbsp;transformation while on the bus to Oakland. He and I are now writing&nbsp;companions and friends.</p>\n<p>Peter de Blanc, a mathematician, programmer and musician, generously&nbsp;vacated his studio for me to stay in.</p>\n<p>Aubrey de Grey, the founder of the SENS Foundation for rejuvenation technology,&nbsp;&nbsp;is committed to the search for effective treatments for the disease of&nbsp;aging. We met to discuss fundraising and marketing. His focus is&nbsp;inspiring.</p>\n<p>On Saturday night, we went to a housewarming party for Divia and&nbsp;Will Eden. &nbsp;Their new house, in San Francisco, has a big back yard,&nbsp;and huge kitchen, is nicknamed Asgard.</p>\n<p>At the housewarming party, everyone I spoke with had a clarity of&nbsp;intelligence and purpose which was invigorating to experience. There were&nbsp;approximately equal numbers of women and men at the party. &nbsp;I spoke with&nbsp;a woman, who's name I don't remember, who is on her second successful&nbsp;startup as a developer, the first, a javascript based mockup tool,&nbsp;she accomplished while learning to program. &nbsp;Charles, a physicist with&nbsp;SpaceX, red haired, and intense, who copt nick-named The Anti-Thor,&nbsp;because he works on preventing lightning strikes from damaging launch&nbsp;vehicles. &nbsp;Valentine Michael Smith, intense, athletic, with a direct gaze,&nbsp;is working on combining martial arts and rationality to teach both of&nbsp;them faster and more effectively. He demonstrated the difference between&nbsp;compulsion and direction through two soft pushes on my sternum. I hope&nbsp;his project develops quickly, I'd learn new skills studying with him.</p>\n<p>Eliezer Yudkowsky, is a smart, funny, big bear of a guy, who both copt&nbsp;and I liked immediately. I hope that we will have the chance to get to&nbsp;know each other better.</p>\n<p>Outside of the Less Wrong connections, I met several other people,&nbsp;including a young musician with a great Magic deck, and at the coffee&nbsp;shop, I struck up a conversation with a couple, she is a photographer, and he is a musician, and she is 30 years older than&nbsp;him. Berkeley has high openness.</p>\n<p>Also I was able to connect with a friend who I know through open source, who&nbsp;took me for lunch at the Thai Buddhist Temple, which has a pay as you&nbsp;like buffet lunch.</p>\n<p>It was a full, but relaxed, 4 day trip. Shannon has a transformative&nbsp;talent of seeing others as powerful agents directing their lives, which&nbsp;encourages one to see themselves as a powerful interesting being. I'm&nbsp;so glad I took the chance to accept her offer. I'm glad to have her as&nbsp;a friend.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zz3HWyByyKF64Sfns": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xQoMYN7ZKoKTA4NqP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 43, "extendedScore": null, "score": 9.427197981676916e-07, "legacy": true, "legacyId": "17644", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZRaRvGPW49oGRSFtC", "zuJmtSqt3TsnBTYyu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-14T21:54:10.719Z", "modifiedAt": null, "url": null, "title": "What Longevity Research Most Excites You?", "slug": "what-longevity-research-most-excites-you", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:58.894Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "christina", "createdAt": "2011-07-09T06:48:34.638Z", "isAdmin": false, "displayName": "christina"}, "userId": "mWYhP5BngbQYEf7FF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MeN2bgFXA7gMoqP6b/what-longevity-research-most-excites-you", "pageUrlRelative": "/posts/MeN2bgFXA7gMoqP6b/what-longevity-research-most-excites-you", "linkUrl": "https://www.lesswrong.com/posts/MeN2bgFXA7gMoqP6b/what-longevity-research-most-excites-you", "postedAtFormatted": "Saturday, July 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20Longevity%20Research%20Most%20Excites%20You%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20Longevity%20Research%20Most%20Excites%20You%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMeN2bgFXA7gMoqP6b%2Fwhat-longevity-research-most-excites-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20Longevity%20Research%20Most%20Excites%20You%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMeN2bgFXA7gMoqP6b%2Fwhat-longevity-research-most-excites-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMeN2bgFXA7gMoqP6b%2Fwhat-longevity-research-most-excites-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<p>For those who are interested in this topic, I'm just wondering what longevity research today looks most promising to you and why.&nbsp; Whether that's SENS, cryonics, nanotech, brain uploading, etc is fine with me.&nbsp; Any links to actual research papers would also be greatly appreciated.&nbsp; I'm very interested in longevity, and am curious to see if anyone else would like to offer some thoughts on the current state of the art.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MeN2bgFXA7gMoqP6b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 7, "extendedScore": null, "score": 9.428012189589555e-07, "legacy": true, "legacyId": "17590", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-14T23:33:07.974Z", "modifiedAt": null, "url": null, "title": "Why could you be optimistic that the Singularity is Near?", "slug": "why-could-you-be-optimistic-that-the-singularity-is-near", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:29.994Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/veEumGEQAsDknw9PC/why-could-you-be-optimistic-that-the-singularity-is-near", "pageUrlRelative": "/posts/veEumGEQAsDknw9PC/why-could-you-be-optimistic-that-the-singularity-is-near", "linkUrl": "https://www.lesswrong.com/posts/veEumGEQAsDknw9PC/why-could-you-be-optimistic-that-the-singularity-is-near", "postedAtFormatted": "Saturday, July 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20could%20you%20be%20optimistic%20that%20the%20Singularity%20is%20Near%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20could%20you%20be%20optimistic%20that%20the%20Singularity%20is%20Near%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FveEumGEQAsDknw9PC%2Fwhy-could-you-be-optimistic-that-the-singularity-is-near%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20could%20you%20be%20optimistic%20that%20the%20Singularity%20is%20Near%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FveEumGEQAsDknw9PC%2Fwhy-could-you-be-optimistic-that-the-singularity-is-near", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FveEumGEQAsDknw9PC%2Fwhy-could-you-be-optimistic-that-the-singularity-is-near", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<p>A while ago I <a href=\"http://www.gwern.net/Mistakes#near-singularity\">wrote briefly</a> on why the Singularity might not be near and my estimates badly off. I saw it linked the other day, and realized that pessimism seemed to be trendy lately, which meant I ought to work on why one might be optimistic instead: <a href=\"http://www.gwern.net/Mistakes#counter-point\">http://www.gwern.net/Mistakes#counter-point</a></p>\n<p>(Summary: long-sought AI goals have been recently achieved, global economic growth &amp; political stability continues, and some resource crunches have turned into surpluses - all contrary to long-standing pessimistic forecasts.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zHjC29kkPmsdo7WTr": 1, "3RnEKrsNgNEDxuNnw": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "veEumGEQAsDknw9PC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 34, "extendedScore": null, "score": 6.9e-05, "legacy": true, "legacyId": "17645", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-15T05:17:42.293Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Setting Up Metaethics", "slug": "seq-rerun-setting-up-metaethics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:39.153Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2XkMCuStTjttqZdq5/seq-rerun-setting-up-metaethics", "pageUrlRelative": "/posts/2XkMCuStTjttqZdq5/seq-rerun-setting-up-metaethics", "linkUrl": "https://www.lesswrong.com/posts/2XkMCuStTjttqZdq5/seq-rerun-setting-up-metaethics", "postedAtFormatted": "Sunday, July 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Setting%20Up%20Metaethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Setting%20Up%20Metaethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2XkMCuStTjttqZdq5%2Fseq-rerun-setting-up-metaethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Setting%20Up%20Metaethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2XkMCuStTjttqZdq5%2Fseq-rerun-setting-up-metaethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2XkMCuStTjttqZdq5%2Fseq-rerun-setting-up-metaethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<p>Today's post, <a href=\"/lw/sl/setting_up_metaethics/\">Setting Up Metaethics</a> was originally published on 28 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Setting_Up_Metaethics\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>What exactly does a correct theory of metaethics need to look like?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dlv/seq_rerun_changing_your_metaethics/\">Changing Your Metaethics</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2XkMCuStTjttqZdq5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.430111881548728e-07, "legacy": true, "legacyId": "17646", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["T7tYmfD9j25uLwqYk", "CBu8DvBdwQcDd33Ta", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-15T08:58:59.631Z", "modifiedAt": null, "url": null, "title": "Magic players: \"How do I lose?\"", "slug": "magic-players-how-do-i-lose", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:51.193Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tDSpuCyKve7rpPG6d/magic-players-how-do-i-lose", "pageUrlRelative": "/posts/tDSpuCyKve7rpPG6d/magic-players-how-do-i-lose", "linkUrl": "https://www.lesswrong.com/posts/tDSpuCyKve7rpPG6d/magic-players-how-do-i-lose", "postedAtFormatted": "Sunday, July 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Magic%20players%3A%20%22How%20do%20I%20lose%3F%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMagic%20players%3A%20%22How%20do%20I%20lose%3F%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtDSpuCyKve7rpPG6d%2Fmagic-players-how-do-i-lose%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Magic%20players%3A%20%22How%20do%20I%20lose%3F%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtDSpuCyKve7rpPG6d%2Fmagic-players-how-do-i-lose", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtDSpuCyKve7rpPG6d%2Fmagic-players-how-do-i-lose", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 567, "htmlBody": "<p>An excellent habit that I've noticed among professional players of the game Magic: The Gathering is asking the question \"how do I lose?\" - a sort of strategic <a href=\"/lw/iw/positive_bias_look_into_the_dark/\">looking into the dark</a>.</p>\n<p>Imagine this situation: you have an army ready to destroy your opponent in two turns. &nbsp;Your opponent has no creatures under his command. &nbsp;Victory seems inevitable. &nbsp;And so you ask \"how do I lose?\"</p>\n<p>Because your victory is now the default, the options for your opponent are very limited. &nbsp;If you have a big army, they need to play a card that can deal with lots of creatures at once. &nbsp;If you have a good idea what their deck contains, you can often narrow it down to a single card that they need to play in order to turn the game around. &nbsp;And once you know how you could lose, you can plan to avoid it.</p>\n<p>For example, suppose your opponent was playing white. &nbsp;Then their card of choice to destroy a big army would be <a href=\"http://gatherer.wizards.com/Handlers/Image.ashx?multiverseid=129808&amp;type=card\">Wrath of God</a>. &nbsp;That card is the way you could lose. &nbsp;But now that you know that, you can avoid losing to Wrath of God by keeping creature cards in your hand so you can rebuild your army - you'll still win if he doesn't play it, since you winning is the default. &nbsp;But you've made it harder to lose. &nbsp;This is a bit of an advanced technique, since <em>not</em>&nbsp;playing all your cards is counterintuitive.</p>\n<p>A related question is \"how do I win?\" &nbsp;This is the question you ask when you're near to losing. &nbsp;And like above, this question is good to ask because when you're really behind, only a few cards will let you come back. &nbsp;And once you know what those cards are, you can plan for them.</p>\n<p>For example, suppose you have a single creature on your side. &nbsp;The opponent is attacking you with a big army. &nbsp;You have a choice:&nbsp;you can let the attack through and lose in two turns, or&nbsp;you can send your creature out to die in your defense and lose in three turns. &nbsp;If you were trying to postpone losing, you would send out the creature. &nbsp;But you're more likely to actually <em>win</em>&nbsp;if you keep your forces alive - you might draw a sword that makes your creature stronger, or a way to weaken their army, or something. &nbsp;And so you ask \"how do I win?\" to remind yourself of that.</p>\n<p>&nbsp;</p>\n<p>This sort of thinking is highly generalizable. &nbsp;The next time you're, say, packing for a vacation and feel like everything's going great, that's a good time to ask: \"How do I lose? &nbsp;Well, by leaving my wallet behind or by having the car break down - everything else can be fixed. &nbsp;So I'll&nbsp;go put my wallet in my pocket right now, and&nbsp;check the oil and coolant levels in the car.\"</p>\n<p>An analogy is that when you ask \"how do I win?\" you get to disregard your impending loss because you're \"standing on the floor\" - there's a fixed result that you get if you don't win, like calling a tow truck if you're in trouble in the car, or canceling your vacation and staying home. &nbsp;Similarly when you ask \"how do I lose?\" you should be standing on the&nbsp;ceiling, as it were - you're about to achieve a goal that doesn't need to be improved upon, so now's the time to be careful about potential Wraths of God.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RLQumypPQGPYg9t6G": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tDSpuCyKve7rpPG6d", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 53, "extendedScore": null, "score": 0.000129, "legacy": true, "legacyId": "17647", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 53, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rmAbiEKQDpDnZzcRf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-15T17:01:42.358Z", "modifiedAt": null, "url": null, "title": "Bargaining and Auctions", "slug": "bargaining-and-auctions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:34.592Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/um7w5RogAHhxGy8Ti/bargaining-and-auctions", "pageUrlRelative": "/posts/um7w5RogAHhxGy8Ti/bargaining-and-auctions", "linkUrl": "https://www.lesswrong.com/posts/um7w5RogAHhxGy8Ti/bargaining-and-auctions", "postedAtFormatted": "Sunday, July 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bargaining%20and%20Auctions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABargaining%20and%20Auctions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fum7w5RogAHhxGy8Ti%2Fbargaining-and-auctions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bargaining%20and%20Auctions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fum7w5RogAHhxGy8Ti%2Fbargaining-and-auctions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fum7w5RogAHhxGy8Ti%2Fbargaining-and-auctions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2260, "htmlBody": "<p>Some people have things. Other people want them. Economists agree that the eventual price will be set by supply and demand, but both parties have tragically misplaced their copies of the <em>Big Book Of Levels Of Supply And Demand For All Goods</em>. They're going to have to decide on a price by themselves.<br /><br />When the transaction can be modeled by the interaction of one seller and one buyer, this kind of decision usually looks like bargaining. When it's best modeled as one seller and multiple buyers (or vice versa), the decision usually looks like an auction. Many buyers and many sellers produce a marketplace, but this is complicated and we'll stick to bargains and auctions for now.<br /><br />Simple bargains bear some similarity to the Ultimatum Game. Suppose an antique dealer has a table she values at $50, and I go to the antique store and fall in love with it, believing it will add $400 worth of classiness to my room. The dealer should never sell for less than $50, and I should never buy for more than $400, but any value in between would benefit both of us. More specifically, it would give us a combined $350 profit. The remaining question is how to divide that $350 pot.<br /><br />If I make an offer to buy at $60, I'm proposing to split the pot \"$10 for you, $340 for me\". If the dealer makes a counter-offer of $225, she's offering \"$175 for you, $175 for me\" - or an even split.<br /><br />Each round of bargaining resembles the Ultimatum Game because one player proposes to split a pot, and the other player accepts or rejects. If the other player rejects the offer (for example, the dealer refuses to sell it for $60) then the deal falls through and neither of us gets any money.<br /><br />But bargaining is unlike the Ultimatum Game for several reasons. First, neither player is the designated \"offer-maker\"; either player may begin by making an offer. Second, the game doesn't end after one round; if the dealer rejects my offer, she can make a counter-offer of her own. Third, and maybe most important, neither player is exactly sure about the size of the pot: I don't walk in knowing that the dealer bought the table for $50, and I may not really be sure I value the table at $400.<br /><br />Our intuition tells us that the fairest method is to split the profits evenly at a price of $225. This number forms a useful Schelling point (remember those?) that prevents the hassle of further bargaining.<br /><br />The <a href=\"http://www.amazon.com/The-Art-Strategy-Theorists-Business/dp/0393062430\"><em>Art of Strategy</em></a> (see the beginning of Ch. 11) includes a proof that an even split is the rational choice under certain artificial assumptions. Imagine a store selling souvenirs for the 2012 Olympics. They make $1000/day each of the sixteen days the Olympics are going on. Unfortunately, the day before the Olympics, the workers decide to strike; the store will make no money without workers, and they don't have enough time to hire scabs.<br /><br />Suppose Britain has some very strange labor laws that mandate the following negotiation procedure: on each odd numbered day of the Olympics, the labor union representative will approach the boss and make an offer; the boss can either accept it or reject it. On each even numbered day, the boss makes the offer to the labor union.<br /><br />So if the negotiations were to drag on to the sixteenth and last day of the Olympics, on that even-numbered day the boss would approach the labor union rep. They're both the sort of straw man rationalists who would take 99-1 splits on the Ultimatum Game, so she offers the labor union rep $1 of the $1000. Since it's the last day of the Olympics and she's a straw man rationalist, the rep accepts.<br /><br />But on the fifteenth day of the Olympics, the labor union rep will approach the boss. She knows that if no deal is struck today, she'll end out with $1 and the boss will end out with $999. She has to convince the boss to accept a deal on the fifteenth day instead of waiting until the sixteenth. So she offers $1 of the profits from the fifteenth day to the boss, with the labor union keeping the rest; now their totals are $1000 for the workers, $1000 for the boss. Since $1000 is better than $999, the boss agrees to these terms and the strike is ended on the fifteenth day.<br /><br />We can see by this logic that on odd numbered days the boss and workers get the same amount, and on even numbered days the boss gets more than the workers but the ratio converges to 1:1 as the length of the negotiations increase. If they were negotiating an indefinite contract, then even if the boss made the first move we might expect her to offer an even split.<br /><br />So both some intuitive and some mathematical arguments lead us to converge on this idea of an even split of the sort that gives us the table for $225. But if I want to be a &ldquo;hard bargainer&rdquo; - the kind of person who manages to get the table for less than $225 - I have a couple of things I could try.<br /><br />I could deceive the seller as to how much I valued the table. This is a pretty traditional bargaining tactic: &ldquo;That old piece of junk? I'd be doing you a favor for taking it off your hands.&rdquo; Here I'm implicitly claiming that the dealer must have paid less than $50, and that I would get less than $400 worth of value. If the dealer paid $20 and I'd only value it to the tune of $300, then splitting the profit evenly would mean a final price of $160. The dealer could then be expected to counter my move with his own claim as to the table's value: &ldquo;$160? Do I look like I was born yesterday? This table was old in the time of the Norman Conquest! Its wood comes from a tree that grows on an enchanted island in the Freptane Sea which appears for only one day every seven years!&rdquo; The final price might be determined by how plausible we each considered the other's claims.<br /><br />Or I could rig the Ultimatum Game. Used car dealerships are notorious for adding on &ldquo;extras&rdquo; after you've agreed on a price over the phone (&ldquo;Well yes, we agreed the car was $5999, but if you want a steering wheel, that costs another $200.&rdquo;) Somebody (possibly an LWer?) proposed showing up to the car dealership without any cash or credit cards, just a check made out for the agreed-upon amount; the dealer now has no choice but to either take the money or forget about the whole deal. In theory, I could go to the antique dealer with a check made out for $60 and he wouldn't have a lot of options (though do remember that people <a href=\"/lw/dgc/interlude_for_behavioral_economics/\">usually reject</a> ultimata of below about 70-30). The classic bargaining tactic of &ldquo;I am but a poor chimney sweep with only a few dollars to my name and seven small children to feed and I could never afford a price above $60&rdquo; seems closely related to this strategy.<br /><br />And although we're still technically talking about transactions with only one buyer and seller, the mere threat of another seller can change the balance of power drastically. Suppose I tell the dealer I know of another dealer who sells modern art for a fixed price of $300, and that the modern art would add exactly as much classiness to my room as this antique table - that is, I only want one of the two and I'm&nbsp; indifferent between them. Now we're no longer talking about coming up with a price between $50 and $400 - anything over $300 and I'll reject it and go to the other guy. Now we're talking about splitting the $250 profit between $50 and $300, and if we split it evenly I should expect to pay $175.<br /><br />(why not $299? After all, the dealer knows $299 is better than my other offer. Because we're still playing the Ultimatum Game, that's why. And if it was $299, then having a second option - art that I like as much as the table - would actually make my bargaining position worse - after all, I was getting it for $225 before.)<br /><br />Negotiation gurus call this backup option the BATNA (<a href=\"http://en.wikipedia.org/wiki/BATNA\">&ldquo;Best Alternative To Negotiated Agreement&rdquo;</a>) and consider it a useful thing to have. If only one participant in the negotiation has a BATNA greater than zero, that person is less desperate, needs the agreement less, and can hold out for a better deal - just as my $300 art allowed me to lower the asking price of the table from $225 to $175. <br /><br />This &ldquo;one buyer, one seller&rdquo; model is artificial, but from here we can start to see how the real world existence of other buyers and sellers serve as BATNAs for both parties and how such negotiations eventually create the supply and demand of the marketplace.<br /><br />The remaining case is one seller and multiple buyers (or vice versa). Here the seller's BATNA is &ldquo;sell it to the other guy&rdquo;, and so a successful buyer must beat the other guy's price. In practice, this takes the form of an auction (why is this different than the previous example? Partly because in the previous example, we were comparing a negotiable commodity - the table - to a fixed price commodity - the art.)<br /><br />How much should you bid at an auction? In the so-called English auction (the classic auction where a crazy man stands at the front shouting &ldquo;Eighty!!! Eighty!!! We have eighty!!! Do I hear eighty-five?!? Eighty-five?!? Eighty-five to the man in the straw hat!!! Do I hear ninety?!?) the answer should be pretty obvious: keep bidding infinitesmally more than the last guy until you reach your value for the product, then stop. For example, with the $400 table, keep bidding until the price approaches $400.<br /><br />But what about a sealed-bid auction, where everyone hands the auctioneer their bid and the auctioneer gives the product to the highest? Or what about the so-called &ldquo;Dutch auction&rdquo; where the auctioneer starts high and goes lower until someone bites (&ldquo;A hundred?!? Anyone for a hundred?!? No?!? Ninety-five?!? Anyone for...yes?!? Sold for ninety-five to the man in the straw hat!!!).<br /><br />The rookie mistake is to bid the amount you value the product. Remember, economists define &ldquo;the amount you value the product&rdquo; as &ldquo;the price at which you would be indifferent between having the product and just keeping the money&rdquo;. If you go to an auction planning to bid your true value, you should expect to get absolutely zero benefit out of the experience. Instead, you should bid infinitesimally more than what you predict the next highest bidder will pay, as long as this is below your value. <br /><br />Thus, the auction beloved by economists as perhaps the purest example of <a href=\"http://en.wikipedia.org/wiki/Auction#Types\">auction forms</a> is the Vickrey, in which everyone submits a sealed bid, the highest bidder wins, and she pays the amount of the second-highest bid. This auction has a certain very elegant property, which is that here the dominant strategy is to bid your true value. Why? <br /><br />Suppose you value a table at $400. If you try to game the system by bidding $350 instead of $400, you may lose out&nbsp; and can at best break even. Why? Because if the highest other bid was above $400, you wouldn't win the table in either case, and your ploy profits you nothing. And if the highest other bid was between $350 and $400 (let's say $375), now you lose the table and make $0 profit, as opposed to the $25 profit you would have made if you had bid your true value of $400, won, and paid the second-highest bid of $375. And if everyone else is below $350 (let's say $300) then you would have paid $300 in either case, and again your ploy profits you nothing. Bid above your true valuation (let's say $450) and you face similar consequences: either you wouldn't have gotten the table anyway, you get the table for the same amount as before, or you get the table for a value between $400 and $450 and now you're taking a loss.<br /><br />In the real world, English, Dutch, sealed-bid and Vickrey auctions all differ a little in ways like how much information they give the bidders about each other, or whether people get caught up in the excitement of bidding, or what to do when you don't really know your true valuation. But in simplified rational models, they all end at an identical price: the true valuation of the second-highest bidder. <br /><br />In conclusion, the gentlemanly way to bargain is to split the difference in profits between your and your partner's best alternative to an agreement, and gentlemanly auctions tend to end at the value of the second-highest participant. Some less gentlemanly alternatives are also available and will be discussed later.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "um7w5RogAHhxGy8Ti", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 53, "extendedScore": null, "score": 0.000115, "legacy": true, "legacyId": "17648", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 53, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Yy7mgec8tsbTAuTqb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-15T17:29:43.466Z", "modifiedAt": null, "url": null, "title": "Tool/Agent distinction in the light of the AI box experiment", "slug": "tool-agent-distinction-in-the-light-of-the-ai-box-experiment", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:39.262Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jost", "createdAt": "2012-06-23T12:35:00.581Z", "isAdmin": false, "displayName": "Jost"}, "userId": "exmf7eMBRCBnQhWt5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/43DAsQzBRPgxsCreS/tool-agent-distinction-in-the-light-of-the-ai-box-experiment", "pageUrlRelative": "/posts/43DAsQzBRPgxsCreS/tool-agent-distinction-in-the-light-of-the-ai-box-experiment", "linkUrl": "https://www.lesswrong.com/posts/43DAsQzBRPgxsCreS/tool-agent-distinction-in-the-light-of-the-ai-box-experiment", "postedAtFormatted": "Sunday, July 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tool%2FAgent%20distinction%20in%20the%20light%20of%20the%20AI%20box%20experiment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATool%2FAgent%20distinction%20in%20the%20light%20of%20the%20AI%20box%20experiment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F43DAsQzBRPgxsCreS%2Ftool-agent-distinction-in-the-light-of-the-ai-box-experiment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tool%2FAgent%20distinction%20in%20the%20light%20of%20the%20AI%20box%20experiment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F43DAsQzBRPgxsCreS%2Ftool-agent-distinction-in-the-light-of-the-ai-box-experiment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F43DAsQzBRPgxsCreS%2Ftool-agent-distinction-in-the-light-of-the-ai-box-experiment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 320, "htmlBody": "<p>This article poses questions on the distinction between Tool AGI and Agent AGI, which was&nbsp;described very concisely by Holden Karnofsky in his recent <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">Thoughts on the Singularity Institute</a>&nbsp;post:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 18px;\">In short, Google Maps is not an&nbsp;<em style=\"font-style: italic;\">agent</em>, taking actions in order to maximize a utility parameter. It is a&nbsp;<em style=\"font-style: italic;\">tool</em>, generating information and then displaying it in a user-friendly manner for me to consider, use and export or discard as I wish.</span></p>\n</blockquote>\n<p>For me, this instantly raised one question: What if a Tool AGI becomes/is self-aware (<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 18px;\">which, for the purposes of this post, I define as &ldquo;able to have goals that are distinct from the goals of the outside world&rdquo;</span>) and starts manipulating its results in a way that is non-obvious to its user? Or, even worse: What if the Tool AGI <em>makes</em> its user do things (which I do not expect to be much more difficult than succeding in the <a href=\"http://yudkowsky.net/singularity/aibox\">AI box experiment</a>)?</p>\n<p>My first reaction was to flinch away by telling myself: &ldquo;But <em>of course</em> a Tool would never become self-aware! Self-awareness is too <em>complex</em> to just happen unintentionally!&rdquo;</p>\n<p>But some uncertainty survived and was strenghtened by Eliezer's <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">reply to Holden</a>:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 18px;\">[Tool AGI] starts sounding much scarier once you try to say something more formal and internally-causal like \"Model the user and the universe, predict the degree of correspondence between the user's model and the universe, and select from among possible explanation-actions on this basis.\"</span></p>\n</blockquote>\n<p>After all, &ldquo;Self-awareness is too&nbsp;<em>complex</em>&nbsp;to just happen unintentionally!&rdquo; is just a bunch of English words expressing my <a href=\"http://yourlogicalfallacyis.com/personal-incredulity\">personal incredulity</a>. It's not a valid argument.</p>\n<p>So, can we make the argument, that self-awareness will not happen unintentionally?</p>\n<p>If we can't make that argument, can we stop Tool AGIs from potentially becoming a <em>Weak Agent AGI</em>&nbsp;which acts through its human user?</p>\n<p>If we can't do that, how meaningful is the distinction between a Weak Agent AGI (a.k.a. Tool AGI) and an Agent AGI?</p>\n<p>&nbsp;</p>\n<p><span style=\"color:#bbb;\">For more, see the&nbsp;<a href=\"/lw/cfd/tools_versus_agents/\">Tools versus Agents</a>&nbsp;post by Stuart_Armstrong, which points to similar questions.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "43DAsQzBRPgxsCreS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -7, "extendedScore": null, "score": 9.433579136262224e-07, "legacy": true, "legacyId": "17189", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6SGqkCgHuNr7d4yJm", "sizjfDgCgAsuLJQmm", "nAwTGhgrdxE85Bjmg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-16T04:22:27.255Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Meaning of Right", "slug": "seq-rerun-the-meaning-of-right", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:06.336Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/naf75ycixrG3odTB4/seq-rerun-the-meaning-of-right", "pageUrlRelative": "/posts/naf75ycixrG3odTB4/seq-rerun-the-meaning-of-right", "linkUrl": "https://www.lesswrong.com/posts/naf75ycixrG3odTB4/seq-rerun-the-meaning-of-right", "postedAtFormatted": "Monday, July 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Meaning%20of%20Right&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Meaning%20of%20Right%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnaf75ycixrG3odTB4%2Fseq-rerun-the-meaning-of-right%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Meaning%20of%20Right%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnaf75ycixrG3odTB4%2Fseq-rerun-the-meaning-of-right", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnaf75ycixrG3odTB4%2Fseq-rerun-the-meaning-of-right", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 148, "htmlBody": "<p>Today's post, <a href=\"/lw/sm/the_meaning_of_right/\">The Meaning of Right</a> was originally published on 29 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#The_Meaning_of_Right\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Eliezer's long-awaited theory of meta-ethics.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dm6/seq_rerun_setting_up_metaethics/\">Setting Up Metaethics</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "naf75ycixrG3odTB4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 9.436672732331959e-07, "legacy": true, "legacyId": "17657", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fG3g3764tSubr6xvs", "2XkMCuStTjttqZdq5", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-16T09:10:43.863Z", "modifiedAt": null, "url": null, "title": "Revisiting SI's 2011 strategic plan: How are we doing?", "slug": "revisiting-si-s-2011-strategic-plan-how-are-we-doing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:54.583Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FD85FtP3tyHAarvwx/revisiting-si-s-2011-strategic-plan-how-are-we-doing", "pageUrlRelative": "/posts/FD85FtP3tyHAarvwx/revisiting-si-s-2011-strategic-plan-how-are-we-doing", "linkUrl": "https://www.lesswrong.com/posts/FD85FtP3tyHAarvwx/revisiting-si-s-2011-strategic-plan-how-are-we-doing", "postedAtFormatted": "Monday, July 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Revisiting%20SI's%202011%20strategic%20plan%3A%20How%20are%20we%20doing%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARevisiting%20SI's%202011%20strategic%20plan%3A%20How%20are%20we%20doing%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFD85FtP3tyHAarvwx%2Frevisiting-si-s-2011-strategic-plan-how-are-we-doing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Revisiting%20SI's%202011%20strategic%20plan%3A%20How%20are%20we%20doing%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFD85FtP3tyHAarvwx%2Frevisiting-si-s-2011-strategic-plan-how-are-we-doing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFD85FtP3tyHAarvwx%2Frevisiting-si-s-2011-strategic-plan-how-are-we-doing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1975, "htmlBody": "<p><a href=\"http://intelligence.org/blog/category/monthly-progress/\"><img style=\"float: right;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2012/07/logo-200x166.jpg\" alt=\"\" width=\"200\" height=\"166\" />Progress updates</a> are nice, but without a previously defined metric for success it's hard to know whether an organization's achievements are noteworthy or not. Is SI making good progress, or underwhelming progress?</p>\n<p>Luckily, in August 2011 we published a <a href=\"http://intelligence.org/files/strategicplan20112.pdf\">strategic plan</a> that outlined lots of specific goals. It's now almost August 2012, so we can check our progress against the standard set nearly one year ago. The plan doesn't specify a timeline for the stated goals, but I remember hoping that we could do most of them by the end of 2012, while understanding that we should list more goals than we could actually accomplish given current resources.</p>\n<p>Let's walk through the goals in that strategic plan, one by one. (Or, you can&nbsp;<a href=\"#summary\">skip</a> to the \"summary and path forward\" section at the end.)</p>\n<p>&nbsp;</p>\n<p><em>1.1.&nbsp;Clarify the open problems relevant to our core mission</em></p>\n<p>This was accomplished to some degree with <a href=\"http://lukeprog.com/SaveTheWorld.html\">So You Want to Save the World</a>, and is on track to be accomplished to a greater degree with Eliezer's sequence \"Open Problems in Friendly AI,\" which you should begin seeing late in August.</p>\n<p>&nbsp;</p>\n<p><em>1.2.&nbsp;Identify and recruit researcher candidates who can solve research problems.</em></p>\n<p>Several strategies for doing this were listed, but the only one worth doing at our current level of funding was to recruit more research associates and hire more researchers. Since August 2011 we have done both, adding half a dozen research associates and hiring nearly a dozen remote researchers, including a few who are working full-time on papers and other projects (e.g. Kaj Sotala).</p>\n<p>&nbsp;</p>\n<p><em>1.3.&nbsp;Use researchers and research associates to solve open problems related to Friendly AI&nbsp;theory.</em></p>\n<p>I never planned to be doing this by the end of 2012; it's more of a long-term goal. A first step in this direction is to have Eliezer transition back to FAI work, e.g. with his \"Open Problems in Friendly AI\" Summit 2011 talk and forthcoming blog sequence. And actually, SI research associate Vladimir Slepnev has been making interesting progress in LW-style decision theory, and is working on a paper explicating one of his results. (Some credit is due to Vladimir Nesov and others.)</p>\n<p>&nbsp;</p>\n<p><em>1.4.&nbsp;Estimate current AI risk levels.</em></p>\n<p>Alas, we haven't done much of this. There's some analysis in <a href=\"http://intelligence.org/files/IE-EI.pdf\">Intelligence Explosion: Evidence and Import</a>, <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Reply to Holden on Tool AI</a>, and <a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/#disjunctive\">Reply to Holden on The Singularity Institute</a>. Also, Anna is working on a simple model of AI risk in MATLAB (or some similar program). But I would have liked to have the cash to hire a researcher to continue things like <a href=\"/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">AI Risk and Opportunity: A Strategic Analysis</a>.</p>\n<p>&nbsp;</p>\n<p><em>2.1.&nbsp;Continue operation of the Singularity Summit, which is beginning to yield a profit while&nbsp;also reaching more people with our message.</em></p>\n<p>We did run Singularity Summit 2011, and Singularity Summit 2012 is on track to be noticeably more fun and professional than all past Summits. (So, <a href=\"https://www.thinkreg.com/events/singularity/\">register now</a>!)</p>\n<p>The strategic plan listed subgoals of gaining corporate sponsors and <em>possibly</em>&nbsp;expanding the Summit outside the USA. We gained corporate sponsors for Summit 2011, and are on track to gain even more of them for Summit 2012. Early in 2011 we also pursued an opportunity to host the first Singularity Summit in Europe, but the financing didn't quite come through.</p>\n<p>&nbsp;</p>\n<p><em>2.2&nbsp;Cultivate LessWrong.com and the greater rationality community as a resource for&nbsp;Singularity Institute.</em></p>\n<p>The strategic plan lists 5 subgoals, all of which we achieved. SI (a) used LessWrong to recruit additional supporters, (b) made use of LessWrong for collaborative problem solving (e.g. <a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle/\">this</a> and <a href=\"/lw/9o7/formulas_of_arithmetic_that_behave_like_decision/\">this</a>), (c) published lots of top-level posts, (d) and published <em><a href=\"/lw/crs/how_to_run_a_successful_less_wrong_meetup/\">How to Run a Successful Less Wrong Meetup Group</a></em>. The early efforts of <a href=\"http://appliedrationality.org/\">CFAR</a>, and our presence at (e.g.) Skepticon IV, made headway on 2.2.e: \"Encourage improvements in critical thinking in the wider world. We need a larger&nbsp;community of critical thinkers for use in recruiting, project implementation, and&nbsp;fundraising.\"</p>\n<p>&nbsp;</p>\n<p><em>2.3.&nbsp;Spread our message and clarify our arguments with public-facing academic deliverables.</em></p>\n<p>We did exceptionally well on this, though <a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/#clarity\">much more is needed</a>. In addition to detailed posts like <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Reply to Holden on Tool AI</a> and <a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/\">Reply to Holden on the Singularity Institute</a>, SI has <a href=\"/lw/axr/three_new_papers_on_ai_risk/627o\">more peer-reviewed publications in 2012 than in all past years combined</a>.</p>\n<p>&nbsp;</p>\n<p><em>2.4.&nbsp;Build more relationships with the optimal philanthropy, humanist, and critical thinking&nbsp;communities, which share many of our values.</em></p>\n<p>Though this work has been mostly invisible, Carl Shulman has spent dozens of hours on building relationships with the optimal philanthropy community. We've also built relationships with the humanist and critical thinking communities, through our presence at Skepticon IV but especially through the early activities of CFAR.</p>\n<p>&nbsp;</p>\n<p><em>2.5.&nbsp;Cultivate and expand Singularity Institute&rsquo;s Volunteer Program.</em></p>\n<p>SI's volunteer program got a <a href=\"http://www.singularityvolunteers.org/\">new website</a> (though we'd like to launch another redesign soon), and we estimate that SI volunteers have done 2x-5x more work per month this year than in the past few years.</p>\n<p>&nbsp;</p>\n<p><em>2.6. Improve Singularity Institute&rsquo;s web presence.</em></p>\n<p>Done. We got a new domain, Singularity.org, and put up a <a href=\"http://intelligence.org/\">new website</a> there. We produced additional introductory materials, like <a href=\"http://friendly-ai.com/\">Friendly-AI.com</a> and <a href=\"http://intelligenceexplosion.com/\">IntelligenceExplosion.com</a>. We produced lots of \"landing pages,\" for example our <a href=\"http://intelligence.org/techsummaries/\">tech summaries</a>. We did not, however, complete subgoals (d) and (e) &mdash; \"Continue to produce articles on targeted websites and other venues\" and \"Produce high-quality videos to explain Singularity Institute&rsquo;s mission\" &mdash; because their ROI isn't high enough to do at our current funding level.</p>\n<p>&nbsp;</p>\n<p><em>2.7.&nbsp;Apply for grants, especially ones that are given to other organizations and researchers&nbsp;concerned with the safety of future technologies (e.g. synthetic biology and&nbsp;nanotechnology).</em></p>\n<p>This one was always meant as a longer-range goal. SI still needs to be \"fixed up\" in certain ways before this is worth trying.</p>\n<p>&nbsp;</p>\n<p><em>2.8.&nbsp;Continue targeted interactions with the public.</em></p>\n<p>We didn't do much of this, either. In particular, Eliezer's rationality books are on hold for now; we have the author of a best-selling science book on retainer to take a crack at Eliezer's rationality books this fall, after he completes his current project.</p>\n<p>&nbsp;</p>\n<p><em>2.9.&nbsp;Improve interactions with current and past donors.</em></p>\n<p>Success. We created and cleaned up our donor database, communicated more regularly with our support base (previously via <a href=\"http://intelligence.org/blog/category/monthly-progress/\">monthly updates</a> and now our shiny new newsletter, which you can sign up for <a href=\"http://intelligence.org/\">here</a>), and updated our <a href=\"http://intelligence.org/topdonors/\">top donors</a> list.</p>\n<p>&nbsp;</p>\n<p><em>3.1.&nbsp;Encourage a new organization to begin rationality instruction similar to what Singularity&nbsp;Institute did in 2011 with Rationality Minicamp and Rationality Boot Camp.</em></p>\n<p>This is perhaps the single most impressive thing we did this year, in the sense that it required dozens of smaller pieces to all work, and work together. The organization is now called the Center for Applied Rationality (CFAR), and it was recently approved for 501c3 status. It has its own <a href=\"http://appliedrationality.org/\">website</a>, has been running extremely well-reviewed <a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/\">rationality retreats</a>, and has lots more exciting stuff going on that hasn't been described online yet. <a href=\"http://appliedrationality.org/#newsletter\"><strong>Sign up for CFAR's newsletter</strong></a> to get these juicy details when they are written up.</p>\n<p>&nbsp;</p>\n<p><em>3.2. Use Charity Navigator&rsquo;s guidelines to improve financial and organizational transparency&nbsp;and efficiency.</em></p>\n<p>There are 9 subgoals listed here. We've since decided we <em>don't</em>&nbsp;want to grow to five independent board members (subgoal b) at this time, because a smaller board runs more efficiently. (I've now heard too many nightmare stories about trying to get things done with a large board.) We <em>did</em>&nbsp;achieve (a), (d), (e), (g), (h), and (i).&nbsp;Subgoal (c) is a longer term goal that we are working toward (we need a professional bookkeeper to clean up our internal processes before we can have a hired CPA audit, and we're interviewing bookkeepers now). Subgoal (f) &mdash; a records retention policy &mdash; is in the works.</p>\n<p>&nbsp;</p>\n<p><em>3.3. Ensure a proper orientation for new Singularity Institute staff and visiting fellows.</em></p>\n<p>This is in process; we're creating orientation materials.</p>\n<p>&nbsp;</p>\n<p><em>3.4.&nbsp;Secure lines of credit to increase liquidity and smooth out the recurring cash-flow&nbsp;pinches that result from having to do things like make payroll and rent event spaces.</em></p>\n<p>We've done this.</p>\n<p>&nbsp;</p>\n<p><em>3.5. Improve safe return on financial reserves</em></p>\n<p>For starters, we put a large chunk of our resources in an ING Direct high-interest savings account.</p>\n<p>&nbsp;</p>\n<p><em>3.6.&nbsp;Ensure high standards for staff effectiveness.</em></p>\n<p>There are two subgoals here. Subgoal (b) was to have staff maintain work logs, which we've been doing for many months now. Subgoal (a) is more ambiguous. We haven't given people job descriptions because at such a small organization, such roles change quickly. But I do provide stronger management of SI staff and projects than ever before, and this clarifies the expectations for our staff, often including task and project deadlines.</p>\n<p>&nbsp;</p>\n<p><em>3.7.&nbsp;When hiring, advertise for applications to find the best candidates.</em></p>\n<p>We've been doing this for several months now, e.g. <a href=\"http://intelligence.org/opportunities/\">here</a>&nbsp;and <a href=\"/lw/bke/the_singularity_institute_still_needs_remote/\">here</a>.</p>\n<p>&nbsp;</p>\n<h3 id=\"summary\">Summary</h3>\n<p>That's it for the main list! Now let's check in on what we said <strong>our top priorities for 2011-2012</strong>&nbsp;were:</p>\n<ol>\n<li><em>Public-facing research on creating a positive singularity</em>. Check.&nbsp;<a href=\"/lw/axr/three_new_papers_on_ai_risk/627o\">SI has more peer-reviewed publications in 2012 than in all past years combined</a>.</li>\n<li><em>Outreach / education / fundraising</em>. Check. Especially, through <a href=\"http://appliedrationality.org/\">CFAR</a>.</li>\n<li><em>Improved organizational effectiveness</em>. Check. <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6jzn\">Lots of good progress</a> on this.</li>\n<li><em>Singularity Summit</em>. <a href=\"http://singularitysummit.com/\">Check</a>.</li>\n</ol>\n<p>In summary, I think SI is a bit behind where I hoped we'd be by now, though this is largely because we've poured so much into launching <a href=\"http://appliedrationality.org/\">CFAR</a>, and as a result, CFAR has turned out to be significantly more cool at launch than I had anticipated.</p>\n<p>Fundraising has been a challenge. One donor failed to actually give their $46,000 pledge despite repeated reminders and requests, and our support base is (understandably) anxious to see a shift from movement-building work to FAI research, a shift I have been fighting for since I was made Executive Director. (Note that spinning off rationality work to CFAR is a substantial part of trimming SI down into being primarily an FAI research institute.)</p>\n<p>Reforming SI into a more efficient, effective organization has been my greatest challenge. Frankly, SI was in <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6l4h\">pretty bad shape</a> when Louie and I arrived as interns in April 2011, and there have been an incredible number of holes to dig SI out of &mdash; and several more remain. (In contrast, it has been a <em>joy</em>&nbsp;to help set up CFAR properly <em>from the very beginning</em>, with all the right organizational tools and processes in place.) Reforming SI presents a fundraising problem, because reforming SI is time consuming and sometimes costly, but is generally unexciting to donors. I can see the light at the end of the tunnel, though. We won't reach it if we can't improve our fundraising success in the next 3-6 months, but it's close enough that I can see it.</p>\n<p>SI's path forward, from my point of view, looks like this:</p>\n<ol>\n<li>We finish launching CFAR, which takes over the rationality work SI was doing. (Before January 2013.)</li>\n<li>We change how the Singularity Summit is planned and run so that it pulls our core staff away from core mission work to a lesser degree. (Before January 2013.)</li>\n<li>Eliezer writes the \"Open Problems in Friendly AI\" sequence. (Before January 2013.)</li>\n<li>We hire 1-2 researchers to produce technical write-ups from <a href=\"http://intelligence.org/files/TDT.pdf\">Eliezer's TDT article</a> and from his \"Open Problems in Friendly AI\" sequence. (Beginning September 2012, except that right now we don't have the cash to hire the 1-2 people who I know who&nbsp;<em>could</em>&nbsp;do this and who <em>want</em>&nbsp;to do this as soon as we have the money to hire them.)</li>\n<li>With the \"Open FAI Problems\" sequence and the technical write-ups in hand, we greatly expand our efforts to show math/compsci researchers that there is a tractable, technical research program in FAI theory, and as a result some researchers work on the sexiest of these problems from their departments, and some other math researchers take more seriously the prospect of being <em>hired</em> by SI to do technical research in FAI theory. (Beginning, roughly, in April 2013.)&nbsp;Also: There won't be classes on x-risk at <a href=\"http://appliedrationality.org/sparc.html\">SPARC</a>&nbsp;(rationality camp for young elite math talent), but some SPARC students might end up being interested in FAI stuff by osmosis.&nbsp;</li>\n<li>With a more tightly honed SI, improved fundraising practices, and visible mission-central research happening, SI is able to attract more funding and hire even more FAI researchers. (Beginning, roughly, in September 2013.)</li>\n</ol>\n<p>If you want to help us make this happen, please <a href=\"http://intelligence.org/blog/2012/07/03/summer-challenge/\">donate during our July matching drive!</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1, "zcvsZQWJBFK6SxK4K": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FD85FtP3tyHAarvwx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 46, "extendedScore": null, "score": 9.438039583118275e-07, "legacy": true, "legacyId": "17649", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sizjfDgCgAsuLJQmm", "i2XoqtYEykc4XWp9B", "Bj244uWzDBXvE2N2S", "yX9pMZik7r38da7Fc", "qMuAazqwJvkvo8teR", "5GskScdvYXBpL78wL", "fkhbBE2ZTSytvsy9x", "T5WjwPRPHy8mPrJcJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-16T11:01:12.406Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne Social Meetup", "slug": "meetup-melbourne-social-meetup-16", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:40.217Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yurifury", "createdAt": "2010-09-23T04:07:21.609Z", "isAdmin": false, "displayName": "Yurifury"}, "userId": "uSLxvtxdCksnaWaQB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iuDgFkP7ApRc4SkiN/meetup-melbourne-social-meetup-16", "pageUrlRelative": "/posts/iuDgFkP7ApRc4SkiN/meetup-melbourne-social-meetup-16", "linkUrl": "https://www.lesswrong.com/posts/iuDgFkP7ApRc4SkiN/meetup-melbourne-social-meetup-16", "postedAtFormatted": "Monday, July 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20Social%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20Social%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiuDgFkP7ApRc4SkiN%2Fmeetup-melbourne-social-meetup-16%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20Social%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiuDgFkP7ApRc4SkiN%2Fmeetup-melbourne-social-meetup-16", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiuDgFkP7ApRc4SkiN%2Fmeetup-melbourne-social-meetup-16", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/bu'>Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 July 2012 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Ben's house - see mailing list - Carlton, VIC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's next social meetup is on Friday, 20th of July. Once again, it'll be at Ben's house - see the mailing list for location details, or I can give you the address - you can call me at 0401 584 380, or email me at yuri@tbqh.net.\nWe'll organise some sort of take-away for dinner. BYO drinks.\nIt will help attendance numbers to comment, saying you'll be attending. We look forward to seeing you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/bu'>Melbourne Social Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iuDgFkP7ApRc4SkiN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.438563491813593e-07, "legacy": true, "legacyId": "17667", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup\">Discussion article for the meetup : <a href=\"/meetups/bu\">Melbourne Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 July 2012 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Ben's house - see mailing list - Carlton, VIC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Melbourne's next social meetup is on Friday, 20th of July. Once again, it'll be at Ben's house - see the mailing list for location details, or I can give you the address - you can call me at 0401 584 380, or email me at yuri@tbqh.net.\nWe'll organise some sort of take-away for dinner. BYO drinks.\nIt will help attendance numbers to comment, saying you'll be attending. We look forward to seeing you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_Social_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/bu\">Melbourne Social Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne Social Meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_Social_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-16T12:47:22.516Z", "modifiedAt": null, "url": null, "title": "Open Thread, July 16-31, 2012", "slug": "open-thread-july-16-31-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:54.141Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rAyKNKAahNftzoRGo/open-thread-july-16-31-2012", "pageUrlRelative": "/posts/rAyKNKAahNftzoRGo/open-thread-july-16-31-2012", "linkUrl": "https://www.lesswrong.com/posts/rAyKNKAahNftzoRGo/open-thread-july-16-31-2012", "postedAtFormatted": "Monday, July 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20July%2016-31%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20July%2016-31%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrAyKNKAahNftzoRGo%2Fopen-thread-july-16-31-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20July%2016-31%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrAyKNKAahNftzoRGo%2Fopen-thread-july-16-31-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrAyKNKAahNftzoRGo%2Fopen-thread-july-16-31-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post, even in Discussion, it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rAyKNKAahNftzoRGo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 9.439067022886065e-07, "legacy": true, "legacyId": "17668", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 142, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-16T20:03:19.130Z", "modifiedAt": null, "url": null, "title": "What about a line of retreat for the psychologists?", "slug": "what-about-a-line-of-retreat-for-the-psychologists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:56.197Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BT_Uytya", "createdAt": "2011-12-03T16:41:14.863Z", "isAdmin": false, "displayName": "BT_Uytya"}, "userId": "Enh7Ap3zRTQDR4gMH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8f4wKmGbdEFFxxnCh/what-about-a-line-of-retreat-for-the-psychologists", "pageUrlRelative": "/posts/8f4wKmGbdEFFxxnCh/what-about-a-line-of-retreat-for-the-psychologists", "linkUrl": "https://www.lesswrong.com/posts/8f4wKmGbdEFFxxnCh/what-about-a-line-of-retreat-for-the-psychologists", "postedAtFormatted": "Monday, July 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20about%20a%20line%20of%20retreat%20for%20the%20psychologists%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20about%20a%20line%20of%20retreat%20for%20the%20psychologists%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8f4wKmGbdEFFxxnCh%2Fwhat-about-a-line-of-retreat-for-the-psychologists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20about%20a%20line%20of%20retreat%20for%20the%20psychologists%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8f4wKmGbdEFFxxnCh%2Fwhat-about-a-line-of-retreat-for-the-psychologists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8f4wKmGbdEFFxxnCh%2Fwhat-about-a-line-of-retreat-for-the-psychologists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 297, "htmlBody": "<p>The road to the truth is paved with revelations; sometimes those revelations are uncomfortable. Also, you can never go back; it is impossible to unlearn something.</p>\n<p>The problem is, if you go too far, those who fell behind will stop to hear your voice. You want them to be closer, and sometimes the only way to achieve it is to guide them to the truth. But the road is paved with the uncomfortable revelations. Oops.</p>\n<p>So far, I remember two big uncomfortable revelations: the first is that we live beyond the reach of god, and the second is that the psychology isn't nearly effective as everybody thinks.</p>\n<p>If I had been completely honest with the people around, I would have told them about <em><a href=\"/lw/2j/schools_proliferating_without_evidence/\">House of the Cards</a></em>. But I'm not. It is too cruel to say \"Hey, your world-view is wrong and your competence is just an illusion\" to the psychologists and soon-to-be-psychologists. Hence I'm afraid to say even innocent \"Hey, I read a very interesting book yesterday\" to the fellow CS students (because I don't know whether they have psychologist relatives).</p>\n<p>This situation seems very wrong to me, but I understand that the reality is unfair and I'm lucky that I can be an atheist without fear of alienation, unlike the poor souls living in the bible belt. I'm just going to be very careful with my words concerning psychology. Of course I should be more cautious and patient while talking with strangers in *Guardian Of The Truth* mode, no surprise here.</p>\n<p>But still.</p>\n<p>Sometimes I wonder what I'm going to do if I <em>really</em> need to tell somebody that very often psychology is useless and sometimes it is even dangerous. What should I do to prevent their flinching from the truth? How to make the reality look more comfortable to them?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8f4wKmGbdEFFxxnCh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": -19, "extendedScore": null, "score": 9.441135094689385e-07, "legacy": true, "legacyId": "17616", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JnKCaGcgZL4Rsep8m"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-16T21:03:51.886Z", "modifiedAt": null, "url": null, "title": "Tips for Starting Group Houses", "slug": "tips-for-starting-group-houses", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:36.177Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ShannonFriedman", "createdAt": "2012-06-19T16:21:31.296Z", "isAdmin": false, "displayName": "ShannonFriedman"}, "userId": "yzRAjgwgXY3bbapsP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/idWa2z998xv4ozRP6/tips-for-starting-group-houses", "pageUrlRelative": "/posts/idWa2z998xv4ozRP6/tips-for-starting-group-houses", "linkUrl": "https://www.lesswrong.com/posts/idWa2z998xv4ozRP6/tips-for-starting-group-houses", "postedAtFormatted": "Monday, July 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tips%20for%20Starting%20Group%20Houses&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATips%20for%20Starting%20Group%20Houses%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FidWa2z998xv4ozRP6%2Ftips-for-starting-group-houses%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tips%20for%20Starting%20Group%20Houses%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FidWa2z998xv4ozRP6%2Ftips-for-starting-group-houses", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FidWa2z998xv4ozRP6%2Ftips-for-starting-group-houses", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 552, "htmlBody": "<p>I've lived in several intentional communities and have been one of the creators of two that I've lived in. After recently securing Zendo, I wrote up some tips to a friend who is thinking about starting another Berkeley house, which <a href=\"/user/Nisan\">Nisan</a>&nbsp;pointed out might be valuable to share with the community at large, so here it is guys! These tips apply most to shared rental places, as opposed to bought property, although the stuff about vision applies to pretty much any joint venture that people embark on with shared leadership, such as group housing, event planning, and start-ups. The part about using padmapper.com and acting quickly applies a lot to Berkeley in particular, because the housing market in this city is especially messed up with rent control and thus finding good places is particularly difficult. We found our place over a month in advance of move in, and it was about $200-300 cheaper/room than similar places in the vicinity/class we were looking at that showed up during the same timeframe. Using these techniques is how we got it&nbsp;<span style=\"color: #222222; font-family: arial, sans-serif; line-height: 17.77777862548828px;\">&mdash;</span>&nbsp;I called within an hour of the posting thanks to being on email when I got the padmapper alert, set up an appointment to see the property manager right away, and while I was there in person, another person made her an offer of a downpayment on the spot. I'm pretty certain that its because of my pro-activeness and handing her a big stack of rental agreements and credit reports that we got this place rather than the several other interested parties. Many thanks to <a href=\"/user/Kevin\">Kevin Fischer</a>, <a href=\"/user/Louie\">Louie Helm</a>, and&nbsp;<a href=\"/user/Eliezer_Yudkowsky\">Eliezer Yudkowsky</a>&nbsp;for helping me with the rental search and&nbsp;acquisition&nbsp;information!</p>\n<p>&nbsp;</p>\n<p><strong>Tips:</strong></p>\n<ul>\n<li><span style=\"font-size: 12.666666984558105px;\">Arrange meetings with people a week or two in advance over <a href=\"http://doodle.com\">Doodle.com</a> to find dates/times that work for large groups. You might not be able to accomodate everyone even doing that and might need to pick the day that the largest number of people can make it. You can do things like record calls or have people Skype in or do a conference line.</span></li>\n</ul>\n<ul>\n<li><span style=\"font-size: 12.666666984558105px;\">Get credit reports from everyone. There are services for free reports. &nbsp;</span></li>\n</ul>\n<ul>\n<li><span style=\"font-size: 12.666666984558105px;\">Download the standard application form and have everyone fill it out, so that you can deliver both credit reports and applications on the spot when you go to check a place out.</span></li>\n</ul>\n<ul>\n<li><span style=\"font-size: 12.666666984558105px;\">Use&nbsp;</span><a style=\"font-size: 12.666666984558105px; color: #1155cc;\" href=\"http://padmapper.com/\" target=\"_blank\">padmapper.com</a><span style=\"font-size: 12.666666984558105px;\">&nbsp;</span><span style=\"font-size: 12.666666984558105px;\">to track the areas you're interested in. Contact places asap&nbsp;<span style=\"color: #222222; font-family: arial, sans-serif; font-size: small; line-height: 17.77777862548828px;\">&mdash;</span>&nbsp;good places will go in a single day sometimes.</span></li>\n</ul>\n<ul>\n<li><span style=\"font-size: 12.666666984558105px;\">Make sure your group has a unified vision about what they want. If one person wants a pretty place, and one person wants a cheap place, they will block each other and you won't ever reach consensus. Not having a unified vision, and not realizing this, is where the majority of would-be co-housing communities fail. &nbsp;If there's something you want people to have as a house culture, make sure thats in agreement too. I personally requested that people be willing to chip in for a maid and hot tub and maintain a paleo 2.0 kitchen, and these things have all been adopted. Vision can include anything from over-arching life goals to never having dishes cluttering the sink. Explore what you really care about and make sure that everyone explicitly agrees to whatever goals are set and that no one is silently dissatisfied.</span></li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "idWa2z998xv4ozRP6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 20, "extendedScore": null, "score": 9.441422381695569e-07, "legacy": true, "legacyId": "17669", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-17T00:03:54.197Z", "modifiedAt": null, "url": null, "title": "Meetup : Monthly Ohio meetup", "slug": "meetup-monthly-ohio-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:53.120Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/b4oYZ252HTXCryv5m/meetup-monthly-ohio-meetup", "pageUrlRelative": "/posts/b4oYZ252HTXCryv5m/meetup-monthly-ohio-meetup", "linkUrl": "https://www.lesswrong.com/posts/b4oYZ252HTXCryv5m/meetup-monthly-ohio-meetup", "postedAtFormatted": "Tuesday, July 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Monthly%20Ohio%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Monthly%20Ohio%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb4oYZ252HTXCryv5m%2Fmeetup-monthly-ohio-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Monthly%20Ohio%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb4oYZ252HTXCryv5m%2Fmeetup-monthly-ohio-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb4oYZ252HTXCryv5m%2Fmeetup-monthly-ohio-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/bv\">Monthly Ohio meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">22 July 2012 04:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong>&nbsp;<a href=\"http://dubpub.com/home/\">The Dublin Pub</a>,&nbsp;<span class=\"address\">300 Wayne Avenue, Dayton</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Our usual monthly meetup; note the unusual date and place. This month's topic is the next set of minicamp exercises. Erica, Elizabeth, Alex, and Rolf have said they'll be there.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/bv\">Monthly Ohio meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "b4oYZ252HTXCryv5m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 9.442276745899309e-07, "legacy": true, "legacyId": "17670", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Monthly_Ohio_meetup\">Discussion article for the meetup : <a href=\"/meetups/bv\">Monthly Ohio meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">22 July 2012 04:00:00PM (-0400)</span></p>\n<p><strong>WHERE:</strong>&nbsp;<a href=\"http://dubpub.com/home/\">The Dublin Pub</a>,&nbsp;<span class=\"address\">300 Wayne Avenue, Dayton</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Our usual monthly meetup; note the unusual date and place. This month's topic is the next set of minicamp exercises. Erica, Elizabeth, Alex, and Rolf have said they'll be there.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Monthly_Ohio_meetup1\">Discussion article for the meetup : <a href=\"/meetups/bv\">Monthly Ohio meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Monthly Ohio meetup", "anchor": "Discussion_article_for_the_meetup___Monthly_Ohio_meetup", "level": 1}, {"title": "Discussion article for the meetup : Monthly Ohio meetup", "anchor": "Discussion_article_for_the_meetup___Monthly_Ohio_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-17T00:46:26.935Z", "modifiedAt": null, "url": null, "title": "Exploiting the Typical Mind Fallacy for more accurate questioning?", "slug": "exploiting-the-typical-mind-fallacy-for-more-accurate", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:37.142Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Xachariah", "createdAt": "2011-02-03T00:44:58.546Z", "isAdmin": false, "displayName": "Xachariah"}, "userId": "sD2senrXqugoP4wit", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hGe6p5Mk5KwuP49kK/exploiting-the-typical-mind-fallacy-for-more-accurate", "pageUrlRelative": "/posts/hGe6p5Mk5KwuP49kK/exploiting-the-typical-mind-fallacy-for-more-accurate", "linkUrl": "https://www.lesswrong.com/posts/hGe6p5Mk5KwuP49kK/exploiting-the-typical-mind-fallacy-for-more-accurate", "postedAtFormatted": "Tuesday, July 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Exploiting%20the%20Typical%20Mind%20Fallacy%20for%20more%20accurate%20questioning%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExploiting%20the%20Typical%20Mind%20Fallacy%20for%20more%20accurate%20questioning%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhGe6p5Mk5KwuP49kK%2Fexploiting-the-typical-mind-fallacy-for-more-accurate%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Exploiting%20the%20Typical%20Mind%20Fallacy%20for%20more%20accurate%20questioning%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhGe6p5Mk5KwuP49kK%2Fexploiting-the-typical-mind-fallacy-for-more-accurate", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhGe6p5Mk5KwuP49kK%2Fexploiting-the-typical-mind-fallacy-for-more-accurate", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 317, "htmlBody": "<p>I was reading Yvain's <a href=\"/lw/dr/generalizing_from_one_example/\">Generalizing from One Example</a>, which talks about the typical mind fallacy.&nbsp; Basically, it describes how humans assume that all other humans are like them.&nbsp; If a person doesn't cheat on tests, they are more likely to assume others won't cheat on tests either.&nbsp; If a person sees mental images, they'll be more likely to assume that everyone else sees mental images.</p>\n<p>As I'm wont to do, I was thinking about how to <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">make that theory pay rent</a>.&nbsp; It occurred to me that this could definitely be exploitable.&nbsp; If the typical mind fallacy is correct, we should be able to have it go the other way; we can derive information about a person's proclivities based on what they think about other people.</p>\n<p>Eg, most employers ask \"have you ever stolen from a job before,\" and have to deal with misreporting because nobody in their right mind will say yes.&nbsp; However, imagine if the typical mind fallacy was correct.&nbsp; The employers could instead ask \"what do you think the percentage of employees who have stolen from their job is?\" and know that the applicants who responded higher than average were correspondingly more likely to steal, and the applicants who responded lower than average were less likely to cheat.&nbsp; It could cut through all sorts of social desirability distortion effects.&nbsp; You couldn't get the exact likelihood, but it would give more useful information than you would get with a direct question.&nbsp; <em></em></p>\n<p>In hindsight, which is always 20/20, it seems incredibly obvious.&nbsp; I'd be surprised if professional personality tests and sociologists <em>aren't</em> using these types of questions.&nbsp; My google-fu shows no hits, but it's possible I'm just not using the correct term that sociologists use.&nbsp; I'm was wondering if anyone had heard of this questioning method before, and if there's any good research data out there showing just how much you can infer from someone's deviance from the median response.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dBPou4ihoQNY4cquv": 1, "xexCWMyds6QLWognu": 1, "4R8JYu4QF2FqzJxE5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hGe6p5Mk5KwuP49kK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 47, "extendedScore": null, "score": 9.44247866403601e-07, "legacy": true, "legacyId": "17671", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 73, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["baTWMegR42PAsH9qJ", "a7n8GdKiAZRX86T5A"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-17T04:54:20.597Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Interpersonal Morality", "slug": "seq-rerun-interpersonal-morality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:06.363Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qgu4B2FoLkyBTRCtF/seq-rerun-interpersonal-morality", "pageUrlRelative": "/posts/Qgu4B2FoLkyBTRCtF/seq-rerun-interpersonal-morality", "linkUrl": "https://www.lesswrong.com/posts/Qgu4B2FoLkyBTRCtF/seq-rerun-interpersonal-morality", "postedAtFormatted": "Tuesday, July 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Interpersonal%20Morality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Interpersonal%20Morality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQgu4B2FoLkyBTRCtF%2Fseq-rerun-interpersonal-morality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Interpersonal%20Morality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQgu4B2FoLkyBTRCtF%2Fseq-rerun-interpersonal-morality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQgu4B2FoLkyBTRCtF%2Fseq-rerun-interpersonal-morality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<p>Today's post, <a href=\"/lw/sn/interpersonal_morality/\">Interpersonal Morality</a> was originally published on 29 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Interpersonal_Morality\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A few clarifications on how Yudkowsky's theory of metaethics applies to interpersonal interactions.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dmh/seq_rerun_the_meaning_of_right/\">The Meaning of Right</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qgu4B2FoLkyBTRCtF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.443655300680901e-07, "legacy": true, "legacyId": "17683", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7HDtecu4qW9PCsSR6", "naf75ycixrG3odTB4", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-17T07:18:09.229Z", "modifiedAt": null, "url": null, "title": "[LINK] Nick Szabo: Beware Pascal's Scams", "slug": "link-nick-szabo-beware-pascal-s-scams", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:55.084Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aALXhkxmcsPFHdC9P/link-nick-szabo-beware-pascal-s-scams", "pageUrlRelative": "/posts/aALXhkxmcsPFHdC9P/link-nick-szabo-beware-pascal-s-scams", "linkUrl": "https://www.lesswrong.com/posts/aALXhkxmcsPFHdC9P/link-nick-szabo-beware-pascal-s-scams", "postedAtFormatted": "Tuesday, July 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Nick%20Szabo%3A%20Beware%20Pascal's%20Scams&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Nick%20Szabo%3A%20Beware%20Pascal's%20Scams%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaALXhkxmcsPFHdC9P%2Flink-nick-szabo-beware-pascal-s-scams%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Nick%20Szabo%3A%20Beware%20Pascal's%20Scams%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaALXhkxmcsPFHdC9P%2Flink-nick-szabo-beware-pascal-s-scams", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaALXhkxmcsPFHdC9P%2Flink-nick-szabo-beware-pascal-s-scams", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 238, "htmlBody": "<p>Nick Szabo on acting on extremely long odds with claimed high payoffs:</p>\n<p style=\"padding-left: 30px;\"><a href=\"http://unenumerated.blogspot.co.uk/2012/07/pascals-scams.html\">Beware of what I call Pascal's scams:</a> movements or belief systems that ask you to hope for or worry about very improbable outcomes that could have very large positive or negative consequences. (The name comes of course from the infinite-reward Wager proposed by Pascal: these days the&nbsp;large-but-finite&nbsp;versions are far more pernicious).&nbsp; Naive expected value reasoning implies that they are worth the effort: if the odds are 1 in 1,000 that I could win $1 billion, and I am risk and time neutral, then I should expend up to nearly $1 million dollars worth of effort to gain this boon. The problems with these beliefs tend to be at least threefold, all stemming from the general uncertainty, i.e. the poor information or lack of information, from which we abstracted the low probability estimate in the first place: because in the messy real world the low probability estimate is almost always due to low or poor evidence rather than being a lottery with well-defined odds.</p>\n<p>Nick clarifies in the comments that he is indeed talking about singularitarians, including his GMU colleague Robin Hanson. This post appears to revisit a <a href=\"http://unenumerated.blogspot.co.uk/2011/01/singularity.html#c1642399731695430742\">comment</a> on an <a href=\"http://unenumerated.blogspot.co.uk/2011/01/singularity.html\">earlier post</a>:</p>\n<p style=\"padding-left: 30px;\">In other words, just because one comes up with quasi-plausible  catastrophic scenarios does not put the burden of proof on the skeptics  to debunk them or else cough up substantial funds to supposedly combat  these alleged threats.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aALXhkxmcsPFHdC9P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 9, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "17692", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 85, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-17T15:23:52.806Z", "modifiedAt": null, "url": null, "title": "Critical Thinking in Global Challenges - free Coursera class", "slug": "critical-thinking-in-global-challenges-free-coursera-class", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:46.278Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Utopiah", "createdAt": "2012-03-10T14:57:03.414Z", "isAdmin": false, "displayName": "Utopiah"}, "userId": "gBoAdZcTffQnbsMAb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cNe47SME4qkwQwkgS/critical-thinking-in-global-challenges-free-coursera-class", "pageUrlRelative": "/posts/cNe47SME4qkwQwkgS/critical-thinking-in-global-challenges-free-coursera-class", "linkUrl": "https://www.lesswrong.com/posts/cNe47SME4qkwQwkgS/critical-thinking-in-global-challenges-free-coursera-class", "postedAtFormatted": "Tuesday, July 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Critical%20Thinking%20in%20Global%20Challenges%20-%20free%20Coursera%20class&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACritical%20Thinking%20in%20Global%20Challenges%20-%20free%20Coursera%20class%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcNe47SME4qkwQwkgS%2Fcritical-thinking-in-global-challenges-free-coursera-class%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Critical%20Thinking%20in%20Global%20Challenges%20-%20free%20Coursera%20class%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcNe47SME4qkwQwkgS%2Fcritical-thinking-in-global-challenges-free-coursera-class", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcNe47SME4qkwQwkgS%2Fcritical-thinking-in-global-challenges-free-coursera-class", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 35, "htmlBody": "<p>\"develop and enhance your ability to think critically, assess information and develop reasoned arguments in the context of the global challenges facing society today.\"</p>\n<p>starts 28 January 2013</p>\n<p><span>cf https://www.coursera.org/course/criticalthinking<br /></span></p>\n<p>see also http://lesswrong.com/lw/dni/a_beginners_guide_to_irrational_behavior_free/<br /> and http://lesswrong.com/lw/d3w/coursera_behavioural_neurology_course/</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cNe47SME4qkwQwkgS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -6, "extendedScore": null, "score": 9.446644583943479e-07, "legacy": true, "legacyId": "17695", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-17T15:26:55.917Z", "modifiedAt": null, "url": null, "title": "A Beginner's Guide to Irrational Behavior - free Coursera class", "slug": "a-beginner-s-guide-to-irrational-behavior-free-coursera", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:46.640Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Utopiah", "createdAt": "2012-03-10T14:57:03.414Z", "isAdmin": false, "displayName": "Utopiah"}, "userId": "gBoAdZcTffQnbsMAb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NAtzcXPvaaGg9su45/a-beginner-s-guide-to-irrational-behavior-free-coursera", "pageUrlRelative": "/posts/NAtzcXPvaaGg9su45/a-beginner-s-guide-to-irrational-behavior-free-coursera", "linkUrl": "https://www.lesswrong.com/posts/NAtzcXPvaaGg9su45/a-beginner-s-guide-to-irrational-behavior-free-coursera", "postedAtFormatted": "Tuesday, July 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Beginner's%20Guide%20to%20Irrational%20Behavior%20-%20free%20Coursera%20class&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Beginner's%20Guide%20to%20Irrational%20Behavior%20-%20free%20Coursera%20class%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNAtzcXPvaaGg9su45%2Fa-beginner-s-guide-to-irrational-behavior-free-coursera%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Beginner's%20Guide%20to%20Irrational%20Behavior%20-%20free%20Coursera%20class%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNAtzcXPvaaGg9su45%2Fa-beginner-s-guide-to-irrational-behavior-free-coursera", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNAtzcXPvaaGg9su45%2Fa-beginner-s-guide-to-irrational-behavior-free-coursera", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 29, "htmlBody": "<p>\"learn about some of the many ways in which people behave in less than rational ways, and how we might overcome these problems.\"</p>\n<p>starts 25 March 2013</p>\n<p>cf https://www.coursera.org/course/behavioralecon</p>\n<p>see also http://lesswrong.com/lw/d3w/coursera_behavioural_neurology_course/</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NAtzcXPvaaGg9su45", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 0, "extendedScore": null, "score": 9.446659079310463e-07, "legacy": true, "legacyId": "17694", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-17T16:05:04.938Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup", "slug": "meetup-west-la-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oqJXBrRnmg53PCbnR/meetup-west-la-meetup-0", "pageUrlRelative": "/posts/oqJXBrRnmg53PCbnR/meetup-west-la-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/oqJXBrRnmg53PCbnR/meetup-west-la-meetup-0", "postedAtFormatted": "Tuesday, July 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoqJXBrRnmg53PCbnR%2Fmeetup-west-la-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoqJXBrRnmg53PCbnR%2Fmeetup-west-la-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoqJXBrRnmg53PCbnR%2Fmeetup-west-la-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/bw'>West LA Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 July 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, July 18th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> If you have a chance, read an <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">old article</a> and a <a href=\"http://lesswrong.com/recentposts\">recent article</a>, any whose title catches your eye. This week conversation will be unstructured, but those will make for good seeds of conversation.</p>\n\n<p>But don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>We may also find time for a game, like The Resistance.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/bw'>West LA Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oqJXBrRnmg53PCbnR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.446840285959546e-07, "legacy": true, "legacyId": "17696", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup\">Discussion article for the meetup : <a href=\"/meetups/bw\">West LA Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 July 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, July 18th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> If you have a chance, read an <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">old article</a> and a <a href=\"http://lesswrong.com/recentposts\">recent article</a>, any whose title catches your eye. This week conversation will be unstructured, but those will make for good seeds of conversation.</p>\n\n<p>But don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>We may also find time for a game, like The Resistance.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/bw\">West LA Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-17T18:40:18.239Z", "modifiedAt": null, "url": null, "title": "Summary thread for Coursera classes", "slug": "summary-thread-for-coursera-classes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:51.315Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mapnoterritory", "createdAt": "2012-03-14T20:44:02.961Z", "isAdmin": false, "displayName": "mapnoterritory"}, "userId": "EvXiCAYEyRqnXMWAD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4AS8ipga6xfmhwyXn/summary-thread-for-coursera-classes", "pageUrlRelative": "/posts/4AS8ipga6xfmhwyXn/summary-thread-for-coursera-classes", "linkUrl": "https://www.lesswrong.com/posts/4AS8ipga6xfmhwyXn/summary-thread-for-coursera-classes", "postedAtFormatted": "Tuesday, July 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Summary%20thread%20for%20Coursera%20classes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASummary%20thread%20for%20Coursera%20classes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4AS8ipga6xfmhwyXn%2Fsummary-thread-for-coursera-classes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Summary%20thread%20for%20Coursera%20classes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4AS8ipga6xfmhwyXn%2Fsummary-thread-for-coursera-classes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4AS8ipga6xfmhwyXn%2Fsummary-thread-for-coursera-classes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 777, "htmlBody": "<p>Maybe it would be worth to have a single summary thread for&nbsp;<a href=\"http://www.coursera.org\" target=\"_blank\">Coursera</a>&nbsp;(and also other source like Udacity etc.)&nbsp;material.&nbsp;At some future point when the courses are on-line and enough&nbsp;people seen them we could work out a \"LW curiculum\".&nbsp;Here is my subjective list of particularly intersting courses for LW&nbsp;audience:</p>\n<p>A Beginner's Guide to Irrational Behavior<br />Artificial Intelligence Planning<br />Automata<br />Basic Behavioral Neurology<br />Computer Science 101<br />Clinical Problem Solving<br />Critical Thinking in Global Challenges<br />Data Analysis<br />Fantasy and Science Fiction: The Human Mind, Our Modern World<br />Game Theory<br />Human-Computer Interaction<br />Introduction to Genetics and Evolution<br />Introduction to Genome Science<br />Introduction to Mathematical Thinking<br />Machine Learning<br />Microeconomics Principles<br />Model Thinking<br />Nanotechnology: The Basics<br />Networked Life<br />Networks: Friends, Money, and Bytes<br />Neural Networks for Machine Learning<br />Neuroethics<br />Principles of Economics for Scientists<br />Probabilistic Graphical Models<br />Quantum Mechanics and Quantum Computation<br />Rationing and Allocating Scarce Medical Resources<br />Statistics One<br />Think Again: How to Reason and Argue</p>\n<p><br />Please note I haven't picked any programming/algorithm courses - there&nbsp;seem to be quite a lot of nice ones.&nbsp;Subscribe <a href=\"https://www.coursera.org/courses\">here</a>. Plain text list&nbsp;(111 courses):</p>\n<p>A Beginner's Guide to Irrational Behavior<br />A History of the World since 1300<br />Aboriginal Worldviews and Education<br />Algorithms, Part I<br />Algorithms, Part II<br />Algorithms: Design and Analysis, Part 1<br />Algorithms: Design and Analysis, Part 2<br />An Introduction to Interactive Programming in Python<br />An Introduction to Operations Management<br />An Introduction to the U.S. Food System: Perspectives from Public Health<br />Analytic Combinatorics, Part I<br />Analytic Combinatorics, Part II<br />Analytical Chemistry<br />Artificial Intelligence Planning<br />Astrobiology and the Search for Extraterrestrial Life<br />Automata<br />Basic Behavioral Neurology<br />Bioelectricity: A Quantitative Approach<br />Calculus: Single Variable<br />Cardiac Arrest, Hypothermia, and Resuscitation Science<br />Chemistry: Concept Development and Application<br />Clinical Problem Solving<br />Community Change in Public Health<br />Compilers<br />Computational Investing, Part I<br />Computational Photography<br />Computer Architecture<br />Computer Science 101<br />Computer Vision: From 3D Reconstruction to Visual Recognition<br />Computer Vision: The Fundamentals<br />Computing for Data Analysis<br />Contraception: Choices, Culture and Consequences<br />Control of Mobile Robots<br />Creative, Serious and Playful Science of Android Apps<br />Critical Thinking in Global Challenges<br />Cryptography<br />Cryptography II<br />Data Analysis<br />Design: Creation of Artifacts in Society<br />Digital Signal Processing<br />Drugs and the Brain<br />E-learning and Digital Cultures<br />Energy 101<br />Equine Nutrition<br />Fantasy and Science Fiction: The Human Mind, Our Modern World<br />Functional Programming Principles in Scala<br />Fundamentals of Electrical Engineering<br />Fundamentals of Online Education: Planning and Application<br />Fundamentals of Pharmacology<br />Galaxies and Cosmology<br />Game Theory<br />Gamification<br />Greek and Roman Mythology<br />Grow to Greatness: Smart Growth for Private Businesses, Part I<br />Health Policy and the Affordable Care Act<br />Health for All Through Primary Care<br />Healthcare Innovation and Entrepreneurship<br />Heterogeneous Parallel Programming<br />How Things Work 1<br />Human-Computer Interaction<br />Information Security and Risk Management in Context<br />Intermediate Organic Chemistry - Part 1<br />Intermediate Organic Chemistry - Part 2<br />Internet History, Technology, and Security<br />Introduction to Astronomy<br />Introduction to Finance<br />Introduction to Genetics and Evolution<br />Introduction to Genome Science<br />Introduction to Logic<br />Introduction to Mathematical Thinking<br />Introduction to Philosophy<br />Introduction to Sociology<br />Introduction to Sustainability<br />Introduction &agrave; la Programmation Objet (in French)<br />Introductory Human Physiology<br />Introductory Organic Chemistry - Part 1<br />Introductory Organic Chemistry - Part 2<br />Know Thyself<br />Learn to Program: Crafting Quality Code<br />Learn to Program: The Fundamentals<br />Listening to World Music<br />Machine Learning<br />Mathematical Biostatistics Bootcamp<br />Medical Neuroscience<br />Microeconomics Principles<br />Model Thinking<br />Modern &amp; Contemporary American Poetry<br />Nanotechnology: The Basics<br />Natural Language Processing<br />Networked Life<br />Networks: Friends, Money, and Bytes<br />Neural Networks for Machine Learning<br />Neuroethics<br />Nutrition for Health Promotion and Disease Prevention<br />Planet Earth<br />Principles of Economics for Scientists<br />Principles of Obesity Economics<br />Probabilistic Graphical Models<br />Quantum Mechanics and Quantum Computation<br />Rationing and Allocating Scarce Medical Resources<br />Scientific Computing<br />Securing Digital Democracy<br />Social Network Analysis<br />Software Engineering for SaaS<br />Statistics One<br />The Modern World: Global History since 1760<br />The Social Context of Mental Health and Illness<br />Think Again: How to Reason and Argue<br />VLSI CAD: Logic to Layout<br />Vaccine Trials: Methods and Best Practices<br />Vaccines</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4AS8ipga6xfmhwyXn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 9.447577622694748e-07, "legacy": true, "legacyId": "17698", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-17T19:44:40.068Z", "modifiedAt": null, "url": null, "title": "Eliezer apparently wrong about higgs boson", "slug": "eliezer-apparently-wrong-about-higgs-boson", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:52.202Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "duckduckMOO", "createdAt": "2011-11-06T17:29:28.080Z", "isAdmin": false, "displayName": "duckduckMOO"}, "userId": "R8orY4w8kni4HydST", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B295pDvgcPf3R7fnM/eliezer-apparently-wrong-about-higgs-boson", "pageUrlRelative": "/posts/B295pDvgcPf3R7fnM/eliezer-apparently-wrong-about-higgs-boson", "linkUrl": "https://www.lesswrong.com/posts/B295pDvgcPf3R7fnM/eliezer-apparently-wrong-about-higgs-boson", "postedAtFormatted": "Tuesday, July 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Eliezer%20apparently%20wrong%20about%20higgs%20boson&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEliezer%20apparently%20wrong%20about%20higgs%20boson%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB295pDvgcPf3R7fnM%2Feliezer-apparently-wrong-about-higgs-boson%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Eliezer%20apparently%20wrong%20about%20higgs%20boson%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB295pDvgcPf3R7fnM%2Feliezer-apparently-wrong-about-higgs-boson", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB295pDvgcPf3R7fnM%2Feliezer-apparently-wrong-about-higgs-boson", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 169, "htmlBody": "<p>So someone told me that Eliezer Yudkowsky predicted no 5 sigma evidence of the higgs boson, and that 6 sigma evidence had been found. A quick search found the post referred to, and a slightly longer but not particularly thorough search did not find anything discussing it.</p>\n<p>So:</p>\n<p><a href=\"/lw/1dt/open_thread_november_2009/17xb\">http://lesswrong.com/lw/1dt/open_thread_november_2009/17xb</a>&nbsp;(02 November 2009)</p>\n<blockquote>\n<p>I'll go ahead and predict here that the Higgs boson will <em>not</em> be showing up. As best I can put the reason into words: I don't think the modern field of physics has its act sufficiently together to predict that a hitherto undetected quantum field is responsible for mass. They are welcome to prove me wrong.</p>\n<p>(I'll also predict that the LHC will never actually run, but that prediction is (almost entirely) a joke, whereas the first prediction is not.)</p>\n<p>Anyone challenging me to bet on the above is welcome to offer odds.</p>\n</blockquote>\n<p>In the post below rolfandreassen sets the condition of 5 sigma evidence before 2014&nbsp;and offers a bet of $25. In the post below that Eliezer accepts.</p>\n<p>Discuss.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B295pDvgcPf3R7fnM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 0, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "17699", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-18T03:17:31.306Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Humans in Funny Suits", "slug": "seq-rerun-humans-in-funny-suits", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:51.999Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Mo3SapZ92tGJTihtB/seq-rerun-humans-in-funny-suits", "pageUrlRelative": "/posts/Mo3SapZ92tGJTihtB/seq-rerun-humans-in-funny-suits", "linkUrl": "https://www.lesswrong.com/posts/Mo3SapZ92tGJTihtB/seq-rerun-humans-in-funny-suits", "postedAtFormatted": "Wednesday, July 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Humans%20in%20Funny%20Suits&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Humans%20in%20Funny%20Suits%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMo3SapZ92tGJTihtB%2Fseq-rerun-humans-in-funny-suits%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Humans%20in%20Funny%20Suits%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMo3SapZ92tGJTihtB%2Fseq-rerun-humans-in-funny-suits", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMo3SapZ92tGJTihtB%2Fseq-rerun-humans-in-funny-suits", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<p>Today's post, <a href=\"/lw/so/humans_in_funny_suits/\">Humans in Funny Suits</a> was originally published on 30 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Humans_in_Funny_Suits\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It's really hard to imagine aliens that are fundamentally different from human beings.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dn7/seq_rerun_interpersonal_morality/\">Interpersonal Morality</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Mo3SapZ92tGJTihtB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.450035254608403e-07, "legacy": true, "legacyId": "17706", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Zkzzjg3h7hW5Z36hK", "Qgu4B2FoLkyBTRCtF", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-18T15:15:08.313Z", "modifiedAt": null, "url": null, "title": "Meetup : Austin, TX - HPMoR Wrap Party", "slug": "meetup-austin-tx-hpmor-wrap-party", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:31.021Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kNqZWZBhFZHttS2Cj/meetup-austin-tx-hpmor-wrap-party", "pageUrlRelative": "/posts/kNqZWZBhFZHttS2Cj/meetup-austin-tx-hpmor-wrap-party", "linkUrl": "https://www.lesswrong.com/posts/kNqZWZBhFZHttS2Cj/meetup-austin-tx-hpmor-wrap-party", "postedAtFormatted": "Wednesday, July 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Austin%2C%20TX%20-%20HPMoR%20Wrap%20Party&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Austin%2C%20TX%20-%20HPMoR%20Wrap%20Party%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkNqZWZBhFZHttS2Cj%2Fmeetup-austin-tx-hpmor-wrap-party%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Austin%2C%20TX%20-%20HPMoR%20Wrap%20Party%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkNqZWZBhFZHttS2Cj%2Fmeetup-austin-tx-hpmor-wrap-party", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkNqZWZBhFZHttS2Cj%2Fmeetup-austin-tx-hpmor-wrap-party", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/bx'>Austin, TX - HPMoR Wrap Party</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 March 2015 05:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">4212 Hookbilled Kite Drive</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next meetup is <strong>March 14th, 2015</strong> at 6:00PM at a private residence (newcomers, especially from places other than Austin, are welcome!). We'll be celebrating the end of HPMoR; there will be food of both the dinner and snack varieties, as well as a wealth of pies. (Feel free to bring food if you'd like, but we expect to have enough even if you don't.)</p>\n\n<p>The topic of the March 21st meetup will be Fun and Games. (Suggest ideas for future topics!)</p>\n\n<p>If you have any questions or want to get my phone number to make sure you find us, please send me a private message. I'll notice and respond to those much more quickly than I'll notice comments on this meetup announcement.</p>\n\n<p>We have a Google Group <a href=\"https://groups.google.com/forum/#!forum/austin-less-wrong\" rel=\"nofollow\">here</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/bx'>Austin, TX - HPMoR Wrap Party</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kNqZWZBhFZHttS2Cj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 9.453446982082333e-07, "legacy": true, "legacyId": "17722", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Austin__TX___HPMoR_Wrap_Party\">Discussion article for the meetup : <a href=\"/meetups/bx\">Austin, TX - HPMoR Wrap Party</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 March 2015 05:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">4212 Hookbilled Kite Drive</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next meetup is <strong>March 14th, 2015</strong> at 6:00PM at a private residence (newcomers, especially from places other than Austin, are welcome!). We'll be celebrating the end of HPMoR; there will be food of both the dinner and snack varieties, as well as a wealth of pies. (Feel free to bring food if you'd like, but we expect to have enough even if you don't.)</p>\n\n<p>The topic of the March 21st meetup will be Fun and Games. (Suggest ideas for future topics!)</p>\n\n<p>If you have any questions or want to get my phone number to make sure you find us, please send me a private message. I'll notice and respond to those much more quickly than I'll notice comments on this meetup announcement.</p>\n\n<p>We have a Google Group <a href=\"https://groups.google.com/forum/#!forum/austin-less-wrong\" rel=\"nofollow\">here</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Austin__TX___HPMoR_Wrap_Party1\">Discussion article for the meetup : <a href=\"/meetups/bx\">Austin, TX - HPMoR Wrap Party</a></h2>", "sections": [{"title": "Discussion article for the meetup : Austin, TX - HPMoR Wrap Party", "anchor": "Discussion_article_for_the_meetup___Austin__TX___HPMoR_Wrap_Party", "level": 1}, {"title": "Discussion article for the meetup : Austin, TX - HPMoR Wrap Party", "anchor": "Discussion_article_for_the_meetup___Austin__TX___HPMoR_Wrap_Party1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-18T15:22:34.826Z", "modifiedAt": null, "url": null, "title": "What are you counting?", "slug": "what-are-you-counting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:51.555Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OrphanWilde", "createdAt": "2012-06-21T18:24:36.749Z", "isAdmin": false, "displayName": "OrphanWilde"}, "userId": "cdW87AS6fdK2sywL2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LDLDJieiTL8Gowcoi/what-are-you-counting", "pageUrlRelative": "/posts/LDLDJieiTL8Gowcoi/what-are-you-counting", "linkUrl": "https://www.lesswrong.com/posts/LDLDJieiTL8Gowcoi/what-are-you-counting", "postedAtFormatted": "Wednesday, July 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20you%20counting%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20you%20counting%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLDLDJieiTL8Gowcoi%2Fwhat-are-you-counting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20you%20counting%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLDLDJieiTL8Gowcoi%2Fwhat-are-you-counting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLDLDJieiTL8Gowcoi%2Fwhat-are-you-counting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 342, "htmlBody": "<p>Eliezer's post <a href=\"/lw/jr/how_to_convince_me_that_2_2_3/\">How To Convince Me That 2 + 2 = 3</a> has an interesting consideration - if putting two sheep in a field, and putting two more sheep in a field, resulted in three sheep being in the field, would arithmetic hold that two plus two equals three?</p>\n<p>I want to introduce another question. &nbsp;What exactly are you counting?</p>\n<p>Imagine one sheep in one field, and another sheep in another. &nbsp;Now put them together. &nbsp;Do you now have two sheep?</p>\n<p>\"Of course!\"</p>\n<p>Ah, but is that -all- you have?</p>\n<p>\"What?\"</p>\n<p>Two sheep are more than twice as complex as a single sheep. &nbsp;It takes more than twice as many bits to describe two sheep than it takes to describe a single sheep, because, in addition to those two sheep, you now also have to describe their relationship to one another.</p>\n<p>Or, to phrase it slightly differently, does 1+1=2?</p>\n<p>Well, the answer is, it depends on what you're counting.</p>\n<p>If you're counting the number of discrete sheep, 1+1=2. &nbsp;However, why is the number of discrete sheep meaningful?</p>\n<p>If you're a hunter counting, not herded sheep, but prey - two sheep is, roughly, twice as much meat as one sheep. &nbsp;1+1=2. &nbsp;If you're a herder, however, two sheep could be a lot more valuable than one - two sheep can turn into three sheep, if one is female and one is male. &nbsp;The value of two sheep can be more than twice the value of a single sheep. &nbsp;And if you're a hypercomputer running Solomonoff Induction to try to describe sheep positional vectors, two sheep will have a different complexity than twice the complexity of a single sheep.</p>\n<p>Which is not to say that one plus one does not equal two. &nbsp;It is, however, to say that one plus one may not be meaningful as a concept outside a very limited domain.</p>\n<p>Would an alien intelligence have arrived at arithmetic? &nbsp;Depends on what it counts. &nbsp;Is arithmetic correct?</p>\n<p>Well, does a set of two sheep contain only two sheep, or does it also contain their interactions? &nbsp;Depends on your problem domain; 1+1 might just equal 2+i.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LDLDJieiTL8Gowcoi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": -24, "extendedScore": null, "score": 9.453482373852289e-07, "legacy": true, "legacyId": "17723", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6FmqiAgS8h4EJm86s"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-18T17:24:51.381Z", "modifiedAt": null, "url": null, "title": "Welcome to Less Wrong! (July 2012)", "slug": "welcome-to-less-wrong-july-2012", "viewCount": null, "lastCommentedAt": "2021-03-03T21:50:17.111Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h2aZTRmZGAJAhuwS3/welcome-to-less-wrong-july-2012", "pageUrlRelative": "/posts/h2aZTRmZGAJAhuwS3/welcome-to-less-wrong-july-2012", "linkUrl": "https://www.lesswrong.com/posts/h2aZTRmZGAJAhuwS3/welcome-to-less-wrong-july-2012", "postedAtFormatted": "Wednesday, July 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Welcome%20to%20Less%20Wrong!%20(July%202012)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWelcome%20to%20Less%20Wrong!%20(July%202012)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh2aZTRmZGAJAhuwS3%2Fwelcome-to-less-wrong-july-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Welcome%20to%20Less%20Wrong!%20(July%202012)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh2aZTRmZGAJAhuwS3%2Fwelcome-to-less-wrong-july-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh2aZTRmZGAJAhuwS3%2Fwelcome-to-less-wrong-july-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1388, "htmlBody": "<p><span>If you've recently joined the&nbsp;</span><a href=\"/lw/1/about_less_wrong\">Less Wrong community</a><span>, please leave a comment here and introduce yourself. We'd love to know who you are, what you're doing, what you value,&nbsp;</span><a href=\"/lw/2/tell_your_rationalist_origin_story\">how you came to identify as a rationalist</a><span>&nbsp;or how you found us. You can&nbsp;</span><a href=\"/lw/90l/welcome_to_less_wrong_2012/#comments\">skip right to that</a><span>&nbsp;if you like; the rest of this post consists of a few things you might find helpful. More can be found at the&nbsp;</span><a href=\"http://wiki.lesswrong.com/wiki/FAQ\">FAQ</a><span>.</span></p>\n<div>(This is the fourth incarnation of&nbsp;the welcome thread, the first three of which which now have too many comments. The text is by orthonormal from an original by MBlume.)</div>\n<h4><a id=\"more\"></a>A few notes about the site mechanics<br /></h4>\n<div>Less Wrong&nbsp;<strong>comments are threaded</strong>&nbsp;for easy following of multiple conversations. To respond to any comment, click the \"Reply\" link at the bottom of that comment's box. Within the comment box, links and formatting are achieved via&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Comment_formatting\">Markdown syntax</a>&nbsp; (you can click the \"Help\" link below the text box to bring up a primer).</div>\n<div class=\"md\">You may have noticed that all the posts and comments on this site have buttons to&nbsp;<strong>vote them up or down</strong>, and all the users have \"karma\" scores which come from the sum of all their comments and posts. This immediate easy feedback mechanism helps keep arguments from turning into flamewars and helps make the best posts more visible; it's part of what makes discussions on Less Wrong look different from those anywhere else on the Internet.</div>\n<div class=\"md\">However, it can feel really irritating to get downvoted, especially if one doesn't know why. It happens to all of us sometimes, and it's perfectly acceptable to ask for an explanation. (Sometimes it's the unwritten LW etiquette; we have different norms than other forums.) Take note when you're downvoted a lot on one topic, as it often means that several members of the community think you're missing an important point or making a mistake in reasoning&mdash; not just that they disagree with you!<strong>&nbsp;If you've any questions about karma or voting, please feel free to ask here.</strong></div>\n<div class=\"md\"><strong>Replies</strong>&nbsp;to your comments across the site, plus&nbsp;<strong>private messages</strong>&nbsp;from other users, will show up in your&nbsp;<a href=\"/message/inbox\">inbox</a>. You can reach it via the little mail icon beneath your karma score on the upper right of most pages. When you have a new reply or message, it glows red. You can also click on any user's name to view all of their comments and posts.</div>\n<div class=\"md\">It's definitely worth your time&nbsp;<strong>commenting on old posts</strong>; veteran users look through the&nbsp;<a href=\"/comments\">recent comments thread</a>&nbsp;quite often (there's a separate&nbsp;<a href=\"/r/discussion/comments\">recent comments thread for the Discussion section</a>, for whatever reason), and a conversation begun anywhere will pick up contributors that way.&nbsp; There's also a succession of&nbsp;<a href=\"/tag/open_thread\">open comment threads</a>&nbsp;for discussion of anything remotely related to rationality.</div>\n<div class=\"md\">Discussions on Less Wrong tend to end differently than in most other forums; a surprising number end when one participant changes their mind, or when multiple people clarify their views enough and reach agreement. More commonly, though, people will just stop when they've better identified their deeper disagreements, or simply&nbsp;<strong>\"tap out\" of a discussion</strong>&nbsp;that's stopped being productive. (Seriously, you can just write \"I'm tapping out of this thread.\") This is absolutely OK, and it's one good way to avoid the flamewars that plague many sites.<br /></div>\n<div class=\"md\"><strong>EXTRA FEATURES:</strong><br /></div>\n<div class=\"md\">There's actually more than meets the eye here: look near the top of the page for the \"WIKI\", \"DISCUSSION\" and \"SEQUENCES\" links.</div>\n<div class=\"md\"><strong>LW WIKI:</strong>&nbsp;This is our attempt to make searching by topic feasible, as well as to store information like&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Acronyms_used_on_Less_Wrong\">common abbreviations</a>&nbsp;and idioms. It's a good place to look if someone's speaking Greek to you.<br /></div>\n<div class=\"md\"><strong>LW DISCUSSION:</strong>&nbsp;This is a forum just like the top-level one, with two key differences: in the top-level forum, posts require the author to have 20 karma in order to publish, and any upvotes or downvotes on the post are multiplied by 10. Thus there's a lot more informal dialogue in the Discussion section, including some of the more fun conversations here.</div>\n<div class=\"md\"><strong>SEQUENCES:</strong>&nbsp;A&nbsp;<em>huge</em>&nbsp;corpus of material mostly written by Eliezer Yudkowsky in his days of blogging at Overcoming Bias, before Less Wrong was started. Much of the discussion here will casually depend on or refer to ideas brought up in those posts, so reading them can really help with present discussions. Besides which, they're pretty engrossing in my opinion.<br /></div>\n<div>\n<h4>A few notes about the community<br /></h4>\n<div>If you've come to Less Wrong to&nbsp;<strong>discuss a particular topic</strong>, this thread would be a great place to start the conversation. By commenting here, and checking the responses, you'll probably get a good read on what, if anything, has already been said here on that topic, what's widely understood and what you might still need to take some time explaining.</div>\n<div><strong>If your welcome comment starts a huge discussion</strong>, then please move to the next step and&nbsp;<strong>create a LW Discussion post to continue the conversation</strong>; we can fit many more welcomes onto each thread if fewer of them sprout 400+ comments. (To do this: click \"Create new article\" in the upper right corner next to your username, then write the article, then at the bottom take the menu \"Post to\" and change it from \"Drafts\" to \"Less Wrong Discussion\". Then click \"Submit\". When you edit a published post, clicking \"Save and continue\" does correctly update the post.)<br /></div>\n<div>If you want to write a post about a LW-relevant topic, awesome!&nbsp;&nbsp;<strong>I highly recommend you submit your first post to Less Wrong Discussion</strong>; don't worry, you can later promote it from there to the main page if it's well-received. (It's much better to get some feedback before every vote counts for 10 karma- honestly, you don't know what you don't know about the community norms here.)<br /></div>\n<div>If you'd like to connect with other LWers in real life, we have&nbsp;&nbsp;<strong>meetups&nbsp;</strong>&nbsp;in various parts of the world. Check the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups\">wiki page for places with regular meetups</a>, or the&nbsp;<a href=\"/meetups\">upcoming (irregular) meetups page</a>.<br /></div>\n<div>There's also a&nbsp;<a href=\"http://www.facebook.com/home.php#/group.php?gid=144017955332&amp;ref=ts\">Facebook group</a>.&nbsp; If you've your own blog or other online presence, please feel free to link it.</div>\n<p><strong>If English is not your first language</strong>, don't let that make you afraid to post or comment. You can get English help on Discussion- or Main-level posts by sending a PM to one of the following users (use the \"send message\" link on the upper right of their user page). Either put the text of the post in the PM, or just say that you'd like English help and you'll get a response with an email address.&nbsp;<br />*&nbsp;<a href=\"/user/Normal_Anomaly\">Normal_Anomaly</a>&nbsp;<br />*&nbsp;<a href=\"/user/Randaly\">Randaly</a>&nbsp;<br />*&nbsp;<a href=\"/user/shokwave\">shokwave</a>&nbsp;<br />*&nbsp;<a href=\"/user/Barry_Cotter\">Barry Cotter</a></p>\n<p><strong>A note for theists</strong>: you will find the Less Wrong community to be predominantly atheist, though not completely so, and most of us are genuinely respectful of religious people who keep the usual community norms. It's worth saying that we might think religion is off-topic in some places where you think it's on-topic, so be thoughtful about where and how you start explicitly talking about it; some of us are happy to talk about religion, some of us aren't interested. Bear in mind that many of us really, truly have given full consideration to theistic claims and found them to be false, so starting with the most common arguments is pretty likely just to annoy people. Anyhow, it's absolutely OK to mention that you're religious in your welcome post and to invite a discussion there.</p>\n<h4>A list of some posts that are pretty awesome<br /></h4>\n<p>I recommend the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Sequences\">major sequences</a>&nbsp;to everybody, but I realize how daunting they look at first. So for purposes of immediate gratification, the following posts are particularly interesting/illuminating/provocative and don't require any previous reading:</p>\n<ul>\n<li><a href=\"/lw/2bu/your_intuitions_are_not_magic\">Your Intuitions are Not Magic</a></li>\n<li><a href=\"/lw/20/the_apologist_and_the_revolutionary\">The Apologist and the Revolutionary</a></li>\n<li><a href=\"/lw/jr/how_to_convince_me_that_2_2_3\">How to Convince Me that 2 + 2 = 3</a></li>\n<li><a href=\"/lw/vo/lawful_uncertainty\">Lawful Uncertainty</a></li>\n<li><a href=\"/lw/jg/planning_fallacy\">The Planning Fallacy</a></li>\n<li><a href=\"/lw/hw/scope_insensitivity\">Scope Insensitivity</a></li>\n<li><a href=\"/lw/my/the_allais_paradox\">The Allais Paradox</a>&nbsp; (with&nbsp;<a href=\"/lw/mz/zut_allais\">two</a>&nbsp;<a href=\"/lw/n1/allais_malaise\">followups</a>)</li>\n<li><a href=\"/lw/jx/we_change_our_minds_less_often_than_we_think\">We Change Our Minds Less Often Than We Think</a></li>\n<li><a href=\"/lw/2k/the_least_convenient_possible_world\">The Least Convenient Possible World</a></li>\n<li><a href=\"/lw/hu/the_third_alternative\">The Third Alternative</a></li>\n<li><a href=\"/lw/116/the_domain_of_your_utility_function\">The Domain of Your Utility Function</a></li>\n<li><a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality\">Newcomb's Problem and Regret of Rationality</a></li>\n<li><a href=\"/lw/tn/the_true_prisoners_dilemma\">The True Prisoner's Dilemma</a></li>\n<li><a href=\"/lw/kw/the_tragedy_of_group_selectionism\">The Tragedy of Group Selectionism</a></li>\n<li><a href=\"/lw/gz/policy_debates_should_not_appear_onesided\">Policy Debates Should Not Appear One-Sided</a></li>\n<li><a href=\"/lw/qk/that_alien_message\">That Alien Message</a></li>\n</ul>\n<p>More suggestions are welcome! Or just check out the&nbsp;<a href=\"/top/?t=all\">top-rated posts from the history of Less Wrong</a>. Most posts at +50 or more are well worth your time.</p>\n<p>Welcome to Less Wrong, and we look forward to hearing from you throughout the site.</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h2aZTRmZGAJAhuwS3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 31, "extendedScore": null, "score": 9.454060156582874e-07, "legacy": true, "legacyId": "17721", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><span>If you've recently joined the&nbsp;</span><a href=\"/lw/1/about_less_wrong\">Less Wrong community</a><span>, please leave a comment here and introduce yourself. We'd love to know who you are, what you're doing, what you value,&nbsp;</span><a href=\"/lw/2/tell_your_rationalist_origin_story\">how you came to identify as a rationalist</a><span>&nbsp;or how you found us. You can&nbsp;</span><a href=\"/lw/90l/welcome_to_less_wrong_2012/#comments\">skip right to that</a><span>&nbsp;if you like; the rest of this post consists of a few things you might find helpful. More can be found at the&nbsp;</span><a href=\"http://wiki.lesswrong.com/wiki/FAQ\">FAQ</a><span>.</span></p>\n<div>(This is the fourth incarnation of&nbsp;the welcome thread, the first three of which which now have too many comments. The text is by orthonormal from an original by MBlume.)</div>\n<h4 id=\"A_few_notes_about_the_site_mechanics\"><a id=\"more\"></a>A few notes about the site mechanics<br></h4>\n<div>Less Wrong&nbsp;<strong>comments are threaded</strong>&nbsp;for easy following of multiple conversations. To respond to any comment, click the \"Reply\" link at the bottom of that comment's box. Within the comment box, links and formatting are achieved via&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Comment_formatting\">Markdown syntax</a>&nbsp; (you can click the \"Help\" link below the text box to bring up a primer).</div>\n<div class=\"md\">You may have noticed that all the posts and comments on this site have buttons to&nbsp;<strong>vote them up or down</strong>, and all the users have \"karma\" scores which come from the sum of all their comments and posts. This immediate easy feedback mechanism helps keep arguments from turning into flamewars and helps make the best posts more visible; it's part of what makes discussions on Less Wrong look different from those anywhere else on the Internet.</div>\n<div class=\"md\">However, it can feel really irritating to get downvoted, especially if one doesn't know why. It happens to all of us sometimes, and it's perfectly acceptable to ask for an explanation. (Sometimes it's the unwritten LW etiquette; we have different norms than other forums.) Take note when you're downvoted a lot on one topic, as it often means that several members of the community think you're missing an important point or making a mistake in reasoning\u2014 not just that they disagree with you!<strong>&nbsp;If you've any questions about karma or voting, please feel free to ask here.</strong></div>\n<div class=\"md\"><strong>Replies</strong>&nbsp;to your comments across the site, plus&nbsp;<strong>private messages</strong>&nbsp;from other users, will show up in your&nbsp;<a href=\"/message/inbox\">inbox</a>. You can reach it via the little mail icon beneath your karma score on the upper right of most pages. When you have a new reply or message, it glows red. You can also click on any user's name to view all of their comments and posts.</div>\n<div class=\"md\">It's definitely worth your time&nbsp;<strong>commenting on old posts</strong>; veteran users look through the&nbsp;<a href=\"/comments\">recent comments thread</a>&nbsp;quite often (there's a separate&nbsp;<a href=\"/r/discussion/comments\">recent comments thread for the Discussion section</a>, for whatever reason), and a conversation begun anywhere will pick up contributors that way.&nbsp; There's also a succession of&nbsp;<a href=\"/tag/open_thread\">open comment threads</a>&nbsp;for discussion of anything remotely related to rationality.</div>\n<div class=\"md\">Discussions on Less Wrong tend to end differently than in most other forums; a surprising number end when one participant changes their mind, or when multiple people clarify their views enough and reach agreement. More commonly, though, people will just stop when they've better identified their deeper disagreements, or simply&nbsp;<strong>\"tap out\" of a discussion</strong>&nbsp;that's stopped being productive. (Seriously, you can just write \"I'm tapping out of this thread.\") This is absolutely OK, and it's one good way to avoid the flamewars that plague many sites.<br></div>\n<div class=\"md\"><strong>EXTRA FEATURES:</strong><br></div>\n<div class=\"md\">There's actually more than meets the eye here: look near the top of the page for the \"WIKI\", \"DISCUSSION\" and \"SEQUENCES\" links.</div>\n<div class=\"md\"><strong>LW WIKI:</strong>&nbsp;This is our attempt to make searching by topic feasible, as well as to store information like&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Acronyms_used_on_Less_Wrong\">common abbreviations</a>&nbsp;and idioms. It's a good place to look if someone's speaking Greek to you.<br></div>\n<div class=\"md\"><strong>LW DISCUSSION:</strong>&nbsp;This is a forum just like the top-level one, with two key differences: in the top-level forum, posts require the author to have 20 karma in order to publish, and any upvotes or downvotes on the post are multiplied by 10. Thus there's a lot more informal dialogue in the Discussion section, including some of the more fun conversations here.</div>\n<div class=\"md\"><strong>SEQUENCES:</strong>&nbsp;A&nbsp;<em>huge</em>&nbsp;corpus of material mostly written by Eliezer Yudkowsky in his days of blogging at Overcoming Bias, before Less Wrong was started. Much of the discussion here will casually depend on or refer to ideas brought up in those posts, so reading them can really help with present discussions. Besides which, they're pretty engrossing in my opinion.<br></div>\n<div>\n<h4 id=\"A_few_notes_about_the_community\">A few notes about the community<br></h4>\n<div>If you've come to Less Wrong to&nbsp;<strong>discuss a particular topic</strong>, this thread would be a great place to start the conversation. By commenting here, and checking the responses, you'll probably get a good read on what, if anything, has already been said here on that topic, what's widely understood and what you might still need to take some time explaining.</div>\n<div><strong>If your welcome comment starts a huge discussion</strong>, then please move to the next step and&nbsp;<strong>create a LW Discussion post to continue the conversation</strong>; we can fit many more welcomes onto each thread if fewer of them sprout 400+ comments. (To do this: click \"Create new article\" in the upper right corner next to your username, then write the article, then at the bottom take the menu \"Post to\" and change it from \"Drafts\" to \"Less Wrong Discussion\". Then click \"Submit\". When you edit a published post, clicking \"Save and continue\" does correctly update the post.)<br></div>\n<div>If you want to write a post about a LW-relevant topic, awesome!&nbsp;&nbsp;<strong>I highly recommend you submit your first post to Less Wrong Discussion</strong>; don't worry, you can later promote it from there to the main page if it's well-received. (It's much better to get some feedback before every vote counts for 10 karma- honestly, you don't know what you don't know about the community norms here.)<br></div>\n<div>If you'd like to connect with other LWers in real life, we have&nbsp;&nbsp;<strong>meetups&nbsp;</strong>&nbsp;in various parts of the world. Check the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups\">wiki page for places with regular meetups</a>, or the&nbsp;<a href=\"/meetups\">upcoming (irregular) meetups page</a>.<br></div>\n<div>There's also a&nbsp;<a href=\"http://www.facebook.com/home.php#/group.php?gid=144017955332&amp;ref=ts\">Facebook group</a>.&nbsp; If you've your own blog or other online presence, please feel free to link it.</div>\n<p><strong>If English is not your first language</strong>, don't let that make you afraid to post or comment. You can get English help on Discussion- or Main-level posts by sending a PM to one of the following users (use the \"send message\" link on the upper right of their user page). Either put the text of the post in the PM, or just say that you'd like English help and you'll get a response with an email address.&nbsp;<br>*&nbsp;<a href=\"/user/Normal_Anomaly\">Normal_Anomaly</a>&nbsp;<br>*&nbsp;<a href=\"/user/Randaly\">Randaly</a>&nbsp;<br>*&nbsp;<a href=\"/user/shokwave\">shokwave</a>&nbsp;<br>*&nbsp;<a href=\"/user/Barry_Cotter\">Barry Cotter</a></p>\n<p><strong>A note for theists</strong>: you will find the Less Wrong community to be predominantly atheist, though not completely so, and most of us are genuinely respectful of religious people who keep the usual community norms. It's worth saying that we might think religion is off-topic in some places where you think it's on-topic, so be thoughtful about where and how you start explicitly talking about it; some of us are happy to talk about religion, some of us aren't interested. Bear in mind that many of us really, truly have given full consideration to theistic claims and found them to be false, so starting with the most common arguments is pretty likely just to annoy people. Anyhow, it's absolutely OK to mention that you're religious in your welcome post and to invite a discussion there.</p>\n<h4 id=\"A_list_of_some_posts_that_are_pretty_awesome\">A list of some posts that are pretty awesome<br></h4>\n<p>I recommend the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Sequences\">major sequences</a>&nbsp;to everybody, but I realize how daunting they look at first. So for purposes of immediate gratification, the following posts are particularly interesting/illuminating/provocative and don't require any previous reading:</p>\n<ul>\n<li><a href=\"/lw/2bu/your_intuitions_are_not_magic\">Your Intuitions are Not Magic</a></li>\n<li><a href=\"/lw/20/the_apologist_and_the_revolutionary\">The Apologist and the Revolutionary</a></li>\n<li><a href=\"/lw/jr/how_to_convince_me_that_2_2_3\">How to Convince Me that 2 + 2 = 3</a></li>\n<li><a href=\"/lw/vo/lawful_uncertainty\">Lawful Uncertainty</a></li>\n<li><a href=\"/lw/jg/planning_fallacy\">The Planning Fallacy</a></li>\n<li><a href=\"/lw/hw/scope_insensitivity\">Scope Insensitivity</a></li>\n<li><a href=\"/lw/my/the_allais_paradox\">The Allais Paradox</a>&nbsp; (with&nbsp;<a href=\"/lw/mz/zut_allais\">two</a>&nbsp;<a href=\"/lw/n1/allais_malaise\">followups</a>)</li>\n<li><a href=\"/lw/jx/we_change_our_minds_less_often_than_we_think\">We Change Our Minds Less Often Than We Think</a></li>\n<li><a href=\"/lw/2k/the_least_convenient_possible_world\">The Least Convenient Possible World</a></li>\n<li><a href=\"/lw/hu/the_third_alternative\">The Third Alternative</a></li>\n<li><a href=\"/lw/116/the_domain_of_your_utility_function\">The Domain of Your Utility Function</a></li>\n<li><a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality\">Newcomb's Problem and Regret of Rationality</a></li>\n<li><a href=\"/lw/tn/the_true_prisoners_dilemma\">The True Prisoner's Dilemma</a></li>\n<li><a href=\"/lw/kw/the_tragedy_of_group_selectionism\">The Tragedy of Group Selectionism</a></li>\n<li><a href=\"/lw/gz/policy_debates_should_not_appear_onesided\">Policy Debates Should Not Appear One-Sided</a></li>\n<li><a href=\"/lw/qk/that_alien_message\">That Alien Message</a></li>\n</ul>\n<p>More suggestions are welcome! Or just check out the&nbsp;<a href=\"/top/?t=all\">top-rated posts from the history of Less Wrong</a>. Most posts at +50 or more are well worth your time.</p>\n<p>Welcome to Less Wrong, and we look forward to hearing from you throughout the site.</p>\n</div>", "sections": [{"title": "A few notes about the site mechanics", "anchor": "A_few_notes_about_the_site_mechanics", "level": 1}, {"title": "A few notes about the community", "anchor": "A_few_notes_about_the_community", "level": 1}, {"title": "A list of some posts that are pretty awesome", "anchor": "A_list_of_some_posts_that_are_pretty_awesome", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "850 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 851, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2om7AHEHtbogJmT5s", "BHMBBFupzb4s8utts", "Psp8ZpYLCDJjshpRb", "ZiQqsgGX6a42Sfpii", "6FmqiAgS8h4EJm86s", "msJA6B9ZjiiZxT6EZ", "CPm5LTwHrvBJCa9h5", "2ftJ38y9SRBCBsCzy", "zJZvoiwydJ5zvzTHK", "zNcLnqHF5rvrTsQJx", "knpAQ4F3gmguxy39z", "buixYfcXBah9hbSNZ", "neQ7eXuaXpiYw7SBy", "erGipespbbzdG5zYb", "xgicQnkrdA5FehhnQ", "6ddcsdA2c2XpNpE5x", "HFyWNBnDNEDsDNLrZ", "QsMJQSFj7WfoTMNgW", "PeSzc9JTBxhaYRp9b", "5wMcKNAwB6X4mp9og"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-18T19:06:55.052Z", "modifiedAt": null, "url": null, "title": "Challenge: change someone's mind", "slug": "challenge-change-someone-s-mind", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:54.571Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Blackened", "createdAt": "2012-05-09T10:50:12.050Z", "isAdmin": false, "displayName": "Blackened"}, "userId": "qNeT4N9rP7tSxwquv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c2zqGg6rMFP6C8Q4m/challenge-change-someone-s-mind", "pageUrlRelative": "/posts/c2zqGg6rMFP6C8Q4m/challenge-change-someone-s-mind", "linkUrl": "https://www.lesswrong.com/posts/c2zqGg6rMFP6C8Q4m/challenge-change-someone-s-mind", "postedAtFormatted": "Wednesday, July 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Challenge%3A%20change%20someone's%20mind&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChallenge%3A%20change%20someone's%20mind%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc2zqGg6rMFP6C8Q4m%2Fchallenge-change-someone-s-mind%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Challenge%3A%20change%20someone's%20mind%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc2zqGg6rMFP6C8Q4m%2Fchallenge-change-someone-s-mind", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc2zqGg6rMFP6C8Q4m%2Fchallenge-change-someone-s-mind", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 471, "htmlBody": "<p>Pick one (or several) of the following. I used specific examples, therefore anything similar still counts.</p>\n<p>1. You have a friendly new acquaintance who is pretty much an average person. He is a theist and doesn't believe Evolution, you have already had a polite debate about that. Convince him to believe in the truth*.</p>\n<p>2. One of your friends is very deeply religious - he has devoted his life to already invested a lot of it in religion. Unexpectedly, he is also highly rational (as a personality) and very intelligent, he studies a technical degree (enjoys it), he has read books about critical thinking (he even knows a little about biases) and he says that he will stop believing in religion if you disprove it. Debating with him so far didn't help (also he isn't too good - he isn't aware of expected value and such ideas). For his own good, convince him to change his mind in the direction of the truth. He is wasting a huge potential and that's not only bad for him, but also for humanity. Also, he will feel more comfortable in his new, more sensible beliefs.</p>\n<p>3. Your brother dislikes you because of his impression of you that was created several years ago and wasn't updated to reflect the changes in your personality. You easily make impressions to other people that are vastly different from his impression of you. Change his impression, so that he sees you truthfully.</p>\n<p>[I have removed 4., because it wasn't about changing the mind of someone who isn't a rationalist, but about coming up with a good psychological mechanism - it deserves an entirely new thread; I suspect that 3 might be too different from 1 and 2, but it's too late to make a so big change to the thread]</p>\n<p>&nbsp;</p>\n<p>I know at least one person for each category. And I haven't been able to change nobody's mind. Have you succeeded in a similar situation? Regardless of whether you have, what strategies do you think would be winning in the 4 situations? If some of them sounds good, I might even try them out and share the results. I'm especially curious about how to approach in #3, because if there is a way, it would come from low-level psychology, which is something I adore.</p>\n<p>So, the aim of this thread is for the participants to try and change someone's mind and then tell the story.</p>\n<p>(also, I'm willing to accept ideas of other templates for classical situations similar to those, in fact I think I had one or two more ideas, but I can't seem to recall them)</p>\n<p>&nbsp;</p>\n<p>*Needless to say, if at any point, anyone proves to you that his direction is in fact the truth, it would be better to change yourself in that direction instead, but that's outside of the scope of the thread.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c2zqGg6rMFP6C8Q4m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -7, "extendedScore": null, "score": 9.454549383008523e-07, "legacy": true, "legacyId": "17724", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-18T19:50:27.973Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley meetup: Argument mapping software", "slug": "meetup-berkeley-meetup-argument-mapping-software", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JTCHqPEEq8hPsz4nF/meetup-berkeley-meetup-argument-mapping-software", "pageUrlRelative": "/posts/JTCHqPEEq8hPsz4nF/meetup-berkeley-meetup-argument-mapping-software", "linkUrl": "https://www.lesswrong.com/posts/JTCHqPEEq8hPsz4nF/meetup-berkeley-meetup-argument-mapping-software", "postedAtFormatted": "Wednesday, July 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%20meetup%3A%20Argument%20mapping%20software&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%20meetup%3A%20Argument%20mapping%20software%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJTCHqPEEq8hPsz4nF%2Fmeetup-berkeley-meetup-argument-mapping-software%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%20meetup%3A%20Argument%20mapping%20software%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJTCHqPEEq8hPsz4nF%2Fmeetup-berkeley-meetup-argument-mapping-software", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJTCHqPEEq8hPsz4nF%2Fmeetup-berkeley-meetup-argument-mapping-software", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/by\">Berkeley meetup: Argument mapping software</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">18 July 2012 07:00:00PM (-0700)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Berkeley, CA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>So at tonight's meetup we will be playing with John's argument mapping software! Feel free to bring your laptop &mdash; we have wifi and the software is web-based at <a href=\"http://www.entailment.org\">www.entailment.org</a>. (It only works with Firefox.)</p>\n<p>The meetup starts at 7pm at Zendo. For directions to Zendo see the <a href=\"http://groups.google.com/group/bayarealesswrong\">mailing list</a> or call me at <img style=\"vertical-align: middle;\" src=\"http://i.imgur.com/Vcafy.png\" alt=\"\" width=\"83\" height=\"18\" />.</p>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JTCHqPEEq8hPsz4nF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 9.454756537147819e-07, "legacy": true, "legacyId": "17725", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-18T23:57:52.193Z", "modifiedAt": null, "url": null, "title": "Take Part in CFAR Rationality Surveys", "slug": "take-part-in-cfar-rationality-surveys", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:51.824Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Unnamed", "createdAt": "2009-02-27T06:08:10.900Z", "isAdmin": false, "displayName": "Unnamed"}, "userId": "PdzQ73mN7S4SvRMhu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KJbthz9Lr2PdNpgJ3/take-part-in-cfar-rationality-surveys", "pageUrlRelative": "/posts/KJbthz9Lr2PdNpgJ3/take-part-in-cfar-rationality-surveys", "linkUrl": "https://www.lesswrong.com/posts/KJbthz9Lr2PdNpgJ3/take-part-in-cfar-rationality-surveys", "postedAtFormatted": "Wednesday, July 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Take%20Part%20in%20CFAR%20Rationality%20Surveys&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATake%20Part%20in%20CFAR%20Rationality%20Surveys%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKJbthz9Lr2PdNpgJ3%2Ftake-part-in-cfar-rationality-surveys%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Take%20Part%20in%20CFAR%20Rationality%20Surveys%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKJbthz9Lr2PdNpgJ3%2Ftake-part-in-cfar-rationality-surveys", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKJbthz9Lr2PdNpgJ3%2Ftake-part-in-cfar-rationality-surveys", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 278, "htmlBody": "<p>Posted By: Dan Keys, CFAR Survey Coordinator<br /><br />The <a href=\"http://appliedrationality.org/\">Center for Applied Rationality</a> is trying to develop better methods for measuring and studying the benefits of rationality.&nbsp; We want to be able to test if this rationality stuff actually works.<br /><br />One way that the Less Wrong community can help us with this process is by taking part in online surveys, which we can use for a variety of purposes including:</p>\n<ul>\n<li>seeing what rationality techniques people actually use in their day-to-day lives&nbsp; </li>\n<li>developing &amp; testing measures of how rational people are, and seeing if potential rationality measures correlate with the other variables that you'd expect them to&nbsp; </li>\n<li>comparing people who attend a minicamp with others in the LW community, so that we can learn what value-added the minicamps provide beyond what you get elsewhere&nbsp; </li>\n<li>trying out some of the rationality techniques that we are trying to teach, so we can see how they work&nbsp; </li>\n</ul>\n<p>We have a couple of surveys ready to go now which cover some of these bullet points, and will be developing other surveys over the coming months.<br /><br />If you're interested in taking part in online surveys for CFAR, please go <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dEp2V0ZXZUpuVmdWaWoyVFhqclRxZVE6MQ\">here</a> to fill out a brief form with your contact info; then we will contact you about participating in specific surveys.</p>\n<p>If you have previously filled out a form like this one to participate in CFAR surveys, then we already have your information so you don't need to sign up again.<br /><br />Questions/Issues can be posted in the comments here, PMed to me, or emailed to us at CFARsurveys@gmail.com.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KJbthz9Lr2PdNpgJ3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 26, "extendedScore": null, "score": 9.455933549840007e-07, "legacy": true, "legacyId": "17685", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-19T04:11:13.608Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Detached Lever Fallacy", "slug": "seq-rerun-detached-lever-fallacy", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8J6q7X3scPAJxihy9/seq-rerun-detached-lever-fallacy", "pageUrlRelative": "/posts/8J6q7X3scPAJxihy9/seq-rerun-detached-lever-fallacy", "linkUrl": "https://www.lesswrong.com/posts/8J6q7X3scPAJxihy9/seq-rerun-detached-lever-fallacy", "postedAtFormatted": "Thursday, July 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Detached%20Lever%20Fallacy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Detached%20Lever%20Fallacy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8J6q7X3scPAJxihy9%2Fseq-rerun-detached-lever-fallacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Detached%20Lever%20Fallacy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8J6q7X3scPAJxihy9%2Fseq-rerun-detached-lever-fallacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8J6q7X3scPAJxihy9%2Fseq-rerun-detached-lever-fallacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 166, "htmlBody": "<p>Today's post, <a href=\"/lw/sp/detached_lever_fallacy/\">Detached Lever Fallacy</a> was originally published on 31 July 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There is a lot of machinery hidden beneath the words, and rationalist's taboo is one way to make a step towards exposing it.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dnu/seq_rerun_humans_in_funny_suits/\">Humans in Funny Suits</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8J6q7X3scPAJxihy9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 9.457139151673403e-07, "legacy": true, "legacyId": "17736", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zY4pic7cwQpa9dnyk", "Mo3SapZ92tGJTihtB", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-19T04:35:12.461Z", "modifiedAt": null, "url": null, "title": "Logging progress improving conscientiousness and overcoming procrastination at LessWrong", "slug": "logging-progress-improving-conscientiousness-and-overcoming", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:02.300Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zRaqko67khvD5yQMy/logging-progress-improving-conscientiousness-and-overcoming", "pageUrlRelative": "/posts/zRaqko67khvD5yQMy/logging-progress-improving-conscientiousness-and-overcoming", "linkUrl": "https://www.lesswrong.com/posts/zRaqko67khvD5yQMy/logging-progress-improving-conscientiousness-and-overcoming", "postedAtFormatted": "Thursday, July 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Logging%20progress%20improving%20conscientiousness%20and%20overcoming%20procrastination%20at%20LessWrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALogging%20progress%20improving%20conscientiousness%20and%20overcoming%20procrastination%20at%20LessWrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzRaqko67khvD5yQMy%2Flogging-progress-improving-conscientiousness-and-overcoming%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Logging%20progress%20improving%20conscientiousness%20and%20overcoming%20procrastination%20at%20LessWrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzRaqko67khvD5yQMy%2Flogging-progress-improving-conscientiousness-and-overcoming", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzRaqko67khvD5yQMy%2Flogging-progress-improving-conscientiousness-and-overcoming", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 255, "htmlBody": "<p>I've recently been thinking a lot about working on items in Luke's article <a href=\"/lw/4su/how_to_be_happy/\">How to Be Happy.</a> Here's one in particular I've struggled with a long time:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Improve your&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">conscientiousness</em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">. Conscientiousness involves a variety of tendencies: useful organization, strong work ethic, reliability, planning ahead, etc. Each of these individual skills can be learned. The techniques for&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\" href=\"/lw/3w3/how_to_beat_procrastination/\">overcoming procrastination</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;are useful, here. Some people report that books like&nbsp;</span><em style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"><a style=\"color: #8a8a8b;\" href=\"http://www.amazon.com/Getting-Things-Done-Stress-Free-Productivity/dp/0142000280/\">Getting Things Done</a></em><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">&nbsp;have helped them become more organized and reliable.</span></p>\n</blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">I particularly want to work on avoiding procrastination. I've tried several techniques for avoiding procrastination, but I always seem to relapse away from using them. But I've heard that you're more likely to stick with a plan if you log your progress in a place other people can see. That's what I plan to do in this thread. I encourage others to use it for the same purpose.</span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">I'm not sure how to best exploit LessWrong for this purpose. Tentatively I plan to come back weekly and post comments on my progress, though suggestions for better ways to do this are welcome.</span></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This thread can also serve as a place for sharing information on what techniques for approving conscientiousness, not procrastinating, etc. have worked and not worked for people in the LessWrong community.</span></p>\n<p>Over the next week, I plan on specifically focusing on&nbsp;implementing&nbsp;kalla724's advice on <a href=\"/lw/blr/attention_control_is_critical_for/\">attention control,</a> which worked when I first tried it but I've fallen away from using. I also plan on trying to make better use of productivity-related browser plugins like <a href=\"https://chrome.google.com/webstore/detail/eacjcpfjdlcdggndfhkmlpnhedggdhke\">Delayed Gratification.</a></p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">And again, advice on how to tweak this project greatly appreciated.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zRaqko67khvD5yQMy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "17737", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZbgCx2ntD5eu8Cno9", "RWo4LwFzpHNQCTcYt", "rD57ysqawarsbry6v"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-19T07:13:43.947Z", "modifiedAt": null, "url": null, "title": "Meetup : Madison: Rough Numbers", "slug": "meetup-madison-rough-numbers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:54.336Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ieMwBZzgN9zioq99E/meetup-madison-rough-numbers", "pageUrlRelative": "/posts/ieMwBZzgN9zioq99E/meetup-madison-rough-numbers", "linkUrl": "https://www.lesswrong.com/posts/ieMwBZzgN9zioq99E/meetup-madison-rough-numbers", "postedAtFormatted": "Thursday, July 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Madison%3A%20Rough%20Numbers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Madison%3A%20Rough%20Numbers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FieMwBZzgN9zioq99E%2Fmeetup-madison-rough-numbers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Madison%3A%20Rough%20Numbers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FieMwBZzgN9zioq99E%2Fmeetup-madison-rough-numbers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FieMwBZzgN9zioq99E%2Fmeetup-madison-rough-numbers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/bz'>Madison: Rough Numbers</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 July 2012 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">302 S Mills St, Apt 5, Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><em>Note:</em> If it's convenient, and you have a smartphone, laptop, or tablet, bring it. We'll want to be able to all look up data efficiently, and I want some folks to run races against calculators. :)</p>\n\n<p>I have a whole slew of stuff I'd like to talk about, and possibly even some things to test.</p>\n\n<p>Short topics we'll cover:</p>\n\n<ul>\n<li>Fermi problems: How to get rough numbers for things you're interested in, without actually knowing much hard data.</li>\n<li>Fast mental math: How to compute Fermi estimates in your head, quickly.</li>\n<li>Value of information: How to estimate when doing more work to find more information is likely to be useful.</li>\n</ul>\n\n<p>Some tests/experiments/kind-of-games I'd like to try:</p>\n\n<ul>\n<li>Is fast mental math actually useful if you have a glowing rectangle handy? I suspect it is, and will happily race some calculators to see if it is so.</li>\n<li>In what situations do Fermi estimates even <em>make sense</em>? Do we have questions that are actually better answered with roughly-guessed numbers, rather than data from the nearest source of internet?</li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/bz'>Madison: Rough Numbers</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ieMwBZzgN9zioq99E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 9.458007775440514e-07, "legacy": true, "legacyId": "17745", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Madison__Rough_Numbers\">Discussion article for the meetup : <a href=\"/meetups/bz\">Madison: Rough Numbers</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 July 2012 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">302 S Mills St, Apt 5, Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><em>Note:</em> If it's convenient, and you have a smartphone, laptop, or tablet, bring it. We'll want to be able to all look up data efficiently, and I want some folks to run races against calculators. :)</p>\n\n<p>I have a whole slew of stuff I'd like to talk about, and possibly even some things to test.</p>\n\n<p>Short topics we'll cover:</p>\n\n<ul>\n<li>Fermi problems: How to get rough numbers for things you're interested in, without actually knowing much hard data.</li>\n<li>Fast mental math: How to compute Fermi estimates in your head, quickly.</li>\n<li>Value of information: How to estimate when doing more work to find more information is likely to be useful.</li>\n</ul>\n\n<p>Some tests/experiments/kind-of-games I'd like to try:</p>\n\n<ul>\n<li>Is fast mental math actually useful if you have a glowing rectangle handy? I suspect it is, and will happily race some calculators to see if it is so.</li>\n<li>In what situations do Fermi estimates even <em>make sense</em>? Do we have questions that are actually better answered with roughly-guessed numbers, rather than data from the nearest source of internet?</li>\n</ul></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Madison__Rough_Numbers1\">Discussion article for the meetup : <a href=\"/meetups/bz\">Madison: Rough Numbers</a></h2>", "sections": [{"title": "Discussion article for the meetup : Madison: Rough Numbers", "anchor": "Discussion_article_for_the_meetup___Madison__Rough_Numbers", "level": 1}, {"title": "Discussion article for the meetup : Madison: Rough Numbers", "anchor": "Discussion_article_for_the_meetup___Madison__Rough_Numbers1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-19T10:27:37.541Z", "modifiedAt": null, "url": null, "title": "The Problem Of Apostasy", "slug": "the-problem-of-apostasy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:00.319Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raw_Power", "createdAt": "2010-09-10T23:59:43.621Z", "isAdmin": false, "displayName": "Raw_Power"}, "userId": "kwSqcED9qTanFyNWG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uRZLzNfGEYCmsX95g/the-problem-of-apostasy", "pageUrlRelative": "/posts/uRZLzNfGEYCmsX95g/the-problem-of-apostasy", "linkUrl": "https://www.lesswrong.com/posts/uRZLzNfGEYCmsX95g/the-problem-of-apostasy", "postedAtFormatted": "Thursday, July 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Problem%20Of%20Apostasy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Problem%20Of%20Apostasy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuRZLzNfGEYCmsX95g%2Fthe-problem-of-apostasy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Problem%20Of%20Apostasy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuRZLzNfGEYCmsX95g%2Fthe-problem-of-apostasy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuRZLzNfGEYCmsX95g%2Fthe-problem-of-apostasy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 488, "htmlBody": "<p>So I have been checking laws around the world regarding Apostasy. And I have found extremely troubling data on the approach Muslims take to dealing with apostates. In most cases, publicly stating that you do not, in fact, love Big Brother (specifically, that you do not believe in God, the Prophet, or Islam), after having professed the Profession of Faith being adult and sane (otherwise, you were never a Muslim in the first place), will get you killed.</p>\n<p>Yes, killed. It's one of the only three things traditional Islamic tribunals hand out death penalties for, the others being murder and adultery.&nbsp;</p>\n<p>However, interestingly enough, you are often given three days of detainment to \"think it over\" and \"accept the faith\".&nbsp;</p>\n<p>Some other countries, though, are more forgiving: you are allowed to be a public apostate. But you are still not allowed to proselytize: that remains a crime (in Morocco it's 15 years of prison, and a flogging). Though proselytism is also a crime if you are not a Muslim. I leave to your imagination how precarious the situation of religious minorities is, in this context.</p>\n<p>How little sense all of this makes, from a theological perspective. Forcing someone to \"accept the faith\" at knife point? Forbidding you from arguing against the Lord's (reputedly) absolutely self-evident and miraculously beautiful Word?&nbsp;</p>\n<p>No. These are the patterns of sedition and treason laws. The crime of the Apostate is not one against the Lord (He can take care of Himself, and He certainly can take care of the Apostate) but against the State (existence of a human lord contingent on political regime).&nbsp;</p>\n<p>And the lesswronger asks himself: \"How is that my concern? Please, get to the point.\" The point is that the promotion of rationalism faces a terrible obstacle there. We're not talking \"God Hates You\" placards, or getting fired from your job. We're talking fire range and electric chair.</p>\n<p>\"Sure,\" you say, \"but rationalism is not about atheism.\" And you'd be right. It isn't. It's just a very likely conclusion for the rationalist mind to reach, and, also, our cult leader (:P) is a raging, bitter, passionate atheist. That is enough. If word spreads and authorities find out, just peddling HPMOR might get people jailed. And that's not accounting for the hypothetical (cough) case of a young adult reading the Sequences and getting all hotheaded about it and doing something stupid. Like trying to promote our brand of rationality in such hostile terrain.</p>\n<p>So, let's take this hypothetical (harrumph) youth. They see irrationality around them, obvious and immense, they see the waste and the pain it causes. They'd like to do something about it. How would you advise them to go about it? Would you advise them to, in fact, do nothing at all? &nbsp;</p>\n<p>More importantly, concerning Less Wrong itself, should we try to distance ourselves from atheism and anti-religiousness as such? Is this baggage too inconvenient, or is it too much a part of what we stand for?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uRZLzNfGEYCmsX95g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 7, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "17751", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 123, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-19T19:48:50.749Z", "modifiedAt": null, "url": null, "title": "In Defense of Tone Arguments", "slug": "in-defense-of-tone-arguments", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:09.380Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OrphanWilde", "createdAt": "2012-06-21T18:24:36.749Z", "isAdmin": false, "displayName": "OrphanWilde"}, "userId": "cdW87AS6fdK2sywL2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dXsQHSa62FLuode48/in-defense-of-tone-arguments", "pageUrlRelative": "/posts/dXsQHSa62FLuode48/in-defense-of-tone-arguments", "linkUrl": "https://www.lesswrong.com/posts/dXsQHSa62FLuode48/in-defense-of-tone-arguments", "postedAtFormatted": "Thursday, July 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20In%20Defense%20of%20Tone%20Arguments&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIn%20Defense%20of%20Tone%20Arguments%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdXsQHSa62FLuode48%2Fin-defense-of-tone-arguments%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=In%20Defense%20of%20Tone%20Arguments%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdXsQHSa62FLuode48%2Fin-defense-of-tone-arguments", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdXsQHSa62FLuode48%2Fin-defense-of-tone-arguments", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 687, "htmlBody": "<p>Suppose, for a moment, you're a strong proponent of Glim, a fantastic new philosophy of ethics that will maximize truth, happiness, and all things good, just as soon as 51% of the population accepts it as the true way; once it has achieved majority status, careful models in game theory show that Glim proponents will be significantly more prosperous and happy than non-proponents (although everybody will benefit on average, according to its models), and it will take over.</p>\n<p>Glim has stalled, however; it's stuck at 49% belief, and a new countermovement, antiGlim, has arisen, claiming that Glim is a corrupt moral system with fatal flaws which will destroy the country if it has its way. &nbsp;Belief is starting to creep down, and those who accepted the ideas as plausible but weren't ready to commit are starting to turn away from the movement.</p>\n<p>In response, a senior researcher of Glim ethics has written a scathing condemnation of antiGlim as unpatriotic, evil, and determined to keep the populace in a state of perpetual misery to support its own hegemony. &nbsp;He vehemently denies that there are any flaws in the moral system, and refuses to entertain antiGlim in a public debate.</p>\n<p>In response to this, belief creeps slightly up, but acceptance goes into a freefall.</p>\n<p>You immediately ascertain that the negativity was worse for the movement than the criticisms; you write a response, and are accused of attacking the tone and ignoring the substance of the arguments. &nbsp;Glim and antiGlim leadership proceed into protracted and nasty arguments, until both are highly marginalized, and ignored by the general public. &nbsp;Belief in Glim continues, but when the leaders of antiGlim and Glim finally arrive on a bitterly agreed upon conclusion - the arguments having centered on an actual error in the original formulations of Glim philosophy, they're unable to either get their remaining supports to cooperate, or to get any of the public to listen. &nbsp;Truth, happiness, and all things good never arise, and things get slightly worse, as a result of the error.</p>\n<p>Tone arguments are not necessarily logical errors; they may be invoked by those who agree with the substance of an argument who nevertheless may feel that the argument, as posed, is counterproductive to its intended purpose.</p>\n<p>I have stopped recommending Dawkin's work to people who are on the fence about religion. &nbsp;The God Delusion utterly destroyed his effectiveness at convincing people against religion. &nbsp;(In a world in which they couldn't do an internet search on his name, it might not matter; we don't live in that world, and I assume other people are as likely to investigate somebody as I am.) &nbsp;It doesn't even matter whether his facts are right or not, the way he presents them will put most people on the intellectual defensive.</p>\n<p>If your purpose is to convince people, it's not enough to have good arguments, or good facts; these things can only work if people are receptive to those arguments and those facts. &nbsp;Your first move is your most important - you must try to make that person receptive. &nbsp;And if somebody levels a tone argument at you, your first consideration should not be \"Oh! &nbsp;That's <a href=\"/lw/85h/better_disagreement/\">DH2</a>, it's a fallacy, I can disregard what this person has to say!\" &nbsp;It should be - why are they leveling a tone argument at you to begin with? &nbsp;Are they disagreeing with you on the basis of your tone, or disagreeing with the tone itself?</p>\n<p>Or, in short, the categorical assessment of \"Responding to Tone\" as either a logical fallacy or a poor argument is incorrect, as it starts from an unfounded assumption that the purpose of a tone response is, in fact, to refute the argument. &nbsp;In the few cases I have seen responses to tone which were utilized against an argument, they were in fact ad-hominems, of the formulation \"This person clearly hates [x], and thus can't be expected to have an unbiased perspective.\" &nbsp;Note that this is a particularly persuasive ad-hominem, particularly for somebody who is looking to rationalize their beliefs against an argument - and that this inoculation against argument is precisely the reason you should, in fact, moderate your tone.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 1, "FkzScn5byCs9PxGsA": 1, "MXcpQvaPGtXpB6vkM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dXsQHSa62FLuode48", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 32, "extendedScore": null, "score": 8.9e-05, "legacy": true, "legacyId": "17752", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 175, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FhH8m5n8qGSSHsAgG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-19T23:54:08.814Z", "modifiedAt": null, "url": null, "title": "[LINK] Using procedural memory to thwart \"rubber-hose cryptanalysis\"", "slug": "link-using-procedural-memory-to-thwart-rubber-hose", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:54.719Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ptCDHq2MEgX6pz6oT/link-using-procedural-memory-to-thwart-rubber-hose", "pageUrlRelative": "/posts/ptCDHq2MEgX6pz6oT/link-using-procedural-memory-to-thwart-rubber-hose", "linkUrl": "https://www.lesswrong.com/posts/ptCDHq2MEgX6pz6oT/link-using-procedural-memory-to-thwart-rubber-hose", "postedAtFormatted": "Thursday, July 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Using%20procedural%20memory%20to%20thwart%20%22rubber-hose%20cryptanalysis%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Using%20procedural%20memory%20to%20thwart%20%22rubber-hose%20cryptanalysis%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FptCDHq2MEgX6pz6oT%2Flink-using-procedural-memory-to-thwart-rubber-hose%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Using%20procedural%20memory%20to%20thwart%20%22rubber-hose%20cryptanalysis%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FptCDHq2MEgX6pz6oT%2Flink-using-procedural-memory-to-thwart-rubber-hose", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FptCDHq2MEgX6pz6oT%2Flink-using-procedural-memory-to-thwart-rubber-hose", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 238, "htmlBody": "<p>It's an interesting idea, to fight the standard social engineering attempts by hiding the password from the user. In a sense, all the conscious mind gets is \"********\". The paper is called \"Neuroscience Meets Cryptography:&nbsp;Designing Crypto Primitives Secure Against Rubber Hose Attacks\". Here is a&nbsp;<a href=\"http://www.theregister.co.uk/2012/07/19/neuroscience_as_password_protector/\">popular write-up</a>&nbsp;and the paper&nbsp;<a href=\"http://bojinov.org/professional/usenixsec2012-rubberhose.pdf\">PDF</a>.</p>\n<p>Abstract:</p>\n<p style=\"padding-left: 30px;\">Cryptographic systems often rely on the secrecy of cryptographic keys given to users. Many schemes, however,&nbsp;cannot resist coercion attacks where the user is forcibly asked by an attacker to reveal the key. These attacks,&nbsp;known as rubber hose cryptanalysis, are often the easiest way to defeat cryptography. We present a defense against&nbsp;coercion attacks using the concept of implicit learning from cognitive psychology. Implicit learning refers to&nbsp;learning of patterns without any conscious knowledge of the learned pattern. We use a carefully crafted computer&nbsp;game to plant a secret password in the participant&rsquo;s brain without the participant having any conscious knowledge&nbsp;of the trained password. While the planted secret can be used for authentication, the participant cannot be coerced&nbsp;into revealing it since he or she has no conscious knowledge of it. We performed a number of user studies&nbsp;using Amazon&rsquo;s Mechanical Turk to verify that participants can successfully re-authenticate over time and that&nbsp;they are unable to reconstruct or even recognize short fragments of the planted secret.</p>\n<p>While this&nbsp;approach&nbsp;does nothing against man-in-the-middle attacks, it can probably be evolved into a unique digital signature some day. Cheaper than a retinal scan or a fingerprint, and does not require client-side hardware.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ptCDHq2MEgX6pz6oT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 6, "extendedScore": null, "score": 9.462771697203308e-07, "legacy": true, "legacyId": "17753", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-20T00:07:11.912Z", "modifiedAt": null, "url": null, "title": "Imperfect Voting Systems", "slug": "imperfect-voting-systems", "viewCount": null, "lastCommentedAt": "2021-01-23T13:50:42.504Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xNBRkPNHAGQ6EQaLS/imperfect-voting-systems", "pageUrlRelative": "/posts/xNBRkPNHAGQ6EQaLS/imperfect-voting-systems", "linkUrl": "https://www.lesswrong.com/posts/xNBRkPNHAGQ6EQaLS/imperfect-voting-systems", "postedAtFormatted": "Friday, July 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Imperfect%20Voting%20Systems&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AImperfect%20Voting%20Systems%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxNBRkPNHAGQ6EQaLS%2Fimperfect-voting-systems%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Imperfect%20Voting%20Systems%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxNBRkPNHAGQ6EQaLS%2Fimperfect-voting-systems", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxNBRkPNHAGQ6EQaLS%2Fimperfect-voting-systems", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1756, "htmlBody": "<p>Stalin once (supposedly) said that &ldquo;He who casts the votes determines nothing; he who counts the votes determines everything &ldquo; But he was being insufficiently cynical. He who chooses the voting system may determine just as much as the other two players.<br /><br /><a href=\"http://www.amazon.com/The-Art-Strategy-Theorists-Business/dp/0393337170/ref=sr_1_1?ie=UTF8&amp;qid=1342741561&amp;sr=8-1&amp;keywords=The+Art+of+Strategy\"><em>The Art of Strategy</em></a> gives some good examples of this principle: here's an adaptation of one of them. Three managers are debating whether to give a Distinguished Employee Award to a certain worker. If the worker gets the award, she must receive one of two prizes: a $50 gift certificate, or a $10,000 bonus.<br /><br />One manager loves the employee and wants her to get the $10,000; if she can't get the $10,000, she should at least get a gift certificate. A second manager acknowledges her contribution but is mostly driven by cost-cutting; she'd be happiest giving her the gift certificate, but would rather refuse to recognize her entirely than lose $10,000. And the third manager dislikes her and doesn't want to recognize her at all - but she also doesn't want the company to gain a reputation for stinginess, so if she gets recognized she'd rather give her the $10,000 than be so pathetic as to give her the cheap certificate.<br /><br />The managers arrange a meeting to determine the employee's fate. If the agenda tells them to vote for or against giving her an award, and then proceed to determine the prize afterwards if she wins, then things will not go well for the employee. Why not? Because the managers reason as follows: if she gets the award, Manager 1 and Manager 3 will vote for the $10,000 prize, and Manager 2 will vote for the certificate.&nbsp; Therefore, voting for her to get the award is practically the same as voting for her to get the $10,000 prize. That means Manager 1, who wants her to get the prize, will vote yes on the award, but Managers 2 and 3, who both prefer no award to the $10,000, will strategically vote not to give her the award. Result: she doesn't get recognized for her distinguished service.<br /><br />But suppose the employee involved happens to be the secretary arranging the meeting where the vote will take place. She makes a seemingly trivial change to the agenda: the managers will vote for what the prize should be first, and then vote on whether to give it to her.<br /><br />If the managers decide the appropriate prize is $10,000, then the motion to give the award will fail for exactly the same reasons it did above. But if the managers decide the certificate is appropriate, then Manager 1 and 2, who both prefer the certificate to nothing, will vote in favor of giving the award. So the three managers, thinking strategically, realize that the decision before them, which looks like &ldquo;$10 grand or certificate&rdquo;, is really &ldquo;No award or certificate&rdquo;. Since 1 and 2 both prefer the certificate to nothing, they vote that the certificate is the appropriate prize (even though Manager 1 doesn't really believe this) and the employee ends out with the gift certificate.<br /><br />But if the secretary is really smart, she may set the agenda as follows: The managers first vote whether or not to give $10,000, and if that fails, they next vote whether or not to give the certificate; if both votes fail the employee gets nothing. Here the managers realize that if the first vote (for $10,000) fails, the next vote (certificate or nothing) will pass, since two managers prefer certificate to nothing as mentioned before. So the true choice in the first vote is &ldquo;$10,000 versus certificate&rdquo;. Since two managers (1 and 3) prefer the $10,000 to the certificate, those two start by voting to give the full $10,000, and this is what the employee gets.<br /><br />So we see that all three options are possible outcomes, and that the true power rests not in the hands of any individual manager, but in the secretary who determines how the voting takes place.<br /><br />Americans have a head start in understanding the pitfalls of voting systems thanks to the so-called two party system. Every four years, they face quandaries like \"If leftists like me vote for Nader instead of Gore just because we like him better, are we going to end up electing Bush because we've split the leftist vote?\"<br /><br />Empirically, yes. The 60,000 Florida citizens who voted Green in 2000 didn't elect Nader. However, they did make Gore lose to Bush by a mere 500 votes. The last post discussed a Vickrey auction, a style of auction in which you have have no incentive to bid anything except your true value. Wouldn't it be nice if we had an electoral system with the same property: one where you should always vote for the candidate you actually support? If such a system existed, we would have ample reason to institute it and could rest assured that no modern-day Stalin was manipulating us via the choice of voting system we used.<br /><br />Some countries do claim to have better systems than the simple winner-takes-all approach of the United States. My own adopted homeland of Ireland uses a system called &ldquo;single transferable vote&rdquo; (also called instant-runoff vote), in which voters rank the X candidates from 1 to X. If a candidate has the majority of first preference votes (or a number of first preference votes greater than the number of positions to fill divided by the number of candidates, in elections with multiple potential winners like legislative elections), then that candidate wins and any surplus votes go to their voters' next preference. If no one meets the quota, then the least popular candidate is eliminated and their second preference votes become first preferences. The system continues until all available seats are full.<br /><br />For example, suppose I voted (1: Nader), (2: Gore), (3: Bush). The election officials tally all the votes and find that Gore has 49 million first preferences, Bush has 50 million, and Nader has 5 million. There's only one presidency, so a candidate would have to have a majority of votes (greater than 52 million out of 104 million) to win. Since no one meets that quota, the lowest ranked candidate gets eliminated - in this case, Nader. My vote now goes to my second preference, Gore. If 4 million Nader voters put Gore second versus 1 million who put Bush second, the tally's now at 53 million Gore, 51 million Bush. Gore has greater than 52 million and wins the election - the opposite result from if we'd elected a president the traditional way.<br /><br />Another system called Condorcet voting also uses a list of all candidates ranked in order, but uses the information to run mock runoffs between each of them. So a Condorcet system would use the ballots to run a Gore/Nader match (which Gore would win), a Gore/Bush match (which Gore would win), and a Bush/Nader match (which Bush would win). Since Gore won all of his matches, he becomes President. This becomes complicated when no candidate wins all of his matches (imagine Gore beating Nader, Bush beating Gore, but Nader beating Bush in a sort of Presidential rock-paper-scissors.) Condorcet voting has various options to resolve this; some systems give victory to the candidate whose greatest loss was by the smallest margin, and others to candidates who defeated the greatest number of other candidates.<br /><br />Do these systems avoid the strategic voting that plagues American elections? No. For example, both <a href=\"http://en.wikipedia.org/wiki/Favorite_betrayal_criterion#Instant-runoff_voting\">Single Transferable Vote</a> and <a href=\"http://en.wikipedia.org/wiki/Favorite_betrayal_criterion#Copeland\">Condorcet voting</a> sometimes provide incentives to rank a candidate with a greater chance of winning higher than a candidate you prefer - that is, the same \"vote Gore instead of Nader\" dilemma you get in traditional first-past-the-post.<br /><br />There are many other electoral systems in use around the world, including several more with ranking of candidates, a few that do different sorts of runoffs, and even some that ask you to give a numerical rating to each candidate (for example &ldquo;Nader 10, Gore 6, Bush -100000&rdquo;). Some of them even manage to eliminate the temptation to rank a non-preferred candidate first. But these work only at the expense of incentivizing other strategic manuevers, like defining &ldquo;approved candidate&rdquo; differently or exaggerating the difference between two candidates.<br /><br />So is there any voting system that automatically reflects the will of the populace in every way without encouraging tactical voting? No. Various proofs, including the <a href=\"http://en.wikipedia.org/wiki/Gibbard%E2%80%93Satterthwaite_theorem\">Gibbard-Satterthwaite Theorem</a> and the better-known <a href=\"http://en.wikipedia.org/wiki/Arrow_Impossibility_theorem\">Arrow Impossibility Theorem</a> show that many of the criteria by which we would naturally judge voting systems are mutually incompatible and that all reasonable systems must contain at least some small <a href=\"http://en.wikipedia.org/wiki/Tactical_voting\">element of tactics</a> (one example of an unreasonable system that eliminates tactical voting is picking one ballot at random and determining the results based solely on its preferences; the precise text of the theorem rules out &ldquo;nondeterministic or dictatorial&rdquo; methods).<br /><br />This means that each voting system has its own benefits and drawbacks, and that which one people use is largely a matter of preference. Some of these preferences reflect genuine concern about the differences between voting systems: for example, is it better to make sure your system always elects the Condorcet winner, even if that means the system penalizes candidates who are too similar to other candidates? Is it better to have a system where you can guarantee that participating in the election always makes your candidate more likely to win, or one where you can be sure that everyone voting exactly the opposite will never elect the same candidate?<br /><br />But in practice, these preferences tend to be political and self-interested. This was recently apparent in Britain, which voted last year on <a href=\"http://www.bbc.co.uk/news/uk-politics-13297573\">a referendum to change the voting system</a>. The Liberal Democrats, who were perpetually stuck in the same third-place situation as Nader in the States, supported a change to a form of instant runoff voting which would have made voting Lib Dem a much more palatable option; the two major parties opposed it probably for exactly that reason. <br /><br />Although no single voting system is mathematically perfect, several do seem to do better on the criteria that real people care about; look over Wikipedia's section on the <a href=\"http://en.wikipedia.org/wiki/Voting_systems#Compliance_of_selected_systems_.28table.29\">strengths and weaknesses of different voting systems</a> to see which one looks best.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"mPuSAzJN7CyrMiKrf": 4, "b8FHrKqyXuYGWc6vn": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xNBRkPNHAGQ6EQaLS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 53, "baseScore": 55, "extendedScore": null, "score": 0.000124, "legacy": true, "legacyId": "17754", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 90, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-20T02:45:00.766Z", "modifiedAt": null, "url": null, "title": "Singularity Institute - mainstream media exposure in Australia", "slug": "singularity-institute-mainstream-media-exposure-in-australia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:52.186Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "D_Alex", "createdAt": "2009-07-17T08:21:38.505Z", "isAdmin": false, "displayName": "D_Alex"}, "userId": "Sriopfkdwx2qJBx4G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GMtgdP9hFcuabD7TQ/singularity-institute-mainstream-media-exposure-in-australia", "pageUrlRelative": "/posts/GMtgdP9hFcuabD7TQ/singularity-institute-mainstream-media-exposure-in-australia", "linkUrl": "https://www.lesswrong.com/posts/GMtgdP9hFcuabD7TQ/singularity-institute-mainstream-media-exposure-in-australia", "postedAtFormatted": "Friday, July 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Singularity%20Institute%20-%20mainstream%20media%20exposure%20in%20Australia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingularity%20Institute%20-%20mainstream%20media%20exposure%20in%20Australia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGMtgdP9hFcuabD7TQ%2Fsingularity-institute-mainstream-media-exposure-in-australia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Singularity%20Institute%20-%20mainstream%20media%20exposure%20in%20Australia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGMtgdP9hFcuabD7TQ%2Fsingularity-institute-mainstream-media-exposure-in-australia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGMtgdP9hFcuabD7TQ%2Fsingularity-institute-mainstream-media-exposure-in-australia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 35, "htmlBody": "<p>The Age, one of the most widely read Australian newspapers, has published an article on \"the Singularity\". Mostly features Jaan Tallinn, but SI gets a mention and a link. Comments section makes for interesting reading.</p>\n<p>&nbsp;</p>\n<p><a href=\"http://www.theage.com.au/technology/sci-tech/rise-of-the-machines-20120718-229ev.html\">http://www.theage.com.au/technology/sci-tech/rise-of-the-machines-20120718-229ev.html</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GMtgdP9hFcuabD7TQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 9.463585776507285e-07, "legacy": true, "legacyId": "17720", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-20T03:31:31.705Z", "modifiedAt": null, "url": null, "title": "[LINK] Anchoring Bias In Medicine (NYT)", "slug": "link-anchoring-bias-in-medicine-nyt", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Q9oWZLLfJtXqhi5fq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hgLM6Hptoj8N2rdMA/link-anchoring-bias-in-medicine-nyt", "pageUrlRelative": "/posts/hgLM6Hptoj8N2rdMA/link-anchoring-bias-in-medicine-nyt", "linkUrl": "https://www.lesswrong.com/posts/hgLM6Hptoj8N2rdMA/link-anchoring-bias-in-medicine-nyt", "postedAtFormatted": "Friday, July 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Anchoring%20Bias%20In%20Medicine%20(NYT)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Anchoring%20Bias%20In%20Medicine%20(NYT)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhgLM6Hptoj8N2rdMA%2Flink-anchoring-bias-in-medicine-nyt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Anchoring%20Bias%20In%20Medicine%20(NYT)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhgLM6Hptoj8N2rdMA%2Flink-anchoring-bias-in-medicine-nyt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhgLM6Hptoj8N2rdMA%2Flink-anchoring-bias-in-medicine-nyt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 60, "htmlBody": "<p><a href=\"http://well.blogs.nytimes.com/2012/07/19/falling-into-the-diagnostic-trap/\">http://well.blogs.nytimes.com/2012/07/19/falling-into-the-diagnostic-trap/</a></p>\n<blockquote>\n<p>For the doctors, this was a harrowing lesson in the trap of anchoring bias. It is so easy to slip into it without even knowing. But this case reminded us to keep reciting the mantra: if something doesn&rsquo;t fit, don&rsquo;t try to make it fit. Ask what else might be going on. Don&rsquo;t fall into the trap.</p>\n</blockquote>\n<p>(Wikipedia's article:&nbsp;<a href=\"http://en.wikipedia.org/wiki/Anchoring\">http://en.wikipedia.org/wiki/Anchoring</a> )</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hgLM6Hptoj8N2rdMA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 9.463807418399085e-07, "legacy": true, "legacyId": "17757", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-20T04:10:15.107Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Comedy of Behaviorism", "slug": "seq-rerun-the-comedy-of-behaviorism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:56.979Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jd7fqzvy5xohAnSR6/seq-rerun-the-comedy-of-behaviorism", "pageUrlRelative": "/posts/jd7fqzvy5xohAnSR6/seq-rerun-the-comedy-of-behaviorism", "linkUrl": "https://www.lesswrong.com/posts/jd7fqzvy5xohAnSR6/seq-rerun-the-comedy-of-behaviorism", "postedAtFormatted": "Friday, July 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Comedy%20of%20Behaviorism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Comedy%20of%20Behaviorism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjd7fqzvy5xohAnSR6%2Fseq-rerun-the-comedy-of-behaviorism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Comedy%20of%20Behaviorism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjd7fqzvy5xohAnSR6%2Fseq-rerun-the-comedy-of-behaviorism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjd7fqzvy5xohAnSR6%2Fseq-rerun-the-comedy-of-behaviorism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 193, "htmlBody": "<p>Today's post, <a href=\"/lw/sr/the_comedy_of_behaviorism/\">The Comedy of Behaviorism</a> was originally published on 02 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#The_Comedy_of_Behaviorism\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The behaviorists thought that speaking about anything like a mind, or emotions, or thoughts, was unscientific. After all, they said, you can't observe anger. You can just observe behavior. But, it is possible, using empathy, to correctly predict wide varieties of behavior, which you can't account for by pavlovian conditioning.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/doo/seq_rerun_detached_lever_fallacy/\">Detached Lever Fallacy</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jd7fqzvy5xohAnSR6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 9.463991938105591e-07, "legacy": true, "legacyId": "17758", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9fpWoXpNv83BAHJdc", "8J6q7X3scPAJxihy9", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-20T05:48:12.760Z", "modifiedAt": null, "url": null, "title": "SI's Summer 2012 Matching Drive Ends July 31st", "slug": "si-s-summer-2012-matching-drive-ends-july-31st", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:57.500Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/29EQr3xtc7jpLyobQ/si-s-summer-2012-matching-drive-ends-july-31st", "pageUrlRelative": "/posts/29EQr3xtc7jpLyobQ/si-s-summer-2012-matching-drive-ends-july-31st", "linkUrl": "https://www.lesswrong.com/posts/29EQr3xtc7jpLyobQ/si-s-summer-2012-matching-drive-ends-july-31st", "postedAtFormatted": "Friday, July 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SI's%20Summer%202012%20Matching%20Drive%20Ends%20July%2031st&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASI's%20Summer%202012%20Matching%20Drive%20Ends%20July%2031st%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F29EQr3xtc7jpLyobQ%2Fsi-s-summer-2012-matching-drive-ends-july-31st%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SI's%20Summer%202012%20Matching%20Drive%20Ends%20July%2031st%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F29EQr3xtc7jpLyobQ%2Fsi-s-summer-2012-matching-drive-ends-july-31st", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F29EQr3xtc7jpLyobQ%2Fsi-s-summer-2012-matching-drive-ends-july-31st", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1192, "htmlBody": "<p>The Singularity Institute's <a href=\"http://intelligence.org/blog/2012/07/03/summer-challenge/\">summer 2012 matching drive</a> ends on July 31st! Donate by the end of the month to have your gift matched, dollar for dollar.</p>\n<p>As of this posting, SI has raised $70,000 of the $150,000 goal.</p>\n<p>The announcement <a href=\"http://intelligence.org/blog/2012/07/03/summer-challenge/\">says</a>:</p>\n<blockquote>Since we published our <a href=\"/files/strategicplan20112.pdf\">strategic plan</a> in August 2011, we have <a href=\"/r/discussion/lw/dm9/revisiting_sis_2011_strategic_plan_how_are_we/#summary\">achieved most of the near-term goals outlined therein</a>... \n<ul>\n</ul>\nIn the coming year, the <strong>Singularity Institute plans to do the following</strong>: \n<ul>\n<li><strong>Hold our annual <a href=\"http://singularitysummit.com/\">Singularity Summit</a></strong>, this year in San Francisco!</li>\n<li><strong>Spin off the <a href=\"http://www.appliedrationality.org/\">Center for Applied Rationality</a></strong> as a separate organization focused on rationality training, so that the Singularity Institute can be focused more exclusively on Singularity research and outreach.</li>\n<li><strong>Publish additional <a href=\"/research/\">research</a></strong> on AI risk and Friendly AI.</li>\n<li><strong>Eliezer will write an \"Open Problems in Friendly AI\" sequence</strong> for <em>Less Wrong</em>. <small>(For news on his rationality books, see <a href=\"/lw/d06/intellectual_insularity_and_productivity/6swt\">here</a>.)</small></li>\n<li><strong>Finish <em><a href=\"http://facingthesingularity.com/\">Facing the Singularity</a></em></strong> and publish ebook versions of <em>Facing the Singularity</em> and <em><a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences, 2006-2009</a></em>.</li>\n<li>And much more! For details on what we might do with additional funding, see <a href=\"/lw/cs6/how_to_purchase_ai_risk_reduction/\">How to Purchase AI Risk Reduction</a>.</li>\n</ul>\nIf you're planning to earmark your donation to CFAR (Center for Applied Rationality), here's a preview of <strong>what CFAR plans to do in the next year</strong>: \n<ul>\n<li><strong>Develop additional lessons</strong> teaching the most important and useful parts of rationality. CFAR has already developed and tested <em>over 18 hours of lessons</em> so far, including classes on how to evaluate evidence using Bayesianism, how to make more accurate predictions, how to be more efficient using economics, how to use thought experiments to better understand your own motivations, and much more.</li>\n<li><strong>Run immersive rationality retreats</strong> to teach from our curriculum and to connect aspiring rationalists with each other. CFAR ran pilot retreats in May and June. Participants in the May retreat called it &ldquo;transformative&rdquo; and &ldquo;astonishing,&rdquo; and the average response on the survey question, &ldquo;Are you glad you came? (1-10)&rdquo; was a 9.4. (We don't have the June data yet, but people were similarly enthusiastic about that one.)</li>\n<li><strong>Run SPARC, a camp on the advanced math of rationality</strong> for mathematically gifted high school students. CFAR has a stellar first-year class for SPARC 2012; most students admitted to the program placed in the top 50 on the USA Math Olympiad (or performed equivalently in a similar contest).</li>\n<li><strong>Collect longitudinal data on the effects of rationality training</strong>, to improve our curriculum and to generate promising hypotheses to test and publish, in collaboration with other researchers. CFAR has already launched a one-year randomized controlled study tracking reasoning ability and various metrics of life success, using participants in our June minicamp and a control group.</li>\n<li><strong>Develop apps and games about rationality</strong>, with the dual goals of (a) helping aspiring rationalists practice essential skills, and (b) making rationality fun and intriguing to a much wider audience. CFAR has two apps in beta testing: one training players to update their own beliefs the right amount after hearing other people&rsquo;s beliefs, and another training players to calibrate their level of confidence in their own beliefs. CFAR is working with a developer on several more games training people to avoid cognitive biases.</li>\n<li>And more!<a id=\"more\"></a></li>\n</ul>\n</blockquote>\n<p>In <a href=\"/lw/dm9/revisiting_sis_2011_strategic_plan_how_are_we/\">another post</a>, I compared the goals in our <a href=\"http://intelligence.org/files/strategicplan20112.pdf\">August 2011 strategic plan</a> to our current situation, summarizing:</p>\n<blockquote>\n<p>That's it for the main list! Now let's check in on what we said <strong>our top priorities for 2011-2012</strong>&nbsp;were:</p>\n<ol>\n<li><em>Public-facing research on creating a positive singularity</em>. Check.&nbsp;<a href=\"/lw/axr/three_new_papers_on_ai_risk/627o\">SI has more peer-reviewed publications in 2012 than in all past years combined</a>.</li>\n<li><em>Outreach / education / fundraising</em>. Check. Especially, through <a href=\"http://appliedrationality.org/\">CFAR</a>.</li>\n<li><em>Improved organizational effectiveness</em>. Check. <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6jzn\">Lots of good progress</a> on this.</li>\n<li><em>Singularity Summit</em>. <a href=\"http://singularitysummit.com/\">Check</a>.</li>\n</ol>\n<p>In summary, I think SI is a bit behind where I hoped we'd be by now, though this is largely because we've poured so much into launching <a href=\"http://appliedrationality.org/\">CFAR</a>, and as a result, CFAR has turned out to be significantly more cool at launch than I had anticipated.</p>\n<p>Fundraising has been a challenge. One donor failed to actually give their $46,000 pledge despite repeated reminders and requests, and our support base is (understandably) anxious to see a shift from movement-building work to FAI research, a shift I have been fighting for since I was made Executive Director. (Note that spinning off rationality work to CFAR is a substantial part of trimming SI down into being primarily an FAI research institute.)</p>\n<p>Reforming SI into a more efficient, effective organization has been my greatest challenge. Frankly, SI was in <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6l4h\">pretty bad shape</a> when Louie and I arrived as interns in April 2011, and there have been an incredible number of holes to dig SI out of &mdash; and several more remain. (In contrast, it has been a <em>joy</em>&nbsp;to help set up CFAR properly <em>from the very beginning</em>, with all the right organizational tools and processes in place.) Reforming SI presents a fundraising problem, because reforming SI is time consuming and sometimes costly, but is generally unexciting to donors.</p>\n<p>I can see the light at the end of the tunnel, though. We won't reach it if we can't improve our fundraising success in the next 3-6 months, but it's close enough that I can see it. SI's path forward, from my point of view, looks like this:</p>\n<ol>\n<li>We finish launching CFAR, which takes over the rationality work SI was doing. (Before January 2013.)</li>\n<li>We change how the Singularity Summit is planned and run so that it pulls our core staff away from core mission work to a lesser degree. (Before January 2013.)</li>\n<li>Eliezer writes the \"Open Problems in Friendly AI\" sequence. (Before January 2013.)</li>\n<li>We hire 1-2 researchers to produce technical write-ups from <a href=\"http://intelligence.org/files/TDT.pdf\">Eliezer's TDT article</a> and from his \"Open Problems in Friendly AI\" sequence. (Beginning September 2012, except that right now we don't have the cash to hire the 1-2 people who I know who&nbsp;<em>could</em>&nbsp;do this and who <em>want</em>&nbsp;to do this as soon as we have the money to hire them.)</li>\n<li>With the \"Open FAI Problems\" sequence and the technical write-ups in hand, we greatly expand our efforts to show math/compsci researchers that there is a tractable, technical research program in FAI theory, and as a result some researchers work on the sexiest of these problems from their departments, and some other math researchers take more seriously the prospect of being <em>hired</em> by SI to do technical research in FAI theory. (Beginning, roughly, in April 2013.)&nbsp;Also: There won't be classes on x-risk at <a href=\"http://appliedrationality.org/sparc.html\">SPARC</a>&nbsp;(rationality camp for young elite math talent), but some SPARC students might end up being interested in FAI stuff by osmosis.&nbsp;</li>\n<li>With a more tightly honed SI, improved fundraising practices, and visible mission-central research happening, SI is able to attract more funding and hire even more FAI researchers. (Beginning, roughly, in September 2013.)</li>\n</ol>\n<p>If you want to help us make this happen, please <a href=\"http://intelligence.org/blog/2012/07/03/summer-challenge/\">donate during our July matching drive!</a></p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "29EQr3xtc7jpLyobQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 19, "extendedScore": null, "score": 9.464458757146722e-07, "legacy": true, "legacyId": "17756", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["8LrrHdt8HSvZFKmRb", "FD85FtP3tyHAarvwx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-20T07:08:45.622Z", "modifiedAt": null, "url": null, "title": "Link: Glial cells shown to be involved in working memory", "slug": "link-glial-cells-shown-to-be-involved-in-working-memory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:56.006Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CronoDAS", "createdAt": "2009-02-27T04:42:19.587Z", "isAdmin": false, "displayName": "CronoDAS"}, "userId": "Q2oaNonArzibx5cQN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qFWmZFkZgp3HHi6oB/link-glial-cells-shown-to-be-involved-in-working-memory", "pageUrlRelative": "/posts/qFWmZFkZgp3HHi6oB/link-glial-cells-shown-to-be-involved-in-working-memory", "linkUrl": "https://www.lesswrong.com/posts/qFWmZFkZgp3HHi6oB/link-glial-cells-shown-to-be-involved-in-working-memory", "postedAtFormatted": "Friday, July 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20Glial%20cells%20shown%20to%20be%20involved%20in%20working%20memory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20Glial%20cells%20shown%20to%20be%20involved%20in%20working%20memory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqFWmZFkZgp3HHi6oB%2Flink-glial-cells-shown-to-be-involved-in-working-memory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20Glial%20cells%20shown%20to%20be%20involved%20in%20working%20memory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqFWmZFkZgp3HHi6oB%2Flink-glial-cells-shown-to-be-involved-in-working-memory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqFWmZFkZgp3HHi6oB%2Flink-glial-cells-shown-to-be-involved-in-working-memory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 29, "htmlBody": "<p><a href=\"http://www.scientificamerican.com/article.cfm?id=marijuana-reveals-memory-mechanism\">http://www.scientificamerican.com/article.cfm?id=marijuana-reveals-memory-mechanism</a></p>\n<p>I wonder what the implications are for brain preservation and whole brain emulation? If glial cells are important, then saving and emulating the neurons alone probably won't be enough.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5d63AWNjtFyHprX2k": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qFWmZFkZgp3HHi6oB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 9.464842626081785e-07, "legacy": true, "legacyId": "17769", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-20T08:17:27.145Z", "modifiedAt": null, "url": null, "title": "[link] Prepared to wait? New research challenges the idea that we favour small rewards now over bigger later", "slug": "link-prepared-to-wait-new-research-challenges-the-idea-that", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:53.101Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8KmgYNtA4X8rhaRCL/link-prepared-to-wait-new-research-challenges-the-idea-that", "pageUrlRelative": "/posts/8KmgYNtA4X8rhaRCL/link-prepared-to-wait-new-research-challenges-the-idea-that", "linkUrl": "https://www.lesswrong.com/posts/8KmgYNtA4X8rhaRCL/link-prepared-to-wait-new-research-challenges-the-idea-that", "postedAtFormatted": "Friday, July 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Prepared%20to%20wait%3F%20New%20research%20challenges%20the%20idea%20that%20we%20favour%20small%20rewards%20now%20over%20bigger%20later&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Prepared%20to%20wait%3F%20New%20research%20challenges%20the%20idea%20that%20we%20favour%20small%20rewards%20now%20over%20bigger%20later%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8KmgYNtA4X8rhaRCL%2Flink-prepared-to-wait-new-research-challenges-the-idea-that%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Prepared%20to%20wait%3F%20New%20research%20challenges%20the%20idea%20that%20we%20favour%20small%20rewards%20now%20over%20bigger%20later%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8KmgYNtA4X8rhaRCL%2Flink-prepared-to-wait-new-research-challenges-the-idea-that", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8KmgYNtA4X8rhaRCL%2Flink-prepared-to-wait-new-research-challenges-the-idea-that", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 93, "htmlBody": "<p><a href=\"http://bps-research-digest.blogspot.com/2012/07/prepared-to-wait-new-research.html\">http://bps-research-digest.blogspot.com/2012/07/prepared-to-wait-new-research.html</a></p>\n<blockquote>The old idea that we make decisions like rational agents has given way over the last few decades to a more realistic, psychologically informed picture that recognises the biases and mental short-cuts that sway our thinking. Supposedly one of these is <a href=\"http://en.wikipedia.org/wiki/Hyperbolic_discounting\">hyperbolic discounting</a> - our tendency to place disproportionate value on immediate rewards, whilst progressively undervaluing distant rewards the further in the future they stand. But not so fast, say <a href=\"http://www.wbs.ac.uk/faculty/members/Daniel/Read\">Daniel Read</a> at Warwick Business School and his colleagues with a new paper that fails to find any evidence for the phenomenon.</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8KmgYNtA4X8rhaRCL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 9.465170016465258e-07, "legacy": true, "legacyId": "17771", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-20T15:16:38.128Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Brussels, Sydney", "slug": "weekly-lw-meetups-brussels-sydney", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SBMPixHjgcJfDfAQe/weekly-lw-meetups-brussels-sydney", "pageUrlRelative": "/posts/SBMPixHjgcJfDfAQe/weekly-lw-meetups-brussels-sydney", "linkUrl": "https://www.lesswrong.com/posts/SBMPixHjgcJfDfAQe/weekly-lw-meetups-brussels-sydney", "postedAtFormatted": "Friday, July 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Brussels%2C%20Sydney&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Brussels%2C%20Sydney%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSBMPixHjgcJfDfAQe%2Fweekly-lw-meetups-brussels-sydney%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Brussels%2C%20Sydney%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSBMPixHjgcJfDfAQe%2Fweekly-lw-meetups-brussels-sydney", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSBMPixHjgcJfDfAQe%2Fweekly-lw-meetups-brussels-sydney", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 399, "htmlBody": "<p><strong>This summary was posted to LW main on July 13th, and has now been moved to discussion.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/bf\">Brussels meetup:&nbsp;<span class=\"date\">14 July 2012 12:00PM</span></a></li>\n<li><a href=\"/meetups/bm\">Less Wrong Sydney 16th July Event for Less Wrong:&nbsp;<span class=\"date\">16 July 2012 06:35PM</span></a></li>\n<li><a href=\"/meetups/br\">Washington DC Calibration Games meetup:&nbsp;<span class=\"date\">22 July 2012 03:00AM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bh\">(NYC) A Game of Nomic:&nbsp;<span class=\"date\">21 July 2012 03:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong>&nbsp;</strong><strong></strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SBMPixHjgcJfDfAQe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.467168303856697e-07, "legacy": true, "legacyId": "17774", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-20T17:02:52.624Z", "modifiedAt": null, "url": null, "title": "Mass-murdering neuroscience Ph.D. student", "slug": "mass-murdering-neuroscience-ph-d-student", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:59.024Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "James_Miller", "createdAt": "2009-03-05T17:14:38.674Z", "isAdmin": false, "displayName": "James_Miller"}, "userId": "LzF2X9eB9oS3q4BXG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wXTmeW9WtKbnQzmvx/mass-murdering-neuroscience-ph-d-student", "pageUrlRelative": "/posts/wXTmeW9WtKbnQzmvx/mass-murdering-neuroscience-ph-d-student", "linkUrl": "https://www.lesswrong.com/posts/wXTmeW9WtKbnQzmvx/mass-murdering-neuroscience-ph-d-student", "postedAtFormatted": "Friday, July 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mass-murdering%20neuroscience%20Ph.D.%20student&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMass-murdering%20neuroscience%20Ph.D.%20student%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwXTmeW9WtKbnQzmvx%2Fmass-murdering-neuroscience-ph-d-student%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mass-murdering%20neuroscience%20Ph.D.%20student%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwXTmeW9WtKbnQzmvx%2Fmass-murdering-neuroscience-ph-d-student", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwXTmeW9WtKbnQzmvx%2Fmass-murdering-neuroscience-ph-d-student", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<p>&nbsp;</p>\n<p class=\"MsoNoSpacing\">A Ph.D student in neuroscience&nbsp;<a href=\"http://sg.sports.yahoo.com/blogs/lookout/suspect-neuroscience-phd-student-photo-released-160848135.html\">shot</a> at least 50 people at a showing of the new Batman movie.&nbsp; He also appears to have released some kind of gas from a canister. &nbsp;Because of his educational background this person almost certainly knows a lot about molecular biology.&nbsp; How long will it be (if ever) before a typical&nbsp;bio-science&nbsp;Ph.D will have the capacity to kill, say,a million people?</p>\n<p class=\"MsoNoSpacing\">&nbsp;</p>\n<p class=\"MsoNoSpacing\">Edit: &nbsp;I'm not claiming that this event should cause a fully informed person to update on anything. &nbsp;Rather I was hoping that readers of this blog with strong life-science backgrounds could provide information that would help me and other interested readers assess the probability of future risks. &nbsp;Since this blog often deals with catastrophic risks and the social harms of irrationality and given that the events I described will likely dominate the U.S. news media for a few days I thought my question worth asking. &nbsp;Given the post's Karma rating (currently -4), however, I will update my beliefs about what constitutes an appropriate discussion post.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wXTmeW9WtKbnQzmvx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 7, "extendedScore": null, "score": 9.46767488650161e-07, "legacy": true, "legacyId": "17775", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 117, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-20T19:08:59.468Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup Thursday 7pm *New Place*", "slug": "meetup-fort-collins-colorado-meetup-thursday-7pm-new-place", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:55.500Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AiFZozYAwMzjv6RTk/meetup-fort-collins-colorado-meetup-thursday-7pm-new-place", "pageUrlRelative": "/posts/AiFZozYAwMzjv6RTk/meetup-fort-collins-colorado-meetup-thursday-7pm-new-place", "linkUrl": "https://www.lesswrong.com/posts/AiFZozYAwMzjv6RTk/meetup-fort-collins-colorado-meetup-thursday-7pm-new-place", "postedAtFormatted": "Friday, July 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20*New%20Place*&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20*New%20Place*%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAiFZozYAwMzjv6RTk%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-new-place%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20*New%20Place*%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAiFZozYAwMzjv6RTk%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-new-place", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAiFZozYAwMzjv6RTk%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-new-place", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 64, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/c0'>Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 July 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">112 S. College Ave, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Flinching, try bad, project management, Game Theory. \nAnd, for those who still eat carbs, the best cake in town is not a lie.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/c0'>Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AiFZozYAwMzjv6RTk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.468276287613918e-07, "legacy": true, "legacyId": "17776", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_\">Discussion article for the meetup : <a href=\"/meetups/c0\">Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 July 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">112 S. College Ave, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Flinching, try bad, project management, Game Theory. \nAnd, for those who still eat carbs, the best cake in town is not a lie.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_1\">Discussion article for the meetup : <a href=\"/meetups/c0\">Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Thursday 7pm *New Place*", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Thursday 7pm *New Place*", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-20T20:10:23.755Z", "modifiedAt": null, "url": null, "title": "[LINK] Inferring the rate of psychopathy from roadkill experiment", "slug": "link-inferring-the-rate-of-psychopathy-from-roadkill", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:27.904Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JaneQ", "createdAt": "2012-07-12T06:30:28.862Z", "isAdmin": false, "displayName": "JaneQ"}, "userId": "2bgzxqqsbesfeA3JD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/999hpxegED2ypfzfR/link-inferring-the-rate-of-psychopathy-from-roadkill", "pageUrlRelative": "/posts/999hpxegED2ypfzfR/link-inferring-the-rate-of-psychopathy-from-roadkill", "linkUrl": "https://www.lesswrong.com/posts/999hpxegED2ypfzfR/link-inferring-the-rate-of-psychopathy-from-roadkill", "postedAtFormatted": "Friday, July 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Inferring%20the%20rate%20of%20psychopathy%20from%20roadkill%20experiment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Inferring%20the%20rate%20of%20psychopathy%20from%20roadkill%20experiment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F999hpxegED2ypfzfR%2Flink-inferring-the-rate-of-psychopathy-from-roadkill%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Inferring%20the%20rate%20of%20psychopathy%20from%20roadkill%20experiment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F999hpxegED2ypfzfR%2Flink-inferring-the-rate-of-psychopathy-from-roadkill", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F999hpxegED2ypfzfR%2Flink-inferring-the-rate-of-psychopathy-from-roadkill", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 287, "htmlBody": "<p>Pardon the <a href=\"http://gizmodo.com/5927083/roadkill-experiment-shows-that-six-percent-of-drivers-are-sadistic-animal-killers\">sensationalist headline</a> of that article:</p>\n<blockquote>\n<p>Mark says that \"one thing that might explain the higher numbers here&mdash;in case people question my methods&mdash;is that I used a tarantula.\" Apparently, people seemed pretty eager about hitting a spider. \"If you take that out it goes to 2.8% which is closer to the other turtle vs. snake studies I ended up finding.\"</p>\n<p>It is still quite a surprisingly high number. At least compared to a <a href=\"http://en.wikipedia.org/wiki/Psychopathy#Epidemiology\">2008 study using the Psychopathy Checklist</a>, which discovered that 1.2 percent of the US population were potential psychopaths. 1.2 vs 2.8 is a huge difference.</p>\n</blockquote>\n<p>I was not aware of the other turtle and snake studies.</p>\n<p>Note that with turtle this is the lower bound on percentage of evil; a perfectly amoral person that could e.g. kill for modest and unimportant sum of money or any other reason would still have no incentive to steer to drive over a turtle; and a significant percentage of people would simply fail to notice the turtle entirely.</p>\n<p>This gives interesting prior for mental model of other people. Even at couple percent, psychopathy is much more common than notable intelligence or many other situations considered 'rare' or 'unlikely'. It appears to me that due to the politeness and the necessary good-until-proven-evil strategy, many people act as if they have an incredibly low prior for psychopathy, which permits easy exploitation by psychopaths. There may also be signaling reasons for pretending to have very low prior for psychopathy as one of the groups of people with high prior for psychopathy is psychopaths themselves; pretending easily becomes too natural, though.</p>\n<p>Perhaps adjusting the priors could improve personal safety and robustness with regards to various forms of exploitation, whenever the priors are set incorrectly.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"v4pviL33XGMuTpSNs": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "999hpxegED2ypfzfR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 14, "extendedScore": null, "score": 9.468569133575839e-07, "legacy": true, "legacyId": "17777", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-21T05:26:56.807Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] No Logical Positivist I", "slug": "seq-rerun-no-logical-positivist-i", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:52.733Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xu62ryBkHQkM3DhfL/seq-rerun-no-logical-positivist-i", "pageUrlRelative": "/posts/xu62ryBkHQkM3DhfL/seq-rerun-no-logical-positivist-i", "linkUrl": "https://www.lesswrong.com/posts/xu62ryBkHQkM3DhfL/seq-rerun-no-logical-positivist-i", "postedAtFormatted": "Saturday, July 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20No%20Logical%20Positivist%20I&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20No%20Logical%20Positivist%20I%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxu62ryBkHQkM3DhfL%2Fseq-rerun-no-logical-positivist-i%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20No%20Logical%20Positivist%20I%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxu62ryBkHQkM3DhfL%2Fseq-rerun-no-logical-positivist-i", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxu62ryBkHQkM3DhfL%2Fseq-rerun-no-logical-positivist-i", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>Today's post, <a href=\"/lw/ss/no_logical_positivist_i/\">No Logical Positivist I</a> was originally published on 04 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#No_Logical_Positivist_I\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Logical positivism was based around the idea that the only meaningful statements were those that could be verified by experiment. Unfortunately for positivism, there are meaningful statements that are very likely true and very likely false, and yet cannot be tested.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dpa/seq_rerun_the_comedy_of_behaviorism/\">The Comedy of Behaviorism</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xu62ryBkHQkM3DhfL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 9.471224114642558e-07, "legacy": true, "legacyId": "17785", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dTkWWhQkgxePxbtPE", "jd7fqzvy5xohAnSR6", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-21T18:28:44.692Z", "modifiedAt": null, "url": null, "title": "Work on Security Instead of Friendliness?", "slug": "work-on-security-instead-of-friendliness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:26.083Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m8FjhuELdg7iv6boW/work-on-security-instead-of-friendliness", "pageUrlRelative": "/posts/m8FjhuELdg7iv6boW/work-on-security-instead-of-friendliness", "linkUrl": "https://www.lesswrong.com/posts/m8FjhuELdg7iv6boW/work-on-security-instead-of-friendliness", "postedAtFormatted": "Saturday, July 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Work%20on%20Security%20Instead%20of%20Friendliness%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWork%20on%20Security%20Instead%20of%20Friendliness%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm8FjhuELdg7iv6boW%2Fwork-on-security-instead-of-friendliness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Work%20on%20Security%20Instead%20of%20Friendliness%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm8FjhuELdg7iv6boW%2Fwork-on-security-instead-of-friendliness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm8FjhuELdg7iv6boW%2Fwork-on-security-instead-of-friendliness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 662, "htmlBody": "<blockquote>\n<p>So I submit the only useful questions we can ask are not about AGI, \"goals\", and other such anthropomorphic, infeasible, irrelevant, and/or hopelessly vague ideas. We can only usefully ask computer security questions. For example some researchers I know believe we can achieve <a rel=\"nofollow\" href=\"http://www.hpl.hp.com/techreports/2004/HPL-2004-221.html?jumpid=reg_R1002_USEN\">virus-safe computing</a>. If we can achieve security against malware as strong as we can achieve for symmetric key cryptography, then it doesn't matter how smart the software is or what goals it has: if one-way functions exist no computational entity, classical or quantum, can crack symmetric key crypto based on said functions. And if NP-hard public key crypto exists, similarly for public key crypto. These and other security issues, and in particular the security of property rights, are the only real issues here and the rest is BS.</p>\n</blockquote>\n<p>-- <a href=\"http://unenumerated.blogspot.com/2011/01/singularity.html#7781997206773677029\">Nick Szabo</a></p>\n<p>Nick Szabo and I have very similar backrounds and interests. We both majored in computer science at the University of Washington. We're both very interested in economics and security. We came up with similar ideas about digital money. So why don't I advocate working on security problems while ignoring AGI, goals and Friendliness?</p>\n<p>In fact, I once did think that working on security was the best way to push the future towards a positive Singularity and away from a negative one. I started working on my <a href=\"http://www.cryptopp.com/\">Crypto++ Library</a> shortly after reading Vernor Vinge's A Fire Upon the Deep. I believe it was the first general purpose open source cryptography library, and it's still one of the most popular. (Studying cryptography led me to become involved in the Cypherpunks community with its emphasis on privacy and freedom from government intrusion, but a major reason for me to become interested in cryptography in the first place was a desire to help increase security against future entities similar to the Blight described in Vinge's novel.)</p>\n<p>I've since changed my mind, for two reasons.</p>\n<p><strong>1. The economics of security seems very unfavorable to the defense, in every field <em>except</em> cryptography.</strong></p>\n<p>Studying cryptography gave me hope that improving security could make a difference. But in every other security field, both physical and virtual, little progress is apparent, certainly not enough that humans might hope to defend their property rights against smarter intelligences. Achieving \"security against malware as strong as we can achieve for symmetric key cryptography\" seems quite hopeless in particular. Nick links above to a 2004 technical report titled \"Polaris: Virus Safe Computing for Windows XP\", which is strange considering that it's now 2012 and malware have little trouble with the latest operating systems and their defenses. Also striking to me has been the fact that even dedicated security software like OpenSSH and OpenSSL have had design and coding flaws that introduced security holes to the systems that run them.</p>\n<p>One way to think about Friendly AI is that it's an offensive approach to the problem of security (i.e., take over the world), instead of a defensive one.</p>\n<p><strong>2. Solving the problem of security at a sufficient level of generality requires understanding goals, and is essentially equivalent to solving Friendliness.</strong></p>\n<p>What does it mean to have \"secure property rights\", anyway? If I build an impregnable fortress around me, but an Unfriendly AI causes me to give up my goals in favor of its own by crafting a philosophical argument that is extremely convincing to me but wrong (or more generally, subverts my motivational system in some way), have I retained my \"property rights\"? What if it does the same to one of my robot servants, so that it subtly starts serving the UFAI's interests while thinking it's still serving mine? How does one define whether a human or an AI has been \"subverted\" or is \"secure\", without reference to its \"goals\"? It became apparent to me that fully solving security is not very different from solving Friendliness.</p>\n<p>I would be very interested to know what Nick (and others taking a similar position) thinks after reading the above, or if they've already had similar thoughts but still came to their current conclusions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 2, "ZFrgTgzwEfStg26JL": 2, "MhHM6Rx2b4F8tHTQk": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m8FjhuELdg7iv6boW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 51, "extendedScore": null, "score": 0.000115, "legacy": true, "legacyId": "17793", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 51, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote>\n<p>So I submit the only useful questions we can ask are not about AGI, \"goals\", and other such anthropomorphic, infeasible, irrelevant, and/or hopelessly vague ideas. We can only usefully ask computer security questions. For example some researchers I know believe we can achieve <a rel=\"nofollow\" href=\"http://www.hpl.hp.com/techreports/2004/HPL-2004-221.html?jumpid=reg_R1002_USEN\">virus-safe computing</a>. If we can achieve security against malware as strong as we can achieve for symmetric key cryptography, then it doesn't matter how smart the software is or what goals it has: if one-way functions exist no computational entity, classical or quantum, can crack symmetric key crypto based on said functions. And if NP-hard public key crypto exists, similarly for public key crypto. These and other security issues, and in particular the security of property rights, are the only real issues here and the rest is BS.</p>\n</blockquote>\n<p>-- <a href=\"http://unenumerated.blogspot.com/2011/01/singularity.html#7781997206773677029\">Nick Szabo</a></p>\n<p>Nick Szabo and I have very similar backrounds and interests. We both majored in computer science at the University of Washington. We're both very interested in economics and security. We came up with similar ideas about digital money. So why don't I advocate working on security problems while ignoring AGI, goals and Friendliness?</p>\n<p>In fact, I once did think that working on security was the best way to push the future towards a positive Singularity and away from a negative one. I started working on my <a href=\"http://www.cryptopp.com/\">Crypto++ Library</a> shortly after reading Vernor Vinge's A Fire Upon the Deep. I believe it was the first general purpose open source cryptography library, and it's still one of the most popular. (Studying cryptography led me to become involved in the Cypherpunks community with its emphasis on privacy and freedom from government intrusion, but a major reason for me to become interested in cryptography in the first place was a desire to help increase security against future entities similar to the Blight described in Vinge's novel.)</p>\n<p>I've since changed my mind, for two reasons.</p>\n<p><strong id=\"1__The_economics_of_security_seems_very_unfavorable_to_the_defense__in_every_field_except_cryptography_\">1. The economics of security seems very unfavorable to the defense, in every field <em>except</em> cryptography.</strong></p>\n<p>Studying cryptography gave me hope that improving security could make a difference. But in every other security field, both physical and virtual, little progress is apparent, certainly not enough that humans might hope to defend their property rights against smarter intelligences. Achieving \"security against malware as strong as we can achieve for symmetric key cryptography\" seems quite hopeless in particular. Nick links above to a 2004 technical report titled \"Polaris: Virus Safe Computing for Windows XP\", which is strange considering that it's now 2012 and malware have little trouble with the latest operating systems and their defenses. Also striking to me has been the fact that even dedicated security software like OpenSSH and OpenSSL have had design and coding flaws that introduced security holes to the systems that run them.</p>\n<p>One way to think about Friendly AI is that it's an offensive approach to the problem of security (i.e., take over the world), instead of a defensive one.</p>\n<p><strong id=\"2__Solving_the_problem_of_security_at_a_sufficient_level_of_generality_requires_understanding_goals__and_is_essentially_equivalent_to_solving_Friendliness_\">2. Solving the problem of security at a sufficient level of generality requires understanding goals, and is essentially equivalent to solving Friendliness.</strong></p>\n<p>What does it mean to have \"secure property rights\", anyway? If I build an impregnable fortress around me, but an Unfriendly AI causes me to give up my goals in favor of its own by crafting a philosophical argument that is extremely convincing to me but wrong (or more generally, subverts my motivational system in some way), have I retained my \"property rights\"? What if it does the same to one of my robot servants, so that it subtly starts serving the UFAI's interests while thinking it's still serving mine? How does one define whether a human or an AI has been \"subverted\" or is \"secure\", without reference to its \"goals\"? It became apparent to me that fully solving security is not very different from solving Friendliness.</p>\n<p>I would be very interested to know what Nick (and others taking a similar position) thinks after reading the above, or if they've already had similar thoughts but still came to their current conclusions.</p>", "sections": [{"title": "1. The economics of security seems very unfavorable to the defense, in every field except cryptography.", "anchor": "1__The_economics_of_security_seems_very_unfavorable_to_the_defense__in_every_field_except_cryptography_", "level": 1}, {"title": "2. Solving the problem of security at a sufficient level of generality requires understanding goals, and is essentially equivalent to solving Friendliness.", "anchor": "2__Solving_the_problem_of_security_at_a_sufficient_level_of_generality_requires_understanding_goals__and_is_essentially_equivalent_to_solving_Friendliness_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "107 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 107, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-21T19:55:33.199Z", "modifiedAt": null, "url": null, "title": "HP:MOR and the Radio Fallacy", "slug": "hp-mor-and-the-radio-fallacy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:08.290Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardChappell", "createdAt": "2009-03-22T04:29:28.935Z", "isAdmin": false, "displayName": "RichardChappell"}, "userId": "rW2qN9f2r5DP9urFF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/siFaJYuPDNLCGz2b6/hp-mor-and-the-radio-fallacy", "pageUrlRelative": "/posts/siFaJYuPDNLCGz2b6/hp-mor-and-the-radio-fallacy", "linkUrl": "https://www.lesswrong.com/posts/siFaJYuPDNLCGz2b6/hp-mor-and-the-radio-fallacy", "postedAtFormatted": "Saturday, July 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20HP%3AMOR%20and%20the%20Radio%20Fallacy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHP%3AMOR%20and%20the%20Radio%20Fallacy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsiFaJYuPDNLCGz2b6%2Fhp-mor-and-the-radio-fallacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=HP%3AMOR%20and%20the%20Radio%20Fallacy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsiFaJYuPDNLCGz2b6%2Fhp-mor-and-the-radio-fallacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsiFaJYuPDNLCGz2b6%2Fhp-mor-and-the-radio-fallacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 197, "htmlBody": "<p>No plot spoilers here, just wanted to flag a bit of poor reasoning that shows up in <a class=\"vt-p\" href=\"http://hpmor.com/chapter/39\">Chapter 39</a>:</p>\n<blockquote>\n<p><span style=\"color: #222222; font-family: Habibi, Georgia; font-size: 16px; line-height: 22px;\">I shouldn't have believed it even for all of thirty seconds! Because if people had souls there wouldn't be any such thing as brain damage, if your soul could go on speaking after your whole brain was gone, how could damage to the left cerebral hemisphere take away your ability to talk?</span></p>\n</blockquote>\n<p>This is a surprisingly common fallacy. &nbsp;Just because X depends on Y, it doesn't follow that X depends on <em>nothing but</em> Y. &nbsp;A phenomenon may involve more than just its most obvious failure point.</p>\n<p>To illustrate: Suppose I'm trapped in a box, and my only way to communicate with the outside world is via radio communication. &nbsp;Someone on the other end argues that I don't really exist -- \"There's no person beyond the radio receiver, for if there was then there wouldn't be any such thing as damaged radios!\" &nbsp;Pretty silly, huh? &nbsp;But people say this kind of thing in defense of physicalism <a class=\"vt-p\" href=\"http://www.philosophyetc.net/2008/07/brain-damage-and-physicalism.html\">all the time</a>.</p>\n<p>(N.B. This is not to defend the existence of souls. It's just to point out that this particular argument against them is invalid.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yrg267i4a8EsgYAXp": 1, "NSMKfa8emSbGNXRKD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "siFaJYuPDNLCGz2b6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 27, "extendedScore": null, "score": 8.1e-05, "legacy": true, "legacyId": "17794", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-22T06:10:07.374Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Anthropomorphic Optimism", "slug": "seq-rerun-anthropomorphic-optimism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:53.145Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yZCw2zZwj6MCkGuLr/seq-rerun-anthropomorphic-optimism", "pageUrlRelative": "/posts/yZCw2zZwj6MCkGuLr/seq-rerun-anthropomorphic-optimism", "linkUrl": "https://www.lesswrong.com/posts/yZCw2zZwj6MCkGuLr/seq-rerun-anthropomorphic-optimism", "postedAtFormatted": "Sunday, July 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Anthropomorphic%20Optimism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Anthropomorphic%20Optimism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyZCw2zZwj6MCkGuLr%2Fseq-rerun-anthropomorphic-optimism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Anthropomorphic%20Optimism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyZCw2zZwj6MCkGuLr%2Fseq-rerun-anthropomorphic-optimism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyZCw2zZwj6MCkGuLr%2Fseq-rerun-anthropomorphic-optimism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 165, "htmlBody": "<p>Today's post, <a href=\"/lw/st/anthropomorphic_optimism/\">Anthropomorphic Optimism</a> was originally published on 04 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>You shouldn't bother coming up with clever, persuasive arguments for why evolution will do things the way you prefer. It really isn't listening.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dq1/seq_rerun_no_logical_positivist_i/\">No Logical Positivist I</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yZCw2zZwj6MCkGuLr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 9.478305924779268e-07, "legacy": true, "legacyId": "17795", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RcZeZt8cPk48xxiQ8", "xu62ryBkHQkM3DhfL", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-22T13:52:13.521Z", "modifiedAt": null, "url": null, "title": "Meetup : Dublin, Ireland Meetup", "slug": "meetup-dublin-ireland-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:03.261Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Laoch", "createdAt": "2010-08-03T08:22:01.285Z", "isAdmin": false, "displayName": "Laoch"}, "userId": "jRKuWSW4uPnBJG4xS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6kwSa8jBeibRwjPcR/meetup-dublin-ireland-meetup", "pageUrlRelative": "/posts/6kwSa8jBeibRwjPcR/meetup-dublin-ireland-meetup", "linkUrl": "https://www.lesswrong.com/posts/6kwSa8jBeibRwjPcR/meetup-dublin-ireland-meetup", "postedAtFormatted": "Sunday, July 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Dublin%2C%20Ireland%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Dublin%2C%20Ireland%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6kwSa8jBeibRwjPcR%2Fmeetup-dublin-ireland-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Dublin%2C%20Ireland%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6kwSa8jBeibRwjPcR%2Fmeetup-dublin-ireland-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6kwSa8jBeibRwjPcR%2Fmeetup-dublin-ireland-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/c1'>Dublin, Ireland Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 August 2012 03:00:28PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">College Green</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>For anybody in Ireland, more specifically in Dublin, interested in meeting up.\nCollege Green Starbucks beside the wax museum.\nThis has been postponed a week.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/c1'>Dublin, Ireland Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6kwSa8jBeibRwjPcR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.480514267653994e-07, "legacy": true, "legacyId": "17796", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Dublin__Ireland_Meetup\">Discussion article for the meetup : <a href=\"/meetups/c1\">Dublin, Ireland Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 August 2012 03:00:28PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">College Green</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>For anybody in Ireland, more specifically in Dublin, interested in meeting up.\nCollege Green Starbucks beside the wax museum.\nThis has been postponed a week.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Dublin__Ireland_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/c1\">Dublin, Ireland Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Dublin, Ireland Meetup", "anchor": "Discussion_article_for_the_meetup___Dublin__Ireland_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Dublin, Ireland Meetup", "anchor": "Discussion_article_for_the_meetup___Dublin__Ireland_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "20 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-22T15:05:10.642Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge, MA Meetup", "slug": "meetup-cambridge-ma-meetup-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/E6zAobnYgNoD59NoC/meetup-cambridge-ma-meetup-1", "pageUrlRelative": "/posts/E6zAobnYgNoD59NoC/meetup-cambridge-ma-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/E6zAobnYgNoD59NoC/meetup-cambridge-ma-meetup-1", "postedAtFormatted": "Sunday, July 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%2C%20MA%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%2C%20MA%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE6zAobnYgNoD59NoC%2Fmeetup-cambridge-ma-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%2C%20MA%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE6zAobnYgNoD59NoC%2Fmeetup-cambridge-ma-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE6zAobnYgNoD59NoC%2Fmeetup-cambridge-ma-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 92, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/c2'>Cambridge, MA Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 July 2012 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames St, Cambridge</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The usual schedule is to have meetups on the first and third Sundays of each month, but some cool Less Wrongers are in town who've been away for awhile, so let's also have one tomorrow! (Sunday the 22nd). The usual time and place (2:00pm, 25 Ames St, Cambridge, room number posted at the entrance based on availability). No formal agenda, just socializing and intellectual conversations.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/c2'>Cambridge, MA Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "E6zAobnYgNoD59NoC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 9.480862981265273e-07, "legacy": true, "legacyId": "17797", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA_Meetup\">Discussion article for the meetup : <a href=\"/meetups/c2\">Cambridge, MA Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 July 2012 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames St, Cambridge</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The usual schedule is to have meetups on the first and third Sundays of each month, but some cool Less Wrongers are in town who've been away for awhile, so let's also have one tomorrow! (Sunday the 22nd). The usual time and place (2:00pm, 25 Ames St, Cambridge, room number posted at the entrance based on availability). No formal agenda, just socializing and intellectual conversations.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/c2\">Cambridge, MA Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge, MA Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Cambridge, MA Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-22T21:50:24.055Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels meetup", "slug": "meetup-brussels-meetup-13", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:23.185Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Axel", "createdAt": "2010-11-03T17:32:46.091Z", "isAdmin": false, "displayName": "Axel"}, "userId": "vi498nAvek8eWuMWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yr8kPYYdCh6fvwm9k/meetup-brussels-meetup-13", "pageUrlRelative": "/posts/yr8kPYYdCh6fvwm9k/meetup-brussels-meetup-13", "linkUrl": "https://www.lesswrong.com/posts/yr8kPYYdCh6fvwm9k/meetup-brussels-meetup-13", "postedAtFormatted": "Sunday, July 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyr8kPYYdCh6fvwm9k%2Fmeetup-brussels-meetup-13%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyr8kPYYdCh6fvwm9k%2Fmeetup-brussels-meetup-13", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyr8kPYYdCh6fvwm9k%2Fmeetup-brussels-meetup-13", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 130, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/c3'>Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 August 2012 12:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're trying out a new location this time as the museum cafeteria can be a bit loud sometimes. We're meeting at 'La Fleur en Papier Dor\u00e9' close to the Brussels Central station (this should also improve train travel) We'll be having some nice intelligent discussions and I'll hopefully have a second version of the learning-Bayes-game.</p>\n\n<p>If you're in the neighborhood consider dropping by!</p>\n\n<p>EDIT: I realize the google maps marker is in a slightly different place then the 'La Fleur en Papier Dor\u00e9' marker, this comes from a spelling error on their site. The red marker is the correct one, you can check with street-view.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/c3'>Brussels meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yr8kPYYdCh6fvwm9k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.482800380691567e-07, "legacy": true, "legacyId": "17798", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup\">Discussion article for the meetup : <a href=\"/meetups/c3\">Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 August 2012 12:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're trying out a new location this time as the museum cafeteria can be a bit loud sometimes. We're meeting at 'La Fleur en Papier Dor\u00e9' close to the Brussels Central station (this should also improve train travel) We'll be having some nice intelligent discussions and I'll hopefully have a second version of the learning-Bayes-game.</p>\n\n<p>If you're in the neighborhood consider dropping by!</p>\n\n<p>EDIT: I realize the google maps marker is in a slightly different place then the 'La Fleur en Papier Dor\u00e9' marker, this comes from a spelling error on their site. The red marker is the correct one, you can check with street-view.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup1\">Discussion article for the meetup : <a href=\"/meetups/c3\">Brussels meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup", "level": 1}, {"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-23T01:44:25.808Z", "modifiedAt": null, "url": null, "title": "The Perception-Action Cycle", "slug": "the-perception-action-cycle", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:56.551Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "royf", "createdAt": "2012-05-28T04:41:04.869Z", "isAdmin": false, "displayName": "royf"}, "userId": "Rrw92MqPSK6T8nSBg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/u8JbzpmDWsK9Qk9nM/the-perception-action-cycle", "pageUrlRelative": "/posts/u8JbzpmDWsK9Qk9nM/the-perception-action-cycle", "linkUrl": "https://www.lesswrong.com/posts/u8JbzpmDWsK9Qk9nM/the-perception-action-cycle", "postedAtFormatted": "Monday, July 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Perception-Action%20Cycle&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Perception-Action%20Cycle%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu8JbzpmDWsK9Qk9nM%2Fthe-perception-action-cycle%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Perception-Action%20Cycle%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu8JbzpmDWsK9Qk9nM%2Fthe-perception-action-cycle", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu8JbzpmDWsK9Qk9nM%2Fthe-perception-action-cycle", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 528, "htmlBody": "<p>Would readers be interested in a sequence of posts offering an intuitive explanation of my underway thesis on the application of information theory to reinforcement learning?&nbsp;Please also feel free to comment on the <a href=\"/lw/kh/explainers_shoot_high_aim_low/\">quality</a>&nbsp;of my presentation.</p>\n<p>In this first post I offer a high-level description of the Perception-Action Cycle as an intuitive explanation of reinforcement learning.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>Imagine that the world is divided into two parts: one we shall call the agent and the rest - its environment. Imagine that the two interact in turns. One moment the agent receives information from its environment in the form of an <em>observation</em>. Then the next moment the agent sends out information to its environment in the form of an <em>action</em>. Then it makes another observation, then another action, and so on.</p>\n<p>To break down the cycle, we start with the agent having a <a href=\"http://wiki.lesswrong.com/wiki/Belief\">belief</a> about the state of its environment. This is actually the technical term: the belief is the probability that the agent assigns, <em>implicitly</em>, to each possible state of the environment. The&nbsp;cycle then proceeds in 4 phases.</p>\n<p>In the first phase, the agent makes an observation. Since the observation conveys <a href=\"/lw/o2/mutual_information_and_density_in_thingspace/\">information</a> of the environment, the agent needs to update its belief, ideally using <a href=\"http://yudkowsky.net/rational/bayes\">Bayes' theorem</a>. The agent now has <a href=\"/lw/1y9/information_theory_and_the_symmetry_of_updating/\">more information</a> about the environment.</p>\n<p>In the second phase, the agent uses this new information to update its plan. Note the crucial underlying principle that information about the environment is <a href=\"/lw/85x/value_of_information_four_examples/\">useful in making better plans</a>. This gives a desired fusion between Bayesian updates and decision making.</p>\n<p>In the third phase, the agent executes a step of its plan - a single action. This changes the environment. Some of the things that the agent knew about the previous state of the environment may no longer be true, and the agent is back to having less information.</p>\n<p>In the fourth phase, the agent makes a prediction about future observations. The importance of making a <a href=\"/lw/im/hindsight_devalues_science/\">prediction before a scientific experiment</a> is well understood by philosophers of science. But the importance of constantly making predictions of all of our sensory inputs as a functional part of our cognition, is only now dawning on&nbsp;neuroscientists&nbsp;and machine learning researchers.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>The Perception-Action Cycle is an intuitive explanation of the technical setting of <em>reinforcement learning</em>. Reinforcement learning is a&nbsp;powerful model of machine learning,&nbsp;in which decision making, learning and evaluation occur simultaneously and somewhat implicitly while a learner interacts with its environment. This&nbsp;can be used to describe a wide variety of real-life scenarios, including biological and artificial agents. It&nbsp;is so general, in fact, that our work is still ahead of us if we want it to have&nbsp;<a href=\"/lw/iu/mysterious_answers_to_mysterious_questions/\">any explanatory power</a>, and&nbsp;solving it in the most general form is a computationally hard problem.</p>\n<p>But the Perception-Action Cycle still offers symmetries to explore, analogies to physics to draw, practical learning algorithms to develop; all of which improve its <a href=\"/lw/jp/occams_razor/\">Occam's razor</a> prior score as a good model of intelligence. And to use it to actually explain things, we can narrow it down further. Not everything that it makes possible is equally probable. By applying information theory,&nbsp;a collection of statistical concepts, theorems and methods implied by strong Bayesianism, we can get a better picture of what intelligence is and isn't.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "u8JbzpmDWsK9Qk9nM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 9.48391960774099e-07, "legacy": true, "legacyId": "17113", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2TPph4EGZ6trEbtku", "yLcuygFfMfrfK8KjF", "SEZqJcSm25XpQMhzr", "vADtvr9iDeYsCDfxd", "WnheMGAka4fL99eae", "6i3zToomS86oj9bS6", "f4txACqDWithRi7hs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-23T02:10:21.208Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Biased Boardgames meetup", "slug": "meetup-washington-dc-biased-boardgames-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AEbGysfu8jCDh7gZd/meetup-washington-dc-biased-boardgames-meetup", "pageUrlRelative": "/posts/AEbGysfu8jCDh7gZd/meetup-washington-dc-biased-boardgames-meetup", "linkUrl": "https://www.lesswrong.com/posts/AEbGysfu8jCDh7gZd/meetup-washington-dc-biased-boardgames-meetup", "postedAtFormatted": "Monday, July 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Biased%20Boardgames%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Biased%20Boardgames%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAEbGysfu8jCDh7gZd%2Fmeetup-washington-dc-biased-boardgames-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Biased%20Boardgames%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAEbGysfu8jCDh7gZd%2Fmeetup-washington-dc-biased-boardgames-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAEbGysfu8jCDh7gZd%2Fmeetup-washington-dc-biased-boardgames-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 97, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/c4'>Washington DC Biased Boardgames meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 August 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Arlington, VA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next meetup will be Sunday, August 12th, at 3pm, at a private residence. PM me or email the list for details.</p>\n\n<p>The topic will be <a href=\"http://lesswrong.com/lw/ar2/biased_pandemic/\">biased boardgames</a> . If you don't want to read the link, basically we will be assigning biases for people to exaggerate while we play a game (most likely pandemic, but maybe others). Suggestions for games would be fantastic, as would be bringing them.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/c4'>Washington DC Biased Boardgames meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AEbGysfu8jCDh7gZd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 9.484043598413585e-07, "legacy": true, "legacyId": "17803", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Biased_Boardgames_meetup\">Discussion article for the meetup : <a href=\"/meetups/c4\">Washington DC Biased Boardgames meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 August 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Arlington, VA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next meetup will be Sunday, August 12th, at 3pm, at a private residence. PM me or email the list for details.</p>\n\n<p>The topic will be <a href=\"http://lesswrong.com/lw/ar2/biased_pandemic/\">biased boardgames</a> . If you don't want to read the link, basically we will be assigning biases for people to exaggerate while we play a game (most likely pandemic, but maybe others). Suggestions for games would be fantastic, as would be bringing them.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Biased_Boardgames_meetup1\">Discussion article for the meetup : <a href=\"/meetups/c4\">Washington DC Biased Boardgames meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Biased Boardgames meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Biased_Boardgames_meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Biased Boardgames meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Biased_Boardgames_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["34jf9Z43kBHF7Axz2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-23T02:37:51.422Z", "modifiedAt": null, "url": null, "title": "[Link] You Have No Idea How Wrong You Are", "slug": "link-you-have-no-idea-how-wrong-you-are", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:53.516Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cogitus_Maximus", "createdAt": "2012-03-20T07:30:58.997Z", "isAdmin": false, "displayName": "Cogitus_Maximus"}, "userId": "iRjrqT5kPtMM3Shh5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/r4yJL7M9cCjP9T89Q/link-you-have-no-idea-how-wrong-you-are", "pageUrlRelative": "/posts/r4yJL7M9cCjP9T89Q/link-you-have-no-idea-how-wrong-you-are", "linkUrl": "https://www.lesswrong.com/posts/r4yJL7M9cCjP9T89Q/link-you-have-no-idea-how-wrong-you-are", "postedAtFormatted": "Monday, July 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20You%20Have%20No%20Idea%20How%20Wrong%20You%20Are&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20You%20Have%20No%20Idea%20How%20Wrong%20You%20Are%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr4yJL7M9cCjP9T89Q%2Flink-you-have-no-idea-how-wrong-you-are%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20You%20Have%20No%20Idea%20How%20Wrong%20You%20Are%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr4yJL7M9cCjP9T89Q%2Flink-you-have-no-idea-how-wrong-you-are", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr4yJL7M9cCjP9T89Q%2Flink-you-have-no-idea-how-wrong-you-are", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 18, "htmlBody": "<p><a href=\"https://www.youtube.com/watch?v=E8V8rtdXnLA&amp;feature\">https://www.youtube.com/watch?v=E8V8rtdXnLA&amp;feature</a></p>\n<p>Be sure to make it to the last 5 minutes of the lecture as the tone shifts significantly.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "r4yJL7M9cCjP9T89Q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -5, "extendedScore": null, "score": 9.484175150449052e-07, "legacy": true, "legacyId": "17804", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-23T03:36:18.842Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Contaminated by Optimism", "slug": "seq-rerun-contaminated-by-optimism", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FfHwLcQkN5xgDDrEY/seq-rerun-contaminated-by-optimism", "pageUrlRelative": "/posts/FfHwLcQkN5xgDDrEY/seq-rerun-contaminated-by-optimism", "linkUrl": "https://www.lesswrong.com/posts/FfHwLcQkN5xgDDrEY/seq-rerun-contaminated-by-optimism", "postedAtFormatted": "Monday, July 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Contaminated%20by%20Optimism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Contaminated%20by%20Optimism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFfHwLcQkN5xgDDrEY%2Fseq-rerun-contaminated-by-optimism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Contaminated%20by%20Optimism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFfHwLcQkN5xgDDrEY%2Fseq-rerun-contaminated-by-optimism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFfHwLcQkN5xgDDrEY%2Fseq-rerun-contaminated-by-optimism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 171, "htmlBody": "<p>Today's post, <a href=\"/lw/su/contaminated_by_optimism/\">Contaminated by Optimism</a> was originally published on 06 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Contaminated_by_Optimism\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Avoid situations, as much as you possibly can, in which optimistic thinking suggests ideas for conscious consideration. In real life problems, if you've done that, you've probably already screwed up.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dqb/seq_rerun_anthropomorphic_optimism/\">Anthropomorphic Optimism</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FfHwLcQkN5xgDDrEY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 9.484454766525206e-07, "legacy": true, "legacyId": "17805", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9agCMMd7k7Hy37aCx", "yZCw2zZwj6MCkGuLr", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-23T08:49:29.722Z", "modifiedAt": null, "url": null, "title": "Uploading: what about the carbon-based version?", "slug": "uploading-what-about-the-carbon-based-version", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:09.682Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qNcGzyq5HEWnrx3zP/uploading-what-about-the-carbon-based-version", "pageUrlRelative": "/posts/qNcGzyq5HEWnrx3zP/uploading-what-about-the-carbon-based-version", "linkUrl": "https://www.lesswrong.com/posts/qNcGzyq5HEWnrx3zP/uploading-what-about-the-carbon-based-version", "postedAtFormatted": "Monday, July 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Uploading%3A%20what%20about%20the%20carbon-based%20version%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUploading%3A%20what%20about%20the%20carbon-based%20version%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqNcGzyq5HEWnrx3zP%2Fuploading-what-about-the-carbon-based-version%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Uploading%3A%20what%20about%20the%20carbon-based%20version%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqNcGzyq5HEWnrx3zP%2Fuploading-what-about-the-carbon-based-version", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqNcGzyq5HEWnrx3zP%2Fuploading-what-about-the-carbon-based-version", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 176, "htmlBody": "<p>In <a href=\"http://www.acceleratingfuture.com/michael/blog/2010/05/eliezer-yudkowsky-and-massimo-pigliucci-debate-the-singularity/\">this video</a>, long about 48:00, Eliezer talks about uploading and about how it wouldn't be murder if his meat body were anesthetized before the upload and killed without regaining consciousness.</p>\n<p>It's arguable that it wouldn't be murder, but I'm not clear about why Eliezer would want to do it that way. I've got some guesses about why one might want to not let the meat body wake up (legal and practical complications of a double but diverging identity, the meat version feeling hopelessly envious), but I'm not sure whether either of them apply.</p>\n<p>On the other hand, I can think of a couple of reasons for *not* eliminating the meat version-- one is that two Eliezers would presumably be better than one, though I don't have a strong intuition about the optimum number of Eliezers. The other, which I consider to be more salient, is that the meat version is a backup in case the upload isn't as good as hoped.</p>\n<p>More generally, what would folks here consider to be good enough evidence that uploading was worth doing?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 2, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qNcGzyq5HEWnrx3zP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 12, "extendedScore": null, "score": 9.48595304608702e-07, "legacy": true, "legacyId": "17818", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-23T19:33:02.700Z", "modifiedAt": null, "url": null, "title": "A Marriage Ceremony for Aspiring Rationalists", "slug": "a-marriage-ceremony-for-aspiring-rationalists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:34.508Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TTanRyNTvKXgqqv3u/a-marriage-ceremony-for-aspiring-rationalists", "pageUrlRelative": "/posts/TTanRyNTvKXgqqv3u/a-marriage-ceremony-for-aspiring-rationalists", "linkUrl": "https://www.lesswrong.com/posts/TTanRyNTvKXgqqv3u/a-marriage-ceremony-for-aspiring-rationalists", "postedAtFormatted": "Monday, July 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Marriage%20Ceremony%20for%20Aspiring%20Rationalists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Marriage%20Ceremony%20for%20Aspiring%20Rationalists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTTanRyNTvKXgqqv3u%2Fa-marriage-ceremony-for-aspiring-rationalists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Marriage%20Ceremony%20for%20Aspiring%20Rationalists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTTanRyNTvKXgqqv3u%2Fa-marriage-ceremony-for-aspiring-rationalists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTTanRyNTvKXgqqv3u%2Fa-marriage-ceremony-for-aspiring-rationalists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1242, "htmlBody": "<p>Recently, LWers <a href=\"http://effectivenessforgeeks.com/\">Will Ryan</a> and <a href=\"http://meaningandmagic.com/\">Divia Melwani</a> (now Will and Divia Eden) were married, with Eliezer Yudkowsky officiating.</p>\n<p>I've been to 40+ weddings in my lifetime, and this was my favorite ceremony yet. <a href=\"http://www.youtube.com/watch?v=CUl2oFSWcEU\">Here</a> is the video, and below is the transcript of Eliezer's... what's it called? \"Blessing\"?</p>\n<p>&nbsp;</p>\n<p>\n<blockquote>\n<p>Dearly beloved, we are gathered here upon this day, to bear witness to William Ryan and Divia Melwani, as they bind themselves together in marriage, becoming William and Divia Eden, from this day endeavoring to live their lives as one. If any person can show just cause why these two should not be joined, let them speak now, or forever hold their peace.</p>\n<p>The institution of marriage is as old as <em>Homo sapiens</em>. Donald Brown lists it among the human universals, the parts of culture which are found in almost every tribe that has been studied by anthropologists, alongside such other universals as dancing, storytelling, jealousy, or language. Though we give it a single name, marriage takes many forms.</p>\n<p>In some tribes a man may wed more than one woman. In 0.5% of hunter-gatherer tribes studied, a woman may wed more than one man. In civilized parts of the modern world, men may marry men, or women marry women. A hundred years ago, in what was then considered civilization, marriage was a cruel necessity if you wanted to have a public relationship with anyone. There was only one approved option for anyone who didn't want to live alone - marry a single person of the opposite sex and stay together for 70 years or until one of you died.</p>\n<p>But in this day, and within this community, marriage necessarily takes on a different meaning. 'Until death do you part' is a different concept if you suspect that indefinite lifespan extension may be invented sometime in the next few decades. Once, getting married at age 20 meant you were probably a quarter of the way through your life. In this day, and in this community, you know that you might actually be getting married at zero point zero zero zero and some more zeroes one percent of the way through your life. Our community contains many people in long-term relationships who are not married and are not waiting around to get married.</p>\n<p>Even among those who marry, not every marriage has the same meaning. Some may not be planning to stay together until the stars go out - just enjoy the marriage for however long it lasts. And though marriage is no longer mandatory, the government of this country, in its finite wisdom, has decreed legal benefits for marriage which some of us may not wish to deny ourselves, even if we haven't yet found a perfect romance out of storybooks, even if we might not want a perfect romance out of storybooks.</p>\n<p>Marriage is no longer something that everyone has to do, and there isn&rsquo;t just one kind of marriage, or one meaning of marriage. But at least so far as I can tell from the outside, Will and Divia seem to have a perfect romance, pretty much. And while romances like that exist, the ancient institution of marriage will continue into the future, I think.</p>\n<p>There are stars in the sky above us, even now. Even on a cloudless day you can't see them with your naked eyes, but the right camera would capture them. There is light shining upon this ceremony which is far older than eight and a half minutes. Standing as we do in the light of eternity, it may seem impossible to swear any true promise upon the future, when there are no perfect blessings called down upon a marriage to ensure its success, but only the mortal wills of human beings to guard it.</p>\n<p>And yet there are still some people who are just so adorable together that you look at them and say, \"Yeah, they should go for it.\" I can think of at least three couples like that, though, aside from Will and Divia, I'm not going to name any names. Elizabeth Moon once wrote that courage is inherent in all living things; it is the quality that keeps them alive; it is courage that splits the acorn and sends the rootlet down into soil to search for sustenance. This is not literally true. Acorns don't have brains so they can't experience courage. But I would still praise the idea of courage as a quality that powers all of human life - the daring to do things that you don't know for certain will work, acting under conditions of uncertainty. Even in an unstable world, not knowing how society might change, how you yourself might change, whether life as we know it will still exist at all in 30 years - even though nobody can foresee a thousand years into the future, even if everything goes right - even so, two or more people can still have sufficient confidence, and hope and courage, to try and build something greater out of the union of their lives. Because why not? If someone is already fortunate enough to have a relationship that once would have been called a marriage blessed by Heaven, why should they receive any less joy, or receive it any later, than they would have had in bygone times? How sad would it be to delay a hundred years and then find out that it would have worked after all?</p>\n<p>And one element of marriage which has not changed is the endeavor to raise children. Not every marriage may desire children, but among those who do desire children, a marriage promises those children a stable home, a lasting family, and at least two people who jointly accept full responsibility for every child. For myself - seeing the meaning of this wedding through my own eyes - I would affirm and support above all else the wholehearted decision of Will and Divia to forge a more lasting bond because they both wished to bring a new child into the world. That responsibility is owed to any endeavor of creating a new sentient life. That meaning of marriage has not changed.</p>\n<p>A final question is what marriage now means to the community that bears witness. William and Divia have chosen to bind their lives together. As it is not our place to deny that, neither is it within our power to permit it. There is no higher authority whose blessings must be sought, and we can't wish them good luck because there's no such thing in the universe as luck. We could say, \"We wish you happy lives as the result of your own decisions!\", but wishing doesn't make anything happen. And yet for as long as marriage has existed among the human species, it has been a ceremony performed within sight of the tribe. For tens of thousands of years before humans imagined that the heavens had authority, the tribe has borne witness to marriages. Of you all, then, I will ask that you promise to respect this marriage, and not come between Will and Divia in any way, should you find that possibility within your power; and those of you present who bear them other friendships may vow such other support as lies within your hearts. And let it be known to all the world that what is begun here today, is done brightly, and without shame.</p>\n</blockquote>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 2, "42JnNJ8fqfdCSqdmM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TTanRyNTvKXgqqv3u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 56, "baseScore": 76, "extendedScore": null, "score": 0.000165, "legacy": true, "legacyId": "17819", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 76, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 77, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-23T20:44:53.927Z", "modifiedAt": null, "url": null, "title": "Evolutionary psychology as \"the truth-killer\"", "slug": "evolutionary-psychology-as-the-truth-killer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:30.178Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benedict", "createdAt": "2012-07-22T12:37:54.443Z", "isAdmin": false, "displayName": "Benedict"}, "userId": "uKhH7qA6itrFuJPnz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XaCrStDecTrMLbsty/evolutionary-psychology-as-the-truth-killer", "pageUrlRelative": "/posts/XaCrStDecTrMLbsty/evolutionary-psychology-as-the-truth-killer", "linkUrl": "https://www.lesswrong.com/posts/XaCrStDecTrMLbsty/evolutionary-psychology-as-the-truth-killer", "postedAtFormatted": "Monday, July 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Evolutionary%20psychology%20as%20%22the%20truth-killer%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEvolutionary%20psychology%20as%20%22the%20truth-killer%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXaCrStDecTrMLbsty%2Fevolutionary-psychology-as-the-truth-killer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Evolutionary%20psychology%20as%20%22the%20truth-killer%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXaCrStDecTrMLbsty%2Fevolutionary-psychology-as-the-truth-killer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXaCrStDecTrMLbsty%2Fevolutionary-psychology-as-the-truth-killer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 507, "htmlBody": "<p>So, a little background- I've just come out as an atheist to my dad, a Christian pastor, who's convinced he can \"fix\" my thinking and is bombarding me with a number of flimsy arguments that I'm having trouble articulating a response to, and need help shutting down. The particular issue at the moment deals with non-theistic explanations for human psychology and things like love, morality, and beauty. After attempting to communicate explanations from evolutionary psychology, I was met with amused dismissal of the subject as \"speculation\".&nbsp;</p>\n<p>There's one book in particular he's having me read- The Reason for God by Timothy Keller. In the book, he brings up evolutionary psychology as an alternative to theistic explanations, and immediately dismisses it as apparently self-defeating.</p>\n<p><em>\"Evolutionists say that if God makes sense to us, it is not because he is really there, it's only because that belief helped us survive and so we are hardwired for it. However, if we can't trust our belief-forming faculties to tell us the truth about God, why should we trust them to tell us the truth about anything, including evolutionary science? If our cognitive faculties only tell us what we need to survive, not what is true, why trust them about anything at all?\" -Timothy Keller</em></p>\n<p>The obvious answer is that knowing the truth about things is generally advantageous to survival- but it hardly addresses the underlying assertion- that without [incredibly specific collection of god-beliefs and assorted dogmas], human brains can't arrive at truth because they weren't designed for it. And of course, I'm talking to a guy with an especially exacting definition of \"truth\" (100% certainty about the territory)- I could use an LW post that&nbsp;succinctly&nbsp;discusses the role and definition of truth, there.&nbsp;</p>\n<p>Another thing Dad likes to do is back me into a corner WRT morality and moral relativism- \"Oh, but can you <em>really</em> believe that the act of rape doesn't have an inherent [wrongness]? Are you saying it was justified for [insert historical monster] to do [atrocity] because it would make him reproductively successful?\" Armed only with evolutionary explanations for their behavior, I couldn't really respond- possibly my fault, since I haven't read the Morality sequence on account of I got stuck in the Quantum Physics ultrasequence, and knowing that reality is composed of complex amplitudes flowing between explicit configurations or aaasasdjgasjdga whatever the frig even (I CAN'T) has proven to be staggeringly unhelpful in this situation.</p>\n<p>In addition to particular arguments WRT the question posed, I could also use recommendations for good, well-argued and accessible books on the subject of evolutionary psychology, with a focus on practical experimental results and application- the guy can't be given a book and not read it, so I'm hoping to at least get him to not dismiss the science as \"speculation\" or a joke. It's likely he's aware that the field evolutionary psychology is really prone to hindsight bias and thus ignores it completely, so along with the book, a good article or study demonstrating the accuracy and predictive power of the evolutionary psychological model would be appreciated.</p>\n<p>Thanks!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XaCrStDecTrMLbsty", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 16, "extendedScore": null, "score": 9.4893771590019e-07, "legacy": true, "legacyId": "17820", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 70, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-24T03:27:07.391Z", "modifiedAt": null, "url": null, "title": "Game Theory As A Dark Art", "slug": "game-theory-as-a-dark-art", "viewCount": null, "lastCommentedAt": "2020-10-08T14:57:30.330Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/A2Qam9Bd9xpbb2wLQ/game-theory-as-a-dark-art", "pageUrlRelative": "/posts/A2Qam9Bd9xpbb2wLQ/game-theory-as-a-dark-art", "linkUrl": "https://www.lesswrong.com/posts/A2Qam9Bd9xpbb2wLQ/game-theory-as-a-dark-art", "postedAtFormatted": "Tuesday, July 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Game%20Theory%20As%20A%20Dark%20Art&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGame%20Theory%20As%20A%20Dark%20Art%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA2Qam9Bd9xpbb2wLQ%2Fgame-theory-as-a-dark-art%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Game%20Theory%20As%20A%20Dark%20Art%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA2Qam9Bd9xpbb2wLQ%2Fgame-theory-as-a-dark-art", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA2Qam9Bd9xpbb2wLQ%2Fgame-theory-as-a-dark-art", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3933, "htmlBody": "<p>One of the most charming features of game theory is the almost limitless depths of evil to which it can sink.<br /><br />Your garden-variety evils act against your values. Your better class of evil, like Voldemort and the folk-tale version of Satan, use your greed to trick <em>you</em> into acting against <em>your own</em> values, then grab away the promised reward at the last moment. But even demons and dark wizards can only do this once or twice before most victims wise up and decide that taking their advice is a bad idea. Game theory can force you to betray your deepest principles for no lasting benefit again and again, and still leave you convinced that your behavior was rational.</p>\n<p>Some of the examples in this post probably wouldn't work in reality; they're more of a <em>reductio ad absurdum</em> of the so-called <em>homo economicus</em> who acts free from any feelings of altruism or trust. But others are lifted directly from real life where seemingly intelligent people genuinely fall for them. And even the ones that don't work with real people might be valuable in modeling institutions or governments.<br /><br />Of the following examples, the first three are from <a href=\"http://www.amazon.com/The-Art-Strategy-Theorists-Business/dp/0393062430\"><em>The Art of Strategy</em></a>; the second three are relatively classic problems taken from around the Internet. A few have been mentioned in the comments here already and are reposted for people who didn't catch them the first time.</p>\n<p><a id=\"more\"></a><br /><strong><br />The Evil Plutocrat</strong><br /><br />You are an evil plutocrat who wants to get your pet bill - let's say a law that makes evil plutocrats tax-exempt - through the US Congress. Your usual strategy would be to bribe the Congressmen involved, but that would be pretty costly - Congressmen no longer come cheap. Assume all Congressmen act in their own financial self-interest, but that absent any financial self-interest they will grudgingly default to honestly representing their constituents, who hate your bill (and you personally). Is there any way to ensure Congress passes your bill, without spending any money on bribes at all?<br /><br />Yes. Simply tell all Congressmen that <em>if</em> your bill fails, you will donate some stupendous amount of money to whichever party gave the greatest percent of their votes in favor.<br /><br />Suppose the Democrats try to coordinate among themselves. They say &ldquo;If we all oppose the bill, then if even one Republican supports the bill, the Republicans will get lots of money they can spend on campaigning against us. If only one of us supports the bill, the Republicans may anticipate this strategy and two of them may support it. The only way to ensure the Republicans don't gain a massive windfall and wipe the floor with us next election is for most of us to vote for the bill.&rdquo;<br /><br />Meanwhile, in their meeting, the Republicans think the same thing. The vote ends with most members of Congress supporting your bill, and you don't end up having to pay any money at all.<br /><br /><strong>The Hostile Takeover</strong><br /><br />You are a ruthless businessman who wants to take over a competitor. The competitor's stock costs $100 a share, and there are 1000 shares, distributed among a hundred investors who each own ten. That means the company ought to cost $100,000, but you don't have $100,000. You only have $98,000. Worse, another competitor with $101,000 has made an offer for greater than the value of the company: they will pay $101 per share if they end up getting all of the shares. Can you still manage to take over the company?<br /><br />Yes. You can make what is called a two-tiered offer. Suppose all investors get a chance to sell shares simultaneously. You will pay $105 for 500 shares - better than they could get from your competitor - but only pay $90 for the other 500. If you get fewer than 500 shares, all will sell for $105; if you get more than 500, you will start by distributing the $105 shares evenly among all investors who sold to you, and then distribute out as many of the $90 shares as necessary (leaving some $90 shares behind except when all investors sell to you) . And you will do this whether or not you succeed in taking over the company - if only one person sells you her share, then that one person gets $105. <br /><br />Suppose an investor believes you're not going to succeed in taking over the company. That means you're not going to get over 50% of shares. That means the offer to buy 500 shares for $105 will still be open. That means the investor can either sell her share to you (for $105) or to your competitor (for $101). Clearly, it's in this investor's self-interest to sell to you.<br /><br />Suppose the investor believes you will succeed in taking over the company. That means your competitor will not take over the company, and its $101 offer will not apply. That means that the new value of the shares will be $90, the offer you've made for the second half of shares. So they will get $90 if they don't sell to you. How much will they get if they do sell to you? They can expect half of their ten shares to go for $105 and half to go for $90; they will get a total of $97.50 per share. $97.50 is better than $90, so their incentive is to sell to you. <br /><br />Suppose the investor believes you are right on the cusp of taking over the company, and her decision will determine the outcome. In that case, you have at most 499 shares. When the investor gives you her 10 shares, you will end up with 509 - 500 of which are $105 shares and 9 of which are $90 shares. If these are distributed randomly, investors can expect to make on average $104.73 per share, compared to $101 if your competitor buys the company.<br /><br />Since all investors are thinking along these lines, they all choose to buy shares from you instead of your competitor. You pay out an average of $97.50 per share, and take over the company for $97,500, leaving $500 to spend on the victory party.<br /><br />The stockholders, meanwhile, are left wondering why they just all sold shares for $97.50 when there was someone else who was promising them $101.</p>\n<p><strong>The Hostile Takeover, Part II</strong></p>\n<p>Your next target is a small family-owned corporation that has instituted what they consider to be invincible protection against hostile takeovers. All decisions are made by the Board of Directors, who serve for life. Although shareholders vote in the new members of the Board after one of them dies or retires, Board members can hang on for decades. And all decisions about the Board, impeachment of its members, and enforcement of its bylaws are made by the Board itself, with members voting from newest to most senior.<br /><br />So you go about buying up 51% of the stock in the company, and sure enough, a Board member retires and is replaced by one of your lackeys. This lackey can propose procedural changes to the Board, but they have to be approved by majority vote. And at the moment the other four directors hate you with a vengeance, and anything you propose is likely to be defeated 4-1. You need those other four windbags out of there, and soon, but they're all young and healthy and unlikely to retire of their own accord.<br /><br />The obvious next step is to start looking for a good assassin. But if you can't find one, is there any way you can propose mass forced retirement to the Board and get them to approve it by majority vote? Even better, is there any way you can get them to approve it unanimously, as a big &ldquo;f#@&amp; you&rdquo; to whoever made up this stupid system?<br /><br />Yes. Your lackey proposes as follows: &ldquo;I move that we vote upon the following: that if this motion passes unanimously, all members of the of the Board resign immediately and are given a reasonable compensation; that if this motion passes 4-1 that the Director who voted against it must retire without compensation, and the four directors who voted in favor may stay on the Board; and that if the motion passes 3-2, then the two 'no' voters get no compensation and the three 'yes' voters may remain on the board and will also get a spectacular prize - to wit, our company's 51% share in your company divided up evenly among them.&rdquo;<br /><br />Your lackey then votes &ldquo;yes&rdquo;. The second newest director uses backward reasoning as follows: <br /><br />Suppose that the vote were tied 2-2. The most senior director would prefer to vote &ldquo;yes&rdquo;, because then she gets to stay on the Board and gets a bunch of free stocks. <br /><br />But knowing that, the second most senior director (SMSD) will also vote 'yes'. After all, when the issue reaches the SMSD, there will be one of the following cases:</p>\n<p>1.&nbsp; If there is only one yes vote (your lackey's), the SMSD stands to gain from voting yes, knowing that will produce a 2-2 tie and make the most senior director vote yes to get her spectacular compensation. This means the motion will pass 3-2, and the SMSD will also remain on the board and get spectacular compensation if she votes yes, compared to a best case scenario of remaining on the board if she votes no.</p>\n<p>2. If there are two yes votes, the SMSD must vote yes - otherwise, it will go 2-2 to the most senior director, who will vote yes, the motion will pass 3-2, and the SMSD will be forced to retire without compensation.</p>\n<p>3. And if there are three yes votes, then the motion has already passed, and in all cases where the second most senior director votes &ldquo;no&rdquo;, she is forced to retire without compensation. Therefore, the second most senior director will always vote &ldquo;yes&rdquo;.</p>\n<p>Since your lackey, the most senior director, and the second most senior director will always vote \"yes\", we can see that the other two directors, knowing the motion will pass, must vote \"yes\" as well in order to get any compensation at all. Therefore, the motion passes unanimously and you take over the company at minimal cost.</p>\n<p><strong>The Dollar Auction</strong><br /><br />You are an economics professor who forgot to go to the ATM before leaving for work, and who has only $20 in your pocket. You have a lunch meeting at a very expensive French restaurant, but you're stuck teaching classes until lunchtime and have no way to get money. Can you trick your students into giving you enough money for lunch in exchange for your $20, without lying to them in any way?<br /><br />Yes. You can use what's called an all-pay auction, in which several people bid for an item, as in a traditional auction, but everyone pays their bid regardless of whether they win or lose (in a common variant, only the top two bidders pay their bids).<br /><br />Suppose one student, Alice, bids $1. This seems reasonable - paying $1 to win $20 is a pretty good deal. A second student, Bob, bids $2. Still a good deal if you can get a twenty for a tenth that amount.<br /><br />The bidding keeps going higher, spurred on by the knowledge that getting a $20 for a bid of less than $20 would be pretty cool. At some point, maybe Alice has bid $18 and Bob has bid $19.<br /><br />Alice thinks: &ldquo;What if I raise my bid to $20? Then certainly I would win, since Bob would not pay more than $20 to get $20, but I would only break even. However, breaking even is better than what I'm doing now, since if I stay where I am Bob wins the auction and I pay $18 without getting anything.&rdquo; Therefore Alice bids $20.<br /><br />Bob thinks &ldquo;Well, it sounds pretty silly to bid $21 for a twenty dollar bill. But if I do that and win, I only lose a dollar, as opposed to bowing out now and losing my $19 bid.&rdquo; So Bob bids $21.<br /><br />Alice thinks &ldquo;If I give up now, I'll lose a whole dollar. I know it seems stupid to keep going, but surely Bob has the same intuition and he'll give up soon. So I'll bid $22 and just lose two dollars...&rdquo;<br /><br />It's easy to see that the bidding could in theory go up with no limits but the players' funds, but in practice it rarely goes above $200.<br /><br />...yes, $200. Economist Max Bazerman claims that of about 180 such auctions, <a href=\"http://mfinley.com/experts/bazerman/bazerman.htm\">seven have made him more than $100</a> (ie $50 from both players) and <a href=\"http://ingrimayne.com/econ/info_risk/NastyAuction.html\">his highest take was $407</a> (ie over $200 from both players).<br /><br />In any case, you're probably set for lunch. If you're not, take another $20 from your earnings and try again until you are - the auction gains even <a href=\"http://www.abe.sju.edu/proc2007/Abalyeat.pdf\">more money from people who have seen it before</a> than it does from naive bidders (!) Bazerman, for his part, says he's made a total of $17,000 from the exercise.<br /><br />At that point you're starting to wonder why no one has tried to build a corporation around this, and unsurprisingly, the online auction site Swoopo <a href=\"http://notapottedplant.blogspot.com/2008/12/swoopo-its-brilliant-and-evil-and-gives.html\">appears to be exactly that</a>. More surprisingly, they seem to have gone bankrupt last year, suggesting that maybe H.L. Mencken was wrong and someone <em>has</em> gone broke underestimating people's intelligence.<br /><br /><strong>The Bloodthirsty Pirates</strong><br /><br />You are a pirate captain who has just stolen $17,000, denominated entirely in $20 bills, from a very smug-looking game theorist. By the Pirate Code, you as the captain may choose how the treasure gets distributed among your men. But your first mate, second mate, third mate, and fourth mate all want a share of the treasure, and demand on threat of mutiny the right to approve or reject any distribution you choose.You expect they'll reject anything too lopsided in your favor, which is too bad, because that was totally what you were planning on.<br /><br />You remember one fact that might help you - your crew, being bloodthirsty pirates, all hate each other and actively want one another dead. Unfortunately, their greed seems to have overcome their bloodlust for the moment, and as long as there are advantages to coordinating with one another, you won't be able to turn them against their fellow sailors. Doubly unfortunately, they also actively want you dead.<br /><br />You think quick. &ldquo;Aye,&rdquo; you tell your men with a scowl that could turn blood to ice, &ldquo;ye can have yer votin' system, ye scurvy dogs&rdquo; (you're that kind of pirate). &ldquo;But here's the rules: I propose a distribution. Then you all vote on whether or not to take it. If a majority of you, or even half of you, vote 'yes', then that's how we distribute the treasure. But if you vote 'no', then I walk the plank to punish me for my presumption, and the first mate is the new captain. He proposes a new distribution, and again you vote on it, and if you accept then that's final, and if you reject it he walks the plank and the second mate becomes the new captain. And so on.&rdquo;<br /><br />Your four mates agree to this proposal. What distribution should you propose? Will it be enough to ensure your comfortable retirement in Jamaica full of rum and wenches?<br /><br />Yes. Surprisingly, you can get away with proposing that you get $16,960, your first mate gets nothing, your second mate gets $20, your third mate gets nothing, and your fourth mate gets $20 - and you will still win 3 -2.<br /><br />The fourth mate uses backward reasoning like so: Suppose there were only two pirates left, me and the third mate. The third mate wouldn't have to promise me anything, because if he proposed all $17,000 for himself and none for me, the vote would be 1-1 and according to the original rules a tie passes. Therefore this is a better deal than I would get if it were just me and the third mate.<br /><br />But suppose there were three pirates left, me, the third mate, and the second mate. Then the second mate would be the new captain, and he could propose $16,980 for himself, $0 for the third mate, and $20 for me. If I vote no, then it reduces to the previous case in which I get nothing. Therefore, I should vote yes and get $20. Therefore, the final vote is 2-1 in favor.<br /><br />But suppose there were four pirates left: me, the third mate, the second mate, and the first mate. Then the first mate would be the new captain, and he could propose $16,980 for himself, $20 for the third mate, $0 for the second mate, and $0 for me. The third mate knows that if he votes no, this reduces to the previous case, in which he gets nothing. Therefore, he should vote yes and get $20. Therefore, the final vote is 2-2, and ties pass.<br /><br />(He might also propose $16980 for himself, $0 for the second mate, $0 for the third mate, and $20 for me. But since he knows I am a bloodthirsty pirate who all else being equal wants him dead, I would vote no since I could get a similar deal from the third mate and make the first mate walk the plank in the bargain. Therefore, he would offer the $20 to the third mate.)<br /><br />But in fact there are five pirates left: me, the third mate, the second mate, the first mate, and the captain. The captain has proposed $16,960 for himself, $20 for the second mate, and $20 for me. If I vote no, this reduces to the previous case, in which I get nothing. Therefore, I should vote yes and get $20.<br /><br />(The captain would avoid giving the $20s to the third and fourth rather than to the second and fourth mates for a similar reason to the one given in the previous example - all else being equal, the pirates would prefer to watch him die.)<br /><br />The second mate thinks along the same lines and realizes that if he votes no, this reduces to the case with the first mate, in which the second mate also gets nothing. Therefore, he too votes yes.<br /><br />Since you, as the captain, obviously vote yes as well, the distribution passes 3-2. You end up with $16,980, and your crew, who were so certain of their ability to threaten you into sharing the treasure, each end up with either a single $20 or nothing.<br /><br /><strong>The Prisoners' Dilemma, Redux</strong><br /><br />This sequence previously mentioned the popularity of Prisoners' Dilemmas as gimmicks on TV game shows. In one program, Golden Balls, contestants do various tasks that add money to a central &ldquo;pot&rdquo;. By the end of the game, only two contestants are left, and are offered a Prisoners' Dilemma situation to split the pot between them. If both players choose to &ldquo;Split&rdquo;, the pot is divided 50-50. If one player &ldquo;Splits&rdquo; and the other player &ldquo;Steals&rdquo;, the stealer gets the entire pot. If both players choose to &ldquo;Steal&rdquo;, then no one gets anything. The two players are allowed to talk to each other before making a decision, but like all Prisoner's Dilemmas, the final choice is made simultaneously and in secret.<br /><br />You are a contestant on this show. You are actually not all that evil - you would prefer to split the pot rather than to steal all of it for yourself - but you certainly don't want to trust the other guy to have the same preference. In fact, the other guy looks a bit greedy. You would prefer to be able to rely on the other guy's rational self-interest rather than on his altruism. Is there any tactic you can use before the choice, when you're allowed to communicate freely, in order to make it rational for him to cooperate?<br /><br />Yes. In <a href=\"http://www.schneier.com/blog/archives/2012/04/amazing_round_o.html#c746020\">one episode</a> of Golden Balls, a player named Nick successfully meta-games the game by transforming it from the Prisoner's Dilemma (where defection is rational) to the Ultimatum Game (where cooperation is rational)<br /><br />Nick tells his opponent: &ldquo;I am going to choose 'Steal' on this round.&rdquo; (He then immediately pressed his button; although the show hid which button he pressed, he only needed to demonstrate that he had committed and his mind could no longer be changed) &ldquo;If you also choose 'Steal', then for certain neither of us gets any money. If you choose 'Split', then I get all the money, but immediately after the game, I will give you half of it. You may not trust me on this, and that's understandable, but think it through. First, there's no less reason to think I'm trustworthy than if I had just told you I pressed 'Split' to begin with, the way everyone else on this show does. And second, now if there's any chance whatsoever that I'm trustworthy, then that's some chance of getting the money - as opposed to the zero chance you have of getting the money if you choose 'Steal'.&rdquo;<br /><br />Nick's evaluation is correct. His opponent can either press 'Steal', with a certainty of getting zero, or press 'Split', with a nonzero probability of getting his half of the pot depending on Nick's trustworthiness. <br /><br />But this solution is not quite perfect, in that one can imagine Nick's opponent being very convinced that Nick will cheat him, and deciding he values punishing this defection more than the tiny chance that Nick will play fair. That's why I was so impressed to see cousin_it propose what I think is <a href=\"/lw/bwu/prisoners_dilemma_on_game_show_golden_balls/\">an even better solution</a> on the Less Wrong thread on the matter:</p>\n<blockquote>\n<p>This game has multiple Nash equilibria and cheap talk is allowed, so correlated equilibria are possible. Here's how you implement a correlated equilibrium if your opponent is smart enough:<br /><br />\"We have two minutes to talk, right? I'm going to ask you to flip a coin (visibly to both of us) at the last possible moment, the exact second where we must cease talking. If the coin comes up heads, I promise I'll cooperate, you can just go ahead and claim the whole prize. If the coin comes up tails, I promise I'll defect. Please cooperate in this case, because you have nothing to gain by defecting, and anyway the arrangement is fair, isn't it?\"</p>\n</blockquote>\n<p>This sort of clever thinking is, in my opinion, the best that game theory has to offer. It shows that game theory need not be only a tool of evil for classical figures of villainy like bloodthirsty pirate captains or corporate raiders or economists, but can also be used to create trust and ensure cooperation between parties with common interests.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 3, "XYHzLjwYiqpeqaf4c": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "A2Qam9Bd9xpbb2wLQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 114, "baseScore": 128, "extendedScore": null, "score": 0.000272, "legacy": true, "legacyId": "17829", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 128, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 103, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["owJCtureTddLFTNkj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-24T03:45:22.457Z", "modifiedAt": null, "url": null, "title": "Meetup : Madison: Team Problem-Solving", "slug": "meetup-madison-team-problem-solving", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:56.619Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oG65j6M4gYs7WD2k7/meetup-madison-team-problem-solving", "pageUrlRelative": "/posts/oG65j6M4gYs7WD2k7/meetup-madison-team-problem-solving", "linkUrl": "https://www.lesswrong.com/posts/oG65j6M4gYs7WD2k7/meetup-madison-team-problem-solving", "postedAtFormatted": "Tuesday, July 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Madison%3A%20Team%20Problem-Solving&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Madison%3A%20Team%20Problem-Solving%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoG65j6M4gYs7WD2k7%2Fmeetup-madison-team-problem-solving%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Madison%3A%20Team%20Problem-Solving%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoG65j6M4gYs7WD2k7%2Fmeetup-madison-team-problem-solving", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoG65j6M4gYs7WD2k7%2Fmeetup-madison-team-problem-solving", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 108, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/c5'>Madison: Team Problem-Solving</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 July 2012 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">650 State Street, Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I'll run a handful of problem-solving games for small teams, each of which should take between 5 to 30 minutes. I'll start the first of these games at 7:30 sharp. Hopefully, this will offer a chance to practice some of the practical skills we've discussed in the past few months.</p>\n\n<p>Chatting and coffee, before and after these events.</p>\n\n<p>I'm excited about this. I've been bouncing it around in my head for at least a month now. I think this might be <em>awesome</em>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/c5'>Madison: Team Problem-Solving</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oG65j6M4gYs7WD2k7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 9.491390687148256e-07, "legacy": true, "legacyId": "17830", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Madison__Team_Problem_Solving\">Discussion article for the meetup : <a href=\"/meetups/c5\">Madison: Team Problem-Solving</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 July 2012 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">650 State Street, Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I'll run a handful of problem-solving games for small teams, each of which should take between 5 to 30 minutes. I'll start the first of these games at 7:30 sharp. Hopefully, this will offer a chance to practice some of the practical skills we've discussed in the past few months.</p>\n\n<p>Chatting and coffee, before and after these events.</p>\n\n<p>I'm excited about this. I've been bouncing it around in my head for at least a month now. I think this might be <em>awesome</em>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Madison__Team_Problem_Solving1\">Discussion article for the meetup : <a href=\"/meetups/c5\">Madison: Team Problem-Solving</a></h2>", "sections": [{"title": "Discussion article for the meetup : Madison: Team Problem-Solving", "anchor": "Discussion_article_for_the_meetup___Madison__Team_Problem_Solving", "level": 1}, {"title": "Discussion article for the meetup : Madison: Team Problem-Solving", "anchor": "Discussion_article_for_the_meetup___Madison__Team_Problem_Solving1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-24T04:47:00.643Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Morality as Fixed Computation", "slug": "seq-rerun-morality-as-fixed-computation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:54.023Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HAYM4naACRAukTvRs/seq-rerun-morality-as-fixed-computation", "pageUrlRelative": "/posts/HAYM4naACRAukTvRs/seq-rerun-morality-as-fixed-computation", "linkUrl": "https://www.lesswrong.com/posts/HAYM4naACRAukTvRs/seq-rerun-morality-as-fixed-computation", "postedAtFormatted": "Tuesday, July 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Morality%20as%20Fixed%20Computation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Morality%20as%20Fixed%20Computation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHAYM4naACRAukTvRs%2Fseq-rerun-morality-as-fixed-computation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Morality%20as%20Fixed%20Computation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHAYM4naACRAukTvRs%2Fseq-rerun-morality-as-fixed-computation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHAYM4naACRAukTvRs%2Fseq-rerun-morality-as-fixed-computation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 148, "htmlBody": "<p>Today's post, <a href=\"/lw/sw/morality_as_fixed_computation/\">Morality as Fixed Computation</a> was originally published on 08 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Morality_as_Fixed_Computation\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A clarification about Yudkowsky's metaethics.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dql/seq_rerun_contaminated_by_optimism/\">Contaminated by Optimism</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HAYM4naACRAukTvRs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.491685908642877e-07, "legacy": true, "legacyId": "17831", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FnJPa8E9ZG5xiLLp5", "FfHwLcQkN5xgDDrEY", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-24T08:49:25.064Z", "modifiedAt": null, "url": null, "title": "Group rationality diary, 7/23/12", "slug": "group-rationality-diary-7-23-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:58.316Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cata", "createdAt": "2010-06-02T18:13:22.408Z", "isAdmin": false, "displayName": "cata"}, "userId": "X9jdpCokhLjCMZEc3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bfqpD7ZLq2FCBYfYC/group-rationality-diary-7-23-12", "pageUrlRelative": "/posts/bfqpD7ZLq2FCBYfYC/group-rationality-diary-7-23-12", "linkUrl": "https://www.lesswrong.com/posts/bfqpD7ZLq2FCBYfYC/group-rationality-diary-7-23-12", "postedAtFormatted": "Tuesday, July 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20rationality%20diary%2C%207%2F23%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20rationality%20diary%2C%207%2F23%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbfqpD7ZLq2FCBYfYC%2Fgroup-rationality-diary-7-23-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20rationality%20diary%2C%207%2F23%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbfqpD7ZLq2FCBYfYC%2Fgroup-rationality-diary-7-23-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbfqpD7ZLq2FCBYfYC%2Fgroup-rationality-diary-7-23-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 195, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is the public group instrumental rationality diary for the week of July 23rd. It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">Thanks to everyone who contributes!</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \"><a href=\"/lw/diy/group_rationality_diary_7912/\">Last week's diary</a>; <a href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">archive of prior diaries</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bfqpD7ZLq2FCBYfYC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 9.492847128506609e-07, "legacy": true, "legacyId": "17839", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["as8784LJq7cMfxqoY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-24T15:04:13.938Z", "modifiedAt": null, "url": null, "title": "[Paper] Simulation of a complete cell", "slug": "paper-simulation-of-a-complete-cell", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:55.760Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FjwgF64d2kKMgCJRw/paper-simulation-of-a-complete-cell", "pageUrlRelative": "/posts/FjwgF64d2kKMgCJRw/paper-simulation-of-a-complete-cell", "linkUrl": "https://www.lesswrong.com/posts/FjwgF64d2kKMgCJRw/paper-simulation-of-a-complete-cell", "postedAtFormatted": "Tuesday, July 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BPaper%5D%20Simulation%20of%20a%20complete%20cell&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BPaper%5D%20Simulation%20of%20a%20complete%20cell%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFjwgF64d2kKMgCJRw%2Fpaper-simulation-of-a-complete-cell%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BPaper%5D%20Simulation%20of%20a%20complete%20cell%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFjwgF64d2kKMgCJRw%2Fpaper-simulation-of-a-complete-cell", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFjwgF64d2kKMgCJRw%2Fpaper-simulation-of-a-complete-cell", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 139, "htmlBody": "<p><a href=\"http://dx.doi.org/10.1016/j.cell.2012.05.044\">\"A Whole-Cell Computational Model Predicts Phenotype from Genotype\"</a>&nbsp;by Jonathan Karr et al.</p>\n<p>This paper appeared a few days ago in Cell, and describes a computational simulation of the bacterium&nbsp;<em>Mycoplasma genitalium</em>, conducted at <a href=\"http://covertlab.stanford.edu/subpages/research.php\">this lab</a>. The paper is behind a paywall, but is blogged about <a href=\"http://egtheory.wordpress.com/2012/07/23/whole-cell-model/\">here</a>. The simulation software is freely available from the <a href=\"https://simtk.org/home/wholecell/\">project web site.</a></p>\n<p>From the abstract: \"Here, we present a &lsquo;&lsquo;whole-cell&rsquo;&rsquo; model of the bacterium <em>Mycoplasma genitalium</em>, a human urogenital parasite whose genome contains 525 genes. Our model attempts to: (1) describe the life cycle of a single cell from the level of individual molecules and their interactions; (2) account for the specific function of every annotated gene product; and (3) accurately predict a wide range of observable cellular behaviors.\"</p>\n<p>According to an <a href=\"http://dx.doi.org/10.1016/j.cell.2012.07.001\">editorial commentary</a> in the same issue, this is the first simulation of a complete free-living microbe.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FjwgF64d2kKMgCJRw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 27, "extendedScore": null, "score": 9.494643126128089e-07, "legacy": true, "legacyId": "17840", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-25T05:50:53.138Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley meetup: Rationalist group therapy", "slug": "meetup-berkeley-meetup-rationalist-group-therapy", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y3WazPMQkL8TAGzna/meetup-berkeley-meetup-rationalist-group-therapy", "pageUrlRelative": "/posts/y3WazPMQkL8TAGzna/meetup-berkeley-meetup-rationalist-group-therapy", "linkUrl": "https://www.lesswrong.com/posts/y3WazPMQkL8TAGzna/meetup-berkeley-meetup-rationalist-group-therapy", "postedAtFormatted": "Wednesday, July 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%20meetup%3A%20Rationalist%20group%20therapy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%20meetup%3A%20Rationalist%20group%20therapy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy3WazPMQkL8TAGzna%2Fmeetup-berkeley-meetup-rationalist-group-therapy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%20meetup%3A%20Rationalist%20group%20therapy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy3WazPMQkL8TAGzna%2Fmeetup-berkeley-meetup-rationalist-group-therapy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy3WazPMQkL8TAGzna%2Fmeetup-berkeley-meetup-rationalist-group-therapy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 269, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/c6\">Berkeley meetup: Rationalist group therapy</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">25 July 2012 07:00:00PM (-0700)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Berkeley, CA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>The Berkeley meetup tomorrow (Wednesday) will feature an activity that I hear is popular with the New York meetup. The way it works is this: One person shares with the group a problem they're having in their life, or a goal they want to achieve &mdash; it could be about their career, their family life, their romantic life, or health; it could be a habit they want to form or to break, or something bigger about themselves that they want to change. Then the group discusses the problem and offers insight and advice.</p>\n<p>I did this with the Mountain View meetup and it was great! It's so nice to be able to get advice from a room of particularly sane people.</p>\n<p>The most important thing is to listen to the individual's problem non-judgmentally. We don't want to punish each other for talking openly about our problems, and a normative reaction like \"You're perpetually late to meetings? Ugh, I hate people who do that\" is a strong enough punishment to thwart the whole activity.</p>\n<p>We'll be able to put a handful of people in the hotseat, one at a time, until we get tired and hang out for the rest of the meetup. I won't be there this time but Shannon will be around, and I'll make sure someone is in charge of getting the meetup started.</p>\n<p>The meetup will be at Zendo. Doors open at 7pm and Rationalist Group Therapy begins at 7:30pm. For directions to Zendo, see the <a href=\"http://groups.google.com/group/bayarealesswrong\">mailing list</a> or call Nisan at <img style=\"vertical-align: middle;\" src=\"http://i.imgur.com/Vcafy.png\" alt=\"\" width=\"83\" height=\"18\" />.</p>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y3WazPMQkL8TAGzna", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 9.498894092684224e-07, "legacy": true, "legacyId": "17851", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-25T05:51:15.278Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley meetup: Rationalist group therapy", "slug": "meetup-berkeley-meetup-rationalist-group-therapy-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/asW8jMBqg6Wr6xwCo/meetup-berkeley-meetup-rationalist-group-therapy-0", "pageUrlRelative": "/posts/asW8jMBqg6Wr6xwCo/meetup-berkeley-meetup-rationalist-group-therapy-0", "linkUrl": "https://www.lesswrong.com/posts/asW8jMBqg6Wr6xwCo/meetup-berkeley-meetup-rationalist-group-therapy-0", "postedAtFormatted": "Wednesday, July 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%20meetup%3A%20Rationalist%20group%20therapy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%20meetup%3A%20Rationalist%20group%20therapy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FasW8jMBqg6Wr6xwCo%2Fmeetup-berkeley-meetup-rationalist-group-therapy-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%20meetup%3A%20Rationalist%20group%20therapy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FasW8jMBqg6Wr6xwCo%2Fmeetup-berkeley-meetup-rationalist-group-therapy-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FasW8jMBqg6Wr6xwCo%2Fmeetup-berkeley-meetup-rationalist-group-therapy-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 268, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/c7'>Berkeley meetup: Rationalist group therapy</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 July 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Berkeley meetup tomorrow (Wednesday) will feature an activity that I hear is popular with the New York meetup. The way it works is this: One person shares with the group a problem they're having in their life, or a goal they want to achieve \u2014 it could be about their career, their family life, their romantic life, or health; it could be a habit they want to form or to break, or something bigger about themselves that they want to change. Then the group discusses the problem and offers insight and advice.</p>\n\n<p>I did this with the Mountain View meetup and it was great! It's so nice to be able to get advice from a room of particularly sane people.</p>\n\n<p>The most important thing is to listen to the individual's problem non-judgmentally. We don't want to punish each other for talking openly about our problems, and a normative reaction like \"You're perpetually late to meetings? Ugh, I hate people who do that\" is a strong enough punishment to thwart the whole activity.</p>\n\n<p>We'll be able to put a handful of people in the hotseat, one at a time, until we get tired and hang out for the rest of the meetup. I won't be there this time but Shannon will be around, and I'll make sure someone is in charge of getting the meetup started.</p>\n\n<p>The meetup will be at Zendo. Doors open at 7pm and Rationalist Group Therapy begins at 7:30pm.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/c7'>Berkeley meetup: Rationalist group therapy</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "asW8jMBqg6Wr6xwCo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "17852", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley_meetup__Rationalist_group_therapy\">Discussion article for the meetup : <a href=\"/meetups/c7\">Berkeley meetup: Rationalist group therapy</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 July 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Berkeley meetup tomorrow (Wednesday) will feature an activity that I hear is popular with the New York meetup. The way it works is this: One person shares with the group a problem they're having in their life, or a goal they want to achieve \u2014 it could be about their career, their family life, their romantic life, or health; it could be a habit they want to form or to break, or something bigger about themselves that they want to change. Then the group discusses the problem and offers insight and advice.</p>\n\n<p>I did this with the Mountain View meetup and it was great! It's so nice to be able to get advice from a room of particularly sane people.</p>\n\n<p>The most important thing is to listen to the individual's problem non-judgmentally. We don't want to punish each other for talking openly about our problems, and a normative reaction like \"You're perpetually late to meetings? Ugh, I hate people who do that\" is a strong enough punishment to thwart the whole activity.</p>\n\n<p>We'll be able to put a handful of people in the hotseat, one at a time, until we get tired and hang out for the rest of the meetup. I won't be there this time but Shannon will be around, and I'll make sure someone is in charge of getting the meetup started.</p>\n\n<p>The meetup will be at Zendo. Doors open at 7pm and Rationalist Group Therapy begins at 7:30pm.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley_meetup__Rationalist_group_therapy1\">Discussion article for the meetup : <a href=\"/meetups/c7\">Berkeley meetup: Rationalist group therapy</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley meetup: Rationalist group therapy", "anchor": "Discussion_article_for_the_meetup___Berkeley_meetup__Rationalist_group_therapy", "level": 1}, {"title": "Discussion article for the meetup : Berkeley meetup: Rationalist group therapy", "anchor": "Discussion_article_for_the_meetup___Berkeley_meetup__Rationalist_group_therapy1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-25T06:16:17.180Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Inseparably Right; or, Joy in the Merely Good", "slug": "seq-rerun-inseparably-right-or-joy-in-the-merely-good", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ToajTxpWSKNgPRx5v/seq-rerun-inseparably-right-or-joy-in-the-merely-good", "pageUrlRelative": "/posts/ToajTxpWSKNgPRx5v/seq-rerun-inseparably-right-or-joy-in-the-merely-good", "linkUrl": "https://www.lesswrong.com/posts/ToajTxpWSKNgPRx5v/seq-rerun-inseparably-right-or-joy-in-the-merely-good", "postedAtFormatted": "Wednesday, July 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Inseparably%20Right%3B%20or%2C%20Joy%20in%20the%20Merely%20Good&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Inseparably%20Right%3B%20or%2C%20Joy%20in%20the%20Merely%20Good%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FToajTxpWSKNgPRx5v%2Fseq-rerun-inseparably-right-or-joy-in-the-merely-good%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Inseparably%20Right%3B%20or%2C%20Joy%20in%20the%20Merely%20Good%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FToajTxpWSKNgPRx5v%2Fseq-rerun-inseparably-right-or-joy-in-the-merely-good", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FToajTxpWSKNgPRx5v%2Fseq-rerun-inseparably-right-or-joy-in-the-merely-good", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<p>Today's post, <a href=\"/lw/sx/inseparably_right_or_joy_in_the_merely_good/\">Inseparably Right; or, Joy in the Merely Good</a> was originally published on 09 August 2008 .  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Inseparably_Right.3B_or.2C_Joy_in_the_Merely_Good\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Don't go looking for some pure essence of goodness, distinct from, you know, actual good.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/drb/seq_rerun_morality_as_fixed_computation/\">Morality as Fixed Computation</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ToajTxpWSKNgPRx5v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.499015923255149e-07, "legacy": true, "legacyId": "17853", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JynJ6xfnpq9oN3zpb", "HAYM4naACRAukTvRs", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-25T07:22:17.541Z", "modifiedAt": null, "url": null, "title": "Max Autonomy", "slug": "max-autonomy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:57.183Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blogospheroid", "createdAt": "2009-03-17T08:11:01.816Z", "isAdmin": false, "displayName": "blogospheroid"}, "userId": "dgscyYwrDh3u7dE7h", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7TKyy6p4pnix6kx5T/max-autonomy", "pageUrlRelative": "/posts/7TKyy6p4pnix6kx5T/max-autonomy", "linkUrl": "https://www.lesswrong.com/posts/7TKyy6p4pnix6kx5T/max-autonomy", "postedAtFormatted": "Wednesday, July 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Max%20Autonomy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMax%20Autonomy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7TKyy6p4pnix6kx5T%2Fmax-autonomy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Max%20Autonomy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7TKyy6p4pnix6kx5T%2Fmax-autonomy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7TKyy6p4pnix6kx5T%2Fmax-autonomy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 195, "htmlBody": "<p>I would like to raise a discussion topic in the spirit of trying to quantify risk from uncontrolled / unsupervised software.</p>\n<p>What is the maximum autonomy that has been granted to an algorithm according to your best estimates? What is the likely trend in the future?</p>\n<p>The estimates could be in terms of money, human lives, processes, etc.</p>\n<p>Another estimate could be on the time it takes for a human to come in the process and say \"This isn't right\".&nbsp;</p>\n<p>A high speed trading algorithm has a lot of money on the line, but a drone might have lives on the line.&nbsp;</p>\n<p>A lot of business processes might get affected by data coming in via an API from a system that might have had slightly different assumptions resulting in catastrophic events. eg. &nbsp;<a href=\"http://en.wikipedia.org/wiki/2010_Flash_Crash\">http://en.wikipedia.org/wiki/2010_Flash_Crash</a></p>\n<p>The reason this topic might be worth researching is that it is a relatively easy to communicate risk of AGI. There might be many people who have an implicit assumption that whatever software is being deployed in the real world, there are humans to counter balance it. For them, empirical evidence that they are mistaken about the autonomy given to present day software may shift beliefs.&nbsp;</p>\n<p>EDIT : formatting</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7TKyy6p4pnix6kx5T", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 4, "extendedScore": null, "score": 9.499332523748724e-07, "legacy": true, "legacyId": "17855", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-25T15:23:43.470Z", "modifiedAt": null, "url": null, "title": "Leaps of faith in college selection", "slug": "leaps-of-faith-in-college-selection", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:57.063Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "tomme", "createdAt": "2012-03-14T19:51:35.247Z", "isAdmin": false, "displayName": "tomme"}, "userId": "uyGXufxNcB7WRQ63C", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/H6QeyRrkYGjrHsnRh/leaps-of-faith-in-college-selection", "pageUrlRelative": "/posts/H6QeyRrkYGjrHsnRh/leaps-of-faith-in-college-selection", "linkUrl": "https://www.lesswrong.com/posts/H6QeyRrkYGjrHsnRh/leaps-of-faith-in-college-selection", "postedAtFormatted": "Wednesday, July 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Leaps%20of%20faith%20in%20college%20selection&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALeaps%20of%20faith%20in%20college%20selection%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH6QeyRrkYGjrHsnRh%2Fleaps-of-faith-in-college-selection%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Leaps%20of%20faith%20in%20college%20selection%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH6QeyRrkYGjrHsnRh%2Fleaps-of-faith-in-college-selection", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH6QeyRrkYGjrHsnRh%2Fleaps-of-faith-in-college-selection", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>Since this fall I will be applying to college in the USA, I have compiled a hefty list of colleges based on the following criteria:</p>\n<p>-4-year school;</p>\n<p>-co-ed or all men;</p>\n<p>-Biology major;</p>\n<p>-\"full-ride\" financial aid available.</p>\n<p>The problem's that I have quite a lot of choices, hundreds, as a matter of fact. So how should I narrow down my list even further, given that I don't care about other stuff, such as campus size or location?</p>\n<p>Moreover, to how many colleges should I apply? As far as I know, mpst people apply to 6-9 colleges, but some even apply to 20! I guess that by applying to as many colleges possible, my chances of admission go up. But, I probably won't have time to write hundreds of admission essays, or the money to send in my application to all these colleges.</p>\n<p>Lastly, as my objective is to gain admission somewhere, should I only apply to colleges with acceptance rates above a certain percentage? What should that percentage be?</p>\n<p>If anyone would like to take this in private, I'd be more than happy to receive some advice from any member of the community!</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "H6QeyRrkYGjrHsnRh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 5, "extendedScore": null, "score": 9.501642299823776e-07, "legacy": true, "legacyId": "17857", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-25T16:21:25.728Z", "modifiedAt": null, "url": null, "title": "A Description of Entropy", "slug": "a-description-of-entropy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:39.432Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OrphanWilde", "createdAt": "2012-06-21T18:24:36.749Z", "isAdmin": false, "displayName": "OrphanWilde"}, "userId": "cdW87AS6fdK2sywL2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K2CEyGY5FmF9MpRTj/a-description-of-entropy", "pageUrlRelative": "/posts/K2CEyGY5FmF9MpRTj/a-description-of-entropy", "linkUrl": "https://www.lesswrong.com/posts/K2CEyGY5FmF9MpRTj/a-description-of-entropy", "postedAtFormatted": "Wednesday, July 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Description%20of%20Entropy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Description%20of%20Entropy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK2CEyGY5FmF9MpRTj%2Fa-description-of-entropy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Description%20of%20Entropy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK2CEyGY5FmF9MpRTj%2Fa-description-of-entropy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK2CEyGY5FmF9MpRTj%2Fa-description-of-entropy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1666, "htmlBody": "<p>[Edited continually.]</p>\n<p>Abstract: This is a layman's description of entropy as understood in thermodynamics. &nbsp;There is nothing revolutionary in here. &nbsp;I'm not forwarding a new understanding of entropy, everything here can also be found in a thermodynamics textbook. &nbsp;The purpose of this article is not to contradict the scientific understanding of entropy, but to attempt to elaborate it in layman's terms. &nbsp;Entropy is not \"That thing which results in the heat death of the universe,\" which tends to be the way some people, particularly science fiction authors and readers, tend to regard it. &nbsp;Entropy, though it is an abstract description of underlying processes, is, understood as those processes, just as necessary to our existence as gravity. &nbsp;That is the purpose of this article; to clear up that particular misunderstanding which is so pervasive in some circles.</p>\n<p>On TimS's suggestion, I'll frame this description around the basic understanding most people have about entropy: It's that law of thermodynamics which prevents perpetual motion machines from happening. &nbsp;So we'll consider three universes - one with backwards entropy. &nbsp;One with no entropy. &nbsp;And finally the universe we do live in.</p>\n<p>To start with, imagine, for a moment, that entropy were reversed; that the amount of entropy in the universe were constantly -decreasing-.</p>\n<p>What would this imply for us?</p>\n<p>Approximately the same thing that increasing entropy implies; the amount of work we can extract from the universe is finite, dictated not by increasing energy&nbsp;homogeneity, but by decreasing. &nbsp;In a universe in which entropy is reversed, rivers would flow uphill - planets would slowly disintegrate, in point of fact, into galactic clouds of gas. &nbsp;We could extract work from this process, while it lasted, provided we could survive in such an environment to begin with. &nbsp;(We've evolved to extract work in one direction. &nbsp;Our cells would be no more capable of producing work in the other direction than a steam engine, which would require carbon dioxide fed deliberately into it, to be divided into oxygen and carbon.)</p>\n<p>If all of this seems absurd, limit our consideration to a simple thing - Newtonian gravity. &nbsp;Consider a closed system containing two particles, five meters apart, as our initial condition. &nbsp;This is a higher potential energy state - a lower entropy state - than the final condition, in which those particles have collided, and the energy has been reexpressed as atomic vibrations, heat. &nbsp;If entropy were reversed, those particles would never be permitted to collide; the law of physics would forbid it, because it would increase the entropy, in violation of our reversed law of entropy.</p>\n<p>&nbsp;</p>\n<p>In the current universe, the work we can extract from matter is not merely limited by entropy, it is in fact permitted by it; it is the irreversibility&nbsp;of thermodynamic processes which permit machines to work to begin with. &nbsp;A steam engine in which steam is as likely to contribute its engine to deoxidizing carbon and forming it into coal as it is to push a turbine is one in which the turbine moves fitfully, unpredictably, and in no particular direction. &nbsp;The arrow of entropy is the not merely the limit on how much work can happen, it is also the mechanism by which work happens. &nbsp;It does not, in fact, matter which direction it goes; as long as it is predictable, work could be extracted. &nbsp;Rivers wouldn't flow without entropy.</p>\n<p>Entropy is not a property of a system, but a property of forces; the law of entropy can be restated as \"Forces do what forces do.\" &nbsp;We measure entropy strictly in terms of things meaningful to use; it cannot be directly measured, because it doesn't exist.</p>\n<p>Entropy is not an increase in homogeneity. &nbsp;It's not an increase in the number of microstates. &nbsp;These are products of forces.</p>\n<p>And forces can work in opposing directions. &nbsp;The number of -macrostates- is in constant decline; this is also a product of forces. &nbsp;In macroscopic terms, heterogeneity, not homogeneity, is on the rise; consider that, according to Big Bang Theory, an early state of the universe was a nearly uniform cloud of gas. &nbsp;Compare that to the macroscopic state of the universe today, a heterogeneous mess. &nbsp;(It doesn't matter for this argument whether Big Bang Theory is true or not; if true, this behavior is exactly what physics would predict.)</p>\n<p>Statistical mechanics doesn't contradict this, but frames it in terms of probability; when dealing with statistical distributions, i/e, a cloud of gas, it's a way of expressing mathematically what is happening on an individual basis to each of the atoms. &nbsp;If you were capable of modeling each individual atom in the cloud of gas, you would arrive at the same conclusions. &nbsp;Entropy isn't necessarily related to information, although it can be modeled that way very easily in statistical mechanics, because information about a statistical representation of an entropic process does vary in relation to the entropy. &nbsp;(Which means that information-theoretic models can still model entropy for a statistical system.)</p>\n<p>The mathematical models for statistical mechanics entropy and informational entropy are very similar, which has led some people, including myself, to an initial misapprehension that they were describing similar processes; one of my early understandings of entropy was that information about the universe was being encoded into the universe, and that quantum uncertainty had to increase elsewhere in order to accommodate this certainty. &nbsp;I will provide an armchair logic proof of why this isn't necessary below. &nbsp;First, as to why they are similar - this is because they are both modeling, and measuring, uncertainty about particular variables within the system.</p>\n<p>Shannon Entropy - the measurement of entropy in information systems - is a measure of information which can be encoded in a given variable, and thus the measure of information which is lost when that variable is lost. &nbsp;This ties into conventional entropy because of statistical mechanics, which is framed on the concept of a \"microstate\" - that is, a configuration of particles which can result in a \"macrostate,\" which is an observable macroscopic state. &nbsp;(A forest is still a forest if a tree is two inches further to the left; the forest is the macrostate, the state of each individual tree is a microstate. &nbsp;From an airplane, the precise position of an individual tree doesn't matter; the macrostate is the same. &nbsp;A particular macrostate is, loosely speaking, a collection of microstates such that it is casually impossible to identify which exact microstate it is in.)</p>\n<p>A given macrostate of a cloud of gas can encode - provided you could read its microstate - a vast amount of information. &nbsp;The exact amount is not important; \"vast\" is all that you need to know. &nbsp;Statistical mechanics asserts that the most likely macrostate is the one which is described by the most possible microstates, or, to phrase this somewhat differently, the most likely state of matter is the one in which, if you could read and write to the microstate, the most information can be encoded. &nbsp;This may sound like a remarkable claim up until you revisit the definition of a macrostate and one particular word - \"possible.\" &nbsp;(Note that statistical mechanics makes a justified assumption that all possible microstates are equally likely; Liouville's theorem proves this for the case that all possible microstates were at some point in the past equally likely. &nbsp;Or, in other words, all microstates are equally likely, provided you have not actually read the microstate.)</p>\n<p>That \"possible\" is pretty important. &nbsp;Absent quantum fluctuations, which throw a wrench into the layman description (I'm sticking to classical mechanics because I'm judging that introducing the necessary \"degrees of freedom\" to explain behavior there is too messy for a layman), all your particles in a closed system can't, in any microstate, simultaneously adopt a leftward velocity; this state would violate the conservation of momentum. &nbsp;Entropy in statistical mechanics is a different way of -measuring- entropy, but that entropy must still be the product of the laws of physics operating on individual particles. &nbsp;Between point A and a later point B in time for a closed system X, entropy will -never- be less in point B than in point A. &nbsp;The \"statistical\" in \"statistical mechanics\" doesn't grant the possibility that anything can happen; none of the possible microstates include \"lower (mechanical) entropy over time.\" &nbsp;(Again, classical mechanics. &nbsp;For those interested in the quantum version of all this, I'll have to refer you to the concepts \"degrees of freedom\" and then \"gauge theory\". &nbsp;If somebody can come up with a layman's description of that, by all means.)</p>\n<p>[Draft section; will continue pending further review.]</p>\n<p>Earlier I promised you an armchair proof that entropy is not necessarily information. &nbsp;The armchair proof that entropy is not necessarily related to a universal concept of information, or level of system quantum uncertainty, is relatively simple - entropy still increases in a modeled system with 100% information about what is going on within it and no quantum uncertainty. &nbsp;This is not to say that entropy -doesn't- represent system information or total quantum uncertainty, only that these concepts aren't necessary to entropy. &nbsp;It also is not to say that entropic calculations do or do not directly represent -internal- uncertainty; it certainly limits the -amount- of information which can be represented in a universe, consistent with the statistical mechanics interpretation of entropy. &nbsp;An armchair proof that entropy bounds the internal information storage capacity of a system is also relatively trivial; the process of information binding requires work, and entropy limits the amount of work that can be done within a system.</p>\n<p>Entropy is, broadly speaking, a statement of the irreversibility&nbsp;of forces. &nbsp;In a closed solar system, gravity means that eventually, everything will be in a stable configuration; a single concentrated ball, or dead objects orbiting each other. &nbsp;There are several stable configurations, but each are local entropic maximums. &nbsp;The irreversibility of entropy, in a final description, can also be treated as an expression of the fact that stability persists, and instability ends - and importantly that this is true of every scale.</p>\n<p>It's not disorder. &nbsp;It's not homogeneity. &nbsp;It's not the number of states. &nbsp;These expressions of entropy are expressions of particular forces.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K2CEyGY5FmF9MpRTj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": -19, "extendedScore": null, "score": -1.1e-05, "legacy": true, "legacyId": "17856", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-25T18:38:24.194Z", "modifiedAt": null, "url": null, "title": "Meetup : Humanist Open Mic, NYC, Wednesday August 1st", "slug": "meetup-humanist-open-mic-nyc-wednesday-august-1st", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yNQqRsYeQa857QCTS/meetup-humanist-open-mic-nyc-wednesday-august-1st", "pageUrlRelative": "/posts/yNQqRsYeQa857QCTS/meetup-humanist-open-mic-nyc-wednesday-august-1st", "linkUrl": "https://www.lesswrong.com/posts/yNQqRsYeQa857QCTS/meetup-humanist-open-mic-nyc-wednesday-august-1st", "postedAtFormatted": "Wednesday, July 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Humanist%20Open%20Mic%2C%20NYC%2C%20Wednesday%20August%201st&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Humanist%20Open%20Mic%2C%20NYC%2C%20Wednesday%20August%201st%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyNQqRsYeQa857QCTS%2Fmeetup-humanist-open-mic-nyc-wednesday-august-1st%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Humanist%20Open%20Mic%2C%20NYC%2C%20Wednesday%20August%201st%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyNQqRsYeQa857QCTS%2Fmeetup-humanist-open-mic-nyc-wednesday-august-1st", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyNQqRsYeQa857QCTS%2Fmeetup-humanist-open-mic-nyc-wednesday-august-1st", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 182, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/c8'>Humanist Open Mic, NYC, Wednesday August 1st</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 August 2012 02:35:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">159 E Houston Street, New York, NY 10003</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Half a year ago, I wrote extensively about the value of art, ritual and community building. Six months later, after much exploration, learning and work, I've started the Open Mind Open Mic.</p>\n\n<p>The Open Mic is an opportunity for rationally-minded people to share performance art related to things they value. Have a piece to share about rationality, science, ethics or human achievement? It can be a song, story, poem, comedy act... any kind of performance art that excites you. Covers of existing songs that you find appropriate are also encouraged. And if you just want to come and listen, that's great too.</p>\n\n<p>The meetup will feature people from the NY Less Wrong community as well as skeptics from Center for Inquiry, the largest network of humanist meetups in NYC. It will be held in the basement of the National Underground, a bar in downtown Manhattan.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/c8'>Humanist Open Mic, NYC, Wednesday August 1st</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yNQqRsYeQa857QCTS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.502576595066618e-07, "legacy": true, "legacyId": "17859", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Humanist_Open_Mic__NYC__Wednesday_August_1st\">Discussion article for the meetup : <a href=\"/meetups/c8\">Humanist Open Mic, NYC, Wednesday August 1st</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 August 2012 02:35:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">159 E Houston Street, New York, NY 10003</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Half a year ago, I wrote extensively about the value of art, ritual and community building. Six months later, after much exploration, learning and work, I've started the Open Mind Open Mic.</p>\n\n<p>The Open Mic is an opportunity for rationally-minded people to share performance art related to things they value. Have a piece to share about rationality, science, ethics or human achievement? It can be a song, story, poem, comedy act... any kind of performance art that excites you. Covers of existing songs that you find appropriate are also encouraged. And if you just want to come and listen, that's great too.</p>\n\n<p>The meetup will feature people from the NY Less Wrong community as well as skeptics from Center for Inquiry, the largest network of humanist meetups in NYC. It will be held in the basement of the National Underground, a bar in downtown Manhattan.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Humanist_Open_Mic__NYC__Wednesday_August_1st1\">Discussion article for the meetup : <a href=\"/meetups/c8\">Humanist Open Mic, NYC, Wednesday August 1st</a></h2>", "sections": [{"title": "Discussion article for the meetup : Humanist Open Mic, NYC, Wednesday August 1st", "anchor": "Discussion_article_for_the_meetup___Humanist_Open_Mic__NYC__Wednesday_August_1st", "level": 1}, {"title": "Discussion article for the meetup : Humanist Open Mic, NYC, Wednesday August 1st", "anchor": "Discussion_article_for_the_meetup___Humanist_Open_Mic__NYC__Wednesday_August_1st1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-25T19:28:25.901Z", "modifiedAt": null, "url": null, "title": "Article about LW: Faith, Hope, and Singularity: Entering the Matrix with New York\u2019s Futurist Set", "slug": "article-about-lw-faith-hope-and-singularity-entering-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:38.383Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "malo", "createdAt": "2011-12-28T20:01:23.182Z", "isAdmin": false, "displayName": "Malo"}, "userId": "DA863LaqrSGNFs5w5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/44GibdAwuetTXrQgd/article-about-lw-faith-hope-and-singularity-entering-the", "pageUrlRelative": "/posts/44GibdAwuetTXrQgd/article-about-lw-faith-hope-and-singularity-entering-the", "linkUrl": "https://www.lesswrong.com/posts/44GibdAwuetTXrQgd/article-about-lw-faith-hope-and-singularity-entering-the", "postedAtFormatted": "Wednesday, July 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Article%20about%20LW%3A%20Faith%2C%20Hope%2C%20and%20Singularity%3A%20Entering%20the%20Matrix%20with%20New%20York%E2%80%99s%20Futurist%20Set&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArticle%20about%20LW%3A%20Faith%2C%20Hope%2C%20and%20Singularity%3A%20Entering%20the%20Matrix%20with%20New%20York%E2%80%99s%20Futurist%20Set%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F44GibdAwuetTXrQgd%2Farticle-about-lw-faith-hope-and-singularity-entering-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Article%20about%20LW%3A%20Faith%2C%20Hope%2C%20and%20Singularity%3A%20Entering%20the%20Matrix%20with%20New%20York%E2%80%99s%20Futurist%20Set%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F44GibdAwuetTXrQgd%2Farticle-about-lw-faith-hope-and-singularity-entering-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F44GibdAwuetTXrQgd%2Farticle-about-lw-faith-hope-and-singularity-entering-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<p><a title=\"Faith, Hope, and Singularity: Entering the Matrix with New York&rsquo;s Futurist Set\" rel=\"nofollow\" href=\"http://betabeat.com/2012/07/singularity-institute-less-wrong-peter-thiel-eliezer-yudkowsky-ray-kurzweil-harry-potter-methods-of-rationality/?show=all\" target=\"_blank\">Faith, Hope, and Singularity: Entering the Matrix with New York&rsquo;s Futurist Set</a></p>\n<p>To my knowledge LessWrong hasn't received a great deal of media coverage. So, I was surprised when I came across an article via a Facebook friend which also appeared on the cover of the New York Observer today. However, I was disappointed upon reading it, as I don't think it is an accurate reflection of the community. It certainly doesn't reflect my experience with the LW communities in Toronto and Waterloo.&nbsp;</p>\n<p>I thought it would be interesting to see what the broader LessWrong community thought about this article. I think it would make for a good discussion.</p>\n<p>Possible conversation topics:</p>\n<ul>\n<li>This article will likely reach many people that have never heard of LessWrong before. Is this a good introduction to LessWrong for those people?</li>\n<li>Does this article give an accurate characterization of the LessWrong community?</li>\n</ul>\n<div><br /></div>\n<p>Edit 1: Added some clarification about my view on the article.</p>\n<p>Edit 2: Re-added link using &ldquo;nofollow&rdquo; attribute.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "7ow6EFpypbH4hzFuz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "44GibdAwuetTXrQgd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 44, "extendedScore": null, "score": 0.000106, "legacy": true, "legacyId": "17860", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 235, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-26T05:10:04.229Z", "modifiedAt": null, "url": null, "title": "Neuroscience basics for LessWrongians", "slug": "neuroscience-basics-for-lesswrongians", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:41.366Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChrisHallquist", "createdAt": "2011-05-25T19:16:15.462Z", "isAdmin": false, "displayName": "ChrisHallquist"}, "userId": "wvT2xWQqHKxkp9NWN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xWTSHJASRaLABgHWc/neuroscience-basics-for-lesswrongians", "pageUrlRelative": "/posts/xWTSHJASRaLABgHWc/neuroscience-basics-for-lesswrongians", "linkUrl": "https://www.lesswrong.com/posts/xWTSHJASRaLABgHWc/neuroscience-basics-for-lesswrongians", "postedAtFormatted": "Thursday, July 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Neuroscience%20basics%20for%20LessWrongians&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANeuroscience%20basics%20for%20LessWrongians%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxWTSHJASRaLABgHWc%2Fneuroscience-basics-for-lesswrongians%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Neuroscience%20basics%20for%20LessWrongians%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxWTSHJASRaLABgHWc%2Fneuroscience-basics-for-lesswrongians", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxWTSHJASRaLABgHWc%2Fneuroscience-basics-for-lesswrongians", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4020, "htmlBody": "<div id=\"entry_t3_d1h\" class=\"content clear\">\n<p>The origins of this article are in my <a href=\"/lw/bug/partial_transcript_of_the_hansonyudkowsky_june/\">partial transcript</a> of the live June 2011 debate between Robin Hanson and Eliezer Yudkowsky. While I still feel like I don't entirely understand his arguments, a few of his comments about neuroscience made me strongly go, \"no, that's not right.\"</p>\n<p>Furthermore, I've noticed that while LessWrong in general seems to be very strong on the psychological or \"black box\"&nbsp;side of cognitive science, there isn't as much discussion of neuroscience here. This is somewhat understandable. Our current understanding of neuroscience is frustratingly incomplete, and too much journalism on neuroscience is sensationalistic nonsense. However, I think what we do know is worth knowing. (And part of what makes much neuroscience journalism annoying is that it makes a big deal out of things that are <em>totally unsurprising, </em>given what we <em>already know.)</em></p>\n<p>My qualifications to do this: while my degrees are in philosophy, for awhile in undergrad I was a neuroscience major, and ended up taking quite a bit of neuroscience as a result. This means I can assure you that most of what I say here is standard neuroscience which could be found in an introductory textbook like Nichols, Martin, Wallace, &amp; Fuchs' <em>From Neuron to Brain </em>(one of the text books I used as an undergraduate). The only things that might not be totally standard are the conjecture I make about how complex currently-poorly-understood areas of the brain are likely to be, and also some of the points I make in criticism of Eliezer at the end (though I believe these are not a very big jump from current textbook neuroscience.)</p>\n<p>One of the main themes of this article will be specialization within the brain. In particular, we know that the brain is divided into specialized areas at the macro level, and we understand some (though not very much) of the micro-level wiring that supports this specialization. It seems likely that each region of the brain has its own micro-level wiring to support its specialized function, and in some regions the wiring is likely to be quite complex.<a id=\"more\"></a></p>\n<h1>1. Specialization of brain regions</h1>\n<p>One of the best-established facts about the brain is that specific regions handle specific functions. And it isn&rsquo;t just that in each individual, specific brain regions handle specific functions. It&rsquo;s also that which regions handle which functions is consistent across individuals. This is an extremely well-established finding, but it&rsquo;s worth briefly summarizing some of the evidence for it.</p>\n<p>One kind of evidence comes from experiments involving direct electrical stimulation of the brain. This cannot ethically be done on humans without a sound medical reason, but it is used with epileptic patients in order to determine the source of the problem, which is necessary in order to treat epilepsy surgically.&nbsp;</p>\n<p>In epileptic patients, stimulating certain regions of the brain (known as the primary sensory areas) causes the patient to report sensations: sights, sounds, feelings, smells, and tastes. Which sensations are caused by stimulating which regions of the brain is consistent across patients. This is the source of the &ldquo;Penfield homunculus,&rdquo; a map of brain regions which, when stimulated, result in touch sensations which patients describe as feeling like they come from particular parts of the body. Stimulating one region, for example, might consistently lead to a patient reporting a feeling in his left foot.</p>\n<p>Regions of the brain associated with sensations are known as sensory areas or sensory cortex. Other regions of the brain, when stimulated, lead to involuntary muscle movements. Those areas are known as motor areas or motor cortex, and again, which areas correspond to which muscles is consistent across patients. The consistency of the mapping of brain regions across patients is important, because it&rsquo;s evidence of an innate structure to the brain.</p>\n<p>An even more significant kind of evidence comes from studies of patients with brain damage. Brain damage can produce very specific ability losses, and patients with damage to the same areas will typically have similar ability losses. For example, the rear most part of the human cerebral cortex is the primary visual cortex, and damage to it results in a phenomenon known as cortical blindness. That is to say, the patient is blind in spite of having perfectly good eyes. Their other mental abilities may be unaffected.&nbsp;</p>\n<p>That much is not surprising, given what we know from studies involving electrical stimulation, but ability losses from brain damage can be strangely specific. For example, neuroscientists now believe that one function of the temporal lobe is to recognize objects and faces. A key line of evidence for this is that patient with damage to certain parts of the temporal lobe will be unable to identify those things by sight, even though they may be able to describe the objects in great detail. Here is neurologist Oliver Sacks&rsquo; description of an interaction with one such patient, the titular patient in Sacks&rsquo; book <em>The Man Who Mistook His Wife For a Hat:</em></p>\n<blockquote>\n<p>&lsquo;What is this?&rsquo; I asked, holding up a glove.</p>\n<p>&lsquo;May I examine it?&rsquo; he asked, and, taking it from me, he proceeded to examine it as he had examined the geometrical shapes.</p>\n<p>&lsquo;A continuous surface,&rsquo; he announced at last, &lsquo;infolded on itself. It appears to have&rsquo;&mdash;he hesitated&mdash;&rsquo;five outpouchings, if this is the word.&rsquo;</p>\n<p>&lsquo;Yes,&rsquo; I said cautiously. You have given me a description. Now tell me what it is.&rsquo;</p>\n<p>&lsquo;A container of some sort?&rsquo;</p>\n<p>&lsquo;Yes,&rsquo; I said, &lsquo;and what would it contain?&rsquo;</p>\n<p>&lsquo;It would contain its contents!&rsquo; said Dr P., with a laugh. &lsquo;There are many possibilities. It could be a change purse, for example, for coins of five sizes. It could ...&rsquo;</p>\n<p>I interrupted the barmy flow. &lsquo;Does it not look familiar? Do you think it might contain, might fit, a part of your body?&rsquo;</p>\n<p>No light of recognition dawned on his face. (Later, by accident, he got it on, and exclaimed, &lsquo;My God, it&rsquo;s a glove!&rsquo;)</p>\n</blockquote>\n<p>The fact that damage to certain parts of the temporal lobe results in an inability to recognize objects contains an extremely important lesson. For most of us, recognizing objects requires no effort or thought as long as we can see the object clearly. Because it&rsquo;s easy for us, it might be tempting to think it&rsquo;s an inherently easy task, one that shouldn&rsquo;t require hardly any brain matter to perform. Certainly it never occurred to me before I studied neuroscience that object recognition might require a special brain region. But it turns out that tasks that seem easy to us can in fact require such a specialized region.</p>\n<p>Another example of this fact comes from two brain regions involved in language, Broca&rsquo;s area and Wernicke's area. Damage to each area leads to distinct types of difficulties with language, known as Broca&rsquo;s aphasia and Wernicke&rsquo;s aphasia, respectively. Both are strange conditions, and my description of them may not give a full sense of what they are like. Readers might consider searching online for videos of interviews Broca&rsquo;s aphasia and Wernicke&rsquo;s aphasia patients to get a better idea of what the conditions entail.</p>\n<p>Broca&rsquo;s aphasia is a loss of ability to produce language. In one extreme case, one of the original cases studied by Paul Broca, a patient was only able to say the word &ldquo;tan.&rdquo; Other patients may have a less limited vocabulary, but still struggle to come up with words for what they want to say. And even when they can come up with individual words, they may be unable to put them into sentences. However, patients with Broca&rsquo;s aphasia appear to have no difficulty understanding speech, and show awareness of their disability.</p>\n<p>Wernicke&rsquo;s aphasia is even stranger. It is often described as an inability to understand language while still being able to produce language. However, while patients with Wernicke's aphasia may have little difficulty producing complete, grammatically correct sentences, &nbsp;the sentences tend to be nonsensical. And Wernicke&rsquo;s patients often act as if they are completely unaware of their condition. &nbsp;A former professor of mine once described a Wernicke&rsquo;s patient as sounding &ldquo;like a politician,&rdquo; and from watching a video of an interview with the patient, I agreed: I was impressed by his ability to confidently utter nonsense.</p>\n<p>The fact of these two forms of aphasia suggest that Broca&rsquo;s area and Wernicke&rsquo;s area have two very important and distinct roles in our ability to produce and understand language. And I find this fact strange to write about. Like object recognition, language comes naturally to us. As I write this, my intuitive feeling is that the work I am doing comes mainly in the ideas, plus making a few subtle stylistic decisions. I know from neuroscience that I would be unable to write this if I had significant damage to either region. Yet I am totally unconscious of the work they are doing for me.</p>\n<h1>2. Complex, specialized wiring within regions</h1>\n<p>&ldquo;Wiring&rdquo; is a hard metaphor to avoid when talking about the brain, but it is also a potentially misleading one. People often talk about &ldquo;electrical signals&rdquo; in the brain, but unlike electrical signals in human technology, which involves movement of electrons between the atomics of the conductor, signals in the human brain involve movement of ions and small molecules across cell membranes and between cells.&nbsp;</p>\n<p>Furthermore, the first thing most people who know a little bit about neuroscience will think of when they hear the word &ldquo;wiring&rdquo; is axons and dendrites, the long skinny projections along which signals are transmitted from neuron to neuron. But it isn&rsquo;t just the layout of axons and dendrites that matters. Ion channels, and the structures that transport neurotransmitters across cell membranes, are also important.&nbsp;</p>\n<p>These can vary a lot at the synapse, the place where two neurons touch. For example, synapses vary in strength, that is to say, the strength of the one neuron&rsquo;s effect on the other neuron. Synapses can also be excitatory (activity in one leads increased activity in the other) or inhibitory (activity in one leads to decreased activity in the other). And this is just a couple of the ways synapses can vary; the details can be somewhat complicated, and I&rsquo;ll give one example of how the details can be complicated later.</p>\n<p>I say all this just to make clear what I mean when I talk about how the brain&rsquo;s &ldquo;wiring.&rdquo; By &ldquo;wiring,&rdquo; I mean all the features of the physical structures that connect neurons to each other and which are relevant for understanding how the brain works. I mean to all the things I&rsquo;ve mentioned above, and anything I may have omitted. It&rsquo;s important to have a word to talk about this wiring, because what (admittedly little) we understand about how the brain works we understand in terms of this wiring.</p>\n<p>For example, the nervous system actually first begins processing visual information in the retina (the part of the eye at the back where our light receptors are). This is done by what&rsquo;s known as the center-surround system: a patch of light receptors, when activated, excites one neuron, but nearby patches of light receptors, when activated, inhibit that same neuron (sometimes, the excitatory roles and inhibitory roles are reversed).</p>\n<p>The effect of this is that what the neurons are sensitive to is not light itself, but contrast. They&rsquo;re contrast detectors. And what allows them to detect contrast isn&rsquo;t anything magical, it&rsquo;s just the wiring, the way the neurons are connected together.</p>\n<p>This technique of getting neurons to serve specific functions based on how they are wired together shows up in more complicated ways in the brain itself. There&rsquo;s one line of evidence for specialization of brain regions that I saved for this section, because it also tells us about the details of how the brain is wired. That line of evidence is recordings from the brain using electrodes.</p>\n<p>For example, during the 50&rsquo;s David Hubel and Torsten Weisel did experiments where they paralyzed the eye muscles of animals, stuck electrodes in primary visual areas of the animals&rsquo; brains, and then showed the animals various images to see which images would cause electrical signals in the animals&rsquo; primary visual areas. It turned out that the main thing that causes electrical signals in the primary visual areas is lines.</p>\n<p>In particular, a given cell in the primary visual area will have a particular orientation of line which it responds to. It appears that the way these line-orientation detecting cells work is that they receive input from several contrast detecting cells which, themselves, correspond to regions of the retina that are themselves all in a line. A line in the right position and orientation will activate all of the contrast-detecting cells, which in turn activates the line-orientation detecting cell. A line in the right position but wrong orientation will activate only one or a few contrast-detecting cells, not enough to activate the line-orientation detecting cell.</p>\n<p>[If this is unclear, a diagram like the one on <a href=\"http://en.wikipedia.org/wiki/Receptive_field#Retinal_ganglion_cells\">Wikipedia</a> may be helpful, though Wikipedia's diagram may not be the best.]</p>\n<p>Another example of a trick the brain does with neural wiring is locating sounds using what are called &ldquo;interaural time differences.&rdquo; The idea is this: there is a group of neurons that receives input from both ears, and specifically responds to <em>simultaneous</em> input from both ears. However, the axons running from the ears to the neurons in this group of cells vary in length, and therefore they vary in how long it takes them to get a signal from each ear.&nbsp;</p>\n<p>This means that which cells in this group respond to a sound depends on whether or not the sound reaches reaches the ears at the same time or at different times, and (if at different times) on how big the time difference is. If there&rsquo;s no difference, that means the sound came from directly ahead or behind (or above or below). A big difference, with the sound reaching the left ear first, indicates the sound came from the left. A big difference, with the sound reaching the right ear first, indicates the sound came from the left. Small differences indicate something in between.</p>\n<p>[A diagram might be helpful here too, but I'm not sure where to find a good one online.]</p>\n<p>I&rsquo;ve made a point to mention these bits of wiring because they&rsquo;re cases where neuroscientists have a clear understanding of how it is that a particular neuron is able to fire only in response to a particular kind of stimulus. Unfortunately, cases like this are relatively rare. In other cases, however, we at least know that particular neurons respond specifically to more complex stimuli, even though we don&rsquo;t know why. In rats, for example, there are cells in the hippocampus that activate only when the rat is in a particular location; apparently their purpose is to keep track of the rat&rsquo;s location.</p>\n<p>The visual system gives us some especially interesting cases of this sort. We know that the primary visual cortex sends information to other parts of the brain in two broadly-defined pathways, the dorsal pathway and the ventral pathway. The dorsal pathway appears to be responsible for processing information related to position and movement. Some cells in the dorsal pathway, for example, fire only when an animal sees an object moving in a particular direction.&nbsp;</p>\n<p>The most interesting cells of this sort that neuroscientists have found so far, though, are probably some of the cells in the medial temporal lobe, which is part of the ventral pathway. In one study (Quiroga et al. 2005), researchers took epileptic patients who had had electrodes implanted in their brains in order to locate the source of their epilepsy and showed them pictures of various people, objects, and landmarks. What the researchers found is that the neuron or small group of neurons a given electrode was reading from typically only responded to pictures of one person or thing.&nbsp;</p>\n<p>Furthermore, a particular electrode often got readings from very different pictures of a single person or thing, but not similar pictures of different people or things. In one notorious example, they found a neuron that they could only get to respond to either pictures of actress Halle Berry or the text &ldquo;Halle Berry.&rdquo; This included drawings of the actress, as well as pictures of her dressed as Catwoman (a role which she had recently performed at the time the study was performed), but not other drawings or other pictures of Catwoman.</p>\n<p>What&rsquo;s going on here? Based on what we know about the wiring of contrast-detectors and orientation-detectors the following conjecture seems highly likely: if we were to map out the brain completely and then follow the path along which visual information is transmitted, we would find that neurons gradually come to be wired together in more and more complex ways, to allow them to gradually become specific to more and more complex features of visual images. This, I think, is an extremely important inference.</p>\n<p>We know that experience can impact the way the brain is wired. In fact, some aspects of the brain&rsquo;s wiring seem to have evolved specifically to be able to change in response to experience (the main wiring of that sort we know about is called Hebbian synapses, but the details aren&rsquo;t important here). And it is actually somewhat difficult to draw a clear line between features of the brain that are innate and features of the brain that are the product of learning, because some fairly basic features of the brain depend on outside cues in order to develop.</p>\n<p>Here, though, I&rsquo;ll use the word &ldquo;innate&rdquo; to refer to features of the brain that will develop given the overwhelming majority of the conditions animals of a given species actually develop under. Under that definition, a &ldquo;Halle Berry neuron&rdquo; is highly unlikely to be innate, because there isn&rsquo;t enough room in the brain to have a neuron specific to every person a person might possibly learn about. Such neural wiring is almost certainly the result of learning.</p>\n<p>But importantly, the underlying structure that makes such learning possible is probably at least somewhat complicated, and also specialized for that particular kind of learning. This is because such person-specific and object-specific neurons are not found in all regions of the brain, there must be something special about the medial temporal lobe that allows such learning to happen there.</p>\n<p>Similar reasoning applies to regions of the brain that we know even less about. For example, it seems likely that Broca&rsquo;s area and Wernicke&rsquo;s area both contain specialized wiring for handling language, though we have little idea how that wiring might perform its function. Given that humans seem to have a considerable innate knack for learning language (Pinker 2007), it again seems likely that the wiring is somewhat complicated.</p>\n<h1>3. On some problematic comments by Eliezer</h1>\n<p>I agree with Singularity Institute positions on a great deal. After all, I recently made my first donation to the Singularity Institute. But here, I want to point out some problematic neuroscience-related comments in Eliezer's debate with Robin Hanson:</p>\n<blockquote>\n<p>If you actually look at the genome, we&rsquo;ve got about 30,000 genes in here. Most of our 750 megabytes of DNA is repetitive and almost certainly junk, as best we understand it. And the brain is simply not a very complicated artifact by comparison to, say, Windows Vista. Now the complexity that it does have it uses a lot more effectively than Windows Vista does. It probably contains a number of design principles which Microsoft knows not. (And I&rsquo;m not saying it&rsquo;s that small because it&rsquo;s 750 megabytes, I&rsquo;m saying it&rsquo;s gotta be that small because at least 90% of the 750 megabytes is junk and there&rsquo;s only 30,000 genes for the whole body, never mind the brain.)</p>\n<p>That something that simple can be this powerful, and this hard to understand, is a shock. But if you look at the brain design, it&rsquo;s got 52 major areas on each side of the cerebral cortex, distinguishable by the local pattern, the tiles and so on, it just doesn&rsquo;t really look all that complicated. It&rsquo;s very powerful. It&rsquo;s very mysterious. What can say about it is that it probably involves 1,000 different deep, major, mathematical insights into the nature of intelligence that we need to comprehend before we can build it.</p>\n</blockquote>\n<p>Though this is not explicit, there appears to be an inference&nbsp;here that, in order for something so simple to be so powerful, it must incorporate many deep insights into intelligence, though we don&rsquo;t know what most of them are. There are&nbsp;several&nbsp;problems with this argument.</p>\n<div>\n<div>First of all, it is not true that the fact that that brain is divided into only 52 major areas is evidence that it is not very complex, because knowing about the complexity of its macroscopic organization tells us nothing about the complexity of its microscopic wiring. The brain consists of tens of billions of neurons, and a single neuron can make hundreds of synapses with other neurons. The details of how synapses are set up vary greatly. The fact is that under a microscope, the brain at least looks very complex.</div>\n<div><br /></div>\n<div>The argument from the small size of the genome is more plausible, especially if Eliezer is thinking in terms of Kolmogorov complexity, which is based on the size of the smallest computer program needed to build something. However, it does not follow that if the genome is not very complex, the brain must not be very complex, because the brain may be built not just based on the genome, but also based on information from the outside environment. We have good reason to think this is how the brain is actually set up, not just in cases we would normally associate with learning and memory, but with some of the most basic and near-universal features of the brain. For example, in normal mammals, the neurons in the visual cortex are organized into &ldquo;ocular dominance columns,&rdquo; but these fail to form if the animal is raised in darkness.</div>\n<div><br /></div>\n<div>More importantly, there is no reason to think getting a lot of power out of a relatively simple design requires insights into the nature of intelligence itself. To use Eliezer&rsquo;s own example of Windows Vista: imagine if, for some reason, Microsoft decided that it was very important for the next generation of its operating system to be highly compressible. Microsoft tells this to its programmers, and they set about looking for ways to make an operating system do most of what the current version of Windows does while being more compressible. They end up doing a lot of things that are only applicable to their situation, and couldn&rsquo;t be used to make a much more powerful operating system. For example, they might look for ways to recycle pieces of code, and make particular pieces of code do as many different things in the program as possible.</div>\n<div><br /></div>\n<div>In this case, would we say that they had discovered deep insights into how to build powerful operating systems? Well no. And there&rsquo;s reason to think that life on Earth uses similar tricks to get a lot of apparent complexity out of relatively simple genetic codes. Genes code for protein. In a phenomenon known as &ldquo;alternative splicing,&rdquo; there may be several ways to combine the parts of a gene, allowing one gene to code for several proteins. And even a single, specific protein may perform several roles within an organism. A receptor protein, for example, may be plugged into different signaling cascades in different parts of an organism.</div>\n<div><br /></div>\n<div>Eliezer's comments about the complexity of brain are only a small part of his arguments in the debate, but I worry that comments like these by people concerned with the future of Artificial Intelligence are harmful insofar as they may lead some people (particularly neuroscientists) to conclude AI-related futurism is a bunch of confusions based in ignorance. <em>I </em>don't think it is, but a neuroscientist taking the Hanson-Yudkowsky debate as an introduction to the issues could easily conclude that.</div>\n<div><br /></div>\n<div>Of course, that's not the most important reason for people with an interest in AI to understand the basics of neuroscience. The most important reason is that understanding some neuroscience will help clarify your thinking about the rest of cognitive science.</div>\n</div>\n<p><em>References:</em></p>\n<p>Pinker, S. 2007. <em>The Language Instinct.</em> Harper Perennial Modern Classics.</p>\n<p>Quiroga, R. Q. et al. 2005. <a href=\"http://papers.klab.caltech.edu/175/1/519.pdf\">Invariant visual representation by single neurons in the human brain.</a> <em>Nature,</em> 435, 1102-1107.</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Wi3EopKJ2aNdtxSWg": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xWTSHJASRaLABgHWc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 96, "baseScore": 129, "extendedScore": null, "score": 0.000272, "legacy": true, "legacyId": "16927", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 129, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<div id=\"entry_t3_d1h\" class=\"content clear\">\n<p>The origins of this article are in my <a href=\"/lw/bug/partial_transcript_of_the_hansonyudkowsky_june/\">partial transcript</a> of the live June 2011 debate between Robin Hanson and Eliezer Yudkowsky. While I still feel like I don't entirely understand his arguments, a few of his comments about neuroscience made me strongly go, \"no, that's not right.\"</p>\n<p>Furthermore, I've noticed that while LessWrong in general seems to be very strong on the psychological or \"black box\"&nbsp;side of cognitive science, there isn't as much discussion of neuroscience here. This is somewhat understandable. Our current understanding of neuroscience is frustratingly incomplete, and too much journalism on neuroscience is sensationalistic nonsense. However, I think what we do know is worth knowing. (And part of what makes much neuroscience journalism annoying is that it makes a big deal out of things that are <em>totally unsurprising, </em>given what we <em>already know.)</em></p>\n<p>My qualifications to do this: while my degrees are in philosophy, for awhile in undergrad I was a neuroscience major, and ended up taking quite a bit of neuroscience as a result. This means I can assure you that most of what I say here is standard neuroscience which could be found in an introductory textbook like Nichols, Martin, Wallace, &amp; Fuchs' <em>From Neuron to Brain </em>(one of the text books I used as an undergraduate). The only things that might not be totally standard are the conjecture I make about how complex currently-poorly-understood areas of the brain are likely to be, and also some of the points I make in criticism of Eliezer at the end (though I believe these are not a very big jump from current textbook neuroscience.)</p>\n<p>One of the main themes of this article will be specialization within the brain. In particular, we know that the brain is divided into specialized areas at the macro level, and we understand some (though not very much) of the micro-level wiring that supports this specialization. It seems likely that each region of the brain has its own micro-level wiring to support its specialized function, and in some regions the wiring is likely to be quite complex.<a id=\"more\"></a></p>\n<h1 id=\"1__Specialization_of_brain_regions\">1. Specialization of brain regions</h1>\n<p>One of the best-established facts about the brain is that specific regions handle specific functions. And it isn\u2019t just that in each individual, specific brain regions handle specific functions. It\u2019s also that which regions handle which functions is consistent across individuals. This is an extremely well-established finding, but it\u2019s worth briefly summarizing some of the evidence for it.</p>\n<p>One kind of evidence comes from experiments involving direct electrical stimulation of the brain. This cannot ethically be done on humans without a sound medical reason, but it is used with epileptic patients in order to determine the source of the problem, which is necessary in order to treat epilepsy surgically.&nbsp;</p>\n<p>In epileptic patients, stimulating certain regions of the brain (known as the primary sensory areas) causes the patient to report sensations: sights, sounds, feelings, smells, and tastes. Which sensations are caused by stimulating which regions of the brain is consistent across patients. This is the source of the \u201cPenfield homunculus,\u201d a map of brain regions which, when stimulated, result in touch sensations which patients describe as feeling like they come from particular parts of the body. Stimulating one region, for example, might consistently lead to a patient reporting a feeling in his left foot.</p>\n<p>Regions of the brain associated with sensations are known as sensory areas or sensory cortex. Other regions of the brain, when stimulated, lead to involuntary muscle movements. Those areas are known as motor areas or motor cortex, and again, which areas correspond to which muscles is consistent across patients. The consistency of the mapping of brain regions across patients is important, because it\u2019s evidence of an innate structure to the brain.</p>\n<p>An even more significant kind of evidence comes from studies of patients with brain damage. Brain damage can produce very specific ability losses, and patients with damage to the same areas will typically have similar ability losses. For example, the rear most part of the human cerebral cortex is the primary visual cortex, and damage to it results in a phenomenon known as cortical blindness. That is to say, the patient is blind in spite of having perfectly good eyes. Their other mental abilities may be unaffected.&nbsp;</p>\n<p>That much is not surprising, given what we know from studies involving electrical stimulation, but ability losses from brain damage can be strangely specific. For example, neuroscientists now believe that one function of the temporal lobe is to recognize objects and faces. A key line of evidence for this is that patient with damage to certain parts of the temporal lobe will be unable to identify those things by sight, even though they may be able to describe the objects in great detail. Here is neurologist Oliver Sacks\u2019 description of an interaction with one such patient, the titular patient in Sacks\u2019 book <em>The Man Who Mistook His Wife For a Hat:</em></p>\n<blockquote>\n<p>\u2018What is this?\u2019 I asked, holding up a glove.</p>\n<p>\u2018May I examine it?\u2019 he asked, and, taking it from me, he proceeded to examine it as he had examined the geometrical shapes.</p>\n<p>\u2018A continuous surface,\u2019 he announced at last, \u2018infolded on itself. It appears to have\u2019\u2014he hesitated\u2014\u2019five outpouchings, if this is the word.\u2019</p>\n<p>\u2018Yes,\u2019 I said cautiously. You have given me a description. Now tell me what it is.\u2019</p>\n<p>\u2018A container of some sort?\u2019</p>\n<p>\u2018Yes,\u2019 I said, \u2018and what would it contain?\u2019</p>\n<p>\u2018It would contain its contents!\u2019 said Dr P., with a laugh. \u2018There are many possibilities. It could be a change purse, for example, for coins of five sizes. It could ...\u2019</p>\n<p>I interrupted the barmy flow. \u2018Does it not look familiar? Do you think it might contain, might fit, a part of your body?\u2019</p>\n<p>No light of recognition dawned on his face. (Later, by accident, he got it on, and exclaimed, \u2018My God, it\u2019s a glove!\u2019)</p>\n</blockquote>\n<p>The fact that damage to certain parts of the temporal lobe results in an inability to recognize objects contains an extremely important lesson. For most of us, recognizing objects requires no effort or thought as long as we can see the object clearly. Because it\u2019s easy for us, it might be tempting to think it\u2019s an inherently easy task, one that shouldn\u2019t require hardly any brain matter to perform. Certainly it never occurred to me before I studied neuroscience that object recognition might require a special brain region. But it turns out that tasks that seem easy to us can in fact require such a specialized region.</p>\n<p>Another example of this fact comes from two brain regions involved in language, Broca\u2019s area and Wernicke's area. Damage to each area leads to distinct types of difficulties with language, known as Broca\u2019s aphasia and Wernicke\u2019s aphasia, respectively. Both are strange conditions, and my description of them may not give a full sense of what they are like. Readers might consider searching online for videos of interviews Broca\u2019s aphasia and Wernicke\u2019s aphasia patients to get a better idea of what the conditions entail.</p>\n<p>Broca\u2019s aphasia is a loss of ability to produce language. In one extreme case, one of the original cases studied by Paul Broca, a patient was only able to say the word \u201ctan.\u201d Other patients may have a less limited vocabulary, but still struggle to come up with words for what they want to say. And even when they can come up with individual words, they may be unable to put them into sentences. However, patients with Broca\u2019s aphasia appear to have no difficulty understanding speech, and show awareness of their disability.</p>\n<p>Wernicke\u2019s aphasia is even stranger. It is often described as an inability to understand language while still being able to produce language. However, while patients with Wernicke's aphasia may have little difficulty producing complete, grammatically correct sentences, &nbsp;the sentences tend to be nonsensical. And Wernicke\u2019s patients often act as if they are completely unaware of their condition. &nbsp;A former professor of mine once described a Wernicke\u2019s patient as sounding \u201clike a politician,\u201d and from watching a video of an interview with the patient, I agreed: I was impressed by his ability to confidently utter nonsense.</p>\n<p>The fact of these two forms of aphasia suggest that Broca\u2019s area and Wernicke\u2019s area have two very important and distinct roles in our ability to produce and understand language. And I find this fact strange to write about. Like object recognition, language comes naturally to us. As I write this, my intuitive feeling is that the work I am doing comes mainly in the ideas, plus making a few subtle stylistic decisions. I know from neuroscience that I would be unable to write this if I had significant damage to either region. Yet I am totally unconscious of the work they are doing for me.</p>\n<h1 id=\"2__Complex__specialized_wiring_within_regions\">2. Complex, specialized wiring within regions</h1>\n<p>\u201cWiring\u201d is a hard metaphor to avoid when talking about the brain, but it is also a potentially misleading one. People often talk about \u201celectrical signals\u201d in the brain, but unlike electrical signals in human technology, which involves movement of electrons between the atomics of the conductor, signals in the human brain involve movement of ions and small molecules across cell membranes and between cells.&nbsp;</p>\n<p>Furthermore, the first thing most people who know a little bit about neuroscience will think of when they hear the word \u201cwiring\u201d is axons and dendrites, the long skinny projections along which signals are transmitted from neuron to neuron. But it isn\u2019t just the layout of axons and dendrites that matters. Ion channels, and the structures that transport neurotransmitters across cell membranes, are also important.&nbsp;</p>\n<p>These can vary a lot at the synapse, the place where two neurons touch. For example, synapses vary in strength, that is to say, the strength of the one neuron\u2019s effect on the other neuron. Synapses can also be excitatory (activity in one leads increased activity in the other) or inhibitory (activity in one leads to decreased activity in the other). And this is just a couple of the ways synapses can vary; the details can be somewhat complicated, and I\u2019ll give one example of how the details can be complicated later.</p>\n<p>I say all this just to make clear what I mean when I talk about how the brain\u2019s \u201cwiring.\u201d By \u201cwiring,\u201d I mean all the features of the physical structures that connect neurons to each other and which are relevant for understanding how the brain works. I mean to all the things I\u2019ve mentioned above, and anything I may have omitted. It\u2019s important to have a word to talk about this wiring, because what (admittedly little) we understand about how the brain works we understand in terms of this wiring.</p>\n<p>For example, the nervous system actually first begins processing visual information in the retina (the part of the eye at the back where our light receptors are). This is done by what\u2019s known as the center-surround system: a patch of light receptors, when activated, excites one neuron, but nearby patches of light receptors, when activated, inhibit that same neuron (sometimes, the excitatory roles and inhibitory roles are reversed).</p>\n<p>The effect of this is that what the neurons are sensitive to is not light itself, but contrast. They\u2019re contrast detectors. And what allows them to detect contrast isn\u2019t anything magical, it\u2019s just the wiring, the way the neurons are connected together.</p>\n<p>This technique of getting neurons to serve specific functions based on how they are wired together shows up in more complicated ways in the brain itself. There\u2019s one line of evidence for specialization of brain regions that I saved for this section, because it also tells us about the details of how the brain is wired. That line of evidence is recordings from the brain using electrodes.</p>\n<p>For example, during the 50\u2019s David Hubel and Torsten Weisel did experiments where they paralyzed the eye muscles of animals, stuck electrodes in primary visual areas of the animals\u2019 brains, and then showed the animals various images to see which images would cause electrical signals in the animals\u2019 primary visual areas. It turned out that the main thing that causes electrical signals in the primary visual areas is lines.</p>\n<p>In particular, a given cell in the primary visual area will have a particular orientation of line which it responds to. It appears that the way these line-orientation detecting cells work is that they receive input from several contrast detecting cells which, themselves, correspond to regions of the retina that are themselves all in a line. A line in the right position and orientation will activate all of the contrast-detecting cells, which in turn activates the line-orientation detecting cell. A line in the right position but wrong orientation will activate only one or a few contrast-detecting cells, not enough to activate the line-orientation detecting cell.</p>\n<p>[If this is unclear, a diagram like the one on <a href=\"http://en.wikipedia.org/wiki/Receptive_field#Retinal_ganglion_cells\">Wikipedia</a> may be helpful, though Wikipedia's diagram may not be the best.]</p>\n<p>Another example of a trick the brain does with neural wiring is locating sounds using what are called \u201cinteraural time differences.\u201d The idea is this: there is a group of neurons that receives input from both ears, and specifically responds to <em>simultaneous</em> input from both ears. However, the axons running from the ears to the neurons in this group of cells vary in length, and therefore they vary in how long it takes them to get a signal from each ear.&nbsp;</p>\n<p>This means that which cells in this group respond to a sound depends on whether or not the sound reaches reaches the ears at the same time or at different times, and (if at different times) on how big the time difference is. If there\u2019s no difference, that means the sound came from directly ahead or behind (or above or below). A big difference, with the sound reaching the left ear first, indicates the sound came from the left. A big difference, with the sound reaching the right ear first, indicates the sound came from the left. Small differences indicate something in between.</p>\n<p>[A diagram might be helpful here too, but I'm not sure where to find a good one online.]</p>\n<p>I\u2019ve made a point to mention these bits of wiring because they\u2019re cases where neuroscientists have a clear understanding of how it is that a particular neuron is able to fire only in response to a particular kind of stimulus. Unfortunately, cases like this are relatively rare. In other cases, however, we at least know that particular neurons respond specifically to more complex stimuli, even though we don\u2019t know why. In rats, for example, there are cells in the hippocampus that activate only when the rat is in a particular location; apparently their purpose is to keep track of the rat\u2019s location.</p>\n<p>The visual system gives us some especially interesting cases of this sort. We know that the primary visual cortex sends information to other parts of the brain in two broadly-defined pathways, the dorsal pathway and the ventral pathway. The dorsal pathway appears to be responsible for processing information related to position and movement. Some cells in the dorsal pathway, for example, fire only when an animal sees an object moving in a particular direction.&nbsp;</p>\n<p>The most interesting cells of this sort that neuroscientists have found so far, though, are probably some of the cells in the medial temporal lobe, which is part of the ventral pathway. In one study (Quiroga et al. 2005), researchers took epileptic patients who had had electrodes implanted in their brains in order to locate the source of their epilepsy and showed them pictures of various people, objects, and landmarks. What the researchers found is that the neuron or small group of neurons a given electrode was reading from typically only responded to pictures of one person or thing.&nbsp;</p>\n<p>Furthermore, a particular electrode often got readings from very different pictures of a single person or thing, but not similar pictures of different people or things. In one notorious example, they found a neuron that they could only get to respond to either pictures of actress Halle Berry or the text \u201cHalle Berry.\u201d This included drawings of the actress, as well as pictures of her dressed as Catwoman (a role which she had recently performed at the time the study was performed), but not other drawings or other pictures of Catwoman.</p>\n<p>What\u2019s going on here? Based on what we know about the wiring of contrast-detectors and orientation-detectors the following conjecture seems highly likely: if we were to map out the brain completely and then follow the path along which visual information is transmitted, we would find that neurons gradually come to be wired together in more and more complex ways, to allow them to gradually become specific to more and more complex features of visual images. This, I think, is an extremely important inference.</p>\n<p>We know that experience can impact the way the brain is wired. In fact, some aspects of the brain\u2019s wiring seem to have evolved specifically to be able to change in response to experience (the main wiring of that sort we know about is called Hebbian synapses, but the details aren\u2019t important here). And it is actually somewhat difficult to draw a clear line between features of the brain that are innate and features of the brain that are the product of learning, because some fairly basic features of the brain depend on outside cues in order to develop.</p>\n<p>Here, though, I\u2019ll use the word \u201cinnate\u201d to refer to features of the brain that will develop given the overwhelming majority of the conditions animals of a given species actually develop under. Under that definition, a \u201cHalle Berry neuron\u201d is highly unlikely to be innate, because there isn\u2019t enough room in the brain to have a neuron specific to every person a person might possibly learn about. Such neural wiring is almost certainly the result of learning.</p>\n<p>But importantly, the underlying structure that makes such learning possible is probably at least somewhat complicated, and also specialized for that particular kind of learning. This is because such person-specific and object-specific neurons are not found in all regions of the brain, there must be something special about the medial temporal lobe that allows such learning to happen there.</p>\n<p>Similar reasoning applies to regions of the brain that we know even less about. For example, it seems likely that Broca\u2019s area and Wernicke\u2019s area both contain specialized wiring for handling language, though we have little idea how that wiring might perform its function. Given that humans seem to have a considerable innate knack for learning language (Pinker 2007), it again seems likely that the wiring is somewhat complicated.</p>\n<h1 id=\"3__On_some_problematic_comments_by_Eliezer\">3. On some problematic comments by Eliezer</h1>\n<p>I agree with Singularity Institute positions on a great deal. After all, I recently made my first donation to the Singularity Institute. But here, I want to point out some problematic neuroscience-related comments in Eliezer's debate with Robin Hanson:</p>\n<blockquote>\n<p>If you actually look at the genome, we\u2019ve got about 30,000 genes in here. Most of our 750 megabytes of DNA is repetitive and almost certainly junk, as best we understand it. And the brain is simply not a very complicated artifact by comparison to, say, Windows Vista. Now the complexity that it does have it uses a lot more effectively than Windows Vista does. It probably contains a number of design principles which Microsoft knows not. (And I\u2019m not saying it\u2019s that small because it\u2019s 750 megabytes, I\u2019m saying it\u2019s gotta be that small because at least 90% of the 750 megabytes is junk and there\u2019s only 30,000 genes for the whole body, never mind the brain.)</p>\n<p>That something that simple can be this powerful, and this hard to understand, is a shock. But if you look at the brain design, it\u2019s got 52 major areas on each side of the cerebral cortex, distinguishable by the local pattern, the tiles and so on, it just doesn\u2019t really look all that complicated. It\u2019s very powerful. It\u2019s very mysterious. What can say about it is that it probably involves 1,000 different deep, major, mathematical insights into the nature of intelligence that we need to comprehend before we can build it.</p>\n</blockquote>\n<p>Though this is not explicit, there appears to be an inference&nbsp;here that, in order for something so simple to be so powerful, it must incorporate many deep insights into intelligence, though we don\u2019t know what most of them are. There are&nbsp;several&nbsp;problems with this argument.</p>\n<div>\n<div>First of all, it is not true that the fact that that brain is divided into only 52 major areas is evidence that it is not very complex, because knowing about the complexity of its macroscopic organization tells us nothing about the complexity of its microscopic wiring. The brain consists of tens of billions of neurons, and a single neuron can make hundreds of synapses with other neurons. The details of how synapses are set up vary greatly. The fact is that under a microscope, the brain at least looks very complex.</div>\n<div><br></div>\n<div>The argument from the small size of the genome is more plausible, especially if Eliezer is thinking in terms of Kolmogorov complexity, which is based on the size of the smallest computer program needed to build something. However, it does not follow that if the genome is not very complex, the brain must not be very complex, because the brain may be built not just based on the genome, but also based on information from the outside environment. We have good reason to think this is how the brain is actually set up, not just in cases we would normally associate with learning and memory, but with some of the most basic and near-universal features of the brain. For example, in normal mammals, the neurons in the visual cortex are organized into \u201cocular dominance columns,\u201d but these fail to form if the animal is raised in darkness.</div>\n<div><br></div>\n<div>More importantly, there is no reason to think getting a lot of power out of a relatively simple design requires insights into the nature of intelligence itself. To use Eliezer\u2019s own example of Windows Vista: imagine if, for some reason, Microsoft decided that it was very important for the next generation of its operating system to be highly compressible. Microsoft tells this to its programmers, and they set about looking for ways to make an operating system do most of what the current version of Windows does while being more compressible. They end up doing a lot of things that are only applicable to their situation, and couldn\u2019t be used to make a much more powerful operating system. For example, they might look for ways to recycle pieces of code, and make particular pieces of code do as many different things in the program as possible.</div>\n<div><br></div>\n<div>In this case, would we say that they had discovered deep insights into how to build powerful operating systems? Well no. And there\u2019s reason to think that life on Earth uses similar tricks to get a lot of apparent complexity out of relatively simple genetic codes. Genes code for protein. In a phenomenon known as \u201calternative splicing,\u201d there may be several ways to combine the parts of a gene, allowing one gene to code for several proteins. And even a single, specific protein may perform several roles within an organism. A receptor protein, for example, may be plugged into different signaling cascades in different parts of an organism.</div>\n<div><br></div>\n<div>Eliezer's comments about the complexity of brain are only a small part of his arguments in the debate, but I worry that comments like these by people concerned with the future of Artificial Intelligence are harmful insofar as they may lead some people (particularly neuroscientists) to conclude AI-related futurism is a bunch of confusions based in ignorance. <em>I </em>don't think it is, but a neuroscientist taking the Hanson-Yudkowsky debate as an introduction to the issues could easily conclude that.</div>\n<div><br></div>\n<div>Of course, that's not the most important reason for people with an interest in AI to understand the basics of neuroscience. The most important reason is that understanding some neuroscience will help clarify your thinking about the rest of cognitive science.</div>\n</div>\n<p><em>References:</em></p>\n<p>Pinker, S. 2007. <em>The Language Instinct.</em> Harper Perennial Modern Classics.</p>\n<p>Quiroga, R. Q. et al. 2005. <a href=\"http://papers.klab.caltech.edu/175/1/519.pdf\">Invariant visual representation by single neurons in the human brain.</a> <em>Nature,</em> 435, 1102-1107.</p>\n</div>", "sections": [{"title": "1. Specialization of brain regions", "anchor": "1__Specialization_of_brain_regions", "level": 1}, {"title": "2. Complex, specialized wiring within regions", "anchor": "2__Complex__specialized_wiring_within_regions", "level": 1}, {"title": "3. On some problematic comments by Eliezer", "anchor": "3__On_some_problematic_comments_by_Eliezer", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "103 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 103, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XvmrwJsRMogXrYG3h"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-26T05:39:45.864Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Sorting Pebbles Into Correct Heaps", "slug": "seq-rerun-sorting-pebbles-into-correct-heaps", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:55.743Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7fwwpsRjBjoSqqxwt/seq-rerun-sorting-pebbles-into-correct-heaps", "pageUrlRelative": "/posts/7fwwpsRjBjoSqqxwt/seq-rerun-sorting-pebbles-into-correct-heaps", "linkUrl": "https://www.lesswrong.com/posts/7fwwpsRjBjoSqqxwt/seq-rerun-sorting-pebbles-into-correct-heaps", "postedAtFormatted": "Thursday, July 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Sorting%20Pebbles%20Into%20Correct%20Heaps&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Sorting%20Pebbles%20Into%20Correct%20Heaps%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7fwwpsRjBjoSqqxwt%2Fseq-rerun-sorting-pebbles-into-correct-heaps%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Sorting%20Pebbles%20Into%20Correct%20Heaps%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7fwwpsRjBjoSqqxwt%2Fseq-rerun-sorting-pebbles-into-correct-heaps", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7fwwpsRjBjoSqqxwt%2Fseq-rerun-sorting-pebbles-into-correct-heaps", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 163, "htmlBody": "<p>Today's post, <a href=\"/lw/sy/sorting_pebbles_into_correct_heaps/\">Sorting Pebbles Into Correct Heaps</a> was originally published on 10 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Sorting_Pebbles_Into_Correct_Heaps\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A parable about an imaginary society that don't understand what their values actually are.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/drx/seq_rerun_inseparably_right_or_joy_in_the_merely/\">Inseparably Right; or, Joy in the Merely Good</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7fwwpsRjBjoSqqxwt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 9.505751790996101e-07, "legacy": true, "legacyId": "17873", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mMBTPTjRbsrqbSkZE", "ToajTxpWSKNgPRx5v", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-26T07:20:05.081Z", "modifiedAt": null, "url": null, "title": "The Mere Cable Channel Addition Paradox", "slug": "the-mere-cable-channel-addition-paradox", "viewCount": null, "lastCommentedAt": "2018-08-13T18:54:15.109Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ghatanathoah", "createdAt": "2012-01-27T20:44:00.945Z", "isAdmin": false, "displayName": "Ghatanathoah"}, "userId": "CzhiZYDsQs87MM5yH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/prEZkHYawwnhzswyf/the-mere-cable-channel-addition-paradox", "pageUrlRelative": "/posts/prEZkHYawwnhzswyf/the-mere-cable-channel-addition-paradox", "linkUrl": "https://www.lesswrong.com/posts/prEZkHYawwnhzswyf/the-mere-cable-channel-addition-paradox", "postedAtFormatted": "Thursday, July 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Mere%20Cable%20Channel%20Addition%20Paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Mere%20Cable%20Channel%20Addition%20Paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FprEZkHYawwnhzswyf%2Fthe-mere-cable-channel-addition-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Mere%20Cable%20Channel%20Addition%20Paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FprEZkHYawwnhzswyf%2Fthe-mere-cable-channel-addition-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FprEZkHYawwnhzswyf%2Fthe-mere-cable-channel-addition-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3738, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\">The following is a dialogue intended to illustrate what I think may be a serious logical flaw in some of the conclusions drawn from the famous <a href=\"http://en.wikipedia.org/wiki/Mere_addition_paradox\">Mere Addition Paradox</a>.&nbsp;</p>\n<p class=\"MsoNormal\">EDIT:&nbsp; To make this clearer, the interpretation of the Mere Addition Paradox this post is intended to criticize is the belief that a world consisting of a large population full of lives barely worth living is the <em>optimal </em>world. That is, I am disagreeing with the idea that the best way for a society to use the resources available to it is to create as many lives barely worth living as possible.&nbsp; Several <a href=\"/lw/dso/the_mere_cable_channel_addition_paradox/73ie\">commenters</a> have <a href=\"/lw/dso/the_mere_cable_channel_addition_paradox/73op?context=3\">argued </a>that another interpretation of the Mere Addition Paradox is that a sufficiently large population with a lower quality of life will always be better than a smaller population with a higher quality of life, even if such a society is far from optimal.&nbsp; I agree that my argument does not necessarily refute this interpretation, but think the other interpretation is common enough that it is worth arguing against.</p>\n<p class=\"MsoNormal\">EDIT: On the <a href=\"/r/discussion/lw/dso/the_mere_cable_channel_addition_paradox/73er\">advice of some of the commenters </a>I have added a shorter summary of my argument in non-dialogue form at the end.&nbsp; Since it is shorter I do not think it summarizes my argument as completely as the dialogue, but feel free to read it instead if pressed for time.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>Hi, I'm with R&amp;P cable.<span>&nbsp; </span>We're selling premium cable packages to interested customers.<span>&nbsp; </span>We have two packages to start out with that we're sure you love.<span>&nbsp; </span>Package A+ offers a larger selection of basic cable channels and costs $50.<span>&nbsp; </span>Package B offers a larger variety of exotic channels for connoisseurs,<span>&nbsp; </span>it costs $100.<span>&nbsp; </span>If you buy package A+, however, you'll get a 50% discount on B.<span>&nbsp; </span></p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>That's very nice, but looking at the channel selection, I just don't think that it will provide me with enough utilons.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>: Utilons?<span>&nbsp; </span>What are those?</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>: They're the unit I use to measure the utility I get from something.<span>&nbsp; </span>I'm really good at shopping, so if I spend my money on the things I usually spend it on I usually get 1.5 utilons for every dollar I spend.<span>&nbsp; </span>Now, looking at your cable channels, I've calculated that I will get 10 utilons from buying Package A+ and 100 utilons from buying Package B.<span>&nbsp; </span>Obviously the total is 110, significantly less than the 150 utilons I'd get from spending $100 on other things.<span>&nbsp; </span>It's just not a good deal for me.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>You think so? <span>&nbsp;</span>Well it so happens that I've met people like you in the past and have managed to convince them.<span>&nbsp; </span>Let me tell you about something called the \"Mere Cable Channel Addition Paradox.\"</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>Alright, I've got time, make your case.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>Imagine that the government is going to give you $50.<span>&nbsp; </span>Sounds like a good thing, right?</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>It depends on where it gets the $50 from.<span>&nbsp; </span>What if it defunds a program I think is important?</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>Let's say that it would defund a program that you believe is entirely neutral.<span>&nbsp; </span>The harms the program causes are exactly outweighed by the benefits it brings, leaving a net utility of zero.</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>I can't think of any program like that, but I'll pretend one exists for the sake of the argument.<span>&nbsp; </span>Yes, defunding it and giving me $50 would be a good thing.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>Okay, now imagine the program's beneficiaries put up a stink, and demand the program be re-instituted.<span>&nbsp; </span>That would be bad for you, right?</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>Sure.<span>&nbsp; </span>I'd be out $50 that I could convert into 75 utilons.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>Now imagine that the CEO of R&amp;P Cable Company sleeps with an important senator and arranges a deal.<span>&nbsp; </span>You get the $50, but you have to spend it on Package A+.<span>&nbsp; </span>That would be better than not getting the money at all, right?</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>: Sure.<span>&nbsp; </span>10 utilons is better than zero.<span>&nbsp; </span>But getting to spend the $50 however I wanted would be best of all.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>That's not an option in this thought experiment.<span>&nbsp; </span>Now, imagine that after you use the money you received to buy Package A+, you find out that the 50% discount for Package B still applies.<span>&nbsp; </span>You can get it for $50.<span>&nbsp; </span>Good deal, right?</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>Again, sure.<span>&nbsp; </span>I'd get 100 utilons for $50. Normally I'd only get 75 utilons.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>Well, there you have it.<span>&nbsp; </span>By a <em>mere addition</em> I have demonstrated that a world where you have bought both Package A+ and Package B is better than one where you have neither.<span>&nbsp; </span>The only difference between the hypothetical world I imagined and the world we live in is that in one you are spending money on cable channels.<span>&nbsp; </span>A mere addition.<span>&nbsp; </span>Yet you have admitted that that world is better than this one.<span>&nbsp; </span>So what are you waiting for?<span>&nbsp; </span>Sign up for Package A+ and Package B!</p>\n<p class=\"MsoNormal\">And that's not all.<span>&nbsp; </span>I can keep adding cable packages to get the same result.<span>&nbsp; </span>The end result of my logic, which I think you'll agree is impeccable, is that you purchase Package Z, a package where you spend all the money other than that you need for bare subsistence on cable television packages.</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>That seems like a pretty repugnant conclusion.<span>&nbsp; </span></p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>It still follows from the logic.<span>&nbsp; </span>For every world where you are spending your money on whatever you have calculated generates the most utilons there exists another, better world where you are spending all your money on premium cable channels.</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>I think I found a flaw in your logic.<span>&nbsp; </span>You didn't perform a \"mere addition.\"<span>&nbsp; </span>The hypothetical world differs from ours in two ways, not one.<span>&nbsp; </span>Namely, in this world the government isn't giving me $50.<span>&nbsp; </span>So your world doesn't just differ from this one in terms of how many cable packages I've bought, it also differs in how much money I have to buy them.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>: So can I interest you in a special form of the package?<span>&nbsp; </span>This one is in the form of a legally binding pledge.&nbsp; You pledge that if you ever make an extra $50 in the future you will use it to buy Package A+.</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>No.<span>&nbsp; </span>In the scenario you describe the only reason buying Package A+ has any value is that it is impossible to get utility out of that money any other way.<span>&nbsp; </span>If I just get $50 for some reason it's more efficient for me to spend it normally.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>Are you sure?<span>&nbsp; </span>I've convinced a lot of people with my logic.</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>Like who?</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>Well, there were these two customers named Michael Huemer and Robin Hanson who both accepted my conclusion.<span>&nbsp; </span>They've both mortgaged their homes and started sending as much money to R&amp;P cable as they can.</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>There must be some others who haven't.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>Well, there was this guy named Derek Parfit who seemed disturbed by my conclusion, but couldn't refute it.<span>&nbsp; </span>The best he could do is mutter something about how the best things in his life would gradually be lost if he spent all his money on premium cable.<span>&nbsp; </span>I'm working on him though, I think I'll be able to bring him around eventually.</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>Funny you should mention Derek Parfit.<span>&nbsp; </span>It so happens that the flaw in your \"Mere Cable Channel Addition Paradox\" is exactly the same as the flaw in a famous philosophical argument he made, which he called the \"Mere Addition Paradox.\"</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>Really? Do tell?</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>Parfit posited a population he called \"A\" which had a moderately large population with large amounts of resources, giving them a very high level of utility per person.<span>&nbsp; </span>Then he added a second population, which was totally isolated from the other population.<span>&nbsp; </span>How they were isolated wasn't important, although Parfit suggested maybe they were on separate continents and can't sail across the ocean or something like that.<span>&nbsp; </span>These people don't have nearly as many resources per person as the other population, so each person's level of utility is lower (their lack of resources is the only reason they have lower utility).<span>&nbsp; </span>However, their lives are still just barely worth living.<span>&nbsp; </span>He called the two populations \"A+.\"</p>\n<p class=\"MsoNormal\">Parfit asked if \"A+\" was a better world than \"A.\"<span>&nbsp; </span>He thought it was, since the extra people were totally isolated from the original population they weren't hurting anyone over there by existing.<span>&nbsp; </span>And their lives were worth living.<span>&nbsp; </span>Follow me so far?</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>: I guess I can see the point.</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>: Next Parfit posited a population called \"B,\" which was the same as A+. except that the two populations had merged together.<span>&nbsp; </span>Maybe they got better at sailing across the ocean, it doesn't really matter how.<span>&nbsp; </span>The people share their resources.<span>&nbsp; </span>The result is that everyone in the original population had their utility lowered, while everyone in the second had it raised.<span>&nbsp; </span></p>\n<p class=\"MsoNormal\">Parfit asked if population \"B\" was better than \"A+\" and argued that it was because it had a greater level of equality and total utility.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>: I think I see where this is going.<span>&nbsp; </span>He's going to keep adding more people, isn't he?</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>Yep.<span>&nbsp; </span>He kept adding more and more people until he reached population \"Z,\" a vast population where everyone had so few resources that their lives were barely worth living.<span>&nbsp; </span>This, he argued, was a paradox, because he argued that most people would believe that Z is far worse than A, but he had made a convincing argument that it was better.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>Are you sure that sharing their resources like that would lower the standard of living for the original population?<span>&nbsp; </span>Wouldn't there be economies of scale and such that would allow them to provide more utility even with less resources per person?</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>: <a href=\"/lw/bwp/please_dont_fight_the_hypothetical/\">Please don't fight the hypothetical</a>.<span>&nbsp; </span>We're assuming that it would for the sake of the argument.</p>\n<p class=\"MsoNormal\">Now, Parfit argued that this argument led to the \"Repugnant Conclusion,\" the idea that the best sort of world is one with a large population with lives barely worth living.<span>&nbsp; </span>That confers on people a duty to reproduce as often as possible, even if doing so would lower the quality of their and everyone else's lives.</p>\n<p class=\"MsoNormal\">He claimed that the reason his argument showed this was that he had conducted \"mere addition.\"<span>&nbsp; </span>The populations in his paradox differed in no way other than their size. <span>&nbsp;</span>By merely adding more people he had made the world \"better,\" even if the level of utility per person plummetted.<span>&nbsp; </span>He claimed that \"For every population, A, with a high average level of utility there exists another, better population, B, with more people and a lower average level of utility.\"</p>\n<p class=\"MsoNormal\">Do you see the flaw in Parfit's argument?<span>&nbsp; </span></p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>No, and that kind of disturbs me.<span>&nbsp; </span>I have kids, and I agree that creating new people can add utility to the world.<span>&nbsp; </span>But it seems to me that it's also important to enhance the utility of the people who already exist.<span>&nbsp; </span></p>\n<p class=\"MsoNormal\"><strong>Alice</strong>: That's right.<span>&nbsp; </span>Normal morality tells us that creating new people with lives worth living and enhancing the utility of people that already exist are both good things to use resources on.<span>&nbsp; </span>Our common sense tells us that we should spend resources on both those things.<span>&nbsp; </span>The disturbing thing about the Mere Addition Paradox is that it seems at first glance to indicate that that's not true, that we should only devote resources to creating more people with barely worthwhile lives.<span>&nbsp; </span>I don't agree with that, of course.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>Neither do I. It seems to me that having a large number of worthwhile lives and a high average utility are <a href=\"/lw/cbn/alan_carter_on_the_complexity_of_value/\">both good things </a>and that we should try to increase them both, not just maximize one.</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>You're right, of course.<span>&nbsp; </span>But don't say \"having a<a href=\"/en.wikipedia.org/wiki/Average_and_total_utilitarianism\"> high average utility</a>.\"<span>&nbsp; </span>Say \"use resources to increase the utility of people who already exist.\"</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>What's the difference? They're the same thing, aren't they?</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>Not quite.<span>&nbsp; </span>There are other ways to increase average utility than enhancing the utility of existing people.<span>&nbsp; </span>You could kill all the depressed people, for instance.<span>&nbsp; </span>Plus, if there was a world where everyone was tortured 24 hours a day, you could increase average utility by creating some new people who are only tortured 23 hours a day.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>That's insane!<span>&nbsp; </span>Who could possibly be that literal-minded?</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>You'd be surprised.<span>&nbsp; </span>The point is, a better way to phrase it is \"use resources to increase the utility of people who already exist,\" not \"increase average utility.\"&nbsp; Of course, that <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">still leaves some stuff out</a>, like the fact that it's probably better to increase everyone's utility equally, rather than focus on just one person.<span>&nbsp; </span>But it doesn't lead to killing depressed people, or creating slightly less tortured people in a Hellworld.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>Okay, so what I'm trying to say is that resources should be used to create people, and to improve people's lives.<span>&nbsp; </span>Also equality is good. And that none of these things should completely eclipse the other, they're each too valuable to maximize just one.<span>&nbsp; </span>So a society that increases all of those values should be considered more efficient at generating value than a society that just maximizes one value.<span>&nbsp; </span>Now that we're done getting our terminology straight, will you tell me what Parfit's mistake was?</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>Population \"A\" and population \"A+\" <em>differ in two ways, not one</em>. Think about it.<span>&nbsp; </span>Parfit is clear that the extra people in \"A+\" do not harm the existing people when they are added.<span>&nbsp; </span>That means they do not use any of the original population's resources.<span>&nbsp; </span>So how do they manage to live lives worth living?<span>&nbsp; </span><em>How are they sustaining themselves</em>?</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>They must have their own resources.<span>&nbsp; </span>To use Parfit's example of continents separated by an ocean; <span>&nbsp;</span>each continent must have its own set of resources.</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>Exactly.<span>&nbsp; </span>So \"A+\" differs from \"A\" both in the size of its population, and the amount of resources it has access to.<span>&nbsp; </span>Parfit was not \"merely adding\" people to the population.<span>&nbsp; </span>He was also adding resources.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>: Aren't you the one who is fighting the hypothetical now?</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>I'm not fighting the hypothetical.<span>&nbsp; </span>Fighting the hypothetical consists of challenging the likelihood of the thought experiment happening, or trying to take another option than the ones presented.<span>&nbsp; </span>What I'm doing is challenging the logical coherence of the hypothetical.<span>&nbsp; </span>One of Parfit's unspoken premises is that you need <em>some</em> resources to live a life worth living, so by adding more worthwhile lives he's also implicitly adding resources.<span>&nbsp; </span>If he had just added some extra people to population A without giving them their own continent full of extra resources to live on then \"A+\" would be worse than \"A.\"</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>So the Mere Addition Paradox doesn't confer on us a positive obligation to have as many children as possible, because the amount of resources we have access to doesn't automatically grow with them.<span>&nbsp; </span>I get that.<span>&nbsp; </span>But doesn't it imply that as soon as we get some more resources we have a duty to add some more people whose lives are barely worth living?</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>: No.<span>&nbsp; </span>Adding lives barely worth living uses the extra resources more efficiently than leaving Parfit's second continent empty for all eternity.<span>&nbsp; </span>But, it's not the most efficient way.<span>&nbsp; </span>Not if you believe that creating new people and enhancing the utility of existing people are <em>both</em> important values.<span>&nbsp; </span></p>\n<p class=\"MsoNormal\">Let's take population \"A+\" again.<span>&nbsp; </span>Now imagine that instead of having a population of people with lives barely worth living, the second continent is inhabited by a smaller population with the same very high percentage of resources and utility per person as the population of the first continent.<span>&nbsp; </span>Call it \"A++. \" Would you say \"A++\" was better than \"A+?\"</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>Sure, definitely.<span>&nbsp; </span></p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>How about a world where the two continents exist, but the second one was never inhabited?<span>&nbsp; </span>The people of the first continent then discover the second one and use its resources to improve their level of utility.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>I'm less sure about that one, but I think it might be better than \"A+.\"</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>So what Parfit actually proved was: \"For every population, A, with a high average level of utility there exists another, better population, B, with more people, <em>access to more resources</em> and a lower average level of utility.\"</p>\n<p class=\"MsoNormal\">And I can add my own corollary to that:<span>&nbsp; </span>\"For every population, B, there exists another, better population, C, that has the <em>same access to resources</em> as B, but a <em>smaller population and higher average utility.</em>\"</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>: Okay, I get it.<span>&nbsp; </span>But how does this relate to my cable TV sales pitch?</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>Well, my current situation, where I'm spending my money on normal things is analogous to Parfit's population \"A.\"<span>&nbsp; </span>High utility, and very efficient conversion of resources into utility, but not as many resources.<span>&nbsp; </span>We're assuming, of course, that using resources to both create new people and improve the utility of existing people is more morally efficient than doing just one or the other.</p>\n<p class=\"MsoNormal\">The situation where the government gives me $50 to spend on Package A+ is analogous to Parfit's population A+.<span>&nbsp; </span>I have more resources and more utility.<span>&nbsp; </span>But the resources aren't being converted as efficiently as they could be.<span>&nbsp; </span></p>\n<p class=\"MsoNormal\">The situation where I take the 50% discount and buy Package B is equivalent to Parfit's population B.<span>&nbsp; </span>It's a better situation than A+, but not the most efficient way to use the money.</p>\n<p class=\"MsoNormal\">The situation where I get the $50 from the government to spend on whatever I want is equivalent to <em>my</em> population C.<span>&nbsp; </span>A world with more access to resources than A, but more efficient conversion of resources to utility than A+ or B.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>: So what would a world where the government kept the money be analogous to?</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>: A world where Parfit's second continent was never settled and remained uninhabited for all eternity, its resources never used by anyone.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>: I get it.<span>&nbsp; </span>So the Mere Addition Paradox doesn't prove what Parfit thought it did?<span>&nbsp; </span>We don't have any moral obligation to tile the universe with people whose lives are barely worth living?</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>:<span>&nbsp; </span>Nope, we don't.<span>&nbsp; </span>It's more morally efficient to use a large percentage of our resources to enhance the lives of those who already exist.</p>\n<p class=\"MsoNormal\"><strong>Bob</strong>:<span>&nbsp; </span>This sure has been a fun conversation.<span>&nbsp; </span>Would you like to buy a cable package from me?<span>&nbsp; </span>We have some great deals.</p>\n<p class=\"MsoNormal\"><strong>Alice</strong>: NO!<span>&nbsp; </span></p>\n<p class=\"MsoNormal\"><span>SUMMARY:</span></p>\n<p class=\"MsoNormal\"><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\">My argument is that Parfit&rsquo;s <a href=\"http://en.wikipedia.org/wiki/Mere_addition_paradox\">Mere Addition Paradox</a> doesn&rsquo;t prove what it seems to.<span>&nbsp; </span>The argument behind the Mere Addition Paradox is that you can make the world a better place by the &ldquo;mere addition&rdquo; of extra people, even if their lives are barely worth living.<span>&nbsp; </span>In other words : \"For every population, A, with a high average level of utility there exists another, better population, B, with more people and a lower average level of utility.\" This supposedly leads to the Repugnant Conclusion, the belief that a world full of people whose lives are barely worth living is better than a world with a smaller population where the people lead extremely fulfilled and happy lives.<span>&nbsp; </span></p>\n<p class=\"MsoNormal\">Parfit demonstrates this by moving from world A, consisting of a population full of people with lots of resources and high average utility, and moving to world A+.<span>&nbsp; </span>World A+ has an addition population of people who are isolated from the original population and not even aware of the other&rsquo;s existence. The extra people live lives just barely worth living.<span>&nbsp; </span>Parfit argues that A+ is a better world than A because everyone in it has lives worth living, and the additional people aren&rsquo;t hurting anyone by existing because they are isolated from the original population.</p>\n<p class=\"MsoNormal\">Parfit them moves from World A+ to World B, where the populations are merged and share resources.<span>&nbsp; </span>This lowers the standard of living for the original people and raises it for the newer people.<span>&nbsp; </span>Parfit argues that B must be better than A+, because it has higher total utility and equality. He then keeps adding people until he reaches Z, a world where everyones&rsquo; lives are barely worth living and the population is vast.<span>&nbsp; </span>He argues that this is a paradox because most people would agree that Z is not a desirable world compared to A.</p>\n<p class=\"MsoNormal\">I argue that the Mere Addition Paradox is a flawed argument because it does not just add people, it also adds resources.<span>&nbsp; </span>The fact that the extra people in A+ do not harm the original people of A by existing indicates that their population must have a decent amount of resources to live on, even if it is not as many per person as the population of A.<span>&nbsp; </span>For this reason what the Mere Addition Paradox proves is not that you can make the world better by adding extra people, but rather that you can make it better by adding extra people and <em>resources to support them</em>.<span>&nbsp; </span>I use a series of choices about purchasing cable television packages to illustrate this in concrete terms.</p>\n<p class=\"MsoNormal\">I further argue for a theory of population ethics that values both using resources to create lives worth living, and using resources to enhance the utility of already existing people, and considers the best sort of world to be one where neither of these two values totally dominate the other.<span>&nbsp; </span>By this ethical standard A+ might be better than A because it has more people and resources, even if the average level of utility is lower.<span>&nbsp; </span>However, a world with the same amount of resources as A+, but a lower population and the same, or higher average utility as A is better than A+.</p>\n<p class=\"MsoNormal\">The main unsatisfying thing about my argument is that while it avoids the Repugnant Conclusion in most cases, it might still lead to it, or something close to it, in situations where creating new people and getting new resources are, as <a href=\"/r/discussion/lw/dso/the_mere_cable_channel_addition_paradox/73dz\">one commenter noted</a>, a &ldquo;package deal.&rdquo;<span>&nbsp;&nbsp; </span>In other words, a situation where it is impossible to obtain new resources without creating some new people whose utility levels are below average.<span>&nbsp; </span>However, even in this case, my argument holds that the best world of all is one where it would be possible to obtain the resources without creating new people, or creating a smaller amount of people with higher utility.</p>\n<p class=\"MsoNormal\">In other words, the Mere Addition Paradox does not prove that: \"For every population, A, with a high average level of utility there exists another, better population, B, with more people and a lower average level of utility.\" Instead what the Mere Addition Paradox seems to demonstrate is that: \"For every population, A, with a high average level of utility there exists another, better population, B, with more people,<em> access to more resources</em> and a lower average level of utility.\"<span>&nbsp; </span>Furthermore, my own argument demonstrates that: \"For every population, B, there exists another, better population, C, which has the<em> same access to resources</em> as B, but a<em> smaller population and higher average utility<span style=\"font-family: &quot;Calibri&quot;,&quot;sans-serif&quot;;\">.</span></em>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"L3NcKBNTvQaFXwv9u": 1, "ZTRNmvQGgoYiymYnq": 1, "PDJ6KqJBRzvKPfuS3": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "prEZkHYawwnhzswyf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 109, "baseScore": 99, "extendedScore": null, "score": 0.000209, "legacy": true, "legacyId": "17880", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 106, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 147, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["s9hTXtAPn2ZEAWutr", "yRTvXPB6hmtF9y9Nn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-26T08:15:00.618Z", "modifiedAt": null, "url": null, "title": "What are the boundaries?", "slug": "what-are-the-boundaries", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:57.061Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stabilizer", "createdAt": "2011-12-02T09:36:56.841Z", "isAdmin": false, "displayName": "Stabilizer"}, "userId": "Qa3pLZx3o2TApyfgq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c677TqTHKthi82Xkp/what-are-the-boundaries", "pageUrlRelative": "/posts/c677TqTHKthi82Xkp/what-are-the-boundaries", "linkUrl": "https://www.lesswrong.com/posts/c677TqTHKthi82Xkp/what-are-the-boundaries", "postedAtFormatted": "Thursday, July 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20the%20boundaries%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20the%20boundaries%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc677TqTHKthi82Xkp%2Fwhat-are-the-boundaries%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20the%20boundaries%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc677TqTHKthi82Xkp%2Fwhat-are-the-boundaries", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc677TqTHKthi82Xkp%2Fwhat-are-the-boundaries", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 160, "htmlBody": "<p>Computer science and information theory were separate from physics. Not anymore. People realized that information had to be physical and this had profound consequences, especially in the form of quantum information/computation.</p>\n<p>Psychology and economics were separate. Not anymore. People realized that humans were the core of economic systems and their behaviors fundamentally shape the nature of economies, even at the largest scales. Note the rise of behavioral economics.</p>\n<p>Neuroscience and computer science were separate. Not anymore. People realized that thinking about the brain as a computer is probably the best possible abstraction to understand it.&nbsp;</p>\n<p>Reality exists. There are no intrinsic boundaries in reality. All fields of study are created by humans. But these divisions seem so natural that nobody realizes that the boundaries&nbsp;<em>have </em>to dissolve. The fields <em>have </em>to collide. And when we realize that--or finally have the language and ideas to meaningfully talk about it--we find out all of kinds of crazy, cool stuff.</p>\n<p>So: what collisions are we currently blind to?</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c677TqTHKthi82Xkp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -1, "extendedScore": null, "score": 9.506497394831821e-07, "legacy": true, "legacyId": "17883", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-26T12:09:26.914Z", "modifiedAt": null, "url": null, "title": "Many-worlds implies the future matters more", "slug": "many-worlds-implies-the-future-matters-more", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:31.746Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FYfXYENpGvzhj4yu4/many-worlds-implies-the-future-matters-more", "pageUrlRelative": "/posts/FYfXYENpGvzhj4yu4/many-worlds-implies-the-future-matters-more", "linkUrl": "https://www.lesswrong.com/posts/FYfXYENpGvzhj4yu4/many-worlds-implies-the-future-matters-more", "postedAtFormatted": "Thursday, July 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Many-worlds%20implies%20the%20future%20matters%20more&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMany-worlds%20implies%20the%20future%20matters%20more%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFYfXYENpGvzhj4yu4%2Fmany-worlds-implies-the-future-matters-more%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Many-worlds%20implies%20the%20future%20matters%20more%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFYfXYENpGvzhj4yu4%2Fmany-worlds-implies-the-future-matters-more", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFYfXYENpGvzhj4yu4%2Fmany-worlds-implies-the-future-matters-more", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 330, "htmlBody": "<p>If you believe the <a href=\"http://en.wikipedia.org/wiki/Many-worlds_interpretation\">MWI</a>&nbsp;[1] you should care about the future a lot more than the present. Imagine you're considering whether to take a break and eat some chocolate in an hour or in two. You'll get similar enjoyment out of both choices, so you might think it doesn't matter. But if every quantum event between one and two hours from now will branch the universe, and there are lots of such events, in two hours there would be hugely many more yous to experience your chocolate break than in only one hour. The MWI implies we should be willing to make substantial sacrifices in terms of current happiness for the benefit of our future selves. In other words, your preference for investing probably isn't strong enough.</p>\n<p>In trying to apply this to altruism you do need to be careful. Some charities are more like spending, in that their benefits are mostly in the present, while others are like investing. If I donate to the <a href=\"http://givewell.org/international/top-charities/AMF\">Against Malaria Foundation</a> to distribute mosquito nets, the main benefits are preventing current or near-future people from dying. There are probably some long term effects, like a stronger economy when you have fewer people sick, but they're not the goal or the main effect. On the other hand the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a>, a charity trying to prevent <a href=\"http://www.theatlantic.com/technology/archive/2012/03/were-underestimating-the-risk-of-human-extinction/253821/\">existential risk</a>, is much more like an investment in that nearly all its benefit (which is really hard to predict or quantify) goes to future people. <a href=\"http://www.jefftk.com/news/2012-06-05.html\">Metacharities</a> promoting <a href=\"http://www.reddit.com/r/smartgiving\">effective altruism</a>, like <a href=\"http://80000hours.org/\">80,000 hours</a>, <a href=\"http://www.givingwhatwecan.org/\">Giving What We Can</a>, and <a href=\"http://givewell.org/\">GiveWell</a>, are another sort of investment-like charity, influencing people's future giving. And then there's the option of straight up monetary investing now and donating later.</p>\n<p>If you accept the MWI you should be evaluating your altruistic options primarily on their future effects, with more emphasis on farther-future ones.</p>\n<p><small><em>I also posted this <a href=\"http://www.jefftk.com/news/2012-07-26.html\">on my blog</a></em></small></p>\n<p><br /> [1] Which I still don't know enough about to have an opinion on the truth of.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FYfXYENpGvzhj4yu4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": -10, "extendedScore": null, "score": 9.507623540479414e-07, "legacy": true, "legacyId": "17885", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-26T21:36:57.813Z", "modifiedAt": null, "url": null, "title": "Zero-sum conversion: a cute trick for decision problems", "slug": "zero-sum-conversion-a-cute-trick-for-decision-problems", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:57.574Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Manfred", "createdAt": "2010-10-12T17:53:38.361Z", "isAdmin": false, "displayName": "Manfred"}, "userId": "kmqiDCH9S5EGXxjGg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6zyfBnWahCPvEGR5N/zero-sum-conversion-a-cute-trick-for-decision-problems", "pageUrlRelative": "/posts/6zyfBnWahCPvEGR5N/zero-sum-conversion-a-cute-trick-for-decision-problems", "linkUrl": "https://www.lesswrong.com/posts/6zyfBnWahCPvEGR5N/zero-sum-conversion-a-cute-trick-for-decision-problems", "postedAtFormatted": "Thursday, July 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Zero-sum%20conversion%3A%20a%20cute%20trick%20for%20decision%20problems&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AZero-sum%20conversion%3A%20a%20cute%20trick%20for%20decision%20problems%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6zyfBnWahCPvEGR5N%2Fzero-sum-conversion-a-cute-trick-for-decision-problems%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Zero-sum%20conversion%3A%20a%20cute%20trick%20for%20decision%20problems%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6zyfBnWahCPvEGR5N%2Fzero-sum-conversion-a-cute-trick-for-decision-problems", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6zyfBnWahCPvEGR5N%2Fzero-sum-conversion-a-cute-trick-for-decision-problems", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 532, "htmlBody": "<p>A while ago, we were presented with <a href=\"/lw/3dy/has_anyone_solved_psykoshs_nonanthropic_problem/\">an interesting puzzle</a>, usually just called \"Psy-kosh's non-anthropic problem.\"&nbsp; This problem is not, as is made clear, an anthropic problem, but it generates a similar sort of confusion by having you cooperate with people who think like you, and you're unsure which of these people you are.</p>\n<p>In the linked post, cousin_it declares \"no points for UDT,\" which is why this post is not called a total solution, but a cute trick :)&nbsp; What I call zero-sum conversion is just a way to make the UDT calculations (that is, the things you do when calculating what the actual best choice is) seem obvious - which is good, since they're the ones that give you the right answer.&nbsp; This trick also makes the UDT math obvious on the absent-minded driver problem and the Sleeping Beauty problem (though that's trickier).</p>\n<p>The basic idea is to pretend that your decision is part of a zero-sum game against a non-anthropic, non-cooperating, generally non-confusing opponent.&nbsp; In order to do this, you must construct an imaginary opponent such that for every choice you could make, their expected utility for that choice is the negative, the opposite of your expected utility.&nbsp; Then you simply do the thing your opponent likes least, and it is equivalent to doing the thing you'll like best.</p>\n<p>&nbsp;</p>\n<p>Example in the case of the <a href=\"/lw/3dy/has_anyone_solved_psykoshs_nonanthropic_problem/\">non-anthropic problem</a> (yes, you should probably have that open in another tab):</p>\n<p>Your opponent here is the experimenter, who really dislikes giving money to charity (characterization isn't necessary, but it's fun).&nbsp; For every utilon that you, personally, would get from money going to charity when you say \"yea\" or \"nay,\" the experimenter gets a negative utilon.</p>\n<p>Proof that the experimenter's expected utilities are negative yours is trivial in this case, since the utilities are opposites for every possible outcome, including cases where you're not a decider.&nbsp; But things can be trickier in other problems, since <em>expected</em> utilities can be opposites without the utilities being exactly opposite for all outcomes.&nbsp; For example, what happens in the case where the participants in the non-anthropic problem get individual candybars instead of collective money to charity?</p>\n<p>Anyhow, now that we have our opponent whose expected utilities are the opposite of yours for every decision you make, you just have to make the decision that's worst for your opponent.&nbsp; This is pretty easy, since our opponent doesn't have to deal with any confusing stuff - they just flip a coin, which to them is an ordinary 50/50 situation, and then pay out based on your decision.&nbsp; So their expected value of \"yea\" is -550, while their expected value of \"nay\" is -700.</p>\n<p>This valuation already takes into account cooperation and all that stuff - it's simply correct.&nbsp; It's merely a coincidence that this seems like you didn't update the evidence of whether you're a decider or not.&nbsp; Though, now that you mention it, it's a general fact that in cooperate problems like this, you can construct a suitable opponent by just reversing your utility in all situations, giving you this \"updatelessness.\"</p>\n<p>&nbsp;</p>\n<p>Disclaimer: I haven't looked very hard for people writing up this trick before me.&nbsp; <a href=\"http://meteuphoric.wordpress.com/\">Katja</a> or someone quite possibly already has this on their blog somewhere.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6zyfBnWahCPvEGR5N", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 14, "extendedScore": null, "score": 9.510350629039102e-07, "legacy": true, "legacyId": "17886", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YZzoWGCJsoRBBbmQg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-27T04:08:38.508Z", "modifiedAt": null, "url": null, "title": "The Criminal Stupidity of Intelligent People ", "slug": "the-criminal-stupidity-of-intelligent-people", "viewCount": null, "lastCommentedAt": "2020-08-25T21:22:17.247Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fare", "createdAt": "2012-07-09T15:23:36.877Z", "isAdmin": false, "displayName": "fare"}, "userId": "GWcS6PkzgGeaM4An2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nrtLTvwuZnenp8TKx/the-criminal-stupidity-of-intelligent-people", "pageUrlRelative": "/posts/nrtLTvwuZnenp8TKx/the-criminal-stupidity-of-intelligent-people", "linkUrl": "https://www.lesswrong.com/posts/nrtLTvwuZnenp8TKx/the-criminal-stupidity-of-intelligent-people", "postedAtFormatted": "Friday, July 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Criminal%20Stupidity%20of%20Intelligent%20People%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Criminal%20Stupidity%20of%20Intelligent%20People%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnrtLTvwuZnenp8TKx%2Fthe-criminal-stupidity-of-intelligent-people%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Criminal%20Stupidity%20of%20Intelligent%20People%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnrtLTvwuZnenp8TKx%2Fthe-criminal-stupidity-of-intelligent-people", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnrtLTvwuZnenp8TKx%2Fthe-criminal-stupidity-of-intelligent-people", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2197, "htmlBody": "<p>What always fascinates me when I meet a group of very intelligent people is the very elaborate bullshit that they believe in. The naive theory of intelligence I first posited when I was a kid was that intelligence is a tool to avoid false beliefs and find the truth. Surrounded by mediocre minds who held obviously absurd beliefs not only without the ability to coherently argue why they held these beliefs, but without the ability of even understanding basic arguments about them, I believed as a child that the vast amount of superstition and false beliefs in the world was due to people both being stupid and following the authority of insufficiently intelligent teachers and leaders. More intelligent people and people following more intelligent authorities would thus automatically hold better beliefs and avoid disproven superstitions. However, as a grown up, I got the opportunity to actually meet and mingle with a whole lot of intelligent people, including many whom I readily admit are vastly more intelligent than I am. And then I had to find that my naive theory of intelligence didn't hold water: intelligent people were just as prone as less intelligent people to believing in obviously absurd superstitions. Only their superstitions would be much more complex, elaborate, rich, and far reaching than an inferior mind's superstitions.</p>\n<p>For instance, I remember a ride with an extremely intelligent and interesting man (RIP Bob Desmarets); he was describing his current pursuit, which struck me as a brilliant mathematical mind's version of mysticism: the difference was that instead of marveling at some trivial picture of an incarnate god like some lesser minds might have done, he was seeking some Ultimate Answer to the Universe in the branching structures of ever more complex algebras of numbers, real numbers, complex numbers, quaternions, octonions, and beyond, in ever higher dimensions (notably in relation to super-string theories). I have no doubt that there <em>is</em> something deep, and probably enlightening and even useful in such theories, and I readily disqualify myself as to the ability to judge the contributions that my friend made to the topic from a technical point of view; no doubt they were brilliant in one way or another. Yet, the way he was talking about this topic immediately triggered the \"crackpot\" flag; he was looking there for much more than could possibly be found, and anyone (like me) capable of acknowledging being too stupid to fathom the Full Glory of these number structures yet able to find some meaning in life could have told that no, this topic doesn't hold key to The Ultimate Source of All Meaning in Life. Bob's intellectual quest, as exaggeratedly exalted as it might have been, and as interesting as it was to his own exceptional mind, was on the grand scale of things but some modestly useful research venue at best, and an inoffensive pastime at worst. Perhaps Bob could conceivably used his vast intellect towards pursuits more useful to you and I; but we didn't own his mind, and we have no claims to lay on the wonders he could have created but failed to by putting his mind into one quest rather than another. First, Do No Harm. Bob didn't harm any one, and his ideas certainly contained no hint of any harm to be done to anyone.</p>\n<p>Unhappily, that is not always the case of every intelligent man's fantasies. Let's consider a discussion I was having recently, that prompted this article. Last week, I joined a dinner-discussion with a <a href=\"/\">lesswrong</a> meetup group: radical believers in rationality and its power to improve life in general and one's own life in particular. As you can imagine, the attendance was largely, though not exclusively, composed of male computer geeks. But then again, any club that accepts me as a member will probably be biased that way: birds of the feather flock together. No doubt, there are plenty of meetup groups with the opposite bias, gathering desperately non-geeky females to the almost exclusion of males. Anyway, the theme of the dinner was \"optimal philanthropy\", or how to give time and money to charities in a way that maximizes the positive impact of your giving. So far, so good.</p>\n<p>But then, I found myself in a most disturbing private side conversation with the organizer, <a href=\"http://www.jefftk.com/\">Jeff Kaufman</a> (a colleague, I later found out), someone I strongly suspect of being in many ways saner and more intelligent than I am. While discussing utilitarian ways of evaluating charitable action, he at some point mentioned some quite intelligent acquaintance of his who believed that morality was about minimizing the suffering of living beings; from there, that acquaintance logically concluded that wiping out all life on earth with sufficient nuclear bombs (or with grey goo) in a surprise simultaneous attack would be the best possible way to optimize the world, though one would have to make triple sure of involving enough destructive power that not one single strand of life should survive or else the suffering would go on and the destruction would have been just gratuitous suffering. We all seemed to agree that this was an absurd and criminal idea, and that we should be glad the guy, brilliant as he may be, doesn't remotely have the ability to implement his crazy scheme; we shuddered though at the idea of a future super-human AI having this ability and being convinced of such theories.</p>\n<p>That was not the disturbing part though. What tipped me off was when Jeff, taking the \"opposite\" stance of \"happiness maximization\" to the discussed acquaintance's \"suffering minimization\", seriously defended the concept of <a href=\"http://wiki.lesswrong.com/wiki/Wireheading\">wireheading</a> as a way that happiness may be maximized in the future: putting humans into vats where the pleasure centers of their brains will be constantly stimulated, possibly using force. Or perhaps instead of humans, using rats, or ants, or some brain cell cultures  or perhaps nano-electronic simulations of such electro-chemical stimulations; in the latter cases, biological humans, being less-efficient forms of happiness substrate, would be done away with or at least not renewed as embodiments of the Holy Happiness to be maximized. He even wrote at least two blog posts on this theme: <a href=\"/r/discussion/lw/dd0/hedonic_vs_preference_utilitarianism_in_the/\">hedonic vs preference utilitarianism in the Context of Wireheading</a>, and <a href=\"/lw/di1/value_of_a_computational_process/\">Value of a Computational Process</a>. In the former, he admits to some doubts, but concludes that <cite>the ways a value system grounded on happiness differ from my intuitions are problems with my intutions.</cite></p>\n<p>I expect that most people would, and rightfully so, find Jeff's ideas as well as his acquaintance's ideas to be ridiculous and absurd on their face; they would judge any attempt to use force to implement them as criminal, and they would consider their fantasied implemention to be the worst of possible mass murders. Of course, I also expect that most people would be incapable of arguing their case rationally against Jeff, who is much more intelligent, educated and knowledgeable in these issues than they are. And yet, though most of them would have to admit their lack of understanding and their absence of a rational response to his arguments, they'd be completely right in rejecting his conclusion and in refusing to hear his arguments, for he is indeed the sorely mistaken one, despite his vast intellectual advantages.</p>\n<p>I wilfully defer any detailed rational refutation of Jeff's idea to <a href=\"http://fare.livejournal.com/168562.html\">some future article</a> (can <em>you</em> without reading mine write a valuable one?). In this post, I rather want to address the meta-point of how to address the seemingly crazy ideas of our intellectual superiors. First, I will invoke the \"conservative\" principle (as I'll call it), well defended by Hayek (who is <a href=\"http://www.lewrockwell.com/orig6/hayek1.html\">not a conservative</a>): we must often reject the well-argued ideas of intelligent people, sometimes more intelligent than we are, sometimes without giving them a detailed hearing, and instead stand by our intuitions, traditions and secular rules, that are the stable fruit of millenia of evolution. We should not lightly reject those rules, certainly not without a clear testable understanding of why they were valid where they are known to have worked, and why they would cease to be in another context. Second, we should not hesitate to use proxy in an eristic argument: if we are to bow to the superior intellect of our better, it should not be without having pitted said presumed intellects against each other in a fair debate to find out if indeed there is a better whose superior arguments can convince the others or reveal their error. Last but not least, beyond mere conservatism or debate, mine is the Libertarian point: there is Universal Law, that everyone must respect, whereby peace between humans is possible inasmuch and only inasmuch as they don't initiate violence against other persons and their property. And as I have argued in another previous essay  (<a href=\"http://fare.tunes.org/liberty/hardscrapple.html\">hardscrapple</a>), this generalizes to maintaining peace between sentient beings of all levels of intelligence, including any future AI that Jeff may be prone to consider. Whatever the one's prevailing or dissenting opinions, the initiation of force is never to be allowed as a means to further any ends. Rather than doubt his intuition, Jeff should have been tipped that his theory was wrong and taken out of context by the very fact that it advocates or condones massive violation of this Universal Law. Criminal urges, mass-criminal at that, are a strong stench that should alert anyone that some ideas have gone astray, even when it might not be immediately obvious where exactly they started parting from the path of sanity.</p>\n<p>Now, you might ask, it is good and well to poke fun at the crazy ideas that some otherwise intelligent people may hold; it may even allow one to wallow in a somewhat justified sense of intellectual superiority over people who otherwise are actually and objectively so one's intellectual superiors. But is there a deeper point? Is it relevant what crazy ideas intellectuals hold, whether inoffensive or criminal? Sadly, it is. As John McCarthy put it, \"Soccer riots kill at most tens. Intellectuals' ideological riots sometimes kill millions.\" Jeff's particular crazy idea may be mostly harmless: the criminal raptures of the overintelligent nerd, that are so elaborate as to be unfathomable to 99.9% of the population, are unlikely to ever spread to enough of the power elite to be implemented. That is, unless by some exceptional circumstance there is a short and brutal transition to power by some overfriendly AI programmed to follow such an idea. On the other hand, the criminal raptures of a majority of the more mediocre intellectual elite, when they further possess simple variants that can intoxicate the ignorant and stupid masses, are not just theoretically able to lead to mass murder, but have historically been the source of all large-scale mass murders so far; and these mass murders can be counted in  <a href=\"http://www.hawaii.edu/powerkills/\">hundreds of millions</a>, over the XXth century only, just for Socialism. Nationalism, Islamism and Social-democracy (the attenuated strand of socialism that now reigns in Western \"Democracies\") count their victims in millions only. And every time, the most well-meaning of intellectuals build and spread the ideologies of these mass-murders. A little initial conceptual mistake, properly amplified, can do that.</p>\n<p>And so I am reminded of the meetings of some communist cells that I attended out of curiosity when I was in high-school. Indeed, trotskyites are very openly recruiting in \"good\" French high-schools. It was amazing the kind of non-sensical crap that these obviously above-average adolescent could repeat. \"The morale of the workers is low.\" Whoa. Or \"The petite-bourgeoisie\" is plotting this or that. Apparently, grossly cut social classes spanning millions of individuals act as one man, either afflicted with depression or making machiavelian plans. Not that any of them knew much of either salaried workers or entrepreneurs but through one-sided socialist literature. If you think that the nonsense of the intellectual elite is inoffensive, consider what happens when some of them actually act on those nonsensical beliefs: you get terrorists who kill tens of people; when they lead ignorant masses, they end up killing millions of people in extermination camps or plain massacres. And when they take control of entire universities, and train generations of scholars, who teach generations of bureaucrats, politicians, journalists, then you suddenly find that all politicians agree on slowly implementing the same totalitarian agenda, one way or another.</p>\n<p>If you think that control of universities by left-wing ideologists is just a French thing, consider how for instance, America just elected a president whose <a href=\"http://www.infowars.com/obama-mentor-wanted-americans-put-in-re-education-camps/\">mentor</a> and <a href=\"http://www.salon.com/2009/10/07/ayers_2/\">ghostwriter</a> was the chief of a terrorist group made of Ivy League educated intellectuals, whose overriding concern about the country they claimed to rule was how to slaughter ten percent of its population in concentration camps. And then consider that the policies of this president's \"right wing\" opponent are indistinguishable from the policies of said president. The violent revolution has given way to the slow replacement of the elite, towards the same totalitarian ideals, coming to you slowly but relentlessly rather than through a single mass criminal event. Welcome to a world where the crazy ideas of intelligent people are imposed by force, cunning and superior organization upon a mass of less intelligent yet less crazy people.</p>\n<p>Ideas have consequences. That's why everyone <a href=\"http://fare.tunes.org/liberty/library/pwni.html\">Needs Philosophy</a>.</p>\n<p>Crossposted from my livejournal: <a href=\"http://fare.livejournal.com/168376.html\">http://fare.livejournal.com/168376.html</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nrtLTvwuZnenp8TKx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": -23, "extendedScore": null, "score": -5.4e-05, "legacy": true, "legacyId": "17888", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 52, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["h9DMcvqhmTeF2MWbd", "4XffsAd4GgvHS7zyD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-27T05:49:14.473Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Moral Error and Moral Disagreement", "slug": "seq-rerun-moral-error-and-moral-disagreement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:57.641Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j5QLY7rx5ZXxRE7K6/seq-rerun-moral-error-and-moral-disagreement", "pageUrlRelative": "/posts/j5QLY7rx5ZXxRE7K6/seq-rerun-moral-error-and-moral-disagreement", "linkUrl": "https://www.lesswrong.com/posts/j5QLY7rx5ZXxRE7K6/seq-rerun-moral-error-and-moral-disagreement", "postedAtFormatted": "Friday, July 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Moral%20Error%20and%20Moral%20Disagreement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Moral%20Error%20and%20Moral%20Disagreement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj5QLY7rx5ZXxRE7K6%2Fseq-rerun-moral-error-and-moral-disagreement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Moral%20Error%20and%20Moral%20Disagreement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj5QLY7rx5ZXxRE7K6%2Fseq-rerun-moral-error-and-moral-disagreement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj5QLY7rx5ZXxRE7K6%2Fseq-rerun-moral-error-and-moral-disagreement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p>Today's post, <a href=\"/lw/sz/moral_error_and_moral_disagreement/\">Moral Error and Moral Disagreement</a> was originally published on 10 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Moral_Error_and_Moral_Disagreement\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>How can you make errors about morality?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dsh/seq_rerun_sorting_pebbles_into_correct_heaps/\">Sorting Pebbles Into Correct Heaps</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j5QLY7rx5ZXxRE7K6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.51271729945822e-07, "legacy": true, "legacyId": "17889", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BkkwXtaTf5LvbA6HB", "7fwwpsRjBjoSqqxwt", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-27T14:24:44.743Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Madison, NYC, Ohio, Washington DC", "slug": "weekly-lw-meetups-madison-nyc-ohio-washington-dc", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jwpgFy9XXb2NLPfFq/weekly-lw-meetups-madison-nyc-ohio-washington-dc", "pageUrlRelative": "/posts/jwpgFy9XXb2NLPfFq/weekly-lw-meetups-madison-nyc-ohio-washington-dc", "linkUrl": "https://www.lesswrong.com/posts/jwpgFy9XXb2NLPfFq/weekly-lw-meetups-madison-nyc-ohio-washington-dc", "postedAtFormatted": "Friday, July 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Madison%2C%20NYC%2C%20Ohio%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Madison%2C%20NYC%2C%20Ohio%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjwpgFy9XXb2NLPfFq%2Fweekly-lw-meetups-madison-nyc-ohio-washington-dc%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Madison%2C%20NYC%2C%20Ohio%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjwpgFy9XXb2NLPfFq%2Fweekly-lw-meetups-madison-nyc-ohio-washington-dc", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjwpgFy9XXb2NLPfFq%2Fweekly-lw-meetups-madison-nyc-ohio-washington-dc", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 394, "htmlBody": "<p><strong>This summary was posted to LW Main on July 20th, and has now been moved to discussion.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/br\">Washington DC Calibration Games meetup:&nbsp;<span class=\"date\">22 July 2012 03:00AM</span></a></li>\n<li><a href=\"/meetups/bz\">Madison: Rough Numbers:&nbsp;<span class=\"date\">22 July 2012 07:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bh\">(NYC) A Game of Nomic:&nbsp;<span class=\"date\">21 July 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/bv\">Monthly Ohio meetup:&nbsp;<span class=\"date\">22 July 2012 04:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong>&nbsp;</strong><strong></strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jwpgFy9XXb2NLPfFq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.515196749516314e-07, "legacy": true, "legacyId": "17773", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-27T17:45:13.390Z", "modifiedAt": null, "url": null, "title": "Is Politics the Mindkiller? An Inconclusive Test", "slug": "is-politics-the-mindkiller-an-inconclusive-test", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:08.560Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OrphanWilde", "createdAt": "2012-06-21T18:24:36.749Z", "isAdmin": false, "displayName": "OrphanWilde"}, "userId": "cdW87AS6fdK2sywL2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/THWYgvqTdyHkmpFgE/is-politics-the-mindkiller-an-inconclusive-test", "pageUrlRelative": "/posts/THWYgvqTdyHkmpFgE/is-politics-the-mindkiller-an-inconclusive-test", "linkUrl": "https://www.lesswrong.com/posts/THWYgvqTdyHkmpFgE/is-politics-the-mindkiller-an-inconclusive-test", "postedAtFormatted": "Friday, July 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20Politics%20the%20Mindkiller%3F%20An%20Inconclusive%20Test&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20Politics%20the%20Mindkiller%3F%20An%20Inconclusive%20Test%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTHWYgvqTdyHkmpFgE%2Fis-politics-the-mindkiller-an-inconclusive-test%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20Politics%20the%20Mindkiller%3F%20An%20Inconclusive%20Test%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTHWYgvqTdyHkmpFgE%2Fis-politics-the-mindkiller-an-inconclusive-test", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTHWYgvqTdyHkmpFgE%2Fis-politics-the-mindkiller-an-inconclusive-test", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 463, "htmlBody": "<p>Or is the convention against discussing politics here silly?</p>\n<p>I propose a test. &nbsp;I'm going to try to lay down some rules on voting on comments for the test here (not that I can force anybody to abide by them):</p>\n<p>1.) Top-level comments should introduce arguments (or ridicule me and/or this test); responses should be responses to those arguments.</p>\n<p>2.) Upvote and downvote based on whether or not you find an argument convincing in the context in which it was raised. &nbsp;This means if it's a good argument against the argument it is responding to, not whether or not there's a good/obvious counterargument to it; if you have a good counterargument, raise it. &nbsp;If it's a convincing argument, and the counterargument is also convincing, upvote both. &nbsp;If both arguments are unconvincing, downvote both.</p>\n<p>3.) Try not to downvote particular comments excessively, if they're legitimate lines of argument. &nbsp;A faulty line of argument provides opportunity for rebuttal, and so for our test has value even then; that is, I want some faulty lines of argument here. &nbsp;If you disagree, please downvote me, instead of the faulty comments, because this post is what you want less of, not those comments. &nbsp;This necessarily implies, for balance, that we not excessively upvote comments. &nbsp;I'd suggest fairly arbitrary limits of 3/-3?</p>\n<p><strong>Edit</strong>: 4.) A single argument per comment would be ideal; as MixedNuts points out <a href=\"/r/discussion/lw/dsv/is_politics_the_mindkiller_an_inconclusive_test/73yp\">here</a>, it's otherwise hard to distinguish between one good and one bad argument, which makes the upvoting/downvoting difficult to evaluate. &nbsp;(My apologies about missing this, folks.)</p>\n<p>I'm going to try really hard not to get personally involved, except to lay down a leading comment posing an argument against abortion, a position I don't hold, for the record. &nbsp;The core of the argument isn't disingenuous, and I hold that this argument is true, it just doesn't lead to my opposing abortion. &nbsp;I do not hold the moral axiom by which I extend the basic argument to argue against abortion, however; I'm playing the devil's advocate to try to help me from getting sucked into the argument while providing an initial point of discussion.</p>\n<p>Which leads me to the next point: If you see a hole in an argument, even if it's an argument for a perspective you agree with, poke through it. &nbsp;The goal is to see whether we can have a constructive political argument here.</p>\n<p>The fact that this is a test, and known to be a test, means this isn't a blind study. &nbsp;Uh, try to act as if you're not being tested?</p>\n<p>After it's gone on a little while, if this post hasn't been hopelessly downvoted and ridiculed (and thus the premise and test discarded as undesirable to begin with), we can put up a poll to see whether people found the political debates helpful, not helpful, and so on.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "THWYgvqTdyHkmpFgE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 19, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "17887", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 277, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-27T19:22:43.416Z", "modifiedAt": null, "url": null, "title": "Notes on the Psychology of Power", "slug": "notes-on-the-psychology-of-power", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:37.344Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ft2Cm9tWtcLNFLrMw/notes-on-the-psychology-of-power", "pageUrlRelative": "/posts/Ft2Cm9tWtcLNFLrMw/notes-on-the-psychology-of-power", "linkUrl": "https://www.lesswrong.com/posts/Ft2Cm9tWtcLNFLrMw/notes-on-the-psychology-of-power", "postedAtFormatted": "Friday, July 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Notes%20on%20the%20Psychology%20of%20Power&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANotes%20on%20the%20Psychology%20of%20Power%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFt2Cm9tWtcLNFLrMw%2Fnotes-on-the-psychology-of-power%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Notes%20on%20the%20Psychology%20of%20Power%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFt2Cm9tWtcLNFLrMw%2Fnotes-on-the-psychology-of-power", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFt2Cm9tWtcLNFLrMw%2Fnotes-on-the-psychology-of-power", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 12134, "htmlBody": "<p>Luke/SI asked me to look into what the academic literature might have to say about people in positions of power. This is a summary of some of the recent psychology results.</p>\n<p>The powerful or elite are: fast-planning abstract thinkers who take action (1) in order to pursue single/minimal objectives, are in favor of strict rules for their stereotyped out-group underlings (2) but are rationalizing (3) &amp; hypocritical when it serves their interests (4), especially when they feel secure in their power. They break social norms (5, 6) or ignore context (1) which turns out to be worsened by disclosure of conflicts of interest (7), and lie fluently without mental or physiological stress (6).</p>\n<p>What are powerful members good for? They can help in shifting among equilibria: solving coordination problems or inducing contributions towards public goods (8), and their abstracted Far perspective can be better than the concrete Near of the weak (9).</p>\n<ol>\n<li>Galinsky et al 2003; Guinote, 2007; Lammers et al 2008; Smith &amp; Bargh, 2008</li>\n<li>Eyal &amp; Liberman</li>\n<li>Rustichini &amp; Villeval 2012</li>\n<li>Lammers et al 2010</li>\n<li>Kleef et al 2011 </li>\n<li>Carney et al 2010</li>\n<li>Cain et al 2005; Cain et al 2011</li>\n<li>Eckel et al 2010</li>\n<li>Slabu et al; Smith &amp; Trope 2006; Smith et al 2008</li>\n</ol>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<p>These benefits may not exceed the costs (is inducing contributions all that useful with improved market mechanisms like <a href=\"https://en.wikipedia.org/wiki/Assurance_contract\">assurance contracts</a> - made increasingly famous thanks to <a href=\"http://en.wikipedia.org/wiki/Kickstarter\">Kickstarter</a>?) Now, to forestall objections from someone like Robin Hanson that these traits - if negative - can be ameliorated by improved technology and organizations and the rest just represents our egalitarian forager prejudice against the elites and corporations who gave us the wealthy modern world, I would point out that these traits look like they would be quite effective at maximizing utility and some selected for in future settings&hellip;</p>\n<p>(Additional cautions include that, in order to control for all sorts of confounds, these are generally small WEIRD samples in laboratory or university settings involving small-scale power shifts, priming, or other cues; as such, <a href=\"http://www.gwern.net/DNB%20FAQ#flaws-in-mainstream-science-and-psychology\">all the usual criticisms apply</a>.)</p>\n<h1 id=\"notes\"><a href=\"#TOC\"><span class=\"header-section-number\">1</span> Notes</a></h1>\n<ol style=\"list-style-type: decimal\">\n<li>key phrases: &ldquo;moral hypocrisy&rdquo;; &ldquo;construal level theory&rdquo; (Near/Far) (<a href=\"http://www.overcomingbias.com/2010/06/near-far-summary.html\">Hanson&rsquo;s visual summary</a>)</li>\n<li>check or blacklist any paper related to <a href=\"/!Wikipedia\">Diederik Stapel</a>! (eg. <a href=\"http://www.nytimes.com/2011/11/03/health/research/noted-dutch-psychologist-stapel-accused-of-research-fraud.html\">&ldquo;Fraud Case Seen as a Red Flag for Psychology Research&rdquo;</a>)</li>\n</ol>\n<h1 id=\"references\"><a href=\"#TOC\"><span class=\"header-section-number\">2</span> References</a></h1>\n<p><a href=\"http://peezer.squarespace.com/storage/moral-reasoning/trust-and-cheating/Psychological%20Science-2010-Lammers-737-44.pdf\">&ldquo;Power increases hypocrisy: Moralizing in reasoning, immorality in behavior&rdquo;</a>, Lammers et al 2010; warning, Stapel! But Lammers says committee cleared this paper.</p>\n<blockquote>\n<p>In five studies, we explored whether power increases moral hypocrisy (i.e., imposing strict moral standards on other people but practicing less strict moral behavior oneself). In Experiment 1, compared with the powerless, the powerful condemned other people&rsquo;s cheating more, but also cheated more themselves. In Experiments 2 through 4, the powerful were more strict in judging other people&rsquo;s moral transgressions than in judging their own transgressions. A final study found that the effect of power on moral hypocrisy depends on the legitimacy of the power: When power was illegitimate, the moral-hypocrisy effect was reversed, with the illegitimately powerful becoming stricter in judging their own behavior than in judging other people&rsquo;s behavior. This pattern, which might be dubbed hypercrisy, was also found among low-power participants in Experiments 3 and 4. We discuss how patterns of hypocrisy and hypercrisy among the powerful and powerless can help perpetuate social inequality.</p>\n<p>&hellip;feelings of power reduce sensitivity to social disapproval (Emerson, 1962; Thibaut &amp; Kelley, 1959), thus reducing the grip of social norms and standards on power holders&rsquo; behavior (Galinsky et al., 2008). As a result, even very strong norms, such as those regulating sexual behavior or compassion, are often ignored by the powerful (Bargh, Raymond, Pryor, &amp; Strack, 1995; Van Kleef et al., 2008).</p>\n<ul>\n<li>Emerson, R.M. (1962). Power-dependence relations. American Sociological Review, 27, 31&ndash;41</li>\n<li>Thibaut, J.W., &amp; Kelley, H.H. (1959). The social psychology of groups. New York: Wiley &amp; Sons</li>\n<li>Galinsky, A.D., Magee, J.C., Gruenfeld, D.H, Whitson, J., &amp; Liljenquist, K.A. (2008). Social power reduces the strength of the situation: Implications for creativity, conformity, and dissonance. Journal of Personality and Social Psychology, 95, 1450&ndash;1466</li>\n</ul>\n<p>Powerful people who feel that their position is illegitimate are less inclined to assertively take what they want (Lammers, Galinsky, Gordijn, &amp; Otten, 2008) and at the same time are less inclined to judge others for doing so, compared with people who feel their power is deserved (Chaurand &amp; Brauer, 2008). Therefore, in our final study, we independently manipulated power and its legitimacy to test whether legitimacy crucially moderates the effect of power on hypocrisy.</p>\n<ul>\n<li>Lammers, J., &amp; Stapel, D.A. (2009). How power influences moral thinking. Journal of Personality and Social Psychology, 97, 279&ndash;289</li>\n<li>Chaurand, N., &amp; Brauer, M. (2008). What determines social control? People&rsquo;s reactions to counternormative behaviors in urban environments. Journal of Applied Social Psychology, 38, 1689&ndash;1715</li>\n</ul>\n</blockquote>\n<p><a href=\"ftp://ftp.gate.cnrs.fr/RePEc/2012/1216.pdf\">&ldquo;Moral Hypocrisy, Power and Social Preferences&rdquo;</a>, Rustichini &amp; Villeval 2012:</p>\n<blockquote>\n<p>We show with a laboratory experiment that individuals adjust their moral principles to the situation and to their actions, just as much as they adjust their actions to their principles. We first elicit the individuals&rsquo; principles regarding the fairness and unfairness of allocations in three different scenarios (a Dictator game, an Ultimatum game, and a Trust game). One week later, the same individuals are invited to play those same games with monetary compensation. Finally in the same session we elicit again their principles regarding the fairness and unfairness of allocations in the same three scenarios.</p>\n<p>Our results show that individuals adjust abstract norms to fit the game, their role and the choices they made. First, norms that appear abstract and universal take into account the bargaining power of the two sides. The strong side bends the norm in its favor and the weak side agrees: Stated fairness is a compromise with power. Second, in most situations, individuals adjust the range of fair shares after playing the game for real money compared with their initial statement. Third, the discrepancy between hypothetical and real behavior is larger in games where real choices have no strategic consequence (Dictator game and second mover in Trust game) than in those where they do (Ultimatum game). Finally the adjustment of principles to actions is mainly the fact of individuals who behave more selfishly and who have a stronger bargaining power.</p>\n<p>&hellip;Individuals destroy the resources of others because of envy (Mui, 1995; Maher, 2010; Charness et al., 2010; Harbring and Irlensbusch, 2011) or for the joy of destruction (Zizzo and Oswald, 2001; Abbink and Sadrieh, 2009); the power of public office sometimes leads politicians to use it for their personal gain (Aidt, 2003); feelings of entitlement push leaders to take more than followers from a common resource (de Cremer and van Dijk, 2005).</p>\n<ul>\n<li>Mui, V.L. (1995). The economics of envy. Journal of Economic Behavior &amp; Organizations, 26(3), 311-336.</li>\n<li>Maher, B. (2010). Research Integrity: Sabotage! Nature, 467, 30 September, 516-518.</li>\n<li>G. Charness, D. Masclet, M.C. Villeval. (2010). Competitive Preferences and Status as an Incentive: Experimental Evidence. IZA Discussion Paper 5034, Bonn</li>\n<li>Harbring, C., Irlensbusch, B. (2011). Sabotage in Tournaments: Evidence from the Laboratory, Management Science, 57(4), 611-627</li>\n<li>Zizzo, D., Oswald, A.J. (2001). Are People Willing to Pay to Reduce Others&rsquo; Incomes? Annales d&rsquo;Economie et de Statistique, 63-64, 39-62</li>\n<li>Abbink, K., Sadrieh, A. (2009). The pleasure of being nasty. Economics Letters, 105(3), 306-308.</li>\n<li>Aidt, T.S. (2003). Economic Analysis of Corruption: A Survey. The Economic Journal, 113, F632-F652.</li>\n<li>de Cremer, D., van Dijk, E. (2005). When and why leaders put themselves first: Leader behaviour in resource allocations as a function of feeling entitled. European Journal of Social Psychology, 35, 553-563.</li>\n</ul>\n<p>social psychologists studying moral hypocrisy have shown that individuals evaluate more negatively the moral transgression of fair principles when this transgression is enacted by others than when enacted by themselves (Valdesolo and deStefano, 2008).</p>\n<ul>\n<li>Valdesolo, P., deStefano, D.A. (2008). The duality of virtue: Deconstructing the moral hypocrite. Journal of Experimental Social Psychology, 44 (5), 1334-1338.</li>\n</ul>\n<p>Such an illusory preference for fairness has been identified by Dana, Weber and Kuang (2007) (see also Larson and Capra, 2009; Grossman, 2010; van der Weele, 2012). Indeed, fairness decreases substantially when the link between fairness and outcome is obfuscated. The choice to play fair is frequently motivated by the willingness to appear fair more than by the willingness to produce a fair outcome and this is why greater anonymity leads to more selfish transfers in the dictator game (Andreoni and Bernheim, 2009; Ariely et al., 2009).</p>\n<ul>\n<li>Dana, J., Weber R.A., Xi Kuang, J. (2007). Exploiting moral wiggle room: experiments demonstrating an illusory preference for fairness. Economic Theory, 33(1), 67-80</li>\n<li>Larson, T., Capra, M. (2009). Exploiting moral wiggle room: Illusory preference for fairness? A comment. Judgment and Decision Making, 4(6), 467-474</li>\n<li>Grossman, Z. (2010). Strategic ignorance and the robustness of social preferences. Working paper, University of California at Santa Barbara</li>\n<li>Van der Weele, J. (2012). When ignorance is innocence: on information avoidance in moral dilemmas. SSRN working paper.</li>\n<li>Andreoni, J., Bernheim, B.D. (2009). Social Image and the 50-50 Norm: A Theoretical and Experimental Analysis of Audience Effects. Econometrica, 77(5), 1607-1636</li>\n<li>Ariely, D., Bracha, A., Meier, S. (2009). Doing Good or Doing Well? Image Motivation and Monetary Incentives in Behaving Prosocially. American Economic Review, 99(1), 544-555</li>\n</ul>\n</blockquote>\n<p><a href=\"http://www.kaynagiminsan.com/wp-content/uploads/2011/06/Social-Psychological-and-Personality-Science-2011-Van-Kleef-1948550611398416.pdf\">&ldquo;Breaking the Rules to Rise to Power: How Norm Violators Gain Power in the Eyes of Others&rdquo;</a>, Kleef et al 2011:</p>\n<blockquote>\n<p>Four studies support this hypothesis. Individuals who took coffee from another person&rsquo;s can (Study 1), violated rules of bookkeeping (Study 2), dropped cigarette ashes on the floor (Study 3), or put their feet on the table (Study 4) were perceived as more powerful than individuals who did not show such behaviors. The effect was mediated by inferences of volitional capacity, and it replicated across different methods (scenario, film clip, face-to-face interaction), different norm violations, and different indices of power (explicit measures, expected emotions, and approach/inhibition tendencies).</p>\n<p>&hellip;&lsquo;&lsquo;Power tends to corrupt, and absolute power corrupts absolutely,&rsquo;&rsquo; wrote Lord Acton to Bishop Mandell Creighton in 1887. This classic adage not only reflects popular sentiments about power; it is also supported by scientific research (e.g., Kipnis, 1972).</p>\n<ul>\n<li>Kipnis, D. (1972). Does power corrupt? Journal of Personality and Social Psychology, 24, 33-41</li>\n</ul>\n<p>Individuals who feel powerful are more likely to act in goal-congruent ways (e.g., by switching off an annoying fan) than those who feel less powerful (Galinsky, Gruenfeld, &amp; Magee, 2003). Powerful individuals are also more likely to take risks (Anderson &amp; Galinsky, 2006), show approach-related tendencies and goal-directed action (Guinote, 2007; Lammers, Galinsky, Gordijn, &amp; Otten, 2008; Smith &amp; Bargh, 2008), express their emotions (Hecht &amp; Lafrance, 1998), act based on their dispositional inclinations (Chen, Lee-Chai, &amp; Bargh, 2001) and momentary desires (Van Kleef &amp; Cote, 2007), and ignore situational pressures (Galinsky et al., 2008).</p>\n<ul>\n<li>Galinsky, A. D., Gruenfeld, D. H., &amp; Magee, J. C. (2003). From power to action. Journal of Personality and Social Psychology, 85, 453-466</li>\n<li>Anderson, C., &amp; Galinsky, A. D. (2006). Power, optimism and risk-taking. European Journal of Social Psychology, 36, 511-536</li>\n<li>Guinote, A. (2007). Power and goal pursuit. Personality and Social Psychology Bulletin, 33, 1076-1087</li>\n<li>Lammers, J., Galinsky, A. D., Gordijn, E. H., &amp; Otten, S. (2008). Illegitimacy moderates the effects of power on approach. Psychological Science, 19, 558-564</li>\n<li>Smith, P. K., &amp; Bargh, J. A. (2008). Nonconscious effects of power on basic approach and avoidance tendencies. Social Cognition, 26, 1-24</li>\n<li>Hecht, M. A., &amp; Lafrance, M. (1998). License or obligation to smile: The effect of power and sex on amount and type of smiling. Personality and Social Psychology Bulletin, 24, 1332-1342</li>\n<li>Chen, S., Lee-Chai, A. Y., &amp; Bargh, J. A. (2001). Relationship orientation as a moderator of the effects of social power. Journal of Personality and Social Psychology, 80, 173-187</li>\n<li>Van Kleef, G. A., &amp; &amp; Cote, S (2007). Expressing anger in conflict: When it helps and when it hurts. Journal of Applied Psychology, 92, 1557-1569</li>\n<li>Galinsky, A. D., Gruenfeld, D. H., Magee, J. C., Whitson, J. A., &amp; Liljenquist, K. A. (2008). Power reduces the press of the situation: Implications for creativity, conformity, and dissonance. Journal of Personality and Social Psychology, 95, 1450-1466</li>\n</ul>\n<p>This behavioral disinhibition makes powerful people more likely to exhibit socially inappropriate behavior. Compared to lower power individuals, powerful individuals are likely to take more cookies from a common plate, eat with their mouths open, and spread crumbs (Keltner et al., 2003); interrupt conversation partners and invade their personal space (DePaulo &amp; Friedman, 1998); fail to take another&rsquo;s perspective (Galinsky, Magee, Inesi, &amp; Gruenfeld, 2006); ignore other people&rsquo;s suffering (Van Kleef et al., 2008); stereotype (Fiske, 1993) and patronize others (Vescio, Gervais, Snyder, &amp; Hoover, 2005); cheat (Lammers, Stapel, &amp; Galinsky, 2010); take credit for the contributions of others (Kipnis, 1972); treat other people as a means to their own ends (Gruenfeld, Inesi, Magee, &amp; Galinsky, 2008); and sexualize and harass low-power women (Bargh, Raymond, Pryor, &amp; Strack, 1995). Powerful people also exhibit more aggression (Haney, Banks, &amp; Zimbardo, 1973), and this is relatively acceptable to others (Porath, Overbeck, &amp; Pearson, 2008). In fact, in several European countries the liberty to violate norms without sanction is perceived as a defining feature of the power holder (Mondillon et al., 2005). Although the powerful impose strict moral standards on others, they practice less strict moral behavior themselves (Lammers et al., 2010).</p>\n<ul>\n<li>Keltner, D., &amp; Gruenfeld, D. H, &amp; Anderson, C. (2003). Power, approach, and inhibition. Psychological Review, 110, 265-284.</li>\n<li>DePaulo, B. M., &amp; Friedman, H. S. (1998). Nonverbal communication. In D. Gilbert, S. T. Fiske, &amp; G. Lindzey (Eds.), Handbook of social psychology (pp.&nbsp;3-40). New York: McGraw-Hill</li>\n<li>Galinsky, A. D., Magee, J. C., Inesi, M. E., &amp; Gruenfeld, D. H. (2006). Power and perspectives not taken. Psychological Science, 17, 1068-1074</li>\n<li>Van Kleef, G. A., Oveis, C., Van Der Lowe, I., LuoKogan, A., Goetz, J., &amp; Keltner, D. (2008). Power, distress, and compassion: Turning a blind eye to the suffering of others. Psychological Science, 19, 1315-1322</li>\n<li>Fiske, S. T. (1993). Controlling other people: The impact of power on stereotyping. American Psychologist, 48, 621-628</li>\n<li>Lammers, J., Stapel, D. A., &amp; Galinsky, A. D. (2010). Power increases hypocrisy: Moralizing in reasoning, immorality in behavior. Psychological Science, 21, 737-744; WARNING: Stapel! Lammers <a href=\"http://lammers.socialpsychology.org/\">states</a> that this paper is untainted:</li>\n</ul>\n<blockquote>\n<p>IMPORTANT: Regarding the scientific fraud of my former supervisor Stapel: the committee Levelt has investigated all my work with Stapel. All my work on the topic of power has been cleared from suspicion of data-fraud. This research is all based on data that I collected myself or collected together with other co-authors (i.e.&nbsp;not Stapel). There is one paper (on racism in legal decisions) where I was misled. This paper contains false data. It is currently being retracted.</p>\n</blockquote>\n<ul>\n<li>Gruenfeld, D. H., Inesi, M. E., Magee, J. C., &amp; Galinsky, A. D. (2008). Power and the objectification of social targets. Journal of Personality and Social Psychology, 95, 111-127</li>\n<li>Bargh, J. A., Raymond, P., Pryor, J. B., &amp; Strack, F. (1995). Attractiveness of the underling: An automatic power-sex association and its consequences for sexual harassment and aggression. Journal of Personality and Social Psychology, 68, 768-781</li>\n<li>Haney, C., Banks, C., &amp; Zimbardo, P. (1973). Interpersonal dynamics in a simulated prison. International Journal of Criminology and Penology, 1, 69-97</li>\n<li>Porath, C. L., Overbeck, J., &amp; Pearson, C. M. (2008). Picking up the gauntlet: How individuals respond to status challenges. Journal of Applied Social Psychology, 38, 1945-1980</li>\n<li>Mondillon, L., Niedenthal, P. M., Brauer, M., Rohman, A., Dalle, N., &amp; Uchida, Y. (2005). Beliefs about power and its relation to emotional experience: A comparison of Japan, France, Germany, and United States. Personality and Social Psychology Bulletin, 31, 1112-1122</li>\n</ul>\n<p>...research on adolescent aggression indicates that bullying behavior is associated with prestige (Savin-Williams, 1976; Sijtsema, Veenstra, Lindenberg, &amp; Salmivalli, 2009).</p>\n<ul>\n<li>Savin-Williams, R. C. (1976). An ethological study of dominance formation and maintenance in a group of human adolescents. Child Development, 47, 972-979</li>\n<li>Sijtsema, J. J., Veenstra, R., Lindenberg, S., &amp; Salmivalli, C. (2009). Empirical test of bullies&rsquo; status goals: Assessing direct goals, aggression, and prestige. Aggressive Behavior, 35, 57-67</li>\n</ul>\n</blockquote>\n<p><a href=\"http://portal.idc.ac.il/en/Symposium/HSPSP/2010/Documents/08-eyal-liberman.pdf\">&ldquo;Morality and Psychological Distance: A Construal Level Theory Perspective&rdquo;</a>, Eyal &amp; Liberman:</p>\n<blockquote>\n<p>In this chapter, we propose one answer to the question of when values and moral principles play a central role in people&rsquo;s judgments and plans. We explore the possibility that values and moral principles are more prominent in judgments and predictions regarding psychologically more distant events. This perspective is based on construal level theory (CLT; Liberman &amp; Trope, 2008; Liberman, Trope, &amp; Stephan, 2007; Trope &amp; Liberman, in press), according to which the construal of psychologically more distant situations highlights more abstract, high-level features. Because values and moral rules tend to be abstract and general, people are more likely to use them in construing, judging, and planning with respect to psychologically more distant situations.</p>\n<p>For example, Nussbaum, Trope, and Liberman (2003, Study 2) conceptualized personal dispositions as high-level construals and situational constrains as low-level construals and demonstrated that people expect others to express their personal dispositions and act consistently across different situations in the distant future more than in the near future. In the study, participants imagined an acquaintance&rsquo;s behavior in four different situations (e.g., a birthday party, waiting in line at the supermarket) in either the near future or the distant future and rated the extent to which the acquaintance would display 15 traits (e.g., behave in a friendly vs.&nbsp;an unfriendly manner) representative of the Big Five personality dimensions (extraversion, agreeableness, conscientiousness, emotional stability, and intellect). Cross- situational consistency was assessed by computing, for each of the 15 traits, the variance in each predicted behavior across the four situations and the correlations among the predicted behaviors in the four situations. As predicted, participants expected others to behave more consistently across distant-future situations than across near-future situations. This finding was replicated with ratings of participants&rsquo; own behavior in different situations: Participants anticipated exhibiting more consistent traits in the distant future than in the near future (Wakslak, Nussbaum, Liberman, &amp; Trope, 2008, Study 5).</p>\n<ul>\n<li>Nussbaum, S., Trope, Y., &amp; Liberman, N. (2003). &ldquo;Creeping dispositionism: The temporal dynamics of behavior prediction&rdquo;. Journal of Personality and Social Psychology, 84, 485-497</li>\n<li>Wakslak, C. J., Nussbaum, S., Liberman, N., &amp; Trope, Y. (2008). &ldquo;Representations of the self in the near and distant future&rdquo;. Journal of Personality and Social Psychology, 95, 757-773</li>\n</ul>\n<p>For each scenario (e.g., national flag), participants chose between two restatements of each action. One restatement referred to an abstract moral principle (high-level construal; e.g., desecrating a national symbol) and the other restatement referred to the means of carrying out the action (low-level construal; e.g., cutting a flag to created rags). We found that distant-future transgressions were identified in moral terms more often than near-future transgressions. These findings suggest that people are more likely to think of a temporally distant action, rather than one in the near term, as having moral implications. CLT predicts similar results for other forms of psychological distance: Situations should be more readily construed in terms of moral principles when they occurred further back in the past, when they apply to more socially or spatially distant individuals or groups, and when they are less likely actually to occur. When the same actions are proximal, they are more likely to be construed in terms that are devoid of moral implications. For example, accepting minority students with lower grades into one&rsquo;s university will be seen as &ldquo;endorsing affirmative action&rdquo; when it is unlikely to be implemented, but it will be seen in more concrete terms (e.g., as &ldquo;making acceptance rules more complicated&rdquo;) when it becomes more likely.</p>\n<p>The vignettes also included situational details that rendered the transgressions harmless (low-level information; e.g., the siblings used contraceptives, they had sex just once, they kept it a secret). Participants were instructed to imagine that the transgressions would occur tomorrow (the near-future condition) or next year (the distant-future condition) and judged the extent of its wrongness. We found that moral transgressions were judged more severely when imagined in the distant future compared to the near future. The same pattern occurred with social distance (Eyal et al., 2008, Study 3), which was manipulated by asking participants to focus either on the feelings and thoughts they experienced while reading about the events (low social distance) or to think about another person they knew, such as a colleague, a friend, or a neighbor, and focus on the feelings and thoughts that this person would experience while reading about the events (high social distance). Notice that the social distance manipulation did not involve judging one&rsquo;s own versus another person&rsquo;s actions, but only one&rsquo;s imagined perspective. Notably, this manipulation does not support interpreting the results in terms of moral hypocrisy, according to which people judge their own moral transgressions less harshly than another person&rsquo;s transgressions because they wish to appear better than others. As predicted, moral transgressions were judged more harshly when imagined from a third person perspective (high social distance) compared to one&rsquo;s own perspective (low social distance). Another study (Eyal et al., 2008, Study 4) examined temporal distance effects on judgments of moral acts. Participants read vignettes that described virtuous acts related to widely accepted moral principles (high-level information; e.g., a couple adopting a disabled child) as well as low-level, situational details that rendered the acts less noble (e.g., the government offering large adoption payments). It was found that these behaviors were judged to be more virtuous when they were described as happening in the distant future rather than the near future.</p>\n<p>Temporal distance from moral transgressions was also found to affect people&rsquo;s emotional responses. Agerstrom and Bjorklund (2009, Studies 1 and 2) asked Swedish participants to imagine situations that involved a threat to human welfare taking place in the near future (today) or in the distant future (in 30 years). For example, one scenario, set in Darfur, Africa, described a woman who was raped and beaten by the Janjaweed militia. Each scenario was followed by a description of a prosocial action that, if taken, could improve the situation (e.g., donate money). Participants rated how wrong it would be for another Swedish citizen not to take the proposed prosocial action given that they had the means to do so. They also rated how angry they would feel if the target person failed to take the prosocial action. It was found that distant-future moral failures were judged more harshly and invoked more anger than near-future moral failures.</p>\n<p>In another study, Agerstrom and Bjorklund (2009) examined whether the greater reliance on moral principles in judgments of distant-future compared to near-future transgressions would generalize to individuals&rsquo; self-perceptions. Participants rated the likelihood of engaging in prosocial actions in reaction to other people&rsquo;s moral transgressions. For example, participants indicated how much money they were willing to donate to help improve the situation in Darfur. As predicted, participants were more likely to express prosocial behavioral intentions when imagining the act occurring in the more distant future. Taken together, these findings suggest that moral rules are more likely to guide people&rsquo;s judgments of distant rather than proximal behaviors.</p>\n<ul>\n<li>Agerstr&ouml;m, J., &amp; Bj&ouml;rklund, F. (2009). Temporal distance and moral concerns: Future morally questionable behavior is perceived as more wrong and evokes stronger prosocial intentions. Basic and Applied Social Psychology, 31, 49-59</li>\n</ul>\n<p>For example, individuals for whom altruism was subordinate in importance to achievement were more likely to refuse to help a fellow student in the distant future than in the near future, whereas individuals for whom achievement was subordinate to altruism were more likely to help a fellow student in the distant future than in the near future. These findings show that secondary values, which are nonetheless part of an individual&rsquo;s self-identity, may mask the influence of central values on near future intentions. Centrality of values may be defined not only within an individual but also within a situation. For example, when medically treating a person from a rival group in a war, the competition is central and mercy is secondary, whereas in a hospital, the reverse is true. An interesting prediction that follows from CLT is that the secondary value will guide behavioral intentions in the near future more than in the distant future. Thus, in a war, benevolence will come into play in near-future plans more than in distant-future plans, leading people to be more merciful than would otherwise be expected. In his poem &ldquo;After the Battle&rdquo;, Victor Hugo tells about his father (&ldquo;that hero with the sweetest smile&rdquo;), an officer in the war against Spain, who encounters a Spaniard soldier asking for something to drink. Although on the battlefield, and although the Spaniard tries to kill him, the officer orders: &ldquo;All the same, give him something to drink.&rdquo;</p>\n</blockquote>\n<p><a href=\"http://www.engaged-zen.org/PDFarchive/PowerLying.pdf\">&ldquo;People with Power are Better Liars&rdquo;</a>, Carney et al 2010:</p>\n<blockquote>\n<p>But lying does not come without cost. Ordinary lie-tellers experience negative emotions, decrements in mental function, and physiological stress. Liars are also at risk of getting caught. Despite people&rsquo;s best attempts to get away with their prevarications, lies are often behaviorally &ldquo;leaked&rdquo; through subtle changes in body movement and speech rate. Power, it seems, enhances the same emotional, cognitive, and physiological systems that lie-telling depletes. People with power enjoy positive emotions, increases in cognitive function (4-5), and physiological resilience such as lower levels of the stress hormone cortisol (6-7). Thus, holding power over others might make it easier for people to tell lies.</p>\n<ol style=\"list-style-type: decimal\">\n<li>D. Keltner, D.H. Gruenfeld, C. Anderson, Psychol Rev.&nbsp;110, 265-284 (2003).</li>\n<li>P.K. Smith, N.B. Jostmann, A.D. Galinsky, W. van Dijk, Psychol Sci. 19, 441-447 (2008).</li>\n<li>R.M. Sapolsky, S.C. Alberts. J. Altmann, J Arch Gen Psychi. 54, 1137-1143 (1997).</li>\n<li>S. Cohen, W.J. Doyle, A. Baum, Psychosom Med. 68, 414-420 (2006)</li>\n</ol>\n<p>Participants were assigned to the role of &ldquo;leader&rdquo; or &ldquo;subordinate&rdquo; and engaged in a series social interactions in which the leader had control over the subordinate&rsquo;s monetary and social outcomes (9)&hellip;.If the individual could successfully convince the experimenter (regardless of whether they were lying) they could keep the $100 in cash. All participants were then interviewed about whether they had stolen the money: half were lying and half were telling the truth. The interviewer (blind to experimental condition) asked all participants the same critical questions (e.g., &ldquo;Did you steal the $100?&rdquo;; &ldquo;Why should I believe you?&rdquo;). After the interview, participants completed measures of moral emotional feelings (rated emotion terms: bashful, guilty, troubled, scornful) and a computerized task assessing degree of cognitive impairment. All participants provided saliva samples before and after the experiment to assess changes in the stress hormone cortisol (9). The interviews were videotaped and coded for two, classic nonverbal markers of deception: one-sided shoulder shrugs and accelerated prosody (9). Low-power individuals showed the expected emotional, cognitive, physiological, and behavioral signs of deception; in contrast, powerful people demonstrated no evidence of lying across emotion, cognition, physiology, or behavior (see Figure). In other words, power acted as a buffer allowing the powerful to lie significantly more easily (less disturbing emotion, less cognitive impairment, less of a rise in the stress hormone cortisol) and more effectively (fewer nonverbal cues associated with lying). Only low-power individuals felt badly after lying (panel A), suffered cognitive impairment (panel B), spiked in levels of the stress hormone cortisol (panel C), and demonstrated nonverbal &ldquo;leakage&rdquo; (more one-sided shoulder shrugs and accelerated prosody; panel D). (9)</p>\n</blockquote>\n<p><a href=\"http://www.bu.edu/law/central/jd/organizations/journals/bulr/documents/LANGEVOORT.pdf\">&ldquo;Psychological perspectives on the fiduciary business&rdquo;</a>, Donald C. Langevoort</p>\n<blockquote>\n<p>But the investment game has been manipulated in numerous ways that produce differing levels of trusting and greater selfishness. One of particular interest is the introduction of the possibility that, at the end of the game, the trustor will learn whether she gets something back but will not know whether this is the result of the trustee&rsquo;s choice or some exogenous force &ndash; e.g., luck.34 Given the opportunity to hide behind the possibility that a return of nothing was just bad luck for the trustor, trustees predictably keep more for themselves, presumably rationalizing the outcome as fair in an uncertain world. The authors of one such study recently drew parallels to financial relationships between investors and securities professionals, because the financial markets generate a great deal of good and bad luck that obscures the value added by professional trustworthiness.35</p>\n<ol style=\"list-style-type: decimal\">\n<li>Radu Vranceanu et al., Trust and Financial Trades: Lessons from an Investment Game Where Reciprocators Can Hide Behind Probabilities 6 (ESSEC Bus. Sch., Working Paper No. 10007, 2010), available at http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1 611666.</li>\n<li>See id. at 14-15.</li>\n</ol>\n<p>Unfortunately, high testosterone levels do not fit well with fiduciary characteristics like empathy and moral decision-making. Emerging research on the subject suggests that testosterone buffers emotional constraints on aggression and risk-taking, leading to a more &ldquo;cold&rdquo; utilitarian calculus and a greater willingness to do harm to gain a preferred outcome.49</p>\n<p>See Dana R. Carney &amp; Malia F. Mason, Decision Making and Testosterone: When the Ends Justify the Means, 46 J. EXPERIMENTAL SOC. PSYCHOL. 668, 668-69 (2010). As the authors point out, the ends need not necessarily be immoral. Id. at 670.</p>\n<p>Power also seems to increase hypocrisy &ndash; insistence on adherence to strict norms by others, while enjoying far greater nimbleness in justifying one&rsquo;s own departures on utilitarian or other rationalized grounds52 &ndash; and optimism and risk-taking.53 Of course, power may be gained in the first place by those skilled at rationalization and willing to take risks, in which case there is a dynamic feedback loop that is likely to generate increasing hypocrisy and hubris over time.</p>\n<ol style=\"list-style-type: decimal\">\n<li>See Joris Lammers et al., Power Increases Hypocrisy: Moralizing in Reasoning, Immorality in Behavior, 21 PSYCHOL. SCI. 737, 738 (2010).</li>\n<li>See Cameron Anderson &amp; Adam D. Galinsky, Power, Optimism, and Risk-taking, 36 EUR. J. SOC. PSYCHOL. 511, 516 (2006). In turn, this pattern may connect to testosterone or other physiological effects. See Carney &amp; Mason, supra note 49, at 668.</li>\n</ol></blockquote>\n<p><a href=\"http://www.jstor.org/stable/10.1086/426699\">&ldquo;The Dirt on Coming Clean: Perverse Effects of Disclosing Conflicts of Interest&rdquo;</a>, Cain et al 2005</p>\n<blockquote>\n<p>Although disclosure is often proposed as a potential solution to these problems, we show that it can have perverse effects. First, people generally do not discount advice from biased advisors as much as they should, even when advisors&rsquo; conflicts of interest are disclosed. Second, disclosure can increase the bias in advice because it leads advisors to feel morally licensed and strategically encouraged to exaggerate their advice even further. As a result, disclosure may fail to solve the problems created by conflicts of interest and may sometimes even make matters worse.</p>\n<p>&hellip;In the domain of medicine, for example, research shows that while many people are ready to acknowledge that doctors might generally be affected by conflicts of interest, few can imagine that their own doctors would be affected (Gibbons et al. 1998). Indeed, it is even possible that disclosure could sometimes increase rather than decrease trust, especially if the person with the conflict of interest is the one who issues the disclosure. Research suggests that when managers offer negative financial disclosures about future earnings, they are regarded as more credible agents, at least in the short term (Lee, Peterson, and Tiedens 2004; Mercer, forthcoming). Thus, if a doctor tells a patient that her research is funded by the manufacturer of the medication that she is prescribing, the patient might then think (perhaps rightly) that the doctor is going out of her way to be open or that she is &ldquo;deeply involved&rdquo; and thus knowledgeable. Thus, disclosure could cause the estimator to place more rather than less weight on the advisor&rsquo;s advice. Third, even when estimators realize that they should make some adjustment for the conflict of interest that is disclosed, such adjustments are likely to be insufficient. As a rule, people have trouble unlearning, ignoring, or suppressing the use of knowledge (such as biased advice) even if they are aware that it is inaccurate (Wilson and Brekke 1994). Research on anchoring, for example, shows that quantitative judgments are often drawn toward numbers (the anchors) that happen to be mentally available. This effect holds even when those anchors are known to be irrelevant (Strack and Mussweiler 1997; Tversky and Kahneman 1974), unreliable (Loftus 1979), or even manipulative (Galinsky and Mussweiler 2001; Hastie, Schkade, and Payne 1999). Research on the &ldquo;curse of knowledge&rdquo; (Camerer, Loewenstein, and Weber 1989) shows that people&rsquo;s judgments are influenced even by information they know they should ignore. And research on what has been called the &ldquo;failure of evidentiary discreditation&rdquo; shows that when the evidence on which beliefs were revised is totally discredited, those beliefs do not revert to their original states but show a persistent effect of the discredited evidence (Skurnik, Moskowitz, and Johnson 2002; Ross, Lepper, and Hubbard 1975). Furthermore, attempts to willfully suppress undesired thoughts can lead to ironic rebound effects, in some cases even increasing the spontaneous use of undesired knowledge (Wegner 1994).</p>\n<p>&hellip;More interesting, and as predicted, all three measures also reveal that disclosure led to greater distortion of advice. The amount that advisors exaggerated, calculated by subtracting advisors&rsquo; own personal estimates from their public suggestions, was significantly greater in the high/disclosed condition than in either of the other two conditions (p&lt;0.05) and significantly greater by the other two measures as well: advisor suggestion minus actual jar values and advisor suggestion minus the average of personal estimates in the accurate condition (p&lt;0.05 for both). In the accurate condition, for example, advisors provided estimators with suggestions of jar values that were, on average, within $1 of their own personal estimates. In the high/undisclosed condition, however, advisors gave suggestions that were $3.32 greater than their own personal estimates, and in the high/disclosed condition, they gave suggestions that were inflated more than twice as much, at more than $7 above their own personal estimates. Disclosure, it appears, did lead advisors to provide estimators with more biased advice.</p>\n<p>&hellip;Although disclosures did increase discounting by estimators, albeit not significantly, this discounting was not sufficient to offset the increase in the bias of the advice they received. As Table 6 (fourth row) shows, estimator discounting increased, on average, less than $2 from the accurate condition to the high/undisclosed condition and less than $2.50 from the high/undisclosed condition to the high/disclosed condition. However, Table 5 (second row) shows that suggestions increased, on average, almost $4 from the accurate condition to the high/undisclosed condition and increased $4 again from the high/undisclosed condition to the high/disclosed condition. Thus, while estimators in the high/disclosed condition discounted suggestions about $4 more than did estimators in the accurate condition, the advice given in the high/disclosed condition was almost $8 higher than advice given in the accurate condition. Instead of correcting for bias, estimates were approximately 28 percent higher in the high/disclosed condition than in the accurate condition (first row of Table 6).</p>\n</blockquote>\n<p><a href=\"http://dl.dropbox.com/u/85192141/2011-cain.pdf\">&ldquo;When Sunlight Fails to Disinfect: Understanding the Perverse Effects of Disclosing Conflicts of Interest&rdquo;</a>, Cain et al 2011</p>\n<blockquote>\n<p>Studies 1 and 2 examine psychological mechanisms (strategic exaggeration, moral licensing) by which disclosure can lead advisors to give more-biased advice. Study 3 shows that disclosure backfires when advice recipients who receive disclosure fail to sufficiently discount and thus fail to mitigate the adverse effects of disclosure on advisor bias. Study 4 identifies one remedy for inadequate discounting of biased advice: explicitly and simultaneously contrasting biased advice with unbiased advice.</p>\n<p>&hellip;Even in one-shot dictator games (Forsythe et al. 1994), research has long shown that many people will share resources and show self-restraint toward anonymous others (Camerer 2003), especially when it is common knowledge that the recipient expects such benevolence (Dana, Cain, and Dawes 2006). Likewise, research on cheating behavior shows that people do not tend to cheat as much as they can get away with, only to the extent that they can rationalize to themselves (Mazar, Amir, and Ariely 2008).</p>\n<p>&hellip;When the welfare of others is a consideration, disclosure might reduce moral concerns. Prior research has suggested that when people demonstrate ethical behavior, they often become more likely to subsequently exhibit ethical lapses (Jordan, Mullen, and Murnighan 2009; Zhong, Liljenquist, and Cain 2009). For example, people who are given an opportunity to demonstrate their own lack of prejudice are more likely to subsequently display discriminatory behavior (Monin and Miller 2001). Likewise, after a conflict of interest has been disclosed, advisors may feel that advisees have been warned and that advisors are &ldquo;morally licensed&rdquo; to provide biased advice.</p>\n<p>&hellip;Disclosure of a conflict of interest can also reduce the perceived immorality of giving biased advice by signaling that bias is widespread and therefore less aberrant (Schultz et al. 2007). If advice recipients&rsquo; expectations affect advisor behavior (Dana et al. 2006), then the lowered expectations for honesty that come with disclosure might allow an advisor to rationalize providing biased advice because that is exactly what the advisee expects, or should expect, to receive.</p>\n<p>&hellip;Why is the call for disclosure so popular despite how it can backfire? One possible explanation is that most people are simply not aware of disclosure&rsquo;s pitfalls. At first glance, disclosure seems like a sensible remedy to a situation in which one party possesses an otherwise hidden incentive to mislead another party. A more cynical explanation would play on the Chicago Theory of Regulation (Becker 1983; Peltzman 1976; Stigler 1971), which posits that regulation typically exists not for the general benefit of society but for the benefit of the regulated groups. These entities might be aware of the ineffectiveness of disclosure but accept it because it benefits them. For example, even though consumer advocates fought hard for warning labels on cigarette packages, the tobacco industry has defended itself against litigation since then by citing the warning labels as evidence that consumers knew the risks. &ldquo;What was intended as a burden on tobacco became a shield instead&rdquo; (Action on Smoking and Health 2001). Moreover, even the regulators may be attracted to disclosure if they see it as absolving them of responsibility for protecting consumers by ostensibly empowering consumers to protect themselves. Disclosure may also be perceived as the lesser of evils for those who might otherwise face more substantive regulation. For example, pharmaceutical firms are often strong proponents of disclosure laws, since it is better for them (and for researchers who receive their funding) if researchers must disclose financial ties to the industry rather than actually having to sever them. This all suggests that disclosure may be problematic for more reasons than those identified by the experiments reported above. It would be a mistake, however, to conclude that disclosure is always counterproductive, as some recent laboratory research illustrates (Church and Kuang 2009; Koch and Schmidt 2009). Research on practical examples of disclosure, summarized in Full Disclosure (Fung, Graham, and Weil 2007), also shows that disclosure can have real beneficial effects. For example, following a spate of highly publicized SUV rollovers, regulations that required auto manufacturers to publicly disclose rollover ratings led to significant and rapid changes in auto design, resulting in a general decrease in the rollover risk for SUVs. Disclosure is likely to be helpful when information is disclosed in an easily digestible form (or is made available to intermediaries, e.g., ratings companies, who process it for consumers) and when it is clear how one should respond to the disclosed information. The rollover ratings met both criteria: the ratings were represented simply as one to five stars, making it easy for consumers to compare&mdash;that is, evaluate jointly&mdash;the relative rollover risks of various SUVs. Even when information isn&rsquo;t presented in such a simple form, disclosure is likely to prove helpful when the recipients are savvy repeat-players who know what to do with the disclosed information, such as institutional investors, experienced attorneys, or managers in government agencies (Church and Kuang 2009; Malmendier and Shanthikumar 2007). Disclosure is much less likely to help individuals such as personal investors, purchasers of insurance, home buyers, or patients, who are unlikely to possess the knowledge or experience to know how much they should discount advice or whether they should get a second opinion in a given conflict-of-interest situation (Malmendier and Shanthikumar 2007).</p>\n</blockquote>\n<p><a href=\"http://www.public-speaking.com/resources/Self-Study/Speaking-Tips/Power-Posing.pdf\">&ldquo;Power Posing: Brief Nonverbal Displays Affect Neuroendocrine Levels and Risk Tolerance&rdquo;</a> Carney et al 2010</p>\n<blockquote>\n<p>As predicted, results revealed that posing in high-power (vs.&nbsp;low-power) nonverbal displays caused neuroendocrine and behavioral changes for both male and female participants: High-power posers experienced elevations in testosterone, decreases in cortisol, and increased feelings of power and tolerance for risk; low-power posers exhibited the opposite pattern. In short, posing in powerful displays caused advantaged and adaptive psychological, physiological, and behavioral changes &ndash; findings that suggest that embodiment extends beyond mere thinking and feeling, to physiology and subsequent behavioral choices.</p>\n<p>&hellip;The neuroendocrine profiles of the powerful differentiate them from the powerless, on two key hormones&mdash;testosterone and cortisol. In humans and other animals, testosterone levels both reflect and reinforce dispositional and situational status and dominance; internal and external cues cause testosterone to rise, increasing dominant behaviors, and these behaviors can elevate testosterone even further (Archer, 2006; Mazur &amp; Booth, 1998). For example, testosterone rises in anticipation of a competition and as a result of a win, but drops following a defeat (e.g., Booth, Shelley, Mazur, Tharp, &amp; Kittok, 1989), and these changes predict the desire to compete again (Mehta &amp; Josephs, 2006). In short, testosterone levels, by reflecting and reinforcing dominance, are closely linked to adaptive responses to challenges.</p>\n<ul>\n<li>Archer, J. (2006). Testosterone and human aggression: An evaluation of the challenge hypothesis. Neuroscience &amp; Biobehavioral Reviews, 30, 319&ndash;345.</li>\n<li>Mazur, A., &amp; Booth, A. (1998). Testosterone and dominance in men. Behavioral &amp; Brain Sciences, 21, 353&ndash;397</li>\n<li>Booth, A., Shelley, G., Mazur, A., Tharp, G., &amp; Kittok, R. (1989). Testosterone and winning and losing in human competition. Hormones and Behavior, 23, 556&ndash;571.</li>\n<li>Mehta, P.H., &amp; Josephs, R.A. (2006). Testosterone change after losing predicts the decision to compete again. Hormones and Behavior, 50, 684&ndash;692</li>\n</ul>\n<p>Power is also linked to the stress hormone cortisol: Power holders show lower basal cortisol levels and lower cortisol reactivity to stressors than powerless people do, and cortisol drops as power is achieved (Abbott et al., 2003; Coe, Mendoza, &amp; Levine, 1979; Sapolsky, Alberts, &amp; Altmann, 1997). Although short-term and acute cortisol elevation is part of an adaptive response to challenges large (e.g., a predator) and small (e.g., waking up), the chronically elevated cortisol levels seen in low-power individuals are associated with negative health consequences, such as impaired immune functioning, hypertension, and memory loss (Sapolsky et al., 1997; Segerstrom &amp; Miller, 2004). Low-power social groups have a higher incidence of stress-related illnesses than high-power social groups do, and this is partially attributable to chronically elevated cortisol (Cohen et al., 2006). Thus, the power holder&rsquo;s typical neuroendocrine profile of high testosterone coupled with low cortisol&mdash;a profile linked to such outcomes as disease resistance (Sapolsky, 2005) and leadership abilities (Mehta &amp; Josephs, 2010)&mdash;appears to be optimally adaptive.</p>\n<ul>\n<li>Abbott, D.H., Keverne, E.B., Bercovitch, F.B., Shively, C.A., Mendoza, S.P., Saltzman, W., et al. (2003). Are subordinates always stressed? A comparative analysis of rank differences in cortisol levels among primates. Hormones and Behavior, 43, 67&ndash;82</li>\n<li>Coe, C.L., Mendoza, S.P., &amp; Levine, S. (1979). Social status constrains the stress response in the squirrel monkey. Physiology &amp; Behavior, 23, 633&ndash;638</li>\n<li>Sapolsky, R.M., Alberts, S.C., &amp; Altmann, J. (1997). Hypercortisolism</li>\n<li>associated with social subordinance or social isolation among</li>\n<li>wild baboons. Archives of General Psychiatry, 54, 1137&ndash;1143</li>\n<li>Segerstrom, S., &amp; Miller, G. (2004). Psychological stress and the human immune system: A meta-analytic study of 30 years of inquiry. Psychological Bulletin, 130, 601&ndash;630</li>\n<li>Cohen, S., Schwartz, J.E., Epel, E., Kirschbaum, C., Sidney, S., &amp; Seeman, T. (2006). Socioeconomic status, race, and diurnal cortisol decline in the Coronary Artery Risk Development in Young Adults (CARDIA) study. Psychosomatic Medicine, 68, 41&ndash;50</li>\n<li>Sapolsky, R.M. (2005). The influence of social hierarchy on primate health. Science, 308, 648&ndash;652.</li>\n</ul>\n<p>It is unequivocal that power is expressed through highly specific, evolved nonverbal displays. Expansive, open postures (widespread limbs and enlargement of occupied space by spreading out) project high power, whereas contractive, closed postures (limbs touching the torso and minimization of occupied space by collapsing the body inward) project low power. All of these patterns have been identified in research on actual and attributed power and its nonverbal correlates (Carney, Hall, &amp; Smith LeBeau, 2005; Darwin, 1872/2009; de Waal, 1998; <a href=\"http://nuweb9.neu.edu/socialinteractionlab/wp-content/uploads/Hall-Coats-Smith-LeBeau-2005.pdf\">Hall, Coats, &amp; Smith LeBeau, 2005</a>).</p>\n<ul>\n<li>Hall, J.A., Coats, E.J., &amp; Smith LeBeau, L. (2005). Nonverbal and the vertical dimension of social relations: A meta-analysis. Psychological Bulletin, 131, 898&ndash;924.</li>\n</ul>\n</blockquote>\n<p><a href=\"http://www.centenary.edu/attachments/psychology/journal/feb2012journalclub.pdf\">&ldquo;Reality at Odds With Perceptions: Narcissistic Leaders and Group Performance&rdquo;</a>, Nevicka et al 2011:</p>\n<blockquote>\n<p>Despite people&rsquo;s positive perceptions of narcissists as leaders, it was previously unknown if and how leaders&rsquo; narcissism is related to the performance of the people they lead. In this study, we used a hidden-profile paradigm to investigate this question and found evidence for discordance between the positive image of narcissists as leaders and the reality of group performance. We hypothesized and found that although narcissistic leaders are perceived as effective because of their displays of authority, a leader&rsquo;s narcissism actually inhibits information exchange between group members and thereby negatively affects group performance. Our findings thus indicate that perceptions and reality can be at odds and have important practical and theoretical implications.</p>\n<p>&hellip;For example, narcissists tend to overestimate their intelligence (Campbell, Rudich, &amp; Sedikides, 2002), creativity (Goncalo, Flynn, &amp; Kim, 2010), academic abilities (Robins &amp; Beer, 2001), and leadership capabilities (Judge, LePine, &amp; Rich, 2006). Generally, other people do not agree with narcissists&rsquo; idealized self-images and perceive narcissists as arrogant, egocentric, overly dominant, and even hostile (Paulhus, 1998). However, the context of leadership constitutes a notable exception in which narcissists tend to be judged positively. For example, individuals with high levels of narcissism receive higher leadership ratings than individuals with low levels of narcissism do (Judge et al., 2006) and tend to emerge as leaders in groups (Brunell et al., 2008; Nevicka, De Hoogh, Van Vianen, Beersma, &amp; McIlwain, 2011). In addition, higher narcissism in U.S. presidents is associated with more positive evaluations of their leadership (Deluga, 1997). It is therefore not surprising that narcissistic characteristics are ascribed to many prominent leaders, such as Nicolas Sarkozy (De Sutter &amp; Immelman, 2008) and Steve Jobs (Robins &amp; Paulhus, 2001).</p>\n<p>&hellip;Of the two prior studies investigating this question, one found no effects of narcissistic leadership on performance (Brunell et al., 2008), and the other showed that organizational performance was merely more volatile, but no worse or better, because of narcissistic leaders&rsquo; risky decision making (Chatterjee &amp; Hambrick, 2007). Unfortunately, neither of these studies examined the effects of narcissistic leaders on group dynamics, communication, and information exchange, factors that are critically important to group decision making (Stasser, 1999), group performance (De Dreu, Nijstad, &amp; van Knippenberg, 2008), and organizational effectiveness (Zaccaro, Rittman, &amp; Marks, 2001)&hellip;Prior research has hinted at a potentially negative effect of narcissistic individuals on group and organizational performance. For example, in one study, individuals with high levels of narcissism allocated more resources to themselves than did individuals with low levels of narcissism&mdash;at a long-term cost to other group members (Campbell, Bush, Brunell, &amp; Shelton, 2005). However, prior research did not provide a clear link between leader&rsquo;s narcissism and group or organizational performance.</p>\n</blockquote>\n<p><a href=\"http://www.letitiaslabu.net/pdf/Slabu_Guinote_Wilkinson_Power_Facilitates_Attentional_Orienting_InPress.pdf\">&ldquo;How quickly can you detect it? Power facilitates attentional orienting&rdquo;</a>, Slabu et al</p>\n<blockquote>\n<p>Participants were assigned to a high power or control role and then performed a computerised spatial cueing task in which they were required to direct their attention to a target that had been preceded by either a valid or invalid location cue. Compared to participants in the control condition, power-holders were better able to override the misinformation provided by invalid cues. This advantage occurred only at 500 ms stimulus onset asynchrony (SOA), whereas at 1000 ms SOA, when there was more time to prepare a response, no differences were found. These findings are taken to support the growing idea that social power affects cognitive flexibility&hellip;Post-test questionnaires confirmed that these effects could not be attributed to differences in positive affect or self-efficacy. We suggest that power most affected performance during invalid trials because these required a greater degree of cognitive flexibility; individuals needed to ignore the cue and unexpectedly orient attention towards the opposite location. In line with this account, the effect was only evident at relatively short SOAs where participants had little time to prepare an appropriate response. At longer SOAs or on valid trials, the need for flexibility was lower which may explain why no effect was seen.</p>\n<p>Social power affects the way in which information is attended and discriminated (Fiske, 1993; Guinote, 2007a). Power holders have more resources and fewer constraints which gives them more attentional resources and allows them to discriminate between relevant and irrelevant information (Guinote, 2007a; Overbeck &amp; Park, 2001). In contrast, powerless people face more constraints and environmental threats (Keltner, Gruenfeld, &amp; Anderson, 2003). Their dependency encourages them to attend to multiple cues in the environment, in search of any potentially useful information. Thus, they treat information more equally, attending not only to the central information but also to the peripheral or distracting information (Slabu &amp; Guinote, 2010). This overflow in information processing makes powerless people less able to respond promptly to specific situational demands, and induces attentional inflexibility (Guinote, 2007a).</p>\n<ul>\n<li>Fiske, S. T. (1993). Controlling other people: The impact of power on stereotyping. American Psychologist, 48(6), 621-628. doi: 10.1037/0003-066X.48.6.621</li>\n<li>Guinote, A. (2007a). Behaviour variability and the Situated Focus Theory of Power. European Review of Social Psychology, 18, 256-295. doi: 10.1080/10463280701692813</li>\n<li>Overbeck, J. R., &amp; Park, B. (2001). When power does not corrupt: Superior individuation processes among powerful perceivers. Journal of Personality and Social Psychology, 81(4), 549-565. doi: 10.1037/0022-3514.81.4.549</li>\n<li>Slabu, L., &amp; Guinote, A. (2010). Getting what you want: Power increases the accessibility of active goals. Journal of Experimental Social Psychology, 46(2), 344-349. doi: 10.1016/j.jesp.2009.10.013</li>\n</ul>\n<p>Research using basic cognitive paradigms supports these claims. For example, Guinote (2007b) showed that high power participants are better able to focus their attention to target objects and ignore the influence of irrelevant background distracters (see also Smith &amp; Trope, 2006). A further outcome of the cognitive flexibility experienced by powerful individuals is the increased ability to adjust their actions in line with changing contextual cues. This includes the ability to suppress dominant responses and implement non-dominant ones when the task calls for non-dominant responses (Guinote, 2007b).</p>\n<ul>\n<li>Guinote, A. (2007b). Power affects basic cognition: Increased attentional inhibition and flexibility. Journal of Experimental Social Psychology, 43(5), 685-697. doi: 10.1016/j.jesp.2006.06.008</li>\n<li>Smith, P. K., &amp; Trope, Y. (2006). You focus on the forest when you&rsquo;re in charge of the trees: Power priming and abstract information processing. Journal of Personality and Social Psychology, 90(4), 578-596. doi: 10.1037/0022-3514.90.4.578</li>\n</ul>\n<p>For example, several studies have shown that having power increases the ability to resolve conflicts and plan action sequences; power-holders are immune to stimulus-response compatibility effects, and are better able to switch attention between the holistic and detailed components of stimuli, as changing task demands dictate (Guinote, 2007b; Smith, Jostmann, Galinsky, &amp; van Dijk, 2008)&hellip; More broadly, our findings build on those reported by Willis, Rodriguez-Bailon and Lupianez (2011) who showed that powerful individuals can make a better use of cues present in the environment to increase their executive control (see also Smith, et al., 2008). Their data support the idea that social power can impact rudimentary processes associated with spatial orienting and control.</p>\n<ul>\n<li>Willis, G. B., Rodr&iacute;guez-Bail&oacute;n, R., Lupi&aacute;&ntilde;ez, J. (2011). The boss is paying attention: Power Affects the Functioning of the Attentional Networks. Social Cognition, 29(2), 166-181.</li>\n</ul>\n</blockquote>\n<p><a href=\"http://www.psych.nyu.edu/tropelab/publications/SmithTrope2006.pdf\">&ldquo;You focus on the forest when you&rsquo;re in charge of the trees: Power priming and abstract information processing&rdquo;</a>, Smith&amp; Trope 2006</p>\n<blockquote>\n<p>Elevated power increases the psychological distance one feels from others, and this distance, according to construal level theory (Y. Trope &amp; N. Liberman, 2003), should lead to more abstract information processing. Thus, high power should be associated with more abstract thinking&mdash;focusing on primary aspects of stimuli and detecting patterns and structure to extract the gist, as well as categorizing stimuli at a higher level&mdash;relative to low power. In 6 experiments involving both conceptual and perceptual tasks, priming high power led to more abstract processing than did priming low power, even when this led to worse performance. Experiment 7 revealed that in line with past neuropsychological research on abstract thinking, priming high power also led to greater relative right-hemispheric activation.</p>\n<ul>\n<li>Trope, Y., &amp; Liberman, N. (2003). Temporal construal. Psychological Review, 110, 403&ndash; 421</li>\n</ul>\n<p>Though the abstraction hypothesis has not been directly tested, there is some research that supports it. For example, in Overbeck and Park&rsquo;s (2001) experiments, high- and low-power participants interacted via e-mail with several different targets holding the opposite power role and received various kinds of information from them. Some of this information was relevant to the task at hand (e.g., Jim waited until the last minute to try to schedule a meeting), and some was irrelevant (e.g., Jim just started a jazz ensemble). Not only did participants in the high-power role recall more information overall than did the low-power participants, but they were especially superior at recalling relevant information. Thus, high-power participants focused more on primary information, a hallmark of abstract thinking.</p>\n<ul>\n<li>Overbeck, J. R., &amp; Park, B. (2001). When power does not corrupt: Superior individuation processes among powerful perceivers. Journal of Personality and Social Psychology, 81, 549 &ndash;565.</li>\n</ul>\n<p>Portuguese participants used more abstract language to describe both their ethnic group and an outgroup when they were part of the majority (i.e., a higher power group) than when they were part of the minority (i.e., a lower power group; Guinote, 2001). Similarly, participants who played the role of judges during a task used more abstract, trait-like language in referring to themselves than did participants who were workers (Guinote, Judd, &amp; Brauer, 2002).</p>\n<ul>\n<li>Guinote, A. (2001). The perception of group variability in a non-minority and a minority context: When adaptation leads to outgroup differentiation. British Journal of Social Psychology, 40, 117&ndash;132.</li>\n<li>Guinote, A., Judd, C. M., &amp; Brauer, M. (2002). Effects of power on perceived and objective group variability: Evidence that more powerful groups are more variable. Journal of Personality and Social Psychology, 82, 708 &ndash;721</li>\n</ul>\n<p>Powerholders, more than the powerless, should thus be guided by their primary, overriding goals rather than by subordinate, incidental concerns. This would mean that powerholders are more likely to act in accordance with their core attitudes and values (Chen et al., 2001). Indeed, individuals placed in high-power roles or those higher in personality dominance have been found to express their true attitudes more during a discussion than have participants lower in power or dominance (Anderson &amp; Berdahl, 2002). Such goal-driven behavior also has implications for stereotyping. Powerholders should be more likely to stereotype those beneath them when such stereotyping is seen as an effective means to their goals. Evidence for this has already been found in the context of the Social Influence Strategy \u03eb Stereotype Match hypothesis (Vescio, Snyder, &amp; Butz, 2003).</p>\n<ul>\n<li>Chen, S., Lee-Chai, A. Y., &amp; Bargh, J. A. (2001). Relationship orientation as a moderator of the effects of social power. Journal of Personality and Social Psychology, 80, 173-187</li>\n<li>Anderson, C., &amp; Berdahl, J. L. (2002). The experience of power: Examining the effects of power on approach and inhibition tendencies. Journal of Personality and Social Psychology, 83, 1362&ndash;1377</li>\n<li>Vescio, T. K., Snyder, M., &amp; Butz, D. A. (2003). Power in stereotypically masculine domains: A social influence strategy \u03eb stereotype match model. Journal of Personality and Social Psychology, 85, 1062&ndash;1078.</li>\n</ul>\n</blockquote>\n<p><a href=\"http://dl.dropbox.com/u/85192141/2008-smith.pdf\">&ldquo;Powerful People Make Good Decisions Even When They Consciously Think&rdquo;</a>, Smith et al 2008</p>\n<blockquote>\n<p>Thought condition again had different effects on performance for the two priming conditions, F(1, 161) 54.67, prep 5 .91, Zp 2 1&frasl;4 :03 (see Fig. 1). Low-power participants performed significantly better after unconscious thought than after conscious thought, prep 5 .96. High-power participants performed equally well in both thought conditions and did not differ from low-power participants in the unconscious-thought condition, Fs &lt; 1. Furthermore, our manipulations did not significantly affect participants&rsquo; confidence in and certainty of their attitudes, preps &lt; .70, their reported effort or motivation, preps &lt; .84, or the amount of apartment information they correctly recalled, Fs &lt; 1. Differences in performance could not be attributed to depth of processing. When given problems requiring a complex decision, high-power participants were equally good at identifying the better choice after conscious versus unconscious thought, whereas the performance of low-power participants suffered when they consciously deliberated. These results provide further evidence that conscious and unconscious thought differ in the type of processing that occurs. The powerful seem to be able to handle so many impactful decisions, without making excessive errors, in part because they generally think more abstractly.</p>\n</blockquote>\n<p><a href=\"http://cbees.utdallas.edu/papers/EckelFatasWilson03_07_10complete.pdf\">&ldquo;Cooperation and Status in Organizations&rdquo;</a>, Eckel et al 2010</p>\n<blockquote>\n<p>We further manipulate status by allocating the central position to the person who earns the highest, or the lowest, score on a trivia quiz. These high-status and low-status treatments are compared, and we find that the effect of organizational structure &ndash; the existence of a central position &ndash; depends on the status of the central player. Higher status players are attended to and mimicked more systematically. Punishment has differential effects in the two treatments, and is least effective in the high-status case.</p>\n<p>In this study, we ask whether social status serves as a useful mechanism for solving public goods problems. Status can act as a coordinating device, as it does in pure coordination games, with higher-status individuals more likely to be mimicked (followed) by others. In addition, in a setting with costly punishment, social status may enhance the effectiveness of punishment and reduce anti-social punishment, enhancing overall efficiency&hellip;Status is awarded by the experimenter using scores on a general-knowledge trivia quiz that is unrelated to the experimental game. The central position is given to either the high scorer (high-status treatment) or the low scorer (low-status treatment). Subjects play two games: a standard linear voluntary contribution mechanism (VCM) and a VCM with costly punishment. We find that higher-status central players are more likely to be &ldquo;followed&rdquo; in the key situation when the peripheral player is contributing less than the central player. We also find that high status central players punish less, and peripheral players are more responsive to punishment by a higher-status central player&hellip;Our results suggest that punishment, while important to enforcing cooperative norms in many social dilemmas, does not boost contributions in all instances. Punishment is used more readily by low-status groups, and increases overall contributions only among low-status groups. However this seems to be primarily a main effect of the punishment institution, as there is little evidence that punishment tokens levied actually increase contributions in low-status groups; indeed there is weak evidence that the response to punishment is greater in high-status groups. Retaliatory punishment of central players is seen only in the low-status groups. An unexpected consequence of these differences is that punishment is not efficiency- enhancing when the status of the central player is high. Costly punishment is used less in these groups, but contributions are not higher than without punishment. This generates a flat contribution pattern, and no differences between the VCM with and without punishment opportunities. At the other extreme, low status central players punish and are heavily punished, and make significantly less money in the experiment than any other type of subject. But the reaction of low status groups to the new environment generates a significant increase in the provision of the public good.</p>\n<p>Second, high-status agents may have a strong influence on others, as others seek their company and guidance, affecting choices and decision making by lower-status individuals. Thus high-status individuals are more likely to be mimicked or deferred to (Ball et al. 2001, Kumru and Vesterlund 2005). Imitating or learning from higher- status exemplars can help solve coordination problems (Eckel and Wilson 2007); the behavior of the higher-status individual provides an example that is observed and can be followed by others.</p>\n<ul>\n<li>Ball, S., C. Eckel, P. Grossman and W. Zame (2001) &ldquo;Status in markets&rdquo; The Quarterly Journal of Economics 116, 161-188</li>\n<li>Kumru, C. and L. Vesterlund (2005) &ldquo;The effect of status on voluntary contribution&rdquo; Working paper, Department of Economics, University of Pittsburgh.</li>\n<li>Eckel, C. and R. Wilson (2007) &ldquo;Social learning in coordination games: Does status matter?&rdquo; Experimental Economics, 10, 317-330</li>\n</ul>\n<p>Gil-White and Henrich (2001) argue that attending to and mimicking high status individuals is a valuable strategy in a world where successful individuals may have superior information. Cultural transmission is enhanced when higher-status, successful individuals are copied by others. Copying successful individuals has evolutionary payoffs, so that humans may have evolved a preference for paying attention to and learning from high-status agents (see also Boyd and Richerson 2002, Boyd et al. 2003). Bala and Goyal (1998) capture the essence of the idea of attending to a high-status agent in a model where the presence of a commonly-observed agent, which they term the &ldquo;royal family&rdquo;, can have a significant impact on which among multiple equilibria is selected&hellip;Experimental research confirms the tendency of individuals to mimic high-status agents. Eckel and Wilson (2001) show that a commonly observed agent can influence equilibrium selection in a coordination game&hellip;Imitation makes the population of subjects more likely to reach a Pareto-superior, but risk- dominated, equilibrium, an outcome that rarely occurs otherwise (Cooper et al. 1990). Kumru and Vesterlund (2005) show a related result, with high-status first-movers more likely to be mimicked in a 2-person sequential voluntary contribution game. In their setting, high status enhances the ability of leaders to increase total contributions.</p>\n<ul>\n<li>Gil-White, F. and J. Henrich (2001) &ldquo;The evolution of prestige: Freely conferred deference as a mechanism for enhancing the benefits of cultural transmission&rdquo; Evolution and Human Behavior 22,165-196</li>\n<li>Boyd, R., and P. Richerson (2002) &ldquo;Group beneficial norms spread rapidly in a structured population&rdquo; Journal of Theoretical Biology 215, 287-296</li>\n<li>Boyd, R., H. Gintis, S. Bowles, and P. Richerson (2003) &ldquo;The evolution of altruistic punishment&rdquo; Proceedings of the National Academy of Sciences (USA) 100, 3531-3535.</li>\n<li>Bala, V. and S. Goyal (1998) &ldquo;Learning from neighbors&rdquo; Review of Economic Studies 65, 595-621</li>\n<li>Eckel, Catherine C., and Rick K. Wilson (2001) &ldquo;Social learning in a social hierarchy: An experimental study.&rdquo; Rice University, Unpublished manuscript</li>\n<li>Cooper, R., D. DeJong, R. Forsythe and T. Ross (1990). &ldquo;Selection criteria in coordination games: some experimental results. American Economic Review 80, 218-233</li>\n<li>Kumru, C. and L. Vesterlund (2005) &ldquo;The effect of status on voluntary contribution&rdquo; Working paper, Department of Economics, University of Pittsburgh</li>\n</ul>\n</blockquote>\n<p>Another good set of studies focusing on rich/powerful behavior.</p>\n<p>2 of the primary researchers write in a 2012 <em>NYT</em> op-ed <a href=\"http://www.nytimes.com/roomfordebate/2012/03/15/does-morality-have-a-place-on-wall-street/greed-on-wall-street-prevents-good-from-happening\">&ldquo;Greed Prevents Good&rdquo;</a></p>\n<blockquote>\n<p>Now, some 25 years later, seven studies we conducted [Piff et al 2012], some on this same campus, have proved the opposite, that greed, far from being good, undermines moral behavior&hellip;.Unethical behaviors among the wealthy are as timeless and pervasive as the ethical principles that try to rein them in. Our research pinpointed why wealth produces unethical conduct with such regularity: greed. Across studies, wealthier subjects expressed the conviction that greed is moral, echoing [Ivan] Boesky and Gekko and their intellectual companions (e.g., Ayn Rand). And it was their greed-is-good attitudes, we found, that gave rise to their unethical behavior. Wealth gives rise to a me-first mentality, and the ideology of unbridled self-interest serves as its lofty justification. Greg Smith is to be applauded for calling out the culture of greed at Goldman Sachs. It is a knockout blow, one as important as Ivan Boesky&rsquo;s proclamation nearly a generation ago. Nobel laureate Milton Friedman famously argued that the single social responsibility of business is to increase profits as long as &ldquo;it stays within the rules of the game.&rdquo; The problem is, when greed for profits is the bottom line, the rules may fall by the wayside.</p>\n</blockquote>\n<p>Relevant studies:</p>\n<ul>\n<li>\n<p><a href=\"http://greatergood.berkeley.edu/dacherkeltner/docs/kraus.inpress.pdf\">Kraus &amp; Keltner 2009</a>, &ldquo;Signs of socioeconomic status: a thin-slicing approach&rdquo;:</p>\n<blockquote>\n<p>Videos of 60-s slices of these interactions were coded for nonverbal cues of disengagement and engagement, and estimates of participants&rsquo; SES were provided by naive observers who viewed these videos. As predicted by analyses of resource dependence and power, upper-SES participants displayed more disengagement cues (e.g., doodling) and fewer engagement cues (e.g., head nods, laughs) than did lower-SES participants&hellip;.Research relevant to this hypothesis is limited, but suggestive. For example, in a meta-analytic review of status and nonverbal behavior, upper SES individuals were found to speak in ways that are less attentive to the audience, for example, with fewer turn-inviting pauses (<a title=\"Nonverbal and the vertical dimension of social relations: A meta-analysis\" href=\"http://nuweb9.neu.edu/socialinteractionlab/wp-content/uploads/Hall-Coats-Smith-LeBeau-2005.pdf\">Hall et al., 2005</a>)&hellip;SES was measured objectively using self-reports of family income and education (e.g., Lachman &amp; Weaver, 1998). [They used undergraduates, <em>not</em> people who had personally clawed into power.]</p>\n</blockquote>\nConsistent with the previously cited studies about how acting rude or defecting is perceived as power.</li>\n<li>\n<p><a href=\"http://www.rotman.utoronto.ca/facbios/file/Kraus%20C%C3%B4t%C3%A9%20Keltner%20PS%202010.pdf\">Kraus et al 2010</a> &ldquo;Social Class, Contextualism, and Empathic Accuracy&rdquo;:</p>\n<blockquote>\n<p>Recent research suggests that lower-class individuals favor explanations of personal and political outcomes that are oriented to features of the external environment. We extended this work by testing the hypothesis that, as a result, individuals of a lower social class are more empathically accurate in judging the emotions of other people. In three studies, lower-class individuals (compared with upper-class individuals) received higher scores on a test of empathic accuracy (Study 1), judged the emotions of an interaction partner more accurately (Study 2), and made more accurate inferences about emotion from static images of muscle movements in the eyes (Study 3). Moreover, the association between social class and empathic accuracy was explained by the tendency for lower-class individuals to explain social events in terms of features of the external environment.</p>\n</blockquote>\nSee the previous discussions of blame, self-centeredness, lack of empathy, and rule-breaking; related: fundamental attribution bias.</li>\n<li>\n<p><a href=\"http://www.krauslab.com/Stellaretal.Emotion.2012.pdf\">Stellar et al 2012</a>, &ldquo;Class and compassion: socioeconomic factors predict responses to suffering&rdquo;:</p>\n<blockquote>\n<p>Previous research indicates that lower-class individuals experience elevated negative emotions as compared with their upper-class counterparts. We examine how the environments of lower-class individuals can also promote greater compassionate responding-that is, concern for the suffering or well-being of others. In the present research, we investigate class-based differences in dispositional compassion and its activation in situations wherein others are suffering. Across studies, relative to their upper-class counterparts, lower-class individuals reported elevated dispositional compassion (Study 1), as well as greater self-reported compassion during a compassion-inducing video (Study 2) and for another person during a social interaction (Study 3). Lower-class individuals also exhibited heart rate deceleration-a physiological response associated with orienting to the social environment and engaging with others-during the compassion-inducing video (Study 2)&hellip;For example, when describing environmental trends in economic inequality and everyday life outcomes (e.g., being laid off from work), undergraduates of lower subjective socioeconomic status&mdash;measured by ranking oneself in society in terms of income, education, and job status relative to others&mdash;attribute the causes of economic inequality to more external reasons (e.g., political influence, educational opportunity) than dispositional reasons (e.g., hard work, talent), relative to their upper-class counterparts (Kraus et al., 2009)&hellip;Converging evidence also suggests that lower-class individuals favor an interdependent view of the self, whereas upper-class individuals are more inclined to espouse beliefs in an individuals&rsquo; independence and autonomy (Stephens, Fryberg, &amp; Markus, 2011; Stephens, Markus, &amp; Townsend, 2007). For instance, in one study lower-class university students, whose parents&rsquo; highest level of education was a high school diploma, tended to make choices that helped them blend in with others (e.g., by choosing a pen that resembled other pens; Stephens et al., 2007). In contrast, upper-class individuals, whose parents graduated from college, tended to prefer choices that helped them stand out (e.g., by choosing a unique pen). In recent work, Stephens and colleagues (2011) suggest that stronger relational norms among working-class individuals result in a less positive perception of individual choice, which favors an individual&rsquo;s own needs.</p>\n</blockquote>\n</li>\n<li>\n<p><a href=\"http://www.pnas.org/content/109/11/4086.full.pdf\">Piff et al 2012</a>, &ldquo;Higher social class predicts increased unethical behavior&rdquo;:</p>\n<blockquote>\n<p>In studies 1 and 2, upper-class individuals were more likely to break the law while driving, relative to lower-class individuals. In follow-up laboratory studies, upper-class individuals were more likely to exhibit unethical decision-making tendencies (study 3), take valued goods from others (study 4), lie in a negotiation (study 5), cheat to increase their chances of winning a prize (study 6), and endorse unethical behavior at work (study 7) than were lower-class individuals. Mediator and moderator data demonstrated that upper-class individuals&rsquo; unethical tendencies are accounted for, in part, by their more favorable attitudes toward greed&hellip;Individuals from upper-class backgrounds are also less generous and altruistic. In one study, upper-class individuals proved more selfish in an economic game, keeping significantly more laboratory credits&mdash;which they believed would later be exchanged for cash&mdash;than did lower-class participants, who shared more of their credits with a stranger (7). These results parallel nationwide survey data showing that upper-class households donate a smaller proportion of their incomes to charity than do lower-class households (10)&hellip;Research finds that individuals motivated by greed tend to abandon moral principles in their pursuit of self-interest (13). In one study, a financial incentive caused people to be more willing to deceive and cheat others for personal gain (14). In another study, the mere presence of money led individuals to be more likely to cheat in an anagram task to receive a larger financial reward (1)&hellip;Why are upper-class individuals more prone to unethical behavior, from violating traffic codes to taking public goods to lying? This finding is likely to be a multiply determined effect involving both structural and psychological factors. Upper-class individuals&rsquo; relative independence from others and increased privacy in their professions (3) may provide fewer structural constraints and decreased perceptions of risk associated with committing unethical acts (8). The availability of resources to deal with the downstream costs of unethical behavior may increase the likelihood of such acts among the upper class. In addition, independent self-construals among the upper class (22) may shape feelings of entitlement and inattention to the consequences of one&rsquo;s actions on others (23). A reduced concern for others&rsquo; evaluations (24) and increased goal-focus (25) could further instigate unethical tendencies among upper-class individuals. Together, these factors may give rise to a set of culturally shared norms among upper-class individuals that facilitates unethical behavior.</p>\n</blockquote>\n<ul>\n<li>7: Piff PK, Kraus MW, C&ocirc;t&eacute; S, Cheng BH, Keltner D (2010) <a href=\"http://www-2.rotman.utoronto.ca/phd/file/Piffetal.pdf\">&ldquo;Having less, giving more: The influence of social class on prosocial behavior&rdquo;</a>. J Pers Soc Psychol 99:771&ndash;784.</li>\n<li>10: Independent Sector (2002) <em>Giving and Volunteering in the United States</em> (Independent Sector, Washington, DC).</li>\n<li>13: Steinel W, De Dreu CKW (2004) <a href=\"http://econ.ucdenver.edu/beckman/Tiffany/steinel-misrpresentation.pdf\">&ldquo;Social motives and strategic misrepresentation in social decision making&rdquo;</a>. J Pers Soc Psychol 86:419&ndash;434</li>\n<li>14: Aquino K, Freeman D, Reed A, II, Felps W, Lim VK (2009) <a href=\"http://bschool.nus.edu/departments/ManagementNOrganization/publication/VLimpublist/testing-a-social-cognitive-model(2009).pdf\">&ldquo;Testing a social-cognitive model of moral behavior: The interactive influence of situations and moral identity centrality&rdquo;</a>. J Pers Soc Psychol 97:123&ndash;141</li>\n<li>1: Gino F, Pierce L (2009) <a href=\"http://dl.dropbox.com/u/85192141/2009-gino.pdf\">&ldquo;The abundance effect: Unethical behavior in the presence of wealth&rdquo;</a>. Organ Behav Hum Dec 109:142&ndash;155</li>\n</ul>\n</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dBPou4ihoQNY4cquv": 1, "FkzScn5byCs9PxGsA": 1, "GQyPQcdEQF4zXhJBq": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ft2Cm9tWtcLNFLrMw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 59, "extendedScore": null, "score": 0.000125, "legacy": true, "legacyId": "17908", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 59, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Luke/SI asked me to look into what the academic literature might have to say about people in positions of power. This is a summary of some of the recent psychology results.</p>\n<p>The powerful or elite are: fast-planning abstract thinkers who take action (1) in order to pursue single/minimal objectives, are in favor of strict rules for their stereotyped out-group underlings (2) but are rationalizing (3) &amp; hypocritical when it serves their interests (4), especially when they feel secure in their power. They break social norms (5, 6) or ignore context (1) which turns out to be worsened by disclosure of conflicts of interest (7), and lie fluently without mental or physiological stress (6).</p>\n<p>What are powerful members good for? They can help in shifting among equilibria: solving coordination problems or inducing contributions towards public goods (8), and their abstracted Far perspective can be better than the concrete Near of the weak (9).</p>\n<ol>\n<li>Galinsky et al 2003; Guinote, 2007; Lammers et al 2008; Smith &amp; Bargh, 2008</li>\n<li>Eyal &amp; Liberman</li>\n<li>Rustichini &amp; Villeval 2012</li>\n<li>Lammers et al 2010</li>\n<li>Kleef et al 2011 </li>\n<li>Carney et al 2010</li>\n<li>Cain et al 2005; Cain et al 2011</li>\n<li>Eckel et al 2010</li>\n<li>Slabu et al; Smith &amp; Trope 2006; Smith et al 2008</li>\n</ol>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<p>These benefits may not exceed the costs (is inducing contributions all that useful with improved market mechanisms like <a href=\"https://en.wikipedia.org/wiki/Assurance_contract\">assurance contracts</a> - made increasingly famous thanks to <a href=\"http://en.wikipedia.org/wiki/Kickstarter\">Kickstarter</a>?) Now, to forestall objections from someone like Robin Hanson that these traits - if negative - can be ameliorated by improved technology and organizations and the rest just represents our egalitarian forager prejudice against the elites and corporations who gave us the wealthy modern world, I would point out that these traits look like they would be quite effective at maximizing utility and some selected for in future settings\u2026</p>\n<p>(Additional cautions include that, in order to control for all sorts of confounds, these are generally small WEIRD samples in laboratory or university settings involving small-scale power shifts, priming, or other cues; as such, <a href=\"http://www.gwern.net/DNB%20FAQ#flaws-in-mainstream-science-and-psychology\">all the usual criticisms apply</a>.)</p>\n<h1 id=\"1_Notes\"><a href=\"#TOC\"><span class=\"header-section-number\">1</span> Notes</a></h1>\n<ol style=\"list-style-type: decimal\">\n<li>key phrases: \u201cmoral hypocrisy\u201d; \u201cconstrual level theory\u201d (Near/Far) (<a href=\"http://www.overcomingbias.com/2010/06/near-far-summary.html\">Hanson\u2019s visual summary</a>)</li>\n<li>check or blacklist any paper related to <a href=\"/!Wikipedia\">Diederik Stapel</a>! (eg. <a href=\"http://www.nytimes.com/2011/11/03/health/research/noted-dutch-psychologist-stapel-accused-of-research-fraud.html\">\u201cFraud Case Seen as a Red Flag for Psychology Research\u201d</a>)</li>\n</ol>\n<h1 id=\"2_References\"><a href=\"#TOC\"><span class=\"header-section-number\">2</span> References</a></h1>\n<p><a href=\"http://peezer.squarespace.com/storage/moral-reasoning/trust-and-cheating/Psychological%20Science-2010-Lammers-737-44.pdf\">\u201cPower increases hypocrisy: Moralizing in reasoning, immorality in behavior\u201d</a>, Lammers et al 2010; warning, Stapel! But Lammers says committee cleared this paper.</p>\n<blockquote>\n<p>In five studies, we explored whether power increases moral hypocrisy (i.e., imposing strict moral standards on other people but practicing less strict moral behavior oneself). In Experiment 1, compared with the powerless, the powerful condemned other people\u2019s cheating more, but also cheated more themselves. In Experiments 2 through 4, the powerful were more strict in judging other people\u2019s moral transgressions than in judging their own transgressions. A final study found that the effect of power on moral hypocrisy depends on the legitimacy of the power: When power was illegitimate, the moral-hypocrisy effect was reversed, with the illegitimately powerful becoming stricter in judging their own behavior than in judging other people\u2019s behavior. This pattern, which might be dubbed hypercrisy, was also found among low-power participants in Experiments 3 and 4. We discuss how patterns of hypocrisy and hypercrisy among the powerful and powerless can help perpetuate social inequality.</p>\n<p>\u2026feelings of power reduce sensitivity to social disapproval (Emerson, 1962; Thibaut &amp; Kelley, 1959), thus reducing the grip of social norms and standards on power holders\u2019 behavior (Galinsky et al., 2008). As a result, even very strong norms, such as those regulating sexual behavior or compassion, are often ignored by the powerful (Bargh, Raymond, Pryor, &amp; Strack, 1995; Van Kleef et al., 2008).</p>\n<ul>\n<li>Emerson, R.M. (1962). Power-dependence relations. American Sociological Review, 27, 31\u201341</li>\n<li>Thibaut, J.W., &amp; Kelley, H.H. (1959). The social psychology of groups. New York: Wiley &amp; Sons</li>\n<li>Galinsky, A.D., Magee, J.C., Gruenfeld, D.H, Whitson, J., &amp; Liljenquist, K.A. (2008). Social power reduces the strength of the situation: Implications for creativity, conformity, and dissonance. Journal of Personality and Social Psychology, 95, 1450\u20131466</li>\n</ul>\n<p>Powerful people who feel that their position is illegitimate are less inclined to assertively take what they want (Lammers, Galinsky, Gordijn, &amp; Otten, 2008) and at the same time are less inclined to judge others for doing so, compared with people who feel their power is deserved (Chaurand &amp; Brauer, 2008). Therefore, in our final study, we independently manipulated power and its legitimacy to test whether legitimacy crucially moderates the effect of power on hypocrisy.</p>\n<ul>\n<li>Lammers, J., &amp; Stapel, D.A. (2009). How power influences moral thinking. Journal of Personality and Social Psychology, 97, 279\u2013289</li>\n<li>Chaurand, N., &amp; Brauer, M. (2008). What determines social control? People\u2019s reactions to counternormative behaviors in urban environments. Journal of Applied Social Psychology, 38, 1689\u20131715</li>\n</ul>\n</blockquote>\n<p><a href=\"ftp://ftp.gate.cnrs.fr/RePEc/2012/1216.pdf\">\u201cMoral Hypocrisy, Power and Social Preferences\u201d</a>, Rustichini &amp; Villeval 2012:</p>\n<blockquote>\n<p>We show with a laboratory experiment that individuals adjust their moral principles to the situation and to their actions, just as much as they adjust their actions to their principles. We first elicit the individuals\u2019 principles regarding the fairness and unfairness of allocations in three different scenarios (a Dictator game, an Ultimatum game, and a Trust game). One week later, the same individuals are invited to play those same games with monetary compensation. Finally in the same session we elicit again their principles regarding the fairness and unfairness of allocations in the same three scenarios.</p>\n<p>Our results show that individuals adjust abstract norms to fit the game, their role and the choices they made. First, norms that appear abstract and universal take into account the bargaining power of the two sides. The strong side bends the norm in its favor and the weak side agrees: Stated fairness is a compromise with power. Second, in most situations, individuals adjust the range of fair shares after playing the game for real money compared with their initial statement. Third, the discrepancy between hypothetical and real behavior is larger in games where real choices have no strategic consequence (Dictator game and second mover in Trust game) than in those where they do (Ultimatum game). Finally the adjustment of principles to actions is mainly the fact of individuals who behave more selfishly and who have a stronger bargaining power.</p>\n<p>\u2026Individuals destroy the resources of others because of envy (Mui, 1995; Maher, 2010; Charness et al., 2010; Harbring and Irlensbusch, 2011) or for the joy of destruction (Zizzo and Oswald, 2001; Abbink and Sadrieh, 2009); the power of public office sometimes leads politicians to use it for their personal gain (Aidt, 2003); feelings of entitlement push leaders to take more than followers from a common resource (de Cremer and van Dijk, 2005).</p>\n<ul>\n<li>Mui, V.L. (1995). The economics of envy. Journal of Economic Behavior &amp; Organizations, 26(3), 311-336.</li>\n<li>Maher, B. (2010). Research Integrity: Sabotage! Nature, 467, 30 September, 516-518.</li>\n<li>G. Charness, D. Masclet, M.C. Villeval. (2010). Competitive Preferences and Status as an Incentive: Experimental Evidence. IZA Discussion Paper 5034, Bonn</li>\n<li>Harbring, C., Irlensbusch, B. (2011). Sabotage in Tournaments: Evidence from the Laboratory, Management Science, 57(4), 611-627</li>\n<li>Zizzo, D., Oswald, A.J. (2001). Are People Willing to Pay to Reduce Others\u2019 Incomes? Annales d\u2019Economie et de Statistique, 63-64, 39-62</li>\n<li>Abbink, K., Sadrieh, A. (2009). The pleasure of being nasty. Economics Letters, 105(3), 306-308.</li>\n<li>Aidt, T.S. (2003). Economic Analysis of Corruption: A Survey. The Economic Journal, 113, F632-F652.</li>\n<li>de Cremer, D., van Dijk, E. (2005). When and why leaders put themselves first: Leader behaviour in resource allocations as a function of feeling entitled. European Journal of Social Psychology, 35, 553-563.</li>\n</ul>\n<p>social psychologists studying moral hypocrisy have shown that individuals evaluate more negatively the moral transgression of fair principles when this transgression is enacted by others than when enacted by themselves (Valdesolo and deStefano, 2008).</p>\n<ul>\n<li>Valdesolo, P., deStefano, D.A. (2008). The duality of virtue: Deconstructing the moral hypocrite. Journal of Experimental Social Psychology, 44 (5), 1334-1338.</li>\n</ul>\n<p>Such an illusory preference for fairness has been identified by Dana, Weber and Kuang (2007) (see also Larson and Capra, 2009; Grossman, 2010; van der Weele, 2012). Indeed, fairness decreases substantially when the link between fairness and outcome is obfuscated. The choice to play fair is frequently motivated by the willingness to appear fair more than by the willingness to produce a fair outcome and this is why greater anonymity leads to more selfish transfers in the dictator game (Andreoni and Bernheim, 2009; Ariely et al., 2009).</p>\n<ul>\n<li>Dana, J., Weber R.A., Xi Kuang, J. (2007). Exploiting moral wiggle room: experiments demonstrating an illusory preference for fairness. Economic Theory, 33(1), 67-80</li>\n<li>Larson, T., Capra, M. (2009). Exploiting moral wiggle room: Illusory preference for fairness? A comment. Judgment and Decision Making, 4(6), 467-474</li>\n<li>Grossman, Z. (2010). Strategic ignorance and the robustness of social preferences. Working paper, University of California at Santa Barbara</li>\n<li>Van der Weele, J. (2012). When ignorance is innocence: on information avoidance in moral dilemmas. SSRN working paper.</li>\n<li>Andreoni, J., Bernheim, B.D. (2009). Social Image and the 50-50 Norm: A Theoretical and Experimental Analysis of Audience Effects. Econometrica, 77(5), 1607-1636</li>\n<li>Ariely, D., Bracha, A., Meier, S. (2009). Doing Good or Doing Well? Image Motivation and Monetary Incentives in Behaving Prosocially. American Economic Review, 99(1), 544-555</li>\n</ul>\n</blockquote>\n<p><a href=\"http://www.kaynagiminsan.com/wp-content/uploads/2011/06/Social-Psychological-and-Personality-Science-2011-Van-Kleef-1948550611398416.pdf\">\u201cBreaking the Rules to Rise to Power: How Norm Violators Gain Power in the Eyes of Others\u201d</a>, Kleef et al 2011:</p>\n<blockquote>\n<p>Four studies support this hypothesis. Individuals who took coffee from another person\u2019s can (Study 1), violated rules of bookkeeping (Study 2), dropped cigarette ashes on the floor (Study 3), or put their feet on the table (Study 4) were perceived as more powerful than individuals who did not show such behaviors. The effect was mediated by inferences of volitional capacity, and it replicated across different methods (scenario, film clip, face-to-face interaction), different norm violations, and different indices of power (explicit measures, expected emotions, and approach/inhibition tendencies).</p>\n<p>\u2026\u2018\u2018Power tends to corrupt, and absolute power corrupts absolutely,\u2019\u2019 wrote Lord Acton to Bishop Mandell Creighton in 1887. This classic adage not only reflects popular sentiments about power; it is also supported by scientific research (e.g., Kipnis, 1972).</p>\n<ul>\n<li>Kipnis, D. (1972). Does power corrupt? Journal of Personality and Social Psychology, 24, 33-41</li>\n</ul>\n<p>Individuals who feel powerful are more likely to act in goal-congruent ways (e.g., by switching off an annoying fan) than those who feel less powerful (Galinsky, Gruenfeld, &amp; Magee, 2003). Powerful individuals are also more likely to take risks (Anderson &amp; Galinsky, 2006), show approach-related tendencies and goal-directed action (Guinote, 2007; Lammers, Galinsky, Gordijn, &amp; Otten, 2008; Smith &amp; Bargh, 2008), express their emotions (Hecht &amp; Lafrance, 1998), act based on their dispositional inclinations (Chen, Lee-Chai, &amp; Bargh, 2001) and momentary desires (Van Kleef &amp; Cote, 2007), and ignore situational pressures (Galinsky et al., 2008).</p>\n<ul>\n<li>Galinsky, A. D., Gruenfeld, D. H., &amp; Magee, J. C. (2003). From power to action. Journal of Personality and Social Psychology, 85, 453-466</li>\n<li>Anderson, C., &amp; Galinsky, A. D. (2006). Power, optimism and risk-taking. European Journal of Social Psychology, 36, 511-536</li>\n<li>Guinote, A. (2007). Power and goal pursuit. Personality and Social Psychology Bulletin, 33, 1076-1087</li>\n<li>Lammers, J., Galinsky, A. D., Gordijn, E. H., &amp; Otten, S. (2008). Illegitimacy moderates the effects of power on approach. Psychological Science, 19, 558-564</li>\n<li>Smith, P. K., &amp; Bargh, J. A. (2008). Nonconscious effects of power on basic approach and avoidance tendencies. Social Cognition, 26, 1-24</li>\n<li>Hecht, M. A., &amp; Lafrance, M. (1998). License or obligation to smile: The effect of power and sex on amount and type of smiling. Personality and Social Psychology Bulletin, 24, 1332-1342</li>\n<li>Chen, S., Lee-Chai, A. Y., &amp; Bargh, J. A. (2001). Relationship orientation as a moderator of the effects of social power. Journal of Personality and Social Psychology, 80, 173-187</li>\n<li>Van Kleef, G. A., &amp; &amp; Cote, S (2007). Expressing anger in conflict: When it helps and when it hurts. Journal of Applied Psychology, 92, 1557-1569</li>\n<li>Galinsky, A. D., Gruenfeld, D. H., Magee, J. C., Whitson, J. A., &amp; Liljenquist, K. A. (2008). Power reduces the press of the situation: Implications for creativity, conformity, and dissonance. Journal of Personality and Social Psychology, 95, 1450-1466</li>\n</ul>\n<p>This behavioral disinhibition makes powerful people more likely to exhibit socially inappropriate behavior. Compared to lower power individuals, powerful individuals are likely to take more cookies from a common plate, eat with their mouths open, and spread crumbs (Keltner et al., 2003); interrupt conversation partners and invade their personal space (DePaulo &amp; Friedman, 1998); fail to take another\u2019s perspective (Galinsky, Magee, Inesi, &amp; Gruenfeld, 2006); ignore other people\u2019s suffering (Van Kleef et al., 2008); stereotype (Fiske, 1993) and patronize others (Vescio, Gervais, Snyder, &amp; Hoover, 2005); cheat (Lammers, Stapel, &amp; Galinsky, 2010); take credit for the contributions of others (Kipnis, 1972); treat other people as a means to their own ends (Gruenfeld, Inesi, Magee, &amp; Galinsky, 2008); and sexualize and harass low-power women (Bargh, Raymond, Pryor, &amp; Strack, 1995). Powerful people also exhibit more aggression (Haney, Banks, &amp; Zimbardo, 1973), and this is relatively acceptable to others (Porath, Overbeck, &amp; Pearson, 2008). In fact, in several European countries the liberty to violate norms without sanction is perceived as a defining feature of the power holder (Mondillon et al., 2005). Although the powerful impose strict moral standards on others, they practice less strict moral behavior themselves (Lammers et al., 2010).</p>\n<ul>\n<li>Keltner, D., &amp; Gruenfeld, D. H, &amp; Anderson, C. (2003). Power, approach, and inhibition. Psychological Review, 110, 265-284.</li>\n<li>DePaulo, B. M., &amp; Friedman, H. S. (1998). Nonverbal communication. In D. Gilbert, S. T. Fiske, &amp; G. Lindzey (Eds.), Handbook of social psychology (pp.&nbsp;3-40). New York: McGraw-Hill</li>\n<li>Galinsky, A. D., Magee, J. C., Inesi, M. E., &amp; Gruenfeld, D. H. (2006). Power and perspectives not taken. Psychological Science, 17, 1068-1074</li>\n<li>Van Kleef, G. A., Oveis, C., Van Der Lowe, I., LuoKogan, A., Goetz, J., &amp; Keltner, D. (2008). Power, distress, and compassion: Turning a blind eye to the suffering of others. Psychological Science, 19, 1315-1322</li>\n<li>Fiske, S. T. (1993). Controlling other people: The impact of power on stereotyping. American Psychologist, 48, 621-628</li>\n<li>Lammers, J., Stapel, D. A., &amp; Galinsky, A. D. (2010). Power increases hypocrisy: Moralizing in reasoning, immorality in behavior. Psychological Science, 21, 737-744; WARNING: Stapel! Lammers <a href=\"http://lammers.socialpsychology.org/\">states</a> that this paper is untainted:</li>\n</ul>\n<blockquote>\n<p>IMPORTANT: Regarding the scientific fraud of my former supervisor Stapel: the committee Levelt has investigated all my work with Stapel. All my work on the topic of power has been cleared from suspicion of data-fraud. This research is all based on data that I collected myself or collected together with other co-authors (i.e.&nbsp;not Stapel). There is one paper (on racism in legal decisions) where I was misled. This paper contains false data. It is currently being retracted.</p>\n</blockquote>\n<ul>\n<li>Gruenfeld, D. H., Inesi, M. E., Magee, J. C., &amp; Galinsky, A. D. (2008). Power and the objectification of social targets. Journal of Personality and Social Psychology, 95, 111-127</li>\n<li>Bargh, J. A., Raymond, P., Pryor, J. B., &amp; Strack, F. (1995). Attractiveness of the underling: An automatic power-sex association and its consequences for sexual harassment and aggression. Journal of Personality and Social Psychology, 68, 768-781</li>\n<li>Haney, C., Banks, C., &amp; Zimbardo, P. (1973). Interpersonal dynamics in a simulated prison. International Journal of Criminology and Penology, 1, 69-97</li>\n<li>Porath, C. L., Overbeck, J., &amp; Pearson, C. M. (2008). Picking up the gauntlet: How individuals respond to status challenges. Journal of Applied Social Psychology, 38, 1945-1980</li>\n<li>Mondillon, L., Niedenthal, P. M., Brauer, M., Rohman, A., Dalle, N., &amp; Uchida, Y. (2005). Beliefs about power and its relation to emotional experience: A comparison of Japan, France, Germany, and United States. Personality and Social Psychology Bulletin, 31, 1112-1122</li>\n</ul>\n<p>...research on adolescent aggression indicates that bullying behavior is associated with prestige (Savin-Williams, 1976; Sijtsema, Veenstra, Lindenberg, &amp; Salmivalli, 2009).</p>\n<ul>\n<li>Savin-Williams, R. C. (1976). An ethological study of dominance formation and maintenance in a group of human adolescents. Child Development, 47, 972-979</li>\n<li>Sijtsema, J. J., Veenstra, R., Lindenberg, S., &amp; Salmivalli, C. (2009). Empirical test of bullies\u2019 status goals: Assessing direct goals, aggression, and prestige. Aggressive Behavior, 35, 57-67</li>\n</ul>\n</blockquote>\n<p><a href=\"http://portal.idc.ac.il/en/Symposium/HSPSP/2010/Documents/08-eyal-liberman.pdf\">\u201cMorality and Psychological Distance: A Construal Level Theory Perspective\u201d</a>, Eyal &amp; Liberman:</p>\n<blockquote>\n<p>In this chapter, we propose one answer to the question of when values and moral principles play a central role in people\u2019s judgments and plans. We explore the possibility that values and moral principles are more prominent in judgments and predictions regarding psychologically more distant events. This perspective is based on construal level theory (CLT; Liberman &amp; Trope, 2008; Liberman, Trope, &amp; Stephan, 2007; Trope &amp; Liberman, in press), according to which the construal of psychologically more distant situations highlights more abstract, high-level features. Because values and moral rules tend to be abstract and general, people are more likely to use them in construing, judging, and planning with respect to psychologically more distant situations.</p>\n<p>For example, Nussbaum, Trope, and Liberman (2003, Study 2) conceptualized personal dispositions as high-level construals and situational constrains as low-level construals and demonstrated that people expect others to express their personal dispositions and act consistently across different situations in the distant future more than in the near future. In the study, participants imagined an acquaintance\u2019s behavior in four different situations (e.g., a birthday party, waiting in line at the supermarket) in either the near future or the distant future and rated the extent to which the acquaintance would display 15 traits (e.g., behave in a friendly vs.&nbsp;an unfriendly manner) representative of the Big Five personality dimensions (extraversion, agreeableness, conscientiousness, emotional stability, and intellect). Cross- situational consistency was assessed by computing, for each of the 15 traits, the variance in each predicted behavior across the four situations and the correlations among the predicted behaviors in the four situations. As predicted, participants expected others to behave more consistently across distant-future situations than across near-future situations. This finding was replicated with ratings of participants\u2019 own behavior in different situations: Participants anticipated exhibiting more consistent traits in the distant future than in the near future (Wakslak, Nussbaum, Liberman, &amp; Trope, 2008, Study 5).</p>\n<ul>\n<li>Nussbaum, S., Trope, Y., &amp; Liberman, N. (2003). \u201cCreeping dispositionism: The temporal dynamics of behavior prediction\u201d. Journal of Personality and Social Psychology, 84, 485-497</li>\n<li>Wakslak, C. J., Nussbaum, S., Liberman, N., &amp; Trope, Y. (2008). \u201cRepresentations of the self in the near and distant future\u201d. Journal of Personality and Social Psychology, 95, 757-773</li>\n</ul>\n<p>For each scenario (e.g., national flag), participants chose between two restatements of each action. One restatement referred to an abstract moral principle (high-level construal; e.g., desecrating a national symbol) and the other restatement referred to the means of carrying out the action (low-level construal; e.g., cutting a flag to created rags). We found that distant-future transgressions were identified in moral terms more often than near-future transgressions. These findings suggest that people are more likely to think of a temporally distant action, rather than one in the near term, as having moral implications. CLT predicts similar results for other forms of psychological distance: Situations should be more readily construed in terms of moral principles when they occurred further back in the past, when they apply to more socially or spatially distant individuals or groups, and when they are less likely actually to occur. When the same actions are proximal, they are more likely to be construed in terms that are devoid of moral implications. For example, accepting minority students with lower grades into one\u2019s university will be seen as \u201cendorsing affirmative action\u201d when it is unlikely to be implemented, but it will be seen in more concrete terms (e.g., as \u201cmaking acceptance rules more complicated\u201d) when it becomes more likely.</p>\n<p>The vignettes also included situational details that rendered the transgressions harmless (low-level information; e.g., the siblings used contraceptives, they had sex just once, they kept it a secret). Participants were instructed to imagine that the transgressions would occur tomorrow (the near-future condition) or next year (the distant-future condition) and judged the extent of its wrongness. We found that moral transgressions were judged more severely when imagined in the distant future compared to the near future. The same pattern occurred with social distance (Eyal et al., 2008, Study 3), which was manipulated by asking participants to focus either on the feelings and thoughts they experienced while reading about the events (low social distance) or to think about another person they knew, such as a colleague, a friend, or a neighbor, and focus on the feelings and thoughts that this person would experience while reading about the events (high social distance). Notice that the social distance manipulation did not involve judging one\u2019s own versus another person\u2019s actions, but only one\u2019s imagined perspective. Notably, this manipulation does not support interpreting the results in terms of moral hypocrisy, according to which people judge their own moral transgressions less harshly than another person\u2019s transgressions because they wish to appear better than others. As predicted, moral transgressions were judged more harshly when imagined from a third person perspective (high social distance) compared to one\u2019s own perspective (low social distance). Another study (Eyal et al., 2008, Study 4) examined temporal distance effects on judgments of moral acts. Participants read vignettes that described virtuous acts related to widely accepted moral principles (high-level information; e.g., a couple adopting a disabled child) as well as low-level, situational details that rendered the acts less noble (e.g., the government offering large adoption payments). It was found that these behaviors were judged to be more virtuous when they were described as happening in the distant future rather than the near future.</p>\n<p>Temporal distance from moral transgressions was also found to affect people\u2019s emotional responses. Agerstrom and Bjorklund (2009, Studies 1 and 2) asked Swedish participants to imagine situations that involved a threat to human welfare taking place in the near future (today) or in the distant future (in 30 years). For example, one scenario, set in Darfur, Africa, described a woman who was raped and beaten by the Janjaweed militia. Each scenario was followed by a description of a prosocial action that, if taken, could improve the situation (e.g., donate money). Participants rated how wrong it would be for another Swedish citizen not to take the proposed prosocial action given that they had the means to do so. They also rated how angry they would feel if the target person failed to take the prosocial action. It was found that distant-future moral failures were judged more harshly and invoked more anger than near-future moral failures.</p>\n<p>In another study, Agerstrom and Bjorklund (2009) examined whether the greater reliance on moral principles in judgments of distant-future compared to near-future transgressions would generalize to individuals\u2019 self-perceptions. Participants rated the likelihood of engaging in prosocial actions in reaction to other people\u2019s moral transgressions. For example, participants indicated how much money they were willing to donate to help improve the situation in Darfur. As predicted, participants were more likely to express prosocial behavioral intentions when imagining the act occurring in the more distant future. Taken together, these findings suggest that moral rules are more likely to guide people\u2019s judgments of distant rather than proximal behaviors.</p>\n<ul>\n<li>Agerstr\u00f6m, J., &amp; Bj\u00f6rklund, F. (2009). Temporal distance and moral concerns: Future morally questionable behavior is perceived as more wrong and evokes stronger prosocial intentions. Basic and Applied Social Psychology, 31, 49-59</li>\n</ul>\n<p>For example, individuals for whom altruism was subordinate in importance to achievement were more likely to refuse to help a fellow student in the distant future than in the near future, whereas individuals for whom achievement was subordinate to altruism were more likely to help a fellow student in the distant future than in the near future. These findings show that secondary values, which are nonetheless part of an individual\u2019s self-identity, may mask the influence of central values on near future intentions. Centrality of values may be defined not only within an individual but also within a situation. For example, when medically treating a person from a rival group in a war, the competition is central and mercy is secondary, whereas in a hospital, the reverse is true. An interesting prediction that follows from CLT is that the secondary value will guide behavioral intentions in the near future more than in the distant future. Thus, in a war, benevolence will come into play in near-future plans more than in distant-future plans, leading people to be more merciful than would otherwise be expected. In his poem \u201cAfter the Battle\u201d, Victor Hugo tells about his father (\u201cthat hero with the sweetest smile\u201d), an officer in the war against Spain, who encounters a Spaniard soldier asking for something to drink. Although on the battlefield, and although the Spaniard tries to kill him, the officer orders: \u201cAll the same, give him something to drink.\u201d</p>\n</blockquote>\n<p><a href=\"http://www.engaged-zen.org/PDFarchive/PowerLying.pdf\">\u201cPeople with Power are Better Liars\u201d</a>, Carney et al 2010:</p>\n<blockquote>\n<p>But lying does not come without cost. Ordinary lie-tellers experience negative emotions, decrements in mental function, and physiological stress. Liars are also at risk of getting caught. Despite people\u2019s best attempts to get away with their prevarications, lies are often behaviorally \u201cleaked\u201d through subtle changes in body movement and speech rate. Power, it seems, enhances the same emotional, cognitive, and physiological systems that lie-telling depletes. People with power enjoy positive emotions, increases in cognitive function (4-5), and physiological resilience such as lower levels of the stress hormone cortisol (6-7). Thus, holding power over others might make it easier for people to tell lies.</p>\n<ol style=\"list-style-type: decimal\">\n<li>D. Keltner, D.H. Gruenfeld, C. Anderson, Psychol Rev.&nbsp;110, 265-284 (2003).</li>\n<li>P.K. Smith, N.B. Jostmann, A.D. Galinsky, W. van Dijk, Psychol Sci. 19, 441-447 (2008).</li>\n<li>R.M. Sapolsky, S.C. Alberts. J. Altmann, J Arch Gen Psychi. 54, 1137-1143 (1997).</li>\n<li>S. Cohen, W.J. Doyle, A. Baum, Psychosom Med. 68, 414-420 (2006)</li>\n</ol>\n<p>Participants were assigned to the role of \u201cleader\u201d or \u201csubordinate\u201d and engaged in a series social interactions in which the leader had control over the subordinate\u2019s monetary and social outcomes (9)\u2026.If the individual could successfully convince the experimenter (regardless of whether they were lying) they could keep the $100 in cash. All participants were then interviewed about whether they had stolen the money: half were lying and half were telling the truth. The interviewer (blind to experimental condition) asked all participants the same critical questions (e.g., \u201cDid you steal the $100?\u201d; \u201cWhy should I believe you?\u201d). After the interview, participants completed measures of moral emotional feelings (rated emotion terms: bashful, guilty, troubled, scornful) and a computerized task assessing degree of cognitive impairment. All participants provided saliva samples before and after the experiment to assess changes in the stress hormone cortisol (9). The interviews were videotaped and coded for two, classic nonverbal markers of deception: one-sided shoulder shrugs and accelerated prosody (9). Low-power individuals showed the expected emotional, cognitive, physiological, and behavioral signs of deception; in contrast, powerful people demonstrated no evidence of lying across emotion, cognition, physiology, or behavior (see Figure). In other words, power acted as a buffer allowing the powerful to lie significantly more easily (less disturbing emotion, less cognitive impairment, less of a rise in the stress hormone cortisol) and more effectively (fewer nonverbal cues associated with lying). Only low-power individuals felt badly after lying (panel A), suffered cognitive impairment (panel B), spiked in levels of the stress hormone cortisol (panel C), and demonstrated nonverbal \u201cleakage\u201d (more one-sided shoulder shrugs and accelerated prosody; panel D). (9)</p>\n</blockquote>\n<p><a href=\"http://www.bu.edu/law/central/jd/organizations/journals/bulr/documents/LANGEVOORT.pdf\">\u201cPsychological perspectives on the fiduciary business\u201d</a>, Donald C. Langevoort</p>\n<blockquote>\n<p>But the investment game has been manipulated in numerous ways that produce differing levels of trusting and greater selfishness. One of particular interest is the introduction of the possibility that, at the end of the game, the trustor will learn whether she gets something back but will not know whether this is the result of the trustee\u2019s choice or some exogenous force \u2013 e.g., luck.34 Given the opportunity to hide behind the possibility that a return of nothing was just bad luck for the trustor, trustees predictably keep more for themselves, presumably rationalizing the outcome as fair in an uncertain world. The authors of one such study recently drew parallels to financial relationships between investors and securities professionals, because the financial markets generate a great deal of good and bad luck that obscures the value added by professional trustworthiness.35</p>\n<ol style=\"list-style-type: decimal\">\n<li>Radu Vranceanu et al., Trust and Financial Trades: Lessons from an Investment Game Where Reciprocators Can Hide Behind Probabilities 6 (ESSEC Bus. Sch., Working Paper No. 10007, 2010), available at http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1 611666.</li>\n<li>See id. at 14-15.</li>\n</ol>\n<p>Unfortunately, high testosterone levels do not fit well with fiduciary characteristics like empathy and moral decision-making. Emerging research on the subject suggests that testosterone buffers emotional constraints on aggression and risk-taking, leading to a more \u201ccold\u201d utilitarian calculus and a greater willingness to do harm to gain a preferred outcome.49</p>\n<p>See Dana R. Carney &amp; Malia F. Mason, Decision Making and Testosterone: When the Ends Justify the Means, 46 J. EXPERIMENTAL SOC. PSYCHOL. 668, 668-69 (2010). As the authors point out, the ends need not necessarily be immoral. Id. at 670.</p>\n<p>Power also seems to increase hypocrisy \u2013 insistence on adherence to strict norms by others, while enjoying far greater nimbleness in justifying one\u2019s own departures on utilitarian or other rationalized grounds52 \u2013 and optimism and risk-taking.53 Of course, power may be gained in the first place by those skilled at rationalization and willing to take risks, in which case there is a dynamic feedback loop that is likely to generate increasing hypocrisy and hubris over time.</p>\n<ol style=\"list-style-type: decimal\">\n<li>See Joris Lammers et al., Power Increases Hypocrisy: Moralizing in Reasoning, Immorality in Behavior, 21 PSYCHOL. SCI. 737, 738 (2010).</li>\n<li>See Cameron Anderson &amp; Adam D. Galinsky, Power, Optimism, and Risk-taking, 36 EUR. J. SOC. PSYCHOL. 511, 516 (2006). In turn, this pattern may connect to testosterone or other physiological effects. See Carney &amp; Mason, supra note 49, at 668.</li>\n</ol></blockquote>\n<p><a href=\"http://www.jstor.org/stable/10.1086/426699\">\u201cThe Dirt on Coming Clean: Perverse Effects of Disclosing Conflicts of Interest\u201d</a>, Cain et al 2005</p>\n<blockquote>\n<p>Although disclosure is often proposed as a potential solution to these problems, we show that it can have perverse effects. First, people generally do not discount advice from biased advisors as much as they should, even when advisors\u2019 conflicts of interest are disclosed. Second, disclosure can increase the bias in advice because it leads advisors to feel morally licensed and strategically encouraged to exaggerate their advice even further. As a result, disclosure may fail to solve the problems created by conflicts of interest and may sometimes even make matters worse.</p>\n<p>\u2026In the domain of medicine, for example, research shows that while many people are ready to acknowledge that doctors might generally be affected by conflicts of interest, few can imagine that their own doctors would be affected (Gibbons et al. 1998). Indeed, it is even possible that disclosure could sometimes increase rather than decrease trust, especially if the person with the conflict of interest is the one who issues the disclosure. Research suggests that when managers offer negative financial disclosures about future earnings, they are regarded as more credible agents, at least in the short term (Lee, Peterson, and Tiedens 2004; Mercer, forthcoming). Thus, if a doctor tells a patient that her research is funded by the manufacturer of the medication that she is prescribing, the patient might then think (perhaps rightly) that the doctor is going out of her way to be open or that she is \u201cdeeply involved\u201d and thus knowledgeable. Thus, disclosure could cause the estimator to place more rather than less weight on the advisor\u2019s advice. Third, even when estimators realize that they should make some adjustment for the conflict of interest that is disclosed, such adjustments are likely to be insufficient. As a rule, people have trouble unlearning, ignoring, or suppressing the use of knowledge (such as biased advice) even if they are aware that it is inaccurate (Wilson and Brekke 1994). Research on anchoring, for example, shows that quantitative judgments are often drawn toward numbers (the anchors) that happen to be mentally available. This effect holds even when those anchors are known to be irrelevant (Strack and Mussweiler 1997; Tversky and Kahneman 1974), unreliable (Loftus 1979), or even manipulative (Galinsky and Mussweiler 2001; Hastie, Schkade, and Payne 1999). Research on the \u201ccurse of knowledge\u201d (Camerer, Loewenstein, and Weber 1989) shows that people\u2019s judgments are influenced even by information they know they should ignore. And research on what has been called the \u201cfailure of evidentiary discreditation\u201d shows that when the evidence on which beliefs were revised is totally discredited, those beliefs do not revert to their original states but show a persistent effect of the discredited evidence (Skurnik, Moskowitz, and Johnson 2002; Ross, Lepper, and Hubbard 1975). Furthermore, attempts to willfully suppress undesired thoughts can lead to ironic rebound effects, in some cases even increasing the spontaneous use of undesired knowledge (Wegner 1994).</p>\n<p>\u2026More interesting, and as predicted, all three measures also reveal that disclosure led to greater distortion of advice. The amount that advisors exaggerated, calculated by subtracting advisors\u2019 own personal estimates from their public suggestions, was significantly greater in the high/disclosed condition than in either of the other two conditions (p&lt;0.05) and significantly greater by the other two measures as well: advisor suggestion minus actual jar values and advisor suggestion minus the average of personal estimates in the accurate condition (p&lt;0.05 for both). In the accurate condition, for example, advisors provided estimators with suggestions of jar values that were, on average, within $1 of their own personal estimates. In the high/undisclosed condition, however, advisors gave suggestions that were $3.32 greater than their own personal estimates, and in the high/disclosed condition, they gave suggestions that were inflated more than twice as much, at more than $7 above their own personal estimates. Disclosure, it appears, did lead advisors to provide estimators with more biased advice.</p>\n<p>\u2026Although disclosures did increase discounting by estimators, albeit not significantly, this discounting was not sufficient to offset the increase in the bias of the advice they received. As Table 6 (fourth row) shows, estimator discounting increased, on average, less than $2 from the accurate condition to the high/undisclosed condition and less than $2.50 from the high/undisclosed condition to the high/disclosed condition. However, Table 5 (second row) shows that suggestions increased, on average, almost $4 from the accurate condition to the high/undisclosed condition and increased $4 again from the high/undisclosed condition to the high/disclosed condition. Thus, while estimators in the high/disclosed condition discounted suggestions about $4 more than did estimators in the accurate condition, the advice given in the high/disclosed condition was almost $8 higher than advice given in the accurate condition. Instead of correcting for bias, estimates were approximately 28 percent higher in the high/disclosed condition than in the accurate condition (first row of Table 6).</p>\n</blockquote>\n<p><a href=\"http://dl.dropbox.com/u/85192141/2011-cain.pdf\">\u201cWhen Sunlight Fails to Disinfect: Understanding the Perverse Effects of Disclosing Conflicts of Interest\u201d</a>, Cain et al 2011</p>\n<blockquote>\n<p>Studies 1 and 2 examine psychological mechanisms (strategic exaggeration, moral licensing) by which disclosure can lead advisors to give more-biased advice. Study 3 shows that disclosure backfires when advice recipients who receive disclosure fail to sufficiently discount and thus fail to mitigate the adverse effects of disclosure on advisor bias. Study 4 identifies one remedy for inadequate discounting of biased advice: explicitly and simultaneously contrasting biased advice with unbiased advice.</p>\n<p>\u2026Even in one-shot dictator games (Forsythe et al. 1994), research has long shown that many people will share resources and show self-restraint toward anonymous others (Camerer 2003), especially when it is common knowledge that the recipient expects such benevolence (Dana, Cain, and Dawes 2006). Likewise, research on cheating behavior shows that people do not tend to cheat as much as they can get away with, only to the extent that they can rationalize to themselves (Mazar, Amir, and Ariely 2008).</p>\n<p>\u2026When the welfare of others is a consideration, disclosure might reduce moral concerns. Prior research has suggested that when people demonstrate ethical behavior, they often become more likely to subsequently exhibit ethical lapses (Jordan, Mullen, and Murnighan 2009; Zhong, Liljenquist, and Cain 2009). For example, people who are given an opportunity to demonstrate their own lack of prejudice are more likely to subsequently display discriminatory behavior (Monin and Miller 2001). Likewise, after a conflict of interest has been disclosed, advisors may feel that advisees have been warned and that advisors are \u201cmorally licensed\u201d to provide biased advice.</p>\n<p>\u2026Disclosure of a conflict of interest can also reduce the perceived immorality of giving biased advice by signaling that bias is widespread and therefore less aberrant (Schultz et al. 2007). If advice recipients\u2019 expectations affect advisor behavior (Dana et al. 2006), then the lowered expectations for honesty that come with disclosure might allow an advisor to rationalize providing biased advice because that is exactly what the advisee expects, or should expect, to receive.</p>\n<p>\u2026Why is the call for disclosure so popular despite how it can backfire? One possible explanation is that most people are simply not aware of disclosure\u2019s pitfalls. At first glance, disclosure seems like a sensible remedy to a situation in which one party possesses an otherwise hidden incentive to mislead another party. A more cynical explanation would play on the Chicago Theory of Regulation (Becker 1983; Peltzman 1976; Stigler 1971), which posits that regulation typically exists not for the general benefit of society but for the benefit of the regulated groups. These entities might be aware of the ineffectiveness of disclosure but accept it because it benefits them. For example, even though consumer advocates fought hard for warning labels on cigarette packages, the tobacco industry has defended itself against litigation since then by citing the warning labels as evidence that consumers knew the risks. \u201cWhat was intended as a burden on tobacco became a shield instead\u201d (Action on Smoking and Health 2001). Moreover, even the regulators may be attracted to disclosure if they see it as absolving them of responsibility for protecting consumers by ostensibly empowering consumers to protect themselves. Disclosure may also be perceived as the lesser of evils for those who might otherwise face more substantive regulation. For example, pharmaceutical firms are often strong proponents of disclosure laws, since it is better for them (and for researchers who receive their funding) if researchers must disclose financial ties to the industry rather than actually having to sever them. This all suggests that disclosure may be problematic for more reasons than those identified by the experiments reported above. It would be a mistake, however, to conclude that disclosure is always counterproductive, as some recent laboratory research illustrates (Church and Kuang 2009; Koch and Schmidt 2009). Research on practical examples of disclosure, summarized in Full Disclosure (Fung, Graham, and Weil 2007), also shows that disclosure can have real beneficial effects. For example, following a spate of highly publicized SUV rollovers, regulations that required auto manufacturers to publicly disclose rollover ratings led to significant and rapid changes in auto design, resulting in a general decrease in the rollover risk for SUVs. Disclosure is likely to be helpful when information is disclosed in an easily digestible form (or is made available to intermediaries, e.g., ratings companies, who process it for consumers) and when it is clear how one should respond to the disclosed information. The rollover ratings met both criteria: the ratings were represented simply as one to five stars, making it easy for consumers to compare\u2014that is, evaluate jointly\u2014the relative rollover risks of various SUVs. Even when information isn\u2019t presented in such a simple form, disclosure is likely to prove helpful when the recipients are savvy repeat-players who know what to do with the disclosed information, such as institutional investors, experienced attorneys, or managers in government agencies (Church and Kuang 2009; Malmendier and Shanthikumar 2007). Disclosure is much less likely to help individuals such as personal investors, purchasers of insurance, home buyers, or patients, who are unlikely to possess the knowledge or experience to know how much they should discount advice or whether they should get a second opinion in a given conflict-of-interest situation (Malmendier and Shanthikumar 2007).</p>\n</blockquote>\n<p><a href=\"http://www.public-speaking.com/resources/Self-Study/Speaking-Tips/Power-Posing.pdf\">\u201cPower Posing: Brief Nonverbal Displays Affect Neuroendocrine Levels and Risk Tolerance\u201d</a> Carney et al 2010</p>\n<blockquote>\n<p>As predicted, results revealed that posing in high-power (vs.&nbsp;low-power) nonverbal displays caused neuroendocrine and behavioral changes for both male and female participants: High-power posers experienced elevations in testosterone, decreases in cortisol, and increased feelings of power and tolerance for risk; low-power posers exhibited the opposite pattern. In short, posing in powerful displays caused advantaged and adaptive psychological, physiological, and behavioral changes \u2013 findings that suggest that embodiment extends beyond mere thinking and feeling, to physiology and subsequent behavioral choices.</p>\n<p>\u2026The neuroendocrine profiles of the powerful differentiate them from the powerless, on two key hormones\u2014testosterone and cortisol. In humans and other animals, testosterone levels both reflect and reinforce dispositional and situational status and dominance; internal and external cues cause testosterone to rise, increasing dominant behaviors, and these behaviors can elevate testosterone even further (Archer, 2006; Mazur &amp; Booth, 1998). For example, testosterone rises in anticipation of a competition and as a result of a win, but drops following a defeat (e.g., Booth, Shelley, Mazur, Tharp, &amp; Kittok, 1989), and these changes predict the desire to compete again (Mehta &amp; Josephs, 2006). In short, testosterone levels, by reflecting and reinforcing dominance, are closely linked to adaptive responses to challenges.</p>\n<ul>\n<li>Archer, J. (2006). Testosterone and human aggression: An evaluation of the challenge hypothesis. Neuroscience &amp; Biobehavioral Reviews, 30, 319\u2013345.</li>\n<li>Mazur, A., &amp; Booth, A. (1998). Testosterone and dominance in men. Behavioral &amp; Brain Sciences, 21, 353\u2013397</li>\n<li>Booth, A., Shelley, G., Mazur, A., Tharp, G., &amp; Kittok, R. (1989). Testosterone and winning and losing in human competition. Hormones and Behavior, 23, 556\u2013571.</li>\n<li>Mehta, P.H., &amp; Josephs, R.A. (2006). Testosterone change after losing predicts the decision to compete again. Hormones and Behavior, 50, 684\u2013692</li>\n</ul>\n<p>Power is also linked to the stress hormone cortisol: Power holders show lower basal cortisol levels and lower cortisol reactivity to stressors than powerless people do, and cortisol drops as power is achieved (Abbott et al., 2003; Coe, Mendoza, &amp; Levine, 1979; Sapolsky, Alberts, &amp; Altmann, 1997). Although short-term and acute cortisol elevation is part of an adaptive response to challenges large (e.g., a predator) and small (e.g., waking up), the chronically elevated cortisol levels seen in low-power individuals are associated with negative health consequences, such as impaired immune functioning, hypertension, and memory loss (Sapolsky et al., 1997; Segerstrom &amp; Miller, 2004). Low-power social groups have a higher incidence of stress-related illnesses than high-power social groups do, and this is partially attributable to chronically elevated cortisol (Cohen et al., 2006). Thus, the power holder\u2019s typical neuroendocrine profile of high testosterone coupled with low cortisol\u2014a profile linked to such outcomes as disease resistance (Sapolsky, 2005) and leadership abilities (Mehta &amp; Josephs, 2010)\u2014appears to be optimally adaptive.</p>\n<ul>\n<li>Abbott, D.H., Keverne, E.B., Bercovitch, F.B., Shively, C.A., Mendoza, S.P., Saltzman, W., et al. (2003). Are subordinates always stressed? A comparative analysis of rank differences in cortisol levels among primates. Hormones and Behavior, 43, 67\u201382</li>\n<li>Coe, C.L., Mendoza, S.P., &amp; Levine, S. (1979). Social status constrains the stress response in the squirrel monkey. Physiology &amp; Behavior, 23, 633\u2013638</li>\n<li>Sapolsky, R.M., Alberts, S.C., &amp; Altmann, J. (1997). Hypercortisolism</li>\n<li>associated with social subordinance or social isolation among</li>\n<li>wild baboons. Archives of General Psychiatry, 54, 1137\u20131143</li>\n<li>Segerstrom, S., &amp; Miller, G. (2004). Psychological stress and the human immune system: A meta-analytic study of 30 years of inquiry. Psychological Bulletin, 130, 601\u2013630</li>\n<li>Cohen, S., Schwartz, J.E., Epel, E., Kirschbaum, C., Sidney, S., &amp; Seeman, T. (2006). Socioeconomic status, race, and diurnal cortisol decline in the Coronary Artery Risk Development in Young Adults (CARDIA) study. Psychosomatic Medicine, 68, 41\u201350</li>\n<li>Sapolsky, R.M. (2005). The influence of social hierarchy on primate health. Science, 308, 648\u2013652.</li>\n</ul>\n<p>It is unequivocal that power is expressed through highly specific, evolved nonverbal displays. Expansive, open postures (widespread limbs and enlargement of occupied space by spreading out) project high power, whereas contractive, closed postures (limbs touching the torso and minimization of occupied space by collapsing the body inward) project low power. All of these patterns have been identified in research on actual and attributed power and its nonverbal correlates (Carney, Hall, &amp; Smith LeBeau, 2005; Darwin, 1872/2009; de Waal, 1998; <a href=\"http://nuweb9.neu.edu/socialinteractionlab/wp-content/uploads/Hall-Coats-Smith-LeBeau-2005.pdf\">Hall, Coats, &amp; Smith LeBeau, 2005</a>).</p>\n<ul>\n<li>Hall, J.A., Coats, E.J., &amp; Smith LeBeau, L. (2005). Nonverbal and the vertical dimension of social relations: A meta-analysis. Psychological Bulletin, 131, 898\u2013924.</li>\n</ul>\n</blockquote>\n<p><a href=\"http://www.centenary.edu/attachments/psychology/journal/feb2012journalclub.pdf\">\u201cReality at Odds With Perceptions: Narcissistic Leaders and Group Performance\u201d</a>, Nevicka et al 2011:</p>\n<blockquote>\n<p>Despite people\u2019s positive perceptions of narcissists as leaders, it was previously unknown if and how leaders\u2019 narcissism is related to the performance of the people they lead. In this study, we used a hidden-profile paradigm to investigate this question and found evidence for discordance between the positive image of narcissists as leaders and the reality of group performance. We hypothesized and found that although narcissistic leaders are perceived as effective because of their displays of authority, a leader\u2019s narcissism actually inhibits information exchange between group members and thereby negatively affects group performance. Our findings thus indicate that perceptions and reality can be at odds and have important practical and theoretical implications.</p>\n<p>\u2026For example, narcissists tend to overestimate their intelligence (Campbell, Rudich, &amp; Sedikides, 2002), creativity (Goncalo, Flynn, &amp; Kim, 2010), academic abilities (Robins &amp; Beer, 2001), and leadership capabilities (Judge, LePine, &amp; Rich, 2006). Generally, other people do not agree with narcissists\u2019 idealized self-images and perceive narcissists as arrogant, egocentric, overly dominant, and even hostile (Paulhus, 1998). However, the context of leadership constitutes a notable exception in which narcissists tend to be judged positively. For example, individuals with high levels of narcissism receive higher leadership ratings than individuals with low levels of narcissism do (Judge et al., 2006) and tend to emerge as leaders in groups (Brunell et al., 2008; Nevicka, De Hoogh, Van Vianen, Beersma, &amp; McIlwain, 2011). In addition, higher narcissism in U.S. presidents is associated with more positive evaluations of their leadership (Deluga, 1997). It is therefore not surprising that narcissistic characteristics are ascribed to many prominent leaders, such as Nicolas Sarkozy (De Sutter &amp; Immelman, 2008) and Steve Jobs (Robins &amp; Paulhus, 2001).</p>\n<p>\u2026Of the two prior studies investigating this question, one found no effects of narcissistic leadership on performance (Brunell et al., 2008), and the other showed that organizational performance was merely more volatile, but no worse or better, because of narcissistic leaders\u2019 risky decision making (Chatterjee &amp; Hambrick, 2007). Unfortunately, neither of these studies examined the effects of narcissistic leaders on group dynamics, communication, and information exchange, factors that are critically important to group decision making (Stasser, 1999), group performance (De Dreu, Nijstad, &amp; van Knippenberg, 2008), and organizational effectiveness (Zaccaro, Rittman, &amp; Marks, 2001)\u2026Prior research has hinted at a potentially negative effect of narcissistic individuals on group and organizational performance. For example, in one study, individuals with high levels of narcissism allocated more resources to themselves than did individuals with low levels of narcissism\u2014at a long-term cost to other group members (Campbell, Bush, Brunell, &amp; Shelton, 2005). However, prior research did not provide a clear link between leader\u2019s narcissism and group or organizational performance.</p>\n</blockquote>\n<p><a href=\"http://www.letitiaslabu.net/pdf/Slabu_Guinote_Wilkinson_Power_Facilitates_Attentional_Orienting_InPress.pdf\">\u201cHow quickly can you detect it? Power facilitates attentional orienting\u201d</a>, Slabu et al</p>\n<blockquote>\n<p>Participants were assigned to a high power or control role and then performed a computerised spatial cueing task in which they were required to direct their attention to a target that had been preceded by either a valid or invalid location cue. Compared to participants in the control condition, power-holders were better able to override the misinformation provided by invalid cues. This advantage occurred only at 500 ms stimulus onset asynchrony (SOA), whereas at 1000 ms SOA, when there was more time to prepare a response, no differences were found. These findings are taken to support the growing idea that social power affects cognitive flexibility\u2026Post-test questionnaires confirmed that these effects could not be attributed to differences in positive affect or self-efficacy. We suggest that power most affected performance during invalid trials because these required a greater degree of cognitive flexibility; individuals needed to ignore the cue and unexpectedly orient attention towards the opposite location. In line with this account, the effect was only evident at relatively short SOAs where participants had little time to prepare an appropriate response. At longer SOAs or on valid trials, the need for flexibility was lower which may explain why no effect was seen.</p>\n<p>Social power affects the way in which information is attended and discriminated (Fiske, 1993; Guinote, 2007a). Power holders have more resources and fewer constraints which gives them more attentional resources and allows them to discriminate between relevant and irrelevant information (Guinote, 2007a; Overbeck &amp; Park, 2001). In contrast, powerless people face more constraints and environmental threats (Keltner, Gruenfeld, &amp; Anderson, 2003). Their dependency encourages them to attend to multiple cues in the environment, in search of any potentially useful information. Thus, they treat information more equally, attending not only to the central information but also to the peripheral or distracting information (Slabu &amp; Guinote, 2010). This overflow in information processing makes powerless people less able to respond promptly to specific situational demands, and induces attentional inflexibility (Guinote, 2007a).</p>\n<ul>\n<li>Fiske, S. T. (1993). Controlling other people: The impact of power on stereotyping. American Psychologist, 48(6), 621-628. doi: 10.1037/0003-066X.48.6.621</li>\n<li>Guinote, A. (2007a). Behaviour variability and the Situated Focus Theory of Power. European Review of Social Psychology, 18, 256-295. doi: 10.1080/10463280701692813</li>\n<li>Overbeck, J. R., &amp; Park, B. (2001). When power does not corrupt: Superior individuation processes among powerful perceivers. Journal of Personality and Social Psychology, 81(4), 549-565. doi: 10.1037/0022-3514.81.4.549</li>\n<li>Slabu, L., &amp; Guinote, A. (2010). Getting what you want: Power increases the accessibility of active goals. Journal of Experimental Social Psychology, 46(2), 344-349. doi: 10.1016/j.jesp.2009.10.013</li>\n</ul>\n<p>Research using basic cognitive paradigms supports these claims. For example, Guinote (2007b) showed that high power participants are better able to focus their attention to target objects and ignore the influence of irrelevant background distracters (see also Smith &amp; Trope, 2006). A further outcome of the cognitive flexibility experienced by powerful individuals is the increased ability to adjust their actions in line with changing contextual cues. This includes the ability to suppress dominant responses and implement non-dominant ones when the task calls for non-dominant responses (Guinote, 2007b).</p>\n<ul>\n<li>Guinote, A. (2007b). Power affects basic cognition: Increased attentional inhibition and flexibility. Journal of Experimental Social Psychology, 43(5), 685-697. doi: 10.1016/j.jesp.2006.06.008</li>\n<li>Smith, P. K., &amp; Trope, Y. (2006). You focus on the forest when you\u2019re in charge of the trees: Power priming and abstract information processing. Journal of Personality and Social Psychology, 90(4), 578-596. doi: 10.1037/0022-3514.90.4.578</li>\n</ul>\n<p>For example, several studies have shown that having power increases the ability to resolve conflicts and plan action sequences; power-holders are immune to stimulus-response compatibility effects, and are better able to switch attention between the holistic and detailed components of stimuli, as changing task demands dictate (Guinote, 2007b; Smith, Jostmann, Galinsky, &amp; van Dijk, 2008)\u2026 More broadly, our findings build on those reported by Willis, Rodriguez-Bailon and Lupianez (2011) who showed that powerful individuals can make a better use of cues present in the environment to increase their executive control (see also Smith, et al., 2008). Their data support the idea that social power can impact rudimentary processes associated with spatial orienting and control.</p>\n<ul>\n<li>Willis, G. B., Rodr\u00edguez-Bail\u00f3n, R., Lupi\u00e1\u00f1ez, J. (2011). The boss is paying attention: Power Affects the Functioning of the Attentional Networks. Social Cognition, 29(2), 166-181.</li>\n</ul>\n</blockquote>\n<p><a href=\"http://www.psych.nyu.edu/tropelab/publications/SmithTrope2006.pdf\">\u201cYou focus on the forest when you\u2019re in charge of the trees: Power priming and abstract information processing\u201d</a>, Smith&amp; Trope 2006</p>\n<blockquote>\n<p>Elevated power increases the psychological distance one feels from others, and this distance, according to construal level theory (Y. Trope &amp; N. Liberman, 2003), should lead to more abstract information processing. Thus, high power should be associated with more abstract thinking\u2014focusing on primary aspects of stimuli and detecting patterns and structure to extract the gist, as well as categorizing stimuli at a higher level\u2014relative to low power. In 6 experiments involving both conceptual and perceptual tasks, priming high power led to more abstract processing than did priming low power, even when this led to worse performance. Experiment 7 revealed that in line with past neuropsychological research on abstract thinking, priming high power also led to greater relative right-hemispheric activation.</p>\n<ul>\n<li>Trope, Y., &amp; Liberman, N. (2003). Temporal construal. Psychological Review, 110, 403\u2013 421</li>\n</ul>\n<p>Though the abstraction hypothesis has not been directly tested, there is some research that supports it. For example, in Overbeck and Park\u2019s (2001) experiments, high- and low-power participants interacted via e-mail with several different targets holding the opposite power role and received various kinds of information from them. Some of this information was relevant to the task at hand (e.g., Jim waited until the last minute to try to schedule a meeting), and some was irrelevant (e.g., Jim just started a jazz ensemble). Not only did participants in the high-power role recall more information overall than did the low-power participants, but they were especially superior at recalling relevant information. Thus, high-power participants focused more on primary information, a hallmark of abstract thinking.</p>\n<ul>\n<li>Overbeck, J. R., &amp; Park, B. (2001). When power does not corrupt: Superior individuation processes among powerful perceivers. Journal of Personality and Social Psychology, 81, 549 \u2013565.</li>\n</ul>\n<p>Portuguese participants used more abstract language to describe both their ethnic group and an outgroup when they were part of the majority (i.e., a higher power group) than when they were part of the minority (i.e., a lower power group; Guinote, 2001). Similarly, participants who played the role of judges during a task used more abstract, trait-like language in referring to themselves than did participants who were workers (Guinote, Judd, &amp; Brauer, 2002).</p>\n<ul>\n<li>Guinote, A. (2001). The perception of group variability in a non-minority and a minority context: When adaptation leads to outgroup differentiation. British Journal of Social Psychology, 40, 117\u2013132.</li>\n<li>Guinote, A., Judd, C. M., &amp; Brauer, M. (2002). Effects of power on perceived and objective group variability: Evidence that more powerful groups are more variable. Journal of Personality and Social Psychology, 82, 708 \u2013721</li>\n</ul>\n<p>Powerholders, more than the powerless, should thus be guided by their primary, overriding goals rather than by subordinate, incidental concerns. This would mean that powerholders are more likely to act in accordance with their core attitudes and values (Chen et al., 2001). Indeed, individuals placed in high-power roles or those higher in personality dominance have been found to express their true attitudes more during a discussion than have participants lower in power or dominance (Anderson &amp; Berdahl, 2002). Such goal-driven behavior also has implications for stereotyping. Powerholders should be more likely to stereotype those beneath them when such stereotyping is seen as an effective means to their goals. Evidence for this has already been found in the context of the Social Influence Strategy \u03eb Stereotype Match hypothesis (Vescio, Snyder, &amp; Butz, 2003).</p>\n<ul>\n<li>Chen, S., Lee-Chai, A. Y., &amp; Bargh, J. A. (2001). Relationship orientation as a moderator of the effects of social power. Journal of Personality and Social Psychology, 80, 173-187</li>\n<li>Anderson, C., &amp; Berdahl, J. L. (2002). The experience of power: Examining the effects of power on approach and inhibition tendencies. Journal of Personality and Social Psychology, 83, 1362\u20131377</li>\n<li>Vescio, T. K., Snyder, M., &amp; Butz, D. A. (2003). Power in stereotypically masculine domains: A social influence strategy \u03eb stereotype match model. Journal of Personality and Social Psychology, 85, 1062\u20131078.</li>\n</ul>\n</blockquote>\n<p><a href=\"http://dl.dropbox.com/u/85192141/2008-smith.pdf\">\u201cPowerful People Make Good Decisions Even When They Consciously Think\u201d</a>, Smith et al 2008</p>\n<blockquote>\n<p>Thought condition again had different effects on performance for the two priming conditions, F(1, 161) 54.67, prep 5 .91, Zp 2 1\u20444 :03 (see Fig. 1). Low-power participants performed significantly better after unconscious thought than after conscious thought, prep 5 .96. High-power participants performed equally well in both thought conditions and did not differ from low-power participants in the unconscious-thought condition, Fs &lt; 1. Furthermore, our manipulations did not significantly affect participants\u2019 confidence in and certainty of their attitudes, preps &lt; .70, their reported effort or motivation, preps &lt; .84, or the amount of apartment information they correctly recalled, Fs &lt; 1. Differences in performance could not be attributed to depth of processing. When given problems requiring a complex decision, high-power participants were equally good at identifying the better choice after conscious versus unconscious thought, whereas the performance of low-power participants suffered when they consciously deliberated. These results provide further evidence that conscious and unconscious thought differ in the type of processing that occurs. The powerful seem to be able to handle so many impactful decisions, without making excessive errors, in part because they generally think more abstractly.</p>\n</blockquote>\n<p><a href=\"http://cbees.utdallas.edu/papers/EckelFatasWilson03_07_10complete.pdf\">\u201cCooperation and Status in Organizations\u201d</a>, Eckel et al 2010</p>\n<blockquote>\n<p>We further manipulate status by allocating the central position to the person who earns the highest, or the lowest, score on a trivia quiz. These high-status and low-status treatments are compared, and we find that the effect of organizational structure \u2013 the existence of a central position \u2013 depends on the status of the central player. Higher status players are attended to and mimicked more systematically. Punishment has differential effects in the two treatments, and is least effective in the high-status case.</p>\n<p>In this study, we ask whether social status serves as a useful mechanism for solving public goods problems. Status can act as a coordinating device, as it does in pure coordination games, with higher-status individuals more likely to be mimicked (followed) by others. In addition, in a setting with costly punishment, social status may enhance the effectiveness of punishment and reduce anti-social punishment, enhancing overall efficiency\u2026Status is awarded by the experimenter using scores on a general-knowledge trivia quiz that is unrelated to the experimental game. The central position is given to either the high scorer (high-status treatment) or the low scorer (low-status treatment). Subjects play two games: a standard linear voluntary contribution mechanism (VCM) and a VCM with costly punishment. We find that higher-status central players are more likely to be \u201cfollowed\u201d in the key situation when the peripheral player is contributing less than the central player. We also find that high status central players punish less, and peripheral players are more responsive to punishment by a higher-status central player\u2026Our results suggest that punishment, while important to enforcing cooperative norms in many social dilemmas, does not boost contributions in all instances. Punishment is used more readily by low-status groups, and increases overall contributions only among low-status groups. However this seems to be primarily a main effect of the punishment institution, as there is little evidence that punishment tokens levied actually increase contributions in low-status groups; indeed there is weak evidence that the response to punishment is greater in high-status groups. Retaliatory punishment of central players is seen only in the low-status groups. An unexpected consequence of these differences is that punishment is not efficiency- enhancing when the status of the central player is high. Costly punishment is used less in these groups, but contributions are not higher than without punishment. This generates a flat contribution pattern, and no differences between the VCM with and without punishment opportunities. At the other extreme, low status central players punish and are heavily punished, and make significantly less money in the experiment than any other type of subject. But the reaction of low status groups to the new environment generates a significant increase in the provision of the public good.</p>\n<p>Second, high-status agents may have a strong influence on others, as others seek their company and guidance, affecting choices and decision making by lower-status individuals. Thus high-status individuals are more likely to be mimicked or deferred to (Ball et al. 2001, Kumru and Vesterlund 2005). Imitating or learning from higher- status exemplars can help solve coordination problems (Eckel and Wilson 2007); the behavior of the higher-status individual provides an example that is observed and can be followed by others.</p>\n<ul>\n<li>Ball, S., C. Eckel, P. Grossman and W. Zame (2001) \u201cStatus in markets\u201d The Quarterly Journal of Economics 116, 161-188</li>\n<li>Kumru, C. and L. Vesterlund (2005) \u201cThe effect of status on voluntary contribution\u201d Working paper, Department of Economics, University of Pittsburgh.</li>\n<li>Eckel, C. and R. Wilson (2007) \u201cSocial learning in coordination games: Does status matter?\u201d Experimental Economics, 10, 317-330</li>\n</ul>\n<p>Gil-White and Henrich (2001) argue that attending to and mimicking high status individuals is a valuable strategy in a world where successful individuals may have superior information. Cultural transmission is enhanced when higher-status, successful individuals are copied by others. Copying successful individuals has evolutionary payoffs, so that humans may have evolved a preference for paying attention to and learning from high-status agents (see also Boyd and Richerson 2002, Boyd et al. 2003). Bala and Goyal (1998) capture the essence of the idea of attending to a high-status agent in a model where the presence of a commonly-observed agent, which they term the \u201croyal family\u201d, can have a significant impact on which among multiple equilibria is selected\u2026Experimental research confirms the tendency of individuals to mimic high-status agents. Eckel and Wilson (2001) show that a commonly observed agent can influence equilibrium selection in a coordination game\u2026Imitation makes the population of subjects more likely to reach a Pareto-superior, but risk- dominated, equilibrium, an outcome that rarely occurs otherwise (Cooper et al. 1990). Kumru and Vesterlund (2005) show a related result, with high-status first-movers more likely to be mimicked in a 2-person sequential voluntary contribution game. In their setting, high status enhances the ability of leaders to increase total contributions.</p>\n<ul>\n<li>Gil-White, F. and J. Henrich (2001) \u201cThe evolution of prestige: Freely conferred deference as a mechanism for enhancing the benefits of cultural transmission\u201d Evolution and Human Behavior 22,165-196</li>\n<li>Boyd, R., and P. Richerson (2002) \u201cGroup beneficial norms spread rapidly in a structured population\u201d Journal of Theoretical Biology 215, 287-296</li>\n<li>Boyd, R., H. Gintis, S. Bowles, and P. Richerson (2003) \u201cThe evolution of altruistic punishment\u201d Proceedings of the National Academy of Sciences (USA) 100, 3531-3535.</li>\n<li>Bala, V. and S. Goyal (1998) \u201cLearning from neighbors\u201d Review of Economic Studies 65, 595-621</li>\n<li>Eckel, Catherine C., and Rick K. Wilson (2001) \u201cSocial learning in a social hierarchy: An experimental study.\u201d Rice University, Unpublished manuscript</li>\n<li>Cooper, R., D. DeJong, R. Forsythe and T. Ross (1990). \u201cSelection criteria in coordination games: some experimental results. American Economic Review 80, 218-233</li>\n<li>Kumru, C. and L. Vesterlund (2005) \u201cThe effect of status on voluntary contribution\u201d Working paper, Department of Economics, University of Pittsburgh</li>\n</ul>\n</blockquote>\n<p>Another good set of studies focusing on rich/powerful behavior.</p>\n<p>2 of the primary researchers write in a 2012 <em>NYT</em> op-ed <a href=\"http://www.nytimes.com/roomfordebate/2012/03/15/does-morality-have-a-place-on-wall-street/greed-on-wall-street-prevents-good-from-happening\">\u201cGreed Prevents Good\u201d</a></p>\n<blockquote>\n<p>Now, some 25 years later, seven studies we conducted [Piff et al 2012], some on this same campus, have proved the opposite, that greed, far from being good, undermines moral behavior\u2026.Unethical behaviors among the wealthy are as timeless and pervasive as the ethical principles that try to rein them in. Our research pinpointed why wealth produces unethical conduct with such regularity: greed. Across studies, wealthier subjects expressed the conviction that greed is moral, echoing [Ivan] Boesky and Gekko and their intellectual companions (e.g., Ayn Rand). And it was their greed-is-good attitudes, we found, that gave rise to their unethical behavior. Wealth gives rise to a me-first mentality, and the ideology of unbridled self-interest serves as its lofty justification. Greg Smith is to be applauded for calling out the culture of greed at Goldman Sachs. It is a knockout blow, one as important as Ivan Boesky\u2019s proclamation nearly a generation ago. Nobel laureate Milton Friedman famously argued that the single social responsibility of business is to increase profits as long as \u201cit stays within the rules of the game.\u201d The problem is, when greed for profits is the bottom line, the rules may fall by the wayside.</p>\n</blockquote>\n<p>Relevant studies:</p>\n<ul>\n<li>\n<p><a href=\"http://greatergood.berkeley.edu/dacherkeltner/docs/kraus.inpress.pdf\">Kraus &amp; Keltner 2009</a>, \u201cSigns of socioeconomic status: a thin-slicing approach\u201d:</p>\n<blockquote>\n<p>Videos of 60-s slices of these interactions were coded for nonverbal cues of disengagement and engagement, and estimates of participants\u2019 SES were provided by naive observers who viewed these videos. As predicted by analyses of resource dependence and power, upper-SES participants displayed more disengagement cues (e.g., doodling) and fewer engagement cues (e.g., head nods, laughs) than did lower-SES participants\u2026.Research relevant to this hypothesis is limited, but suggestive. For example, in a meta-analytic review of status and nonverbal behavior, upper SES individuals were found to speak in ways that are less attentive to the audience, for example, with fewer turn-inviting pauses (<a title=\"Nonverbal and the vertical dimension of social relations: A meta-analysis\" href=\"http://nuweb9.neu.edu/socialinteractionlab/wp-content/uploads/Hall-Coats-Smith-LeBeau-2005.pdf\">Hall et al., 2005</a>)\u2026SES was measured objectively using self-reports of family income and education (e.g., Lachman &amp; Weaver, 1998). [They used undergraduates, <em>not</em> people who had personally clawed into power.]</p>\n</blockquote>\nConsistent with the previously cited studies about how acting rude or defecting is perceived as power.</li>\n<li>\n<p><a href=\"http://www.rotman.utoronto.ca/facbios/file/Kraus%20C%C3%B4t%C3%A9%20Keltner%20PS%202010.pdf\">Kraus et al 2010</a> \u201cSocial Class, Contextualism, and Empathic Accuracy\u201d:</p>\n<blockquote>\n<p>Recent research suggests that lower-class individuals favor explanations of personal and political outcomes that are oriented to features of the external environment. We extended this work by testing the hypothesis that, as a result, individuals of a lower social class are more empathically accurate in judging the emotions of other people. In three studies, lower-class individuals (compared with upper-class individuals) received higher scores on a test of empathic accuracy (Study 1), judged the emotions of an interaction partner more accurately (Study 2), and made more accurate inferences about emotion from static images of muscle movements in the eyes (Study 3). Moreover, the association between social class and empathic accuracy was explained by the tendency for lower-class individuals to explain social events in terms of features of the external environment.</p>\n</blockquote>\nSee the previous discussions of blame, self-centeredness, lack of empathy, and rule-breaking; related: fundamental attribution bias.</li>\n<li>\n<p><a href=\"http://www.krauslab.com/Stellaretal.Emotion.2012.pdf\">Stellar et al 2012</a>, \u201cClass and compassion: socioeconomic factors predict responses to suffering\u201d:</p>\n<blockquote>\n<p>Previous research indicates that lower-class individuals experience elevated negative emotions as compared with their upper-class counterparts. We examine how the environments of lower-class individuals can also promote greater compassionate responding-that is, concern for the suffering or well-being of others. In the present research, we investigate class-based differences in dispositional compassion and its activation in situations wherein others are suffering. Across studies, relative to their upper-class counterparts, lower-class individuals reported elevated dispositional compassion (Study 1), as well as greater self-reported compassion during a compassion-inducing video (Study 2) and for another person during a social interaction (Study 3). Lower-class individuals also exhibited heart rate deceleration-a physiological response associated with orienting to the social environment and engaging with others-during the compassion-inducing video (Study 2)\u2026For example, when describing environmental trends in economic inequality and everyday life outcomes (e.g., being laid off from work), undergraduates of lower subjective socioeconomic status\u2014measured by ranking oneself in society in terms of income, education, and job status relative to others\u2014attribute the causes of economic inequality to more external reasons (e.g., political influence, educational opportunity) than dispositional reasons (e.g., hard work, talent), relative to their upper-class counterparts (Kraus et al., 2009)\u2026Converging evidence also suggests that lower-class individuals favor an interdependent view of the self, whereas upper-class individuals are more inclined to espouse beliefs in an individuals\u2019 independence and autonomy (Stephens, Fryberg, &amp; Markus, 2011; Stephens, Markus, &amp; Townsend, 2007). For instance, in one study lower-class university students, whose parents\u2019 highest level of education was a high school diploma, tended to make choices that helped them blend in with others (e.g., by choosing a pen that resembled other pens; Stephens et al., 2007). In contrast, upper-class individuals, whose parents graduated from college, tended to prefer choices that helped them stand out (e.g., by choosing a unique pen). In recent work, Stephens and colleagues (2011) suggest that stronger relational norms among working-class individuals result in a less positive perception of individual choice, which favors an individual\u2019s own needs.</p>\n</blockquote>\n</li>\n<li>\n<p><a href=\"http://www.pnas.org/content/109/11/4086.full.pdf\">Piff et al 2012</a>, \u201cHigher social class predicts increased unethical behavior\u201d:</p>\n<blockquote>\n<p>In studies 1 and 2, upper-class individuals were more likely to break the law while driving, relative to lower-class individuals. In follow-up laboratory studies, upper-class individuals were more likely to exhibit unethical decision-making tendencies (study 3), take valued goods from others (study 4), lie in a negotiation (study 5), cheat to increase their chances of winning a prize (study 6), and endorse unethical behavior at work (study 7) than were lower-class individuals. Mediator and moderator data demonstrated that upper-class individuals\u2019 unethical tendencies are accounted for, in part, by their more favorable attitudes toward greed\u2026Individuals from upper-class backgrounds are also less generous and altruistic. In one study, upper-class individuals proved more selfish in an economic game, keeping significantly more laboratory credits\u2014which they believed would later be exchanged for cash\u2014than did lower-class participants, who shared more of their credits with a stranger (7). These results parallel nationwide survey data showing that upper-class households donate a smaller proportion of their incomes to charity than do lower-class households (10)\u2026Research finds that individuals motivated by greed tend to abandon moral principles in their pursuit of self-interest (13). In one study, a financial incentive caused people to be more willing to deceive and cheat others for personal gain (14). In another study, the mere presence of money led individuals to be more likely to cheat in an anagram task to receive a larger financial reward (1)\u2026Why are upper-class individuals more prone to unethical behavior, from violating traffic codes to taking public goods to lying? This finding is likely to be a multiply determined effect involving both structural and psychological factors. Upper-class individuals\u2019 relative independence from others and increased privacy in their professions (3) may provide fewer structural constraints and decreased perceptions of risk associated with committing unethical acts (8). The availability of resources to deal with the downstream costs of unethical behavior may increase the likelihood of such acts among the upper class. In addition, independent self-construals among the upper class (22) may shape feelings of entitlement and inattention to the consequences of one\u2019s actions on others (23). A reduced concern for others\u2019 evaluations (24) and increased goal-focus (25) could further instigate unethical tendencies among upper-class individuals. Together, these factors may give rise to a set of culturally shared norms among upper-class individuals that facilitates unethical behavior.</p>\n</blockquote>\n<ul>\n<li>7: Piff PK, Kraus MW, C\u00f4t\u00e9 S, Cheng BH, Keltner D (2010) <a href=\"http://www-2.rotman.utoronto.ca/phd/file/Piffetal.pdf\">\u201cHaving less, giving more: The influence of social class on prosocial behavior\u201d</a>. J Pers Soc Psychol 99:771\u2013784.</li>\n<li>10: Independent Sector (2002) <em>Giving and Volunteering in the United States</em> (Independent Sector, Washington, DC).</li>\n<li>13: Steinel W, De Dreu CKW (2004) <a href=\"http://econ.ucdenver.edu/beckman/Tiffany/steinel-misrpresentation.pdf\">\u201cSocial motives and strategic misrepresentation in social decision making\u201d</a>. J Pers Soc Psychol 86:419\u2013434</li>\n<li>14: Aquino K, Freeman D, Reed A, II, Felps W, Lim VK (2009) <a href=\"http://bschool.nus.edu/departments/ManagementNOrganization/publication/VLimpublist/testing-a-social-cognitive-model(2009).pdf\">\u201cTesting a social-cognitive model of moral behavior: The interactive influence of situations and moral identity centrality\u201d</a>. J Pers Soc Psychol 97:123\u2013141</li>\n<li>1: Gino F, Pierce L (2009) <a href=\"http://dl.dropbox.com/u/85192141/2009-gino.pdf\">\u201cThe abundance effect: Unethical behavior in the presence of wealth\u201d</a>. Organ Behav Hum Dec 109:142\u2013155</li>\n</ul>\n</li>\n</ul>", "sections": [{"title": "1 Notes", "anchor": "1_Notes", "level": 1}, {"title": "2 References", "anchor": "2_References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "49 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-28T00:10:14.576Z", "modifiedAt": null, "url": null, "title": "Anyone at Otakon?", "slug": "anyone-at-otakon", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:56.208Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Crux", "createdAt": "2011-09-15T17:49:36.096Z", "isAdmin": false, "displayName": "Crux"}, "userId": "XfQRFDS5eFdeYe6uM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y84sonh2Hn9Z2vxQg/anyone-at-otakon", "pageUrlRelative": "/posts/y84sonh2Hn9Z2vxQg/anyone-at-otakon", "linkUrl": "https://www.lesswrong.com/posts/y84sonh2Hn9Z2vxQg/anyone-at-otakon", "postedAtFormatted": "Saturday, July 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anyone%20at%20Otakon%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnyone%20at%20Otakon%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy84sonh2Hn9Z2vxQg%2Fanyone-at-otakon%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anyone%20at%20Otakon%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy84sonh2Hn9Z2vxQg%2Fanyone-at-otakon", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy84sonh2Hn9Z2vxQg%2Fanyone-at-otakon", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 44, "htmlBody": "<p>Perhaps this is a bit late, as the convention is already underway and those who are here may not be checking Less Wrong, but it may be worth a shot. Would be cool to get a LW meetup going on here if anyone's around.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y84sonh2Hn9Z2vxQg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.518014237747997e-07, "legacy": true, "legacyId": "17909", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-28T06:34:14.755Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Abstracted Idealized Dynamics", "slug": "seq-rerun-abstracted-idealized-dynamics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:58.387Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GvSLtMJEWgii2fKyF/seq-rerun-abstracted-idealized-dynamics", "pageUrlRelative": "/posts/GvSLtMJEWgii2fKyF/seq-rerun-abstracted-idealized-dynamics", "linkUrl": "https://www.lesswrong.com/posts/GvSLtMJEWgii2fKyF/seq-rerun-abstracted-idealized-dynamics", "postedAtFormatted": "Saturday, July 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Abstracted%20Idealized%20Dynamics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Abstracted%20Idealized%20Dynamics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGvSLtMJEWgii2fKyF%2Fseq-rerun-abstracted-idealized-dynamics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Abstracted%20Idealized%20Dynamics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGvSLtMJEWgii2fKyF%2Fseq-rerun-abstracted-idealized-dynamics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGvSLtMJEWgii2fKyF%2Fseq-rerun-abstracted-idealized-dynamics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<p>Today's post, <a href=\"/lw/t0/abstracted_idealized_dynamics/\">Abstracted Idealized Dynamics</a> was originally published on 12 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Abstracted_Idealized_Dynamics\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A bit of explanation on the idea of morality as \"computation\".</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dsx/seq_rerun_moral_error_and_moral_disagreement/\">Moral Error and Moral Disagreement</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GvSLtMJEWgii2fKyF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 9.519862909425178e-07, "legacy": true, "legacyId": "17919", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9KacKm5yBv27rxWnJ", "j5QLY7rx5ZXxRE7K6", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-28T09:14:52.553Z", "modifiedAt": null, "url": null, "title": "The Paperclip [Link]", "slug": "the-paperclip-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:57.109Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HumanFlesh", "createdAt": "2009-08-06T13:57:13.387Z", "isAdmin": false, "displayName": "HumanFlesh"}, "userId": "WnHt66oquDiLQPnEc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/criQvbXkNc7bTAkqE/the-paperclip-link", "pageUrlRelative": "/posts/criQvbXkNc7bTAkqE/the-paperclip-link", "linkUrl": "https://www.lesswrong.com/posts/criQvbXkNc7bTAkqE/the-paperclip-link", "postedAtFormatted": "Saturday, July 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Paperclip%20%5BLink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Paperclip%20%5BLink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcriQvbXkNc7bTAkqE%2Fthe-paperclip-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Paperclip%20%5BLink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcriQvbXkNc7bTAkqE%2Fthe-paperclip-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcriQvbXkNc7bTAkqE%2Fthe-paperclip-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 15, "htmlBody": "<p>&nbsp;</p>\n<p>A discussion on the history of the paperclip can be found in <a href=\"http://www.metafilter.com/118351/an-elegant-loop-within-a-loop-of-springy-steel-wire\">this Metafilter post</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "criQvbXkNc7bTAkqE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -16, "extendedScore": null, "score": 9.520636404800252e-07, "legacy": true, "legacyId": "17923", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-29T00:13:38.238Z", "modifiedAt": null, "url": null, "title": "Reinforcement Learning: A Non-Standard Introduction (Part 1)", "slug": "reinforcement-learning-a-non-standard-introduction-part-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:59.256Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "royf", "createdAt": "2012-05-28T04:41:04.869Z", "isAdmin": false, "displayName": "royf"}, "userId": "Rrw92MqPSK6T8nSBg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HPAjhrbYk6rPbpSXx/reinforcement-learning-a-non-standard-introduction-part-1", "pageUrlRelative": "/posts/HPAjhrbYk6rPbpSXx/reinforcement-learning-a-non-standard-introduction-part-1", "linkUrl": "https://www.lesswrong.com/posts/HPAjhrbYk6rPbpSXx/reinforcement-learning-a-non-standard-introduction-part-1", "postedAtFormatted": "Sunday, July 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reinforcement%20Learning%3A%20A%20Non-Standard%20Introduction%20(Part%201)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReinforcement%20Learning%3A%20A%20Non-Standard%20Introduction%20(Part%201)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHPAjhrbYk6rPbpSXx%2Freinforcement-learning-a-non-standard-introduction-part-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reinforcement%20Learning%3A%20A%20Non-Standard%20Introduction%20(Part%201)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHPAjhrbYk6rPbpSXx%2Freinforcement-learning-a-non-standard-introduction-part-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHPAjhrbYk6rPbpSXx%2Freinforcement-learning-a-non-standard-introduction-part-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 694, "htmlBody": "<p>Imagine that the world is divided into two parts: one we shall call the agent and the rest - its environment. Imagine you could describe in full detail the state of both the agent and the environment. The state of the agent is denoted M: it could be a Mind if you're a philosopher, a Machine if you're researching machine learning, or a Monkey if you're a neuroscientist. Anyway, it's just the Memory of the agent. The state of the rest of the World (or just World, for short) is denoted W.</p>\n<p>These states change over time. In general, when describing the dynamics of a system, we specify how each state is determined by the previous states. So we have probability distributions for the states W<sub style=\"font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\">t</sub>&nbsp;and M<sub style=\"font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\">t</sub>&nbsp;of the world and the agent in time t:</p>\n<p style=\"padding-left: 30px; \">p(W<sub>t</sub>|W<sub>t-1</sub>,M<sub>t-1</sub>)</p>\n<p style=\"padding-left: 30px; \">q(M<sub>t</sub>|W<sub>t-1</sub>,M<sub>t-1</sub>)</p>\n<p>This gives us the probabilities that the world is currently in state W<sub style=\"font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\">t</sub>, and the agent in state M<sub style=\"font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\">t</sub>, given that they previously were in states W<sub style=\"font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\">t-1</sub>&nbsp;and M<sub style=\"font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\">t-1</sub>. This can be illustrated in the following <a href=\"/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/\">Bayesian network</a>&nbsp;(<a href=\"/lw/9jw/michael_nielsen_explains_judea_pearls_causality/\">see also</a>):</p>\n<p><img src=\"http://images.lesswrong.com/t3_dsq_0.png\" alt=\"\" width=\"512\" height=\"208\" /></p>\n<p style=\"text-align: justify;\">Bayesian networks look like they represent causation: that the current state is \"caused\" by the immediately previous state. But what they <em>really</em>&nbsp;represent is statistical <em>in</em>dependence: that the current joint state (W<sub style=\"font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\">t</sub>, M<sub style=\"font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\">t</sub>)&nbsp;depends <em>only</em> on the immediately previous joint state (W<sub style=\"font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\">t-1</sub>, M<sub style=\"font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\">t-1</sub>), and not on any earlier state. So the power of Bayesian networks is in what they <em>don't</em>&nbsp;show, in this case there's no arrow from, say, W<sub style=\"font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\">t-2</sub>&nbsp;to W<sub style=\"font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\">t</sub>.</p>\n<p style=\"text-align: justify;\">The current joint state of the world and the agent represents everything we need to know in order to continue the dynamics forward. Given this state, the past is independent of the future. This property is so important, that it has a name, borrowed from one of its earliest researchers, Markov.</p>\n<p style=\"text-align: justify;\">The Markov property is not enough for our purposes.&nbsp;We are going to make a further assumption, which is that the states of the world and the agent don't both change together. Rather, they take turns changing, and while one does the other remains the same. This gives us the dynamics:</p>\n<p style=\"padding-left: 30px; \">p(W<sub>t</sub>|W<sub>t-1</sub>,M<sub>t-1</sub>)</p>\n<p style=\"padding-left: 30px; \">q(M<sub>t</sub>|M<sub>t-1</sub>,W<sub>t</sub>)</p>\n<p>and the Bayesian network:</p>\n<p style=\"text-align: justify;\"><img src=\"http://images.lesswrong.com/t3_dsq_1.png?v=2ce386464bc49b49fafb8a93946db7f6\" alt=\"\" width=\"678\" height=\"208\" /></p>\n<p style=\"text-align: justify;\">Sometimes this assumption can be readily justified. For example, let's use this model to describe a chess player.</p>\n<p style=\"text-align: justify;\">Suppose that at time t the game has reached state W<sub style=\"font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\">t</sub>&nbsp;where it is our agent's turn to play. Our agent has also reached a decision of what to do next, and its mind is now in state M<sub style=\"font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\">t</sub>, including memory, plan, general knowledge of chess, and all.</p>\n<p style=\"text-align: justify;\">Our agent takes its turn,&nbsp;and then enters stasis: we are going to assume that it's not thinking off-turn. This is true of most existing artificial chess players, and disregarding time constraints their play is not worse off for it. They are not missing out on anything other than time to think. So the agent keeps its state until the opponent has taken its turn. This completes the change of the state of the game from W<sub style=\"font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\">t</sub>&nbsp;to W<sub style=\"font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\">t+1</sub>.</p>\n<p style=\"text-align: justify;\">Now the agent takes a look at the board, and starts thinking up a new strategy to counter the last move of the opponent. If reaches a decision, and commits to its next action. This completes the change of the agent's state from M<sub style=\"font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\">t</sub>&nbsp;to M<sub style=\"font-family: Arial, Helvetica, sans-serif; line-height: 26px; text-align: justify;\">t+1</sub>.</p>\n<p style=\"text-align: justify;\">Chess is a turn-based game. But even in other scenarios, when such division of the dynamics into turns is not a good approximation of the process, our assumption can still be justified. If the length of each time step is taken to be smaller and smaller, the state of each of the parties remains more and more the same during each step, with increasing probability and accuracy. In the limit where we describe a continuous change of state over time, the turn-based assumption disappears, and we are back to the general model.</p>\n<p style=\"text-align: justify;\">&nbsp;</p>\n<hr />\n<p style=\"text-align: justify;\">&nbsp;</p>\n<p style=\"text-align: justify;\">This is the first part of an intuitive and highly non-standard introduction to reinforcement learning. <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">This</a> is more typical of what neuroscientists mean when they use the term. We, on the other hand, will get closer as we move forward to its meaning in machine learning (but not too close).</p>\n<p style=\"text-align: justify;\">In following posts we will continue to assume the Markov property in its turn-based variant. We will describe the model in further detail and explore its decision-making aspect.</p>\n<p style=\"text-align: justify;\"><strong>Continue reading:</strong>&nbsp;<a href=\"/lw/dux/reinforcement_learning_a_nonstandard_introduction/\">Part 2</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2d4": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HPAjhrbYk6rPbpSXx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 33, "extendedScore": null, "score": 9.524966343659197e-07, "legacy": true, "legacyId": "17882", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["miwf7qQTh2HXNnSuq", "pMzQ4zQYgY8jcckSi", "hN2aRnu798yas5b2k", "xdjA6YtE7QBsLYQ3i"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-29T00:27:26.381Z", "modifiedAt": null, "url": null, "title": "[Applications Closed] The Singularity Institute is hiring remote LaTeX editors", "slug": "applications-closed-the-singularity-institute-is-hiring", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:57.498Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "malo", "createdAt": "2011-12-28T20:01:23.182Z", "isAdmin": false, "displayName": "Malo"}, "userId": "DA863LaqrSGNFs5w5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5y2ki9j4TFgjMwCiA/applications-closed-the-singularity-institute-is-hiring", "pageUrlRelative": "/posts/5y2ki9j4TFgjMwCiA/applications-closed-the-singularity-institute-is-hiring", "linkUrl": "https://www.lesswrong.com/posts/5y2ki9j4TFgjMwCiA/applications-closed-the-singularity-institute-is-hiring", "postedAtFormatted": "Sunday, July 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BApplications%20Closed%5D%20The%20Singularity%20Institute%20is%20hiring%20remote%20LaTeX%20editors&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BApplications%20Closed%5D%20The%20Singularity%20Institute%20is%20hiring%20remote%20LaTeX%20editors%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5y2ki9j4TFgjMwCiA%2Fapplications-closed-the-singularity-institute-is-hiring%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BApplications%20Closed%5D%20The%20Singularity%20Institute%20is%20hiring%20remote%20LaTeX%20editors%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5y2ki9j4TFgjMwCiA%2Fapplications-closed-the-singularity-institute-is-hiring", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5y2ki9j4TFgjMwCiA%2Fapplications-closed-the-singularity-institute-is-hiring", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 214, "htmlBody": "<p>The Singularity Institute has recently made the transition to a LaTeX based document production workflow for its publications and <a title=\"SI Reaserch\" href=\"http://intelligence.org/research/\" target=\"_blank\">republished its existing research papers</a>. However, there is still much work to be done and we need more remote LaTeX editors. Some projects currently in the queue include converting both <a href=\"http://wiki.lesswrong.com/wiki/Sequences\" target=\"_blank\">The Sequences</a> and <a href=\"http://facingthesingularity.com\">Facing the Singularity</a> into LaTeX based books.</p>\n<p>As with <a title=\"Remote Researcher Positions\" href=\"/lw/9t8/the_singularity_institute_needs_remote/\" target=\"_blank\">other remote positions</a>, pay is hourly and starts at $14/hr but will increase if you produce a good product.</p>\n<p><strong>Perks:</strong></p>\n<ul>\n<li>Work flexible hours: Complete your work in few large chunks or many small ones&mdash;at 03:00 or 18:00&mdash;it's up to you.</li>\n<li>Work from wherever you please: your home (maybe even in bed), your local coffee shop, a hostel in Nepal, whatever.</li>\n<li>Age and credentials are irrelevant; only product matters.</li>\n<li>Make money while contributing to The Singularity Institute.</li>\n</ul>\n<div><strong>Requirements</strong>:</div>\n<div>\n<ul>\n<li>Experience creating and typesetting LaTeX based documents.</li>\n<li>Good attention to detail (this is more important than being a LaTeX wiz).</li>\n<li>Ability to work autonomously and set your own schedule.</li>\n</ul>\n<div><br /></div>\n<div>If you're interested, <a title=\"Application to the Singularity Institute's LaTeX Team\" href=\"http://tinyurl.com/remote-latex-editors\" target=\"_blank\"><strong>apply here</strong></a>. If you aren't sure you're qualified, err on the side of applying.</div>\n<div><br /></div>\n<div>Edit: Applications have been closed.</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5y2ki9j4TFgjMwCiA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 9.52503286611829e-07, "legacy": true, "legacyId": "17925", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LakrCAaj8rNss2q6j"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-29T05:24:54.448Z", "modifiedAt": null, "url": null, "title": "discounting on the radio", "slug": "discounting-on-the-radio", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jdinkum", "createdAt": "2011-02-13T00:27:51.433Z", "isAdmin": false, "displayName": "jdinkum"}, "userId": "PXAxasBwXYqoEyYNv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mgnT3E5xJn7LFmiis/discounting-on-the-radio", "pageUrlRelative": "/posts/mgnT3E5xJn7LFmiis/discounting-on-the-radio", "linkUrl": "https://www.lesswrong.com/posts/mgnT3E5xJn7LFmiis/discounting-on-the-radio", "postedAtFormatted": "Sunday, July 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20discounting%20on%20the%20radio&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Adiscounting%20on%20the%20radio%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmgnT3E5xJn7LFmiis%2Fdiscounting-on-the-radio%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=discounting%20on%20the%20radio%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmgnT3E5xJn7LFmiis%2Fdiscounting-on-the-radio", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmgnT3E5xJn7LFmiis%2Fdiscounting-on-the-radio", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 44, "htmlBody": "<p>I thought this was an interesting <a href=\"https://www.npr.org/blogs/money/2012/07/20/157105414/episode-388-putting-a-price-tag-on-your-descendants\" target=\"_blank\">radio piece</a>. The economist is interviewed about hyperbolic discounting and existential risk (though not using those exact terms) and how it related to government spending. With a dose of \"Politics is the mindkiller\" thrown in for good measure.</p>\n<p>https://www.npr.org/blogs/money/2012/07/20/157105414/episode-388-putting-a-price-tag-on-your-descendants</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mgnT3E5xJn7LFmiis", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 3, "extendedScore": null, "score": 9.526466753488142e-07, "legacy": true, "legacyId": "17928", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-29T06:19:43.274Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] \"Arbitrary\"", "slug": "seq-rerun-arbitrary", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:57.365Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B89kYYrFbzAdFz9gP/seq-rerun-arbitrary", "pageUrlRelative": "/posts/B89kYYrFbzAdFz9gP/seq-rerun-arbitrary", "linkUrl": "https://www.lesswrong.com/posts/B89kYYrFbzAdFz9gP/seq-rerun-arbitrary", "postedAtFormatted": "Sunday, July 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20%22Arbitrary%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20%22Arbitrary%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB89kYYrFbzAdFz9gP%2Fseq-rerun-arbitrary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20%22Arbitrary%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB89kYYrFbzAdFz9gP%2Fseq-rerun-arbitrary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB89kYYrFbzAdFz9gP%2Fseq-rerun-arbitrary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<p>Today's post, <a href=\"/lw/t1/arbitrary/\">\"Arbitrary\"</a> was originally published on 12 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#.22Arbitrary.22\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>When we say that something is arbitrary, we are saying that it feels like it should come with a justification, but doesn't.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dtr/seq_rerun_abstracted_idealized_dynamics/\">Abstracted Idealized Dynamics</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B89kYYrFbzAdFz9gP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.526731014514745e-07, "legacy": true, "legacyId": "17929", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HacgrDxJx3Xr7uwCR", "GvSLtMJEWgii2fKyF", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-29T09:58:18.172Z", "modifiedAt": null, "url": null, "title": "Why space stopped captivating minds ?", "slug": "why-space-stopped-captivating-minds", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:04.370Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kilobug", "createdAt": "2011-09-02T14:37:51.213Z", "isAdmin": false, "displayName": "kilobug"}, "userId": "7BQMuDSmLE2XRq2ph", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ok59AcDnQK65P9tE9/why-space-stopped-captivating-minds", "pageUrlRelative": "/posts/ok59AcDnQK65P9tE9/why-space-stopped-captivating-minds", "linkUrl": "https://www.lesswrong.com/posts/ok59AcDnQK65P9tE9/why-space-stopped-captivating-minds", "postedAtFormatted": "Sunday, July 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20space%20stopped%20captivating%20minds%20%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20space%20stopped%20captivating%20minds%20%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fok59AcDnQK65P9tE9%2Fwhy-space-stopped-captivating-minds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20space%20stopped%20captivating%20minds%20%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fok59AcDnQK65P9tE9%2Fwhy-space-stopped-captivating-minds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fok59AcDnQK65P9tE9%2Fwhy-space-stopped-captivating-minds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 275, "htmlBody": "<p>This article http://www.huffingtonpost.com/andrew-kessler/why-you-should-be-more-interested-in-mars-than-the-olympics_b_1712462.html -- ok, I admit, I read Slashdot sometimes, no one is perfect ;) -- made me wonder why the awesomeness of space conquest stopped motivating people.</p>\n<p>I remember the tales of my parents at the time of the Apollo landing, it was indeed instilling awe and wonder in the minds of people. It was followed by people like the Olympics or the football competitions are. And nowadays, NASA about to send a nuclear-powered rover to Mars, in a very delicate mission requiring the best of human engineering and scientific skills, and not in line in most media, most people not even aware of it? How did we fall that low?</p>\n<p>Sure there was the Cold War. It definitely played a role, in the amount of resources invested by both sides in space conquest, and in the way the media broadcasted the news.</p>\n<p>But here in France, a country that was mostly neutral during the Cold War (slightly west-aligned, but not part of NATO for most of the Cold War), the interest of people for space was not really partisan. People who were pro-USSR were amazed and cheering for the Appolo mission, people who were pro-USA were amazed and cheering for Gagarin. My brother and I played with (USSR) Sputnik as much as with (USA) space shuttles. We praised equally Neil Armstrong and Yuri Gagarin. I don't think the lack of Cold War explains it all.</p>\n<p>So what happened to the space conquest spirit? How did it disappear? I notice a blank spot on my map (well, not totally blank, but still very fuzzy) of reality, do some of you have clues for how to fill it?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ok59AcDnQK65P9tE9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 15, "extendedScore": null, "score": 4.5e-05, "legacy": true, "legacyId": "17931", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-30T05:30:41.114Z", "modifiedAt": null, "url": null, "title": "[Link] Why prison doesn't work and what to do about it", "slug": "link-why-prison-doesn-t-work-and-what-to-do-about-it", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:00.842Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mesilliac", "createdAt": "2012-01-22T09:52:35.331Z", "isAdmin": false, "displayName": "mesilliac"}, "userId": "s8nFtGr3Phx3HhgfC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NonW42WkLfNh7FBk7/link-why-prison-doesn-t-work-and-what-to-do-about-it", "pageUrlRelative": "/posts/NonW42WkLfNh7FBk7/link-why-prison-doesn-t-work-and-what-to-do-about-it", "linkUrl": "https://www.lesswrong.com/posts/NonW42WkLfNh7FBk7/link-why-prison-doesn-t-work-and-what-to-do-about-it", "postedAtFormatted": "Monday, July 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Why%20prison%20doesn't%20work%20and%20what%20to%20do%20about%20it&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Why%20prison%20doesn't%20work%20and%20what%20to%20do%20about%20it%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNonW42WkLfNh7FBk7%2Flink-why-prison-doesn-t-work-and-what-to-do-about-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Why%20prison%20doesn't%20work%20and%20what%20to%20do%20about%20it%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNonW42WkLfNh7FBk7%2Flink-why-prison-doesn-t-work-and-what-to-do-about-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNonW42WkLfNh7FBk7%2Flink-why-prison-doesn-t-work-and-what-to-do-about-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 304, "htmlBody": "<p>link: <a href=\"http://www.philosophersbeard.org/2012/07/why-prison-doesnt-work-and-what-to-do.html\">http://www.philosophersbeard.org/2012/07/why-prison-doesnt-work-and-what-to-do.html</a></p>\n<p>I came across this short well-reasoned essay via reddit, but most of the discussion there was by people who (<a href=\"http://www.reddit.com/help/faq#Whatdoesthenameredditmean\">ironically</a> enough) appeared not to have read it. The comment section under the article itself was similar. Some seemed not to have read the first paragraph.</p>\n<blockquote>\n<p>Prison time is a very severe punishment. JS Mill likened it to being consigned to a living tomb.* Any society that employs it should do so with care and restraint. Yet we do not. Partly because we think that prison is a humane punishment, it is drastically over-used in many countries, to the point of cruelty. Aside from failing in humanity, prison does not even perform well at the specific functions of a criminal justice system, namely, deterrence, retribution, security, and rehabilitation. We need to reconsider our over-reliance on prison, and reconsider whether other types of punishment, including capital and corporal punishment, may sometimes be more effective and more humane.</p>\n</blockquote>\n<p>The author's arguments against the imprisonment system, and for alternative methods of punishment, are interesting. They touch upon ideas which are hard to consider rationally. The very idea of punishment is something most people appear to find unpleasant. Indeed so it must be if something is to be considered a punishment.</p>\n<blockquote>\n<p>Flogging is barbaric and ugly. Yet that in itself does not mean it is cruel or inhumane or otherwise unfit as a punishment. Punishments, by definition, are supposed to be very unpleasant.</p>\n</blockquote>\n<p>I found some relevant previous discussion on Less Wrong (<a href=\"/lw/4x9/crime_and_punishment/\">Crime and Punishment</a>, <a href=\"/lw/1p/the_wrath_of_kahneman\">The Wrath of Kahneman</a>), and Overcoming Bias (<a href=\"http://www.overcomingbias.com/2010/12/prison-is-cruel.html\">Prison is Cruel</a>), but these seem to be about specifics, and not the system in general.</p>\n<p>I am curious both what the scientific consensus is on punishment systems, and what Less Wrong thinks of them. With such an emotionally charged issue, it's hard to find rational discussion about it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NonW42WkLfNh7FBk7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 16, "extendedScore": null, "score": 9.533441295512957e-07, "legacy": true, "legacyId": "17940", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SCXaRKGhQPkJCMmpm", "YGPzzqqpYcAoyzF4d"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-30T05:47:30.297Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Is Fairness Arbitrary?", "slug": "seq-rerun-is-fairness-arbitrary", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LhWrmdvHn9ZLDW4aK/seq-rerun-is-fairness-arbitrary", "pageUrlRelative": "/posts/LhWrmdvHn9ZLDW4aK/seq-rerun-is-fairness-arbitrary", "linkUrl": "https://www.lesswrong.com/posts/LhWrmdvHn9ZLDW4aK/seq-rerun-is-fairness-arbitrary", "postedAtFormatted": "Monday, July 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Is%20Fairness%20Arbitrary%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Is%20Fairness%20Arbitrary%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLhWrmdvHn9ZLDW4aK%2Fseq-rerun-is-fairness-arbitrary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Is%20Fairness%20Arbitrary%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLhWrmdvHn9ZLDW4aK%2Fseq-rerun-is-fairness-arbitrary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLhWrmdvHn9ZLDW4aK%2Fseq-rerun-is-fairness-arbitrary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 169, "htmlBody": "<p>Today's post, <a href=\"/lw/t2/is_fairness_arbitrary/\">Is Fairness Arbitrary?</a> was originally published on 14 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Is_Fairness_Arbitrary.3F\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>When we say that a fair division of pie among N people is for each person to get 1/N of the pie, we aren't being arbitrary. We're being fair.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/du1/seq_rerun_arbitrary/\">\"Arbitrary\"</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5A5ZGTQovxbay6fpr": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LhWrmdvHn9ZLDW4aK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.533522488202651e-07, "legacy": true, "legacyId": "17941", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["saw8WAML4NEaJ2Wmz", "B89kYYrFbzAdFz9gP", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-30T08:20:12.220Z", "modifiedAt": null, "url": null, "title": "Brain structure - scans reveal an amazingly regular pattern", "slug": "brain-structure-scans-reveal-an-amazingly-regular-pattern", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:57.562Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "D_Alex", "createdAt": "2009-07-17T08:21:38.505Z", "isAdmin": false, "displayName": "D_Alex"}, "userId": "Sriopfkdwx2qJBx4G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Afuu6i4XNac2CCkEb/brain-structure-scans-reveal-an-amazingly-regular-pattern", "pageUrlRelative": "/posts/Afuu6i4XNac2CCkEb/brain-structure-scans-reveal-an-amazingly-regular-pattern", "linkUrl": "https://www.lesswrong.com/posts/Afuu6i4XNac2CCkEb/brain-structure-scans-reveal-an-amazingly-regular-pattern", "postedAtFormatted": "Monday, July 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Brain%20structure%20-%20scans%20reveal%20an%20amazingly%20regular%20pattern&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABrain%20structure%20-%20scans%20reveal%20an%20amazingly%20regular%20pattern%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAfuu6i4XNac2CCkEb%2Fbrain-structure-scans-reveal-an-amazingly-regular-pattern%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Brain%20structure%20-%20scans%20reveal%20an%20amazingly%20regular%20pattern%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAfuu6i4XNac2CCkEb%2Fbrain-structure-scans-reveal-an-amazingly-regular-pattern", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAfuu6i4XNac2CCkEb%2Fbrain-structure-scans-reveal-an-amazingly-regular-pattern", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<p><a href=\"http://www.nih.gov/news/health/mar2012/nimh-29.htm\">http://www.nih.gov/news/health/mar2012/nimh-29.htm</a>&nbsp;has images of human and monkey brain scans which reveal an amazingly grid-like pattern of neuronal connections. What are the implications? Could brain scanning, emulation etc be simpler than would appear from images of tangled up neuronal cells (eg here:&nbsp;<a href=\"http://www.willamette.edu/~gorr/classes/cs449/brain.html\">http://www.willamette.edu/~gorr/classes/cs449/brain.html</a>)?</p>\n<p>(My mother is a retired neuroscientist... I'd ask her for comment, but she is on holidays.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Afuu6i4XNac2CCkEb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 13, "extendedScore": null, "score": 9.534259654439919e-07, "legacy": true, "legacyId": "17956", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-30T14:40:18.062Z", "modifiedAt": null, "url": null, "title": "Meetup : Third Copenhagen Meet-up.", "slug": "meetup-third-copenhagen-meet-up", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:00.193Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Pi96zJvgwPfrcNeyb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DTuzj3Kex4L3jWoAw/meetup-third-copenhagen-meet-up", "pageUrlRelative": "/posts/DTuzj3Kex4L3jWoAw/meetup-third-copenhagen-meet-up", "linkUrl": "https://www.lesswrong.com/posts/DTuzj3Kex4L3jWoAw/meetup-third-copenhagen-meet-up", "postedAtFormatted": "Monday, July 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Third%20Copenhagen%20Meet-up.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Third%20Copenhagen%20Meet-up.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDTuzj3Kex4L3jWoAw%2Fmeetup-third-copenhagen-meet-up%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Third%20Copenhagen%20Meet-up.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDTuzj3Kex4L3jWoAw%2Fmeetup-third-copenhagen-meet-up", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDTuzj3Kex4L3jWoAw%2Fmeetup-third-copenhagen-meet-up", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 101, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/c9'>Third Copenhagen Meet-up.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 August 2012 04:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">K\u00f8bmagergade 52, 1150 K\u00f8benhavn</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The third Copenhagen meet-up date has been decided in the mailing list.</p>\n\n<p>As the last meet-up the location is Studenterhuset, a cafe.</p>\n\n<p>The topic will be discussion of the basic rationality techniques, knowledge sources, books worth reading, answering of unresolved issues and perchance discussion of vision and future plans for the local bayesian conspiracy/phyg.</p>\n\n<p>I will be on location, from 4 PM with a sign of some kind. Looking forward to meeting you.</p>\n\n<p>less-wrong-copenhagen AT googlegroups DOT com</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/c9'>Third Copenhagen Meet-up.</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DTuzj3Kex4L3jWoAw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 9.536095051954066e-07, "legacy": true, "legacyId": "17958", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Third_Copenhagen_Meet_up_\">Discussion article for the meetup : <a href=\"/meetups/c9\">Third Copenhagen Meet-up.</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 August 2012 04:00:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">K\u00f8bmagergade 52, 1150 K\u00f8benhavn</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The third Copenhagen meet-up date has been decided in the mailing list.</p>\n\n<p>As the last meet-up the location is Studenterhuset, a cafe.</p>\n\n<p>The topic will be discussion of the basic rationality techniques, knowledge sources, books worth reading, answering of unresolved issues and perchance discussion of vision and future plans for the local bayesian conspiracy/phyg.</p>\n\n<p>I will be on location, from 4 PM with a sign of some kind. Looking forward to meeting you.</p>\n\n<p>less-wrong-copenhagen AT googlegroups DOT com</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Third_Copenhagen_Meet_up_1\">Discussion article for the meetup : <a href=\"/meetups/c9\">Third Copenhagen Meet-up.</a></h2>", "sections": [{"title": "Discussion article for the meetup : Third Copenhagen Meet-up.", "anchor": "Discussion_article_for_the_meetup___Third_Copenhagen_Meet_up_", "level": 1}, {"title": "Discussion article for the meetup : Third Copenhagen Meet-up.", "anchor": "Discussion_article_for_the_meetup___Third_Copenhagen_Meet_up_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-30T15:22:32.031Z", "modifiedAt": null, "url": null, "title": "AI cooperation is already studied in academia as \"program equilibrium\"", "slug": "ai-cooperation-is-already-studied-in-academia-as-program", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:38.281Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XkNXsi6bsxxFaL5FL/ai-cooperation-is-already-studied-in-academia-as-program", "pageUrlRelative": "/posts/XkNXsi6bsxxFaL5FL/ai-cooperation-is-already-studied-in-academia-as-program", "linkUrl": "https://www.lesswrong.com/posts/XkNXsi6bsxxFaL5FL/ai-cooperation-is-already-studied-in-academia-as-program", "postedAtFormatted": "Monday, July 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20cooperation%20is%20already%20studied%20in%20academia%20as%20%22program%20equilibrium%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20cooperation%20is%20already%20studied%20in%20academia%20as%20%22program%20equilibrium%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXkNXsi6bsxxFaL5FL%2Fai-cooperation-is-already-studied-in-academia-as-program%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20cooperation%20is%20already%20studied%20in%20academia%20as%20%22program%20equilibrium%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXkNXsi6bsxxFaL5FL%2Fai-cooperation-is-already-studied-in-academia-as-program", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXkNXsi6bsxxFaL5FL%2Fai-cooperation-is-already-studied-in-academia-as-program", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 140, "htmlBody": "<p>About a month ago I accidentally found out that the LW idea of <a href=\"/lw/do/reformalizing_pd/\">quining cooperation</a>&nbsp;is already studied in academia:</p>\n<p>1) Moshe Tennenholtz's 2004 paper <a href=\"http://ie.technion.ac.il/~moshet/progeqnote4.pdf\">Program Equilibrium</a>&nbsp;describes the idea of programs cooperating in the Prisoner's Dilemma by inspecting each other's source code.</p>\n<p>2) Lance Fortnow's 2009 paper <a href=\"http://people.cs.uchicago.edu/~fortnow/papers/discount.pdf\">Program Equilibria and Discounted Computation Time</a>&nbsp;describes an analogue of Benja Fallenstein's idea for&nbsp;<a href=\"/lw/14d/an_alternative_approach_to_ai_cooperation/100c\">implementing correlated play</a>, among other things.</p>\n<p>3) Peters and Szentes's 2012 paper <a href=\"http://else.econ.ucl.ac.uk/papers/uploaded/327.pdf\">Definable and Contractible Contracts</a>&nbsp;studies quining cooperation over a wider class of definable (not just computable) functions.</p>\n<p>As far as I know, academia still hasn't discovered Loebian cooperation or the subsequent ideas about formal models of UDT, but I might easily be wrong about that. In any case, the episode has given me a mini-crisis of faith, and a new appreciation of academia. That was a big part of the motivation for my <a href=\"/lw/dg8/should_you_try_to_do_good_work_on_lw/\">previous post</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"chuP2QqQycjD8qakL": 1, "b8FHrKqyXuYGWc6vn": 1, "GY5kPPpCoyt9fnTMn": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XkNXsi6bsxxFaL5FL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 60, "extendedScore": null, "score": 0.000128, "legacy": true, "legacyId": "17959", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 60, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5iK6rsa3MSrMhHQyf", "nFwHtnfLZ9QWia7Zu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-30T17:55:04.788Z", "modifiedAt": null, "url": null, "title": "[Retracted] Simpson's paradox strikes again: there is no great stagnation?", "slug": "retracted-simpson-s-paradox-strikes-again-there-is-no-great", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:00.616Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CarlShulman", "createdAt": "2009-03-01T07:47:12.225Z", "isAdmin": false, "displayName": "CarlShulman"}, "userId": "SguegG9SFXaKTgJLq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YcyjX655BsKr4zkSm/retracted-simpson-s-paradox-strikes-again-there-is-no-great", "pageUrlRelative": "/posts/YcyjX655BsKr4zkSm/retracted-simpson-s-paradox-strikes-again-there-is-no-great", "linkUrl": "https://www.lesswrong.com/posts/YcyjX655BsKr4zkSm/retracted-simpson-s-paradox-strikes-again-there-is-no-great", "postedAtFormatted": "Monday, July 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BRetracted%5D%20Simpson's%20paradox%20strikes%20again%3A%20there%20is%20no%20great%20stagnation%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BRetracted%5D%20Simpson's%20paradox%20strikes%20again%3A%20there%20is%20no%20great%20stagnation%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYcyjX655BsKr4zkSm%2Fretracted-simpson-s-paradox-strikes-again-there-is-no-great%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BRetracted%5D%20Simpson's%20paradox%20strikes%20again%3A%20there%20is%20no%20great%20stagnation%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYcyjX655BsKr4zkSm%2Fretracted-simpson-s-paradox-strikes-again-there-is-no-great", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYcyjX655BsKr4zkSm%2Fretracted-simpson-s-paradox-strikes-again-there-is-no-great", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p><em>ETA: The table linked by Landsburg has been called into serious question by <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Fr%2Fdiscussion%2Flw%2Fduw%2Fsimpsons_paradox_strikes_again_there_is_no_great%2F&amp;v=1&amp;libid=1343691925790&amp;out=http%3A%2F%2Fesoltas.blogspot.com%2F2012%2F07%2Finaccurate-consequences.html&amp;ref=http%3A%2F%2Flesswrong.com%2Fr%2Fdiscussion%2Fnew%2F&amp;title=Simpson's%20paradox%20strikes%20again%3A%20there%20is%20no%20great%20stagnation%20-%20Less%20Wrong%20Discussion&amp;txt=http%3A%2F%2Fesoltas.blogspot.com%2F2012%2F07%2Finaccurate-consequences.html&amp;jsonp=vglnk_jsonp_13436919076491\">Evan Soltas</a> [H.T. CronoDAS]. I edited the post to leave only the table to provide context for the comment discussion of its status.</em></p>\n<p><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">Economist Steve Landsburg has a <a href=\"http://www.thebigquestions.com/2012/07/30/the-numbers-racket/\">post</a>&nbsp;[H.T. <a href=\"http://econlog.econlib.org/archives/2012/07/landsburg_on_me.html\">David Henderson</a>] about the supposed stagnation of median wages in the United States in recent decades. In the linked table median wages have risen for:&nbsp;</span></p>\n<p><img src=\"http://www.landsburg.org/breakdown.gif\" alt=\"\" /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3uE2pXvbcnS9nnZRE": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YcyjX655BsKr4zkSm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 41, "extendedScore": null, "score": 0.000149, "legacy": true, "legacyId": "17960", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-31T00:24:09.440Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup Thursday 7pm *New Place*", "slug": "meetup-fort-collins-colorado-meetup-thursday-7pm-new-place-4", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zuwtKdTbRW8o8NxbX/meetup-fort-collins-colorado-meetup-thursday-7pm-new-place-4", "pageUrlRelative": "/posts/zuwtKdTbRW8o8NxbX/meetup-fort-collins-colorado-meetup-thursday-7pm-new-place-4", "linkUrl": "https://www.lesswrong.com/posts/zuwtKdTbRW8o8NxbX/meetup-fort-collins-colorado-meetup-thursday-7pm-new-place-4", "postedAtFormatted": "Tuesday, July 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20*New%20Place*&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20*New%20Place*%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzuwtKdTbRW8o8NxbX%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-new-place-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20*New%20Place*%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzuwtKdTbRW8o8NxbX%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-new-place-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzuwtKdTbRW8o8NxbX%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-new-place-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ca'>Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 August 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1129 W. Elizabeth St. Fort Collins 80521</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The move to Thursdays is working well. Back to Momo Lomo near campus for more discussions on how to level up in life.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ca'>Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zuwtKdTbRW8o8NxbX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "17962", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_\">Discussion article for the meetup : <a href=\"/meetups/ca\">Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 August 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1129 W. Elizabeth St. Fort Collins 80521</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The move to Thursdays is working well. Back to Momo Lomo near campus for more discussions on how to level up in life.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_1\">Discussion article for the meetup : <a href=\"/meetups/ca\">Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Thursday 7pm *New Place*", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Thursday 7pm *New Place*", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-31T00:24:20.668Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup Thursday 7pm *New Place*", "slug": "meetup-fort-collins-colorado-meetup-thursday-7pm-new-place-3", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/redGJ5i3XQHA9e2FN/meetup-fort-collins-colorado-meetup-thursday-7pm-new-place-3", "pageUrlRelative": "/posts/redGJ5i3XQHA9e2FN/meetup-fort-collins-colorado-meetup-thursday-7pm-new-place-3", "linkUrl": "https://www.lesswrong.com/posts/redGJ5i3XQHA9e2FN/meetup-fort-collins-colorado-meetup-thursday-7pm-new-place-3", "postedAtFormatted": "Tuesday, July 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20*New%20Place*&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20*New%20Place*%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FredGJ5i3XQHA9e2FN%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-new-place-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20*New%20Place*%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FredGJ5i3XQHA9e2FN%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-new-place-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FredGJ5i3XQHA9e2FN%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-new-place-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/cb'>Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 August 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1129 W. Elizabeth St. Fort Collins 80521</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The move to Thursdays is working well. Back to Momo Lomo near campus for more discussions on how to level up in life.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/cb'>Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "redGJ5i3XQHA9e2FN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "17963", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_\">Discussion article for the meetup : <a href=\"/meetups/cb\">Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 August 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1129 W. Elizabeth St. Fort Collins 80521</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The move to Thursdays is working well. Back to Momo Lomo near campus for more discussions on how to level up in life.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_1\">Discussion article for the meetup : <a href=\"/meetups/cb\">Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Thursday 7pm *New Place*", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Thursday 7pm *New Place*", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-31T00:24:33.144Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup Thursday 7pm *New Place*", "slug": "meetup-fort-collins-colorado-meetup-thursday-7pm-new-place-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rGkxYYHinjhpDfLZh/meetup-fort-collins-colorado-meetup-thursday-7pm-new-place-2", "pageUrlRelative": "/posts/rGkxYYHinjhpDfLZh/meetup-fort-collins-colorado-meetup-thursday-7pm-new-place-2", "linkUrl": "https://www.lesswrong.com/posts/rGkxYYHinjhpDfLZh/meetup-fort-collins-colorado-meetup-thursday-7pm-new-place-2", "postedAtFormatted": "Tuesday, July 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20*New%20Place*&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20*New%20Place*%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrGkxYYHinjhpDfLZh%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-new-place-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20*New%20Place*%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrGkxYYHinjhpDfLZh%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-new-place-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrGkxYYHinjhpDfLZh%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm-new-place-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/cc'>Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 August 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1129 W. Elizabeth St. Fort Collins 80521</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The move to Thursdays is working well. Back to Momo Lomo near campus for more discussions on how to level up in life.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/cc'>Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rGkxYYHinjhpDfLZh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "17964", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_\">Discussion article for the meetup : <a href=\"/meetups/cc\">Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 August 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1129 W. Elizabeth St. Fort Collins 80521</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The move to Thursdays is working well. Back to Momo Lomo near campus for more discussions on how to level up in life.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_1\">Discussion article for the meetup : <a href=\"/meetups/cc\">Fort Collins, Colorado Meetup Thursday 7pm *New Place*</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Thursday 7pm *New Place*", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Thursday 7pm *New Place*", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm__New_Place_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-31T00:58:19.700Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge (MA) Meetup", "slug": "meetup-cambridge-ma-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:00.461Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chronophasiac", "createdAt": "2009-04-03T11:25:57.322Z", "isAdmin": false, "displayName": "chronophasiac"}, "userId": "wu2Hs7x6pbfJbMumC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kFJLQLrx5xJB6hhaw/meetup-cambridge-ma-meetup", "pageUrlRelative": "/posts/kFJLQLrx5xJB6hhaw/meetup-cambridge-ma-meetup", "linkUrl": "https://www.lesswrong.com/posts/kFJLQLrx5xJB6hhaw/meetup-cambridge-ma-meetup", "postedAtFormatted": "Tuesday, July 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%20(MA)%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%20(MA)%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFJLQLrx5xJB6hhaw%2Fmeetup-cambridge-ma-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%20(MA)%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFJLQLrx5xJB6hhaw%2Fmeetup-cambridge-ma-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkFJLQLrx5xJB6hhaw%2Fmeetup-cambridge-ma-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/cd'>Cambridge (MA) Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 August 2012 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames St, Cambridge, MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148. Room number subject to change based on availability, signs will be posted with the actual room number.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/cd'>Cambridge (MA) Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kFJLQLrx5xJB6hhaw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 9.539080689837548e-07, "legacy": true, "legacyId": "17965", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA__Meetup\">Discussion article for the meetup : <a href=\"/meetups/cd\">Cambridge (MA) Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 August 2012 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames St, Cambridge, MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148. Room number subject to change based on availability, signs will be posted with the actual room number.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA__Meetup1\">Discussion article for the meetup : <a href=\"/meetups/cd\">Cambridge (MA) Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge (MA) Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA__Meetup", "level": 1}, {"title": "Discussion article for the meetup : Cambridge (MA) Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA__Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-31T00:58:31.528Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge (MA) Meetup", "slug": "meetup-cambridge-ma-meetup-3", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chronophasiac", "createdAt": "2009-04-03T11:25:57.322Z", "isAdmin": false, "displayName": "chronophasiac"}, "userId": "wu2Hs7x6pbfJbMumC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jqBZ4oZvax7ggj4ny/meetup-cambridge-ma-meetup-3", "pageUrlRelative": "/posts/jqBZ4oZvax7ggj4ny/meetup-cambridge-ma-meetup-3", "linkUrl": "https://www.lesswrong.com/posts/jqBZ4oZvax7ggj4ny/meetup-cambridge-ma-meetup-3", "postedAtFormatted": "Tuesday, July 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%20(MA)%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%20(MA)%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjqBZ4oZvax7ggj4ny%2Fmeetup-cambridge-ma-meetup-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%20(MA)%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjqBZ4oZvax7ggj4ny%2Fmeetup-cambridge-ma-meetup-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjqBZ4oZvax7ggj4ny%2Fmeetup-cambridge-ma-meetup-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ce'>Cambridge (MA) Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 August 2012 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames St, Cambridge, MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148. Room number subject to change based on availability, signs will be posted with the actual room number.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ce'>Cambridge (MA) Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jqBZ4oZvax7ggj4ny", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": -8, "extendedScore": null, "score": 9.539081642274985e-07, "legacy": true, "legacyId": "17966", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA__Meetup\">Discussion article for the meetup : <a href=\"/meetups/ce\">Cambridge (MA) Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 August 2012 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames St, Cambridge, MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148. Room number subject to change based on availability, signs will be posted with the actual room number.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA__Meetup1\">Discussion article for the meetup : <a href=\"/meetups/ce\">Cambridge (MA) Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge (MA) Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA__Meetup", "level": 1}, {"title": "Discussion article for the meetup : Cambridge (MA) Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA__Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-31T01:01:51.262Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge (MA) Meetup", "slug": "meetup-cambridge-ma-meetup-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "chronophasiac", "createdAt": "2009-04-03T11:25:57.322Z", "isAdmin": false, "displayName": "chronophasiac"}, "userId": "wu2Hs7x6pbfJbMumC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EqMjmc43qua2bSRHT/meetup-cambridge-ma-meetup-2", "pageUrlRelative": "/posts/EqMjmc43qua2bSRHT/meetup-cambridge-ma-meetup-2", "linkUrl": "https://www.lesswrong.com/posts/EqMjmc43qua2bSRHT/meetup-cambridge-ma-meetup-2", "postedAtFormatted": "Tuesday, July 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%20(MA)%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%20(MA)%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEqMjmc43qua2bSRHT%2Fmeetup-cambridge-ma-meetup-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%20(MA)%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEqMjmc43qua2bSRHT%2Fmeetup-cambridge-ma-meetup-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEqMjmc43qua2bSRHT%2Fmeetup-cambridge-ma-meetup-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/cf'>Cambridge (MA) Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 August 2012 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames St, Cambridge, MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148. Room number subject to change based on availability, signs will be posted with the actual room number.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/cf'>Cambridge (MA) Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EqMjmc43qua2bSRHT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 3, "extendedScore": null, "score": 9.539097728232616e-07, "legacy": true, "legacyId": "17967", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA__Meetup\">Discussion article for the meetup : <a href=\"/meetups/cf\">Cambridge (MA) Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 August 2012 02:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames St, Cambridge, MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148. Room number subject to change based on availability, signs will be posted with the actual room number.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA__Meetup1\">Discussion article for the meetup : <a href=\"/meetups/cf\">Cambridge (MA) Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge (MA) Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA__Meetup", "level": 1}, {"title": "Discussion article for the meetup : Cambridge (MA) Meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA__Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-31T03:45:54.795Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Bedrock of Morality: Arbitrary?", "slug": "seq-rerun-the-bedrock-of-morality-arbitrary", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:04.605Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wb4mx8CmtoYYnttCk/seq-rerun-the-bedrock-of-morality-arbitrary", "pageUrlRelative": "/posts/wb4mx8CmtoYYnttCk/seq-rerun-the-bedrock-of-morality-arbitrary", "linkUrl": "https://www.lesswrong.com/posts/wb4mx8CmtoYYnttCk/seq-rerun-the-bedrock-of-morality-arbitrary", "postedAtFormatted": "Tuesday, July 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Bedrock%20of%20Morality%3A%20Arbitrary%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Bedrock%20of%20Morality%3A%20Arbitrary%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwb4mx8CmtoYYnttCk%2Fseq-rerun-the-bedrock-of-morality-arbitrary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Bedrock%20of%20Morality%3A%20Arbitrary%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwb4mx8CmtoYYnttCk%2Fseq-rerun-the-bedrock-of-morality-arbitrary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwb4mx8CmtoYYnttCk%2Fseq-rerun-the-bedrock-of-morality-arbitrary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<p>Today's post, <a href=\"/lw/t3/the_bedrock_of_morality_arbitrary/\">The Bedrock of Morality: Arbitrary?</a> was originally published on 14 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#The_Bedrock_of_Morality:_Arbitrary.3F\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Humans are built in such a way as to do what is right. Other optimization processes may not. So what?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dud/seq_rerun_is_fairness_arbitrary/\">Is Fairness Arbitrary?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wb4mx8CmtoYYnttCk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.539890562571514e-07, "legacy": true, "legacyId": "17973", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RBszS2jwGM4oghXW4", "LhWrmdvHn9ZLDW4aK", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-31T05:58:01.507Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup", "slug": "meetup-west-la-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8aNEtesnZZZLZATa7/meetup-west-la-meetup", "pageUrlRelative": "/posts/8aNEtesnZZZLZATa7/meetup-west-la-meetup", "linkUrl": "https://www.lesswrong.com/posts/8aNEtesnZZZLZATa7/meetup-west-la-meetup", "postedAtFormatted": "Tuesday, July 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8aNEtesnZZZLZATa7%2Fmeetup-west-la-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8aNEtesnZZZLZATa7%2Fmeetup-west-la-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8aNEtesnZZZLZATa7%2Fmeetup-west-la-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/cg'>West LA Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 August 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, August 1st.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> If you have a chance, read an <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">old article</a> and a <a href=\"http://lesswrong.com/recentposts\">recent article</a>, any whose title catches your eye. This week conversation will be unstructured, but those will make for good seeds of conversation.</p>\n\n<p>But don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>We may also find time for a game, like The Resistance.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/cg'>West LA Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8aNEtesnZZZLZATa7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.540529094187634e-07, "legacy": true, "legacyId": "17974", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup\">Discussion article for the meetup : <a href=\"/meetups/cg\">West LA Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 August 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, August 1st.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> If you have a chance, read an <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">old article</a> and a <a href=\"http://lesswrong.com/recentposts\">recent article</a>, any whose title catches your eye. This week conversation will be unstructured, but those will make for good seeds of conversation.</p>\n\n<p>But don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>We may also find time for a game, like The Resistance.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/cg\">West LA Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-31T05:58:09.567Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup", "slug": "meetup-west-la-meetup-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tmwYgvywtEF9wjuBd/meetup-west-la-meetup-1", "pageUrlRelative": "/posts/tmwYgvywtEF9wjuBd/meetup-west-la-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/tmwYgvywtEF9wjuBd/meetup-west-la-meetup-1", "postedAtFormatted": "Tuesday, July 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtmwYgvywtEF9wjuBd%2Fmeetup-west-la-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtmwYgvywtEF9wjuBd%2Fmeetup-west-la-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtmwYgvywtEF9wjuBd%2Fmeetup-west-la-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ch'>West LA Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 August 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, August 1st.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> If you have a chance, read an <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">old article</a> and a <a href=\"http://lesswrong.com/recentposts\">recent article</a>, any whose title catches your eye. This week conversation will be unstructured, but those will make for good seeds of conversation.</p>\n\n<p>But don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>We may also find time for a game, like The Resistance.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ch'>West LA Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tmwYgvywtEF9wjuBd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "17975", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup\">Discussion article for the meetup : <a href=\"/meetups/ch\">West LA Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 August 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, August 1st.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> If you have a chance, read an <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">old article</a> and a <a href=\"http://lesswrong.com/recentposts\">recent article</a>, any whose title catches your eye. This week conversation will be unstructured, but those will make for good seeds of conversation.</p>\n\n<p>But don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>We may also find time for a game, like The Resistance.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/ch\">West LA Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-31T06:23:59.844Z", "modifiedAt": null, "url": null, "title": "Utilitarianism Subreddit", "slug": "utilitarianism-subreddit", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:57.829Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tuxedage", "createdAt": "2012-03-22T17:13:05.551Z", "isAdmin": false, "displayName": "Tuxedage"}, "userId": "Ezvcs6nqmgXbpD5bN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vxPDKqY8QfEKiNzop/utilitarianism-subreddit", "pageUrlRelative": "/posts/vxPDKqY8QfEKiNzop/utilitarianism-subreddit", "linkUrl": "https://www.lesswrong.com/posts/vxPDKqY8QfEKiNzop/utilitarianism-subreddit", "postedAtFormatted": "Tuesday, July 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Utilitarianism%20Subreddit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUtilitarianism%20Subreddit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvxPDKqY8QfEKiNzop%2Futilitarianism-subreddit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Utilitarianism%20Subreddit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvxPDKqY8QfEKiNzop%2Futilitarianism-subreddit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvxPDKqY8QfEKiNzop%2Futilitarianism-subreddit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 25, "htmlBody": "<p>Utilitarianism seems to be a common topic here. Many here are also familiar with Reddit.</p>\n<p>I suggest checking out<a href=\"http://www.reddit.com/r/utilitarianism\"> /r/utilitarianism</a> and consider subscribing. That is all.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vxPDKqY8QfEKiNzop", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 9, "extendedScore": null, "score": 9.540654633645398e-07, "legacy": true, "legacyId": "17978", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-31T10:32:47.686Z", "modifiedAt": null, "url": null, "title": "Always check your assertions... (Winning the Lottery)", "slug": "always-check-your-assertions-winning-the-lottery", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:30.203Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "brilee", "createdAt": "2009-11-24T14:36:56.816Z", "isAdmin": false, "displayName": "brilee"}, "userId": "bbMiGjzXWpEqRMwe6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uivnZpkiLq5YEKQpR/always-check-your-assertions-winning-the-lottery", "pageUrlRelative": "/posts/uivnZpkiLq5YEKQpR/always-check-your-assertions-winning-the-lottery", "linkUrl": "https://www.lesswrong.com/posts/uivnZpkiLq5YEKQpR/always-check-your-assertions-winning-the-lottery", "postedAtFormatted": "Tuesday, July 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Always%20check%20your%20assertions...%20(Winning%20the%20Lottery)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAlways%20check%20your%20assertions...%20(Winning%20the%20Lottery)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuivnZpkiLq5YEKQpR%2Falways-check-your-assertions-winning-the-lottery%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Always%20check%20your%20assertions...%20(Winning%20the%20Lottery)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuivnZpkiLq5YEKQpR%2Falways-check-your-assertions-winning-the-lottery", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuivnZpkiLq5YEKQpR%2Falways-check-your-assertions-winning-the-lottery", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 74, "htmlBody": "<p>\n<p class=\"p1\">\"In 2005, Dr. Zhang was having an ongoing discussion with friends about the Lottery, with Dr. Zhang taking the view that it offered poor odds and was a tax mainly on poor people. To bolster his argument, he began analyzing the Massachusetts Lottery&rsquo;s various games. But when he got to Cash WinFall, he was shocked to find that during roll-down drawings the odds were in the bettor&rsquo;s favor.\"</p>\n<p class=\"p1\">Full story here - it's rather engrossing.</p>\n</p>\n<p class=\"p1\">http://www.mass.gov/ig/publications/reports-and-recommendations/2012/lottery-cash-winfall-letter-july-2012.pdf</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uivnZpkiLq5YEKQpR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 9.541857369236e-07, "legacy": true, "legacyId": "17987", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-31T10:38:58.987Z", "modifiedAt": null, "url": null, "title": "AGI-12 conference in Oxford in December", "slug": "agi-12-conference-in-oxford-in-december", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:01.065Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BimwSY3DKh2ezP7Ev/agi-12-conference-in-oxford-in-december", "pageUrlRelative": "/posts/BimwSY3DKh2ezP7Ev/agi-12-conference-in-oxford-in-december", "linkUrl": "https://www.lesswrong.com/posts/BimwSY3DKh2ezP7Ev/agi-12-conference-in-oxford-in-december", "postedAtFormatted": "Tuesday, July 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AGI-12%20conference%20in%20Oxford%20in%20December&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAGI-12%20conference%20in%20Oxford%20in%20December%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBimwSY3DKh2ezP7Ev%2Fagi-12-conference-in-oxford-in-december%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AGI-12%20conference%20in%20Oxford%20in%20December%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBimwSY3DKh2ezP7Ev%2Fagi-12-conference-in-oxford-in-december", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBimwSY3DKh2ezP7Ev%2Fagi-12-conference-in-oxford-in-december", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 167, "htmlBody": "<p>The <a href=\"http://www.winterintelligence.org/oxford2012/agi-impacts/cfp/\">AGI</a> <a href=\"/lw/d39/agi_impacts_conference_in_oxford_in_december_with/\">impacts</a> conference in Oxford in December of this year will happen alongside the AGI-12 conference on Artificial General Intelligence. They also have a call for papers, to which some on this list may be interested in submitting:</p>\n<p>\n<h2>AGI-12 Paper Submission Deadline EXTENDED to August 15</h2>\n<p>Some good news for tardy AGI authors!</p>\n<p>As you may recall, the Fifth Conferences on Artificial General&nbsp;Intelligence (AGI-12) will be held Dec 8-11 at Oxford University in&nbsp;the UK. &nbsp;The AGI conferences are the only major conference series&nbsp;dedicated to research on the creation of thinking machines with&nbsp;general intelligence at the human level and ultimately beyond. &nbsp;The&nbsp;full AGI-12 Call for Papers may be found at:</p>\n<p>http://agi-conf.org/2012/call-for-papers/</p>\n<p>Our proceedings publisher for AGI-12, Springer Lecture Notes in AI&nbsp;(LNAI), has informed us that their deadline for receiving the&nbsp;proceedings manuscript from is later than previously thought. &nbsp;So, we&nbsp;have been able to extend the paper submission deadline once more, till&nbsp;August 15, allowing us to round up a few more excellent papers from&nbsp;tardy authors.</p>\n<p>We look forward to seeing you at Oxford in December!</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BimwSY3DKh2ezP7Ev", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 6, "extendedScore": null, "score": 9.541887286227216e-07, "legacy": true, "legacyId": "17988", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XTfPjyJzot4FkQmgX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-31T17:49:32.790Z", "modifiedAt": null, "url": null, "title": "What is the Mantra of Polya?", "slug": "what-is-the-mantra-of-polya", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:57.902Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mark_Eichenlaub", "createdAt": "2010-09-01T17:59:32.486Z", "isAdmin": false, "displayName": "Mark_Eichenlaub"}, "userId": "6mdGZLDekk4835gM6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BFv7fwdSp6w3vGyxD/what-is-the-mantra-of-polya", "pageUrlRelative": "/posts/BFv7fwdSp6w3vGyxD/what-is-the-mantra-of-polya", "linkUrl": "https://www.lesswrong.com/posts/BFv7fwdSp6w3vGyxD/what-is-the-mantra-of-polya", "postedAtFormatted": "Tuesday, July 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20the%20Mantra%20of%20Polya%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20the%20Mantra%20of%20Polya%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBFv7fwdSp6w3vGyxD%2Fwhat-is-the-mantra-of-polya%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20the%20Mantra%20of%20Polya%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBFv7fwdSp6w3vGyxD%2Fwhat-is-the-mantra-of-polya", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBFv7fwdSp6w3vGyxD%2Fwhat-is-the-mantra-of-polya", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 365, "htmlBody": "<p>The other day at dinner, someone showed me <a href=\"http://youtu.be/eCMmmEEyOO0?t=44s\">this video</a> of a slinky dropping. It shoes that the bottom of the slinky stays perfectly stationary for a while after it's been dropped. (The link goes to the 10-second interesting part).</p>\n<p>I spent some time trying to figure out why that happens, but didn't get it. The next day, I spent half an hour writing down the differential equations that describe the slinky's motion and staring at them, with no idea how to proceed. Eventually, I watched the video again with sound, and learned the simple answer, which is that the speed of waves traveling in a slinky is very slow - a few meters per second - and the bottom half sits still until a wave can travel down and inform it that the slinky's been dropped.</p>\n<p>The strange thing is that I already knew this, or at least the idea was familiar to me. Also, while at dinner, someone mentioned the <a href=\"http://en.wikipedia.org/wiki/Ladder_paradox\">\"pole-in-the-barn\"</a> paradox from special relativity, and mentioned the same speed-of-information-in-materials idea in resolving the paradox, but I still didn't make the connection to the problem I was considering.</p>\n<p>I want a simple phrase, similar to \"check consequentialism\", \"take the outside view\", or \"worth it?\" that applies to checking your own thought process while solving problems to stop you from revving your engine in the wrong direction for too long. I realized I've read a book about what to do in such situations. It's George Polya's <em>How to Solve It</em>. (<a href=\"http://www.amazon.com/How-Solve-Mathematical-Princeton-ebook/dp/B0073X0IOA/ref=sr_1_4?ie=UTF8&amp;qid=1343756403&amp;sr=8-4&amp;keywords=how+to+solve+it\">Amazon</a> <a href=\"http://en.wikipedia.org/wiki/How_to_Solve_It\">Wikipedia</a> <a href=\"http://books.google.com/books?id=X3xsgXjTGgoC&amp;lpg=PP1&amp;ots=t5Nt1MtKwg&amp;dq=how%20to%20solve%20it&amp;pg=PP1#v=onepage&amp;q=how%20to%20solve%20it&amp;f=false\">Google Books</a>) I don't have a copy of the book anymore, and I would like to crowdsource creating a short phrase that captures the general mindset endorsed by it. Some questions I remember the book suggesting are</p>\n<p>\n<ul>\n<li>Have you seen a similar problem before?</li>\n<li>What are the unknowns?</li>\n<li>What information do you have?</li>\n<li>Is it obvious that the unknowns are enough information?</li>\n</ul>\n<div>There are more of these listed in the <a href=\"http://en.wikipedia.org/wiki/How_to_Solve_It\">Wikipedia article</a>.</div>\n<div><br /></div>\n<div>Also, \"Mantra of Polya\" doesn't roll off the tongue well (at least I think it doesn't, since I'm not sure how to pronounce \"Polya\"), so a better name for this mnemonic would be good, too.</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BFv7fwdSp6w3vGyxD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 9.543969362712145e-07, "legacy": true, "legacyId": "17990", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-31T19:41:57.276Z", "modifiedAt": null, "url": null, "title": "[Link] Machiavelli in historical context", "slug": "link-machiavelli-in-historical-context", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:58.358Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cyan", "createdAt": "2009-02-27T22:31:08.528Z", "isAdmin": false, "displayName": "Cyan"}, "userId": "eGtDNuhj58ehX9Wgf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7CjJdTx6CgibyjS5C/link-machiavelli-in-historical-context", "pageUrlRelative": "/posts/7CjJdTx6CgibyjS5C/link-machiavelli-in-historical-context", "linkUrl": "https://www.lesswrong.com/posts/7CjJdTx6CgibyjS5C/link-machiavelli-in-historical-context", "postedAtFormatted": "Tuesday, July 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Machiavelli%20in%20historical%20context&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Machiavelli%20in%20historical%20context%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7CjJdTx6CgibyjS5C%2Flink-machiavelli-in-historical-context%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Machiavelli%20in%20historical%20context%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7CjJdTx6CgibyjS5C%2Flink-machiavelli-in-historical-context", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7CjJdTx6CgibyjS5C%2Flink-machiavelli-in-historical-context", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 433, "htmlBody": "<p>In modern usage, the name \"Machiavelli\" is a byword for cynical, selfish scheming. In <a href=\"http://exurbe.com/?p=1429\">this post</a>, a Renaissance scholar places Machiavelli the human being into historical context, illuminating that Machiavelli was not cynical so much as desirous of an accurate map of the territory, and not selfish at all but rather relentlessly goal-oriented.&nbsp;(The post starts slowly -- that's historical context for ya.)&nbsp;In writing <em>Il Principe</em>, Machiavelli (<a href=\"/lw/dvr/link_machiavelli_in_historical_context/74j0\">quite possibly unintentionally</a>) committed to posterity two major breakthroughs, which we would now call (i) the creation of modern political science and history and (ii) the introduction of utilitarian/consequentialist ethics.&nbsp;</p>\n<h3>Consequentialism&nbsp;</h3>\n<p>In 1498, at the age of 29, Machiavelli was made a high official of the Florentine analogue of the State Department/Ministry of Foreign Affairs. His job was to <a href=\"/lw/up/shut_up_and_do_the_impossible/\">shut up and do the impossible</a>:</p>\n<blockquote>\n<ul>\n<li><strong>Goal:</strong>&nbsp;Prevent Florence from being conquered by any of 10+ different incredibly enormous foreign powers.</li>\n<li><strong>Resources:</strong>&nbsp;100 bags of gold, 4 sheep, 1 wood, lots of books and a bust of Caesar.</li>\n<li><strong>Go!</strong></li>\n</ul>\n</blockquote>\n<div>And thus did Machiavelli come to invent consequentialism.</div>\n<h3>Modern Political Science</h3>\n<blockquote>\n<div><strong>1508</strong>. The Italian territories destabilized by the Borgias are ripe for conquest. &nbsp;Everyone in Europe wants to go to war with everyone else and Italy will be the biggest battlefield. &nbsp;Machaivelli&rsquo;s job now is to figure out who to ally with, and who to bribe. &nbsp;If he can&rsquo;t predict the sides there&rsquo;s no way to know where Florence should commit its precious resources. &nbsp;How will it fall out? &nbsp;Will Tudor claims on the French throne drive England to ally with Spain against France? &nbsp;Or will French and Spanish rival claims to Southern Italy lead France to recruit England against the houses of Aragon and Habsburg? &nbsp;Will the Holy Roman Emperor try to seize Milan from the French? &nbsp;Will the Ottomans ally with France to seize and divide the Spanish holdings in the Mediterranean? &nbsp;Will the Swiss finally wake up and notice that they have all the best armies in Europe and could conquer whatever the heck they wanted if they tried? &nbsp;(Seriously, Machiavelli spends a lot of time worrying about this possibility.) &nbsp;All the ambassadors from the great kingdoms and empires meet, and Machiavelli spends frantic months exchanging letters with colleagues evaluating the psychology of every prince, what each has to gain, to lose, to prove. &nbsp;He comes up with several probable scenarios and begins preparations. &nbsp;At last a courier rushes in with the news. &nbsp;The day has come. &nbsp;The alliance has formed. &nbsp;It is: everyone joins forces to attack Venice.</div>\n<div>\n<div>O_O &nbsp; &nbsp; &nbsp;????????</div>\n<div>Conclusion: must invent Modern Political Science.</div>\n</div>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7CjJdTx6CgibyjS5C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 11, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "17991", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>In modern usage, the name \"Machiavelli\" is a byword for cynical, selfish scheming. In <a href=\"http://exurbe.com/?p=1429\">this post</a>, a Renaissance scholar places Machiavelli the human being into historical context, illuminating that Machiavelli was not cynical so much as desirous of an accurate map of the territory, and not selfish at all but rather relentlessly goal-oriented.&nbsp;(The post starts slowly -- that's historical context for ya.)&nbsp;In writing <em>Il Principe</em>, Machiavelli (<a href=\"/lw/dvr/link_machiavelli_in_historical_context/74j0\">quite possibly unintentionally</a>) committed to posterity two major breakthroughs, which we would now call (i) the creation of modern political science and history and (ii) the introduction of utilitarian/consequentialist ethics.&nbsp;</p>\n<h3 id=\"Consequentialism_\">Consequentialism&nbsp;</h3>\n<p>In 1498, at the age of 29, Machiavelli was made a high official of the Florentine analogue of the State Department/Ministry of Foreign Affairs. His job was to <a href=\"/lw/up/shut_up_and_do_the_impossible/\">shut up and do the impossible</a>:</p>\n<blockquote>\n<ul>\n<li><strong>Goal:</strong>&nbsp;Prevent Florence from being conquered by any of 10+ different incredibly enormous foreign powers.</li>\n<li><strong>Resources:</strong>&nbsp;100 bags of gold, 4 sheep, 1 wood, lots of books and a bust of Caesar.</li>\n<li><strong>Go!</strong></li>\n</ul>\n</blockquote>\n<div>And thus did Machiavelli come to invent consequentialism.</div>\n<h3 id=\"Modern_Political_Science\">Modern Political Science</h3>\n<blockquote>\n<div><strong>1508</strong>. The Italian territories destabilized by the Borgias are ripe for conquest. &nbsp;Everyone in Europe wants to go to war with everyone else and Italy will be the biggest battlefield. &nbsp;Machaivelli\u2019s job now is to figure out who to ally with, and who to bribe. &nbsp;If he can\u2019t predict the sides there\u2019s no way to know where Florence should commit its precious resources. &nbsp;How will it fall out? &nbsp;Will Tudor claims on the French throne drive England to ally with Spain against France? &nbsp;Or will French and Spanish rival claims to Southern Italy lead France to recruit England against the houses of Aragon and Habsburg? &nbsp;Will the Holy Roman Emperor try to seize Milan from the French? &nbsp;Will the Ottomans ally with France to seize and divide the Spanish holdings in the Mediterranean? &nbsp;Will the Swiss finally wake up and notice that they have all the best armies in Europe and could conquer whatever the heck they wanted if they tried? &nbsp;(Seriously, Machiavelli spends a lot of time worrying about this possibility.) &nbsp;All the ambassadors from the great kingdoms and empires meet, and Machiavelli spends frantic months exchanging letters with colleagues evaluating the psychology of every prince, what each has to gain, to lose, to prove. &nbsp;He comes up with several probable scenarios and begins preparations. &nbsp;At last a courier rushes in with the news. &nbsp;The day has come. &nbsp;The alliance has formed. &nbsp;It is: everyone joins forces to attack Venice.</div>\n<div>\n<div>O_O &nbsp; &nbsp; &nbsp;????????</div>\n<div>Conclusion: must invent Modern Political Science.</div>\n</div>\n</blockquote>\n<p>&nbsp;</p>", "sections": [{"title": "Consequentialism\u00a0", "anchor": "Consequentialism_", "level": 1}, {"title": "Modern Political Science", "anchor": "Modern_Political_Science", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "17 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nCvvhFBaayaXyuBiD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-07-31T23:12:56.298Z", "modifiedAt": null, "url": null, "title": "Advice please: Cognitive distortion preventing me from accomplishing anything", "slug": "advice-please-cognitive-distortion-preventing-me-from", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:59.958Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "abcd_z", "createdAt": "2011-06-26T00:41:40.672Z", "isAdmin": false, "displayName": "abcd_z"}, "userId": "ntrr3JGG5fDueLnND", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2Ymebu9eMhxp34L7A/advice-please-cognitive-distortion-preventing-me-from", "pageUrlRelative": "/posts/2Ymebu9eMhxp34L7A/advice-please-cognitive-distortion-preventing-me-from", "linkUrl": "https://www.lesswrong.com/posts/2Ymebu9eMhxp34L7A/advice-please-cognitive-distortion-preventing-me-from", "postedAtFormatted": "Tuesday, July 31st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Advice%20please%3A%20Cognitive%20distortion%20preventing%20me%20from%20accomplishing%20anything&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdvice%20please%3A%20Cognitive%20distortion%20preventing%20me%20from%20accomplishing%20anything%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Ymebu9eMhxp34L7A%2Fadvice-please-cognitive-distortion-preventing-me-from%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Advice%20please%3A%20Cognitive%20distortion%20preventing%20me%20from%20accomplishing%20anything%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Ymebu9eMhxp34L7A%2Fadvice-please-cognitive-distortion-preventing-me-from", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2Ymebu9eMhxp34L7A%2Fadvice-please-cognitive-distortion-preventing-me-from", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 101, "htmlBody": "<p>The cognitive distortion is called \"catastrophizing\", I think.</p>\n<p>I'm afraid of unexpected, strongly negative events occurring to me without warning. &nbsp;Nothing specific, just a generalized fear. &nbsp;That fear is crippling me. &nbsp;Worse, there's a part of me that feels that fear is keeping me safe. &nbsp;\"If I let go of that fear,\" it goes, \"I would start doing things and then I wouldn't be safe any more.\"</p>\n<p>I haven't filled out a job application in over a week, because doing so would force me out into the world if i got an interview, and into the world consistently if I got the job.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2Ymebu9eMhxp34L7A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 9.545533718019046e-07, "legacy": true, "legacyId": "17992", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-01T06:29:38.843Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Hot Air Doesn't Disagree", "slug": "seq-rerun-hot-air-doesn-t-disagree", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:04.610Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QGqbpqT4NS59KtmEo/seq-rerun-hot-air-doesn-t-disagree", "pageUrlRelative": "/posts/QGqbpqT4NS59KtmEo/seq-rerun-hot-air-doesn-t-disagree", "linkUrl": "https://www.lesswrong.com/posts/QGqbpqT4NS59KtmEo/seq-rerun-hot-air-doesn-t-disagree", "postedAtFormatted": "Wednesday, August 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Hot%20Air%20Doesn't%20Disagree&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Hot%20Air%20Doesn't%20Disagree%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQGqbpqT4NS59KtmEo%2Fseq-rerun-hot-air-doesn-t-disagree%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Hot%20Air%20Doesn't%20Disagree%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQGqbpqT4NS59KtmEo%2Fseq-rerun-hot-air-doesn-t-disagree", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQGqbpqT4NS59KtmEo%2Fseq-rerun-hot-air-doesn-t-disagree", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<p>Today's post, <a href=\"/lw/t4/hot_air_doesnt_disagree/\">Hot Air Doesn't Disagree</a> was originally published on 16 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Hot_Air_Doesn.27t_Disagree\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>\"Disagreement\" between rabbits and foxes is sheer anthropomorphism. Rocks and hot air don't disagree, even though one decreases in elevation and one increases in elevation.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dv9/seq_rerun_the_bedrock_of_morality_arbitrary/\">The Bedrock of Morality: Arbitrary?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QGqbpqT4NS59KtmEo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.5476469458776e-07, "legacy": true, "legacyId": "18003", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["TMtBb7jTECLtWrKM4", "wb4mx8CmtoYYnttCk", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-01T06:40:27.322Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley meetup: Board game night", "slug": "meetup-berkeley-meetup-board-game-night-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:00.432Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/k3bYZXDA2DYCJYSFc/meetup-berkeley-meetup-board-game-night-0", "pageUrlRelative": "/posts/k3bYZXDA2DYCJYSFc/meetup-berkeley-meetup-board-game-night-0", "linkUrl": "https://www.lesswrong.com/posts/k3bYZXDA2DYCJYSFc/meetup-berkeley-meetup-board-game-night-0", "postedAtFormatted": "Wednesday, August 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%20meetup%3A%20Board%20game%20night&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%20meetup%3A%20Board%20game%20night%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk3bYZXDA2DYCJYSFc%2Fmeetup-berkeley-meetup-board-game-night-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%20meetup%3A%20Board%20game%20night%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk3bYZXDA2DYCJYSFc%2Fmeetup-berkeley-meetup-board-game-night-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk3bYZXDA2DYCJYSFc%2Fmeetup-berkeley-meetup-board-game-night-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 94, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ci'>Berkeley meetup: Board game night</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 August 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello, all! Tomorrow's meetup will be a board game night at Zendo. Zendo's game library has Robo Rally, Smallworld, Dominion, Ticket to Ride, Settlers, Tigrs &amp; Euphrates and some others. If you have a game (like Zendo!) that you'd like to play, bring it along! The meetup begins at 7pm. For directions to Zendo, see the mailing list <a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a> or call Nisan at <a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a> .</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ci'>Berkeley meetup: Board game night</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "k3bYZXDA2DYCJYSFc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 9.547699255982871e-07, "legacy": true, "legacyId": "18004", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley_meetup__Board_game_night\">Discussion article for the meetup : <a href=\"/meetups/ci\">Berkeley meetup: Board game night</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 August 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello, all! Tomorrow's meetup will be a board game night at Zendo. Zendo's game library has Robo Rally, Smallworld, Dominion, Ticket to Ride, Settlers, Tigrs &amp; Euphrates and some others. If you have a game (like Zendo!) that you'd like to play, bring it along! The meetup begins at 7pm. For directions to Zendo, see the mailing list <a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a> or call Nisan at <a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a> .</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley_meetup__Board_game_night1\">Discussion article for the meetup : <a href=\"/meetups/ci\">Berkeley meetup: Board game night</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley meetup: Board game night", "anchor": "Discussion_article_for_the_meetup___Berkeley_meetup__Board_game_night", "level": 1}, {"title": "Discussion article for the meetup : Berkeley meetup: Board game night", "anchor": "Discussion_article_for_the_meetup___Berkeley_meetup__Board_game_night1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-01T10:22:18.849Z", "modifiedAt": null, "url": null, "title": "Why AGI is extremely likely to come before FAI", "slug": "why-agi-is-extremely-likely-to-come-before-fai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:00.545Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "skeptical_lurker", "createdAt": "2010-12-01T13:43:18.347Z", "isAdmin": false, "displayName": "skeptical_lurker"}, "userId": "aKzXqmmQZrJseCGRt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JguLE9pQJiDzAMFmg/why-agi-is-extremely-likely-to-come-before-fai", "pageUrlRelative": "/posts/JguLE9pQJiDzAMFmg/why-agi-is-extremely-likely-to-come-before-fai", "linkUrl": "https://www.lesswrong.com/posts/JguLE9pQJiDzAMFmg/why-agi-is-extremely-likely-to-come-before-fai", "postedAtFormatted": "Wednesday, August 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20AGI%20is%20extremely%20likely%20to%20come%20before%20FAI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20AGI%20is%20extremely%20likely%20to%20come%20before%20FAI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJguLE9pQJiDzAMFmg%2Fwhy-agi-is-extremely-likely-to-come-before-fai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20AGI%20is%20extremely%20likely%20to%20come%20before%20FAI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJguLE9pQJiDzAMFmg%2Fwhy-agi-is-extremely-likely-to-come-before-fai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJguLE9pQJiDzAMFmg%2Fwhy-agi-is-extremely-likely-to-come-before-fai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 687, "htmlBody": "<p>&nbsp;</p>\n<div id=\"entry_t3_dw6\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p><strong>Why AGI is extremely likely to come before FAI</strong></p>\n<p>&nbsp;</p>\n<p>I am using FAI to mean EY's definition of provably friendly AGI implementing CEV. It is possible that the AGI which comes first will not be *provably* friendly, but will never the less turn out to be friendly, although the probability that non-provably friendly AI turns out to be friendly depends on how it is programmed. But this is a subtopic for another article - Part II: what to do if FAI is unlikely to come first.</p>\n<p><br />I realize much of this has been said before. <br /><br /><br /><strong>Complexity of algorithm design:</strong><br /><br />Intuitively, FAI seems orders of magnitude more complex than AGI. If I decided to start trying to program an AGI tomorrow, I would have ideas on how to start, and maybe even make a minuscule amount of progress. Ben Goertzel even has a (somewhat optimistic) [roadmap](<a href=\"http://opencog.org/roadmap/\">http://opencog.org/roadmap/</a>) for AGI in a decade. Meanwhile, afaik FAI is still stuck at the stage of lob&rsquo;s theorem.&nbsp; <br />The fact that EY seems to be focusing on promoting rationality and writing (admittedly awesome) harry potter fanfiction seems to indicate that he doesn&rsquo;t currently know how to write FAI (and nor does anyone else) otherwise he would be focusing on that now, and instead is planning for the long term.<br /><strong><br />Computational complexity</strong><br />CEV requires modelling (and extrapolating) every human mind on the planet, while avoiding the creation of sentient entities. While modelling might be cheaper than ~10^17 flops per human due to short cuts, I doubt it&rsquo;s going to come cheap. Randomly sampling a subset of humanity to extrapolate from, at least initially, could make this problem less severe, although you will get a poorer estimate of humanities utility function. Furthermore, this can be partially circumvented by saying that the AI follows a specific utility function while bootstrapping to enough computing power to implement CEV, but then you have the problem of allowing it to bootstrap safely. Having to prove friendliness of each step in self-improvement strikes me as something that could also be costly. <br />Finally, I get the impression that people are considering using Solomonoff induction. It&rsquo;s uncomputable, and while I realize that there exist approximations, I would imagine that these would be extremely expensive to calculate anything non-trivial. Is there any reason for using SI for FAI more than AGI, e.g. something todo with provability about the programs actions?&nbsp; <br /><strong><br />Infeasibility of relinquishment.</strong><br />If you can&rsquo;t convince Ben Goertzel that FAI is needed, even though he is familiar with the arguments and is an adviser to SIAI, you&rsquo;re not going to get anywhere near a universal consensus on the matter. Furthermore, AI is increasingly being used in financial and possibly soon military applications, and so there are strong incentives to speed the development of AI. While these uses are unlikely to be full AGI, they could provide building blocks &ndash; I can imagine a plausible situation where an advanced AI that predict the stock exchange could easily be modified to be a universal predictor.&nbsp;&nbsp; <br />The most powerful incentive to speed up AI development is the sheer number of people who die every day, and the amount of negentropy lost in the case that the 2nd law of thermodynamics cannot be circumvented. Even if there could be a worldwide ban on non-provably safe AGI, work would still probably continue in secret by people who thought the benefits of an earlier singularity outweighed the risks, and/or were worried about ideologically opposed groups getting their first.<br /><strong><br />Financial bootstrapping</strong><br />If you are ok with running a non-provably friendly AGI, then even in the early stages when, for example, your AI can write simple code or make reasonably accurate predictions but not speak English or make plans, you can use these to earn money, and buy more hardware/programmers.&nbsp; This seems to be part of the approach Ben is taking.</p>\n<p>Coming in Part II: is there any alternative (and doing nothing is not an alternative! even if FAI is unlikely to work its better than giving up!)</p>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JguLE9pQJiDzAMFmg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 6, "extendedScore": null, "score": 9.548773154587022e-07, "legacy": true, "legacyId": "18007", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-01T12:32:23.995Z", "modifiedAt": null, "url": null, "title": "Thoughts on a possible solution to Pascal's Mugging", "slug": "thoughts-on-a-possible-solution-to-pascal-s-mugging", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:37.262Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dolores1984", "createdAt": "2012-04-27T01:13:58.517Z", "isAdmin": false, "displayName": "Dolores1984"}, "userId": "4atqsmycH3WC4Cf5u", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/grwvpsnqScQonMWun/thoughts-on-a-possible-solution-to-pascal-s-mugging", "pageUrlRelative": "/posts/grwvpsnqScQonMWun/thoughts-on-a-possible-solution-to-pascal-s-mugging", "linkUrl": "https://www.lesswrong.com/posts/grwvpsnqScQonMWun/thoughts-on-a-possible-solution-to-pascal-s-mugging", "postedAtFormatted": "Wednesday, August 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thoughts%20on%20a%20possible%20solution%20to%20Pascal's%20Mugging&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThoughts%20on%20a%20possible%20solution%20to%20Pascal's%20Mugging%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgrwvpsnqScQonMWun%2Fthoughts-on-a-possible-solution-to-pascal-s-mugging%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thoughts%20on%20a%20possible%20solution%20to%20Pascal's%20Mugging%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgrwvpsnqScQonMWun%2Fthoughts-on-a-possible-solution-to-pascal-s-mugging", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgrwvpsnqScQonMWun%2Fthoughts-on-a-possible-solution-to-pascal-s-mugging", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1444, "htmlBody": "<p>For those who aren't familiar, Pascal's Mugging is a simple thought experiment that seems to demonstrate an intuitive flaw in naive expected utility maximization. &nbsp;In the classic version, someone walks up to you on the street, and says, 'Hi, I'm an entity outside your current model of the universe with essentially unlimited capabilities. &nbsp;If you don't give me five dollars, I'm going to use my powers to create 3^^^^3 people, and then torture them to death.' &nbsp;(For those not familiar with Knuth up-arrow notation, see <a href=\"http://wiki.lesswrong.com/wiki/Knuth's_up-arrow_notation\">here</a>). &nbsp;The idea being that however small your probability is that the person is telling the truth, they can simply state a number that's grossly larger - &nbsp;and when you <a href=\"http://wiki.lesswrong.com/wiki/Shut_up_and_multiply\">shut up and multiply</a>, expected utility calculations say you should give them the five dollars, along with pretty much anything else they ask for. &nbsp;</p>\n<p>Intuitively, this is nonsense. &nbsp;However, an AI under construction doesn't have a piece of code that lights up when exposed to nonsense. &nbsp;Not unless we program one in. &nbsp;And formalizing why, exactly, we shouldn't listen to the mugger is not as trivial as it sounds. &nbsp;The actual underlying problem has to do with how we handle arbitrarily small probabilities. &nbsp;There are a number of variations you could construct on the original problem that present the same paradoxical results. &nbsp;There are also a number of simple hacks you could undertake that produce the correct results in this particular case, but these are worrying (not to mention unsatisfying) for a number of reasons.</p>\n<p>So, with the background out of the way, let's move on to a potential approach to solving the problem which occurred to me about fifteen minutes ago while I was lying in bed with a bad case of insomnia at about five in the morning. &nbsp;If it winds up being incoherent, I blame sleep deprivation. &nbsp;If not, I take full credit. &nbsp;&nbsp;</p>\n<p>&nbsp;</p>\n<p>Let's take a look at a new thought experiment. &nbsp;Let's say someone comes up to you and tells you that they have magic powers, and will make a magic pony fall out of the sky. &nbsp;Let's say that, through some bizarrely specific priors, you decide that the probability that they're telling the truth (and, therefore, the probability that a magic pony is about to fall from the sky) is exactly 1/2^100. &nbsp;That's all well and good.</p>\n<p>Now, let's say that later that day, someone comes up to you, and hands you a fair quarter and says that if you flip it one hundred times, the probability that you'll get a straight run of heads is 1/2^100. &nbsp;You agree with them, chat about math for a bit, and then leave with their quarter. &nbsp;</p>\n<p>I propose that the probability value in the second case, while superficially&nbsp;identical&nbsp;to the probability value in the first case, represents a fundamentally different kind of claim about reality than the first case. &nbsp;In the first case, you believe, overwhelmingly, that a magic pony will not fall from the sky. &nbsp;You believe, overwhelmingly, that the probability (in underlying reality, divorced from the map and its limitations) is zero. &nbsp;It is only grudgingly that you inch even a tiny morsel of probability into the other hypothesis (that the universe is structured in such a way as to make the probability non-zero). &nbsp;</p>\n<p>In the second case, you also believe, overwhelmingly, that you will not see the event in question (a run of heads). &nbsp;However, you don't believe that the probability is zero. &nbsp;You believe it's 1/2^100. &nbsp;You believe that, through only the lawful operation of the universe that actually exists, you could be surprised, even if it's not likely. &nbsp;You believe that if you ran the experiment in question enough times, you would probably, eventually, see a run of one hundred heads. &nbsp;This is not true for the first case. &nbsp;No matter how many times somebody pulls the pony trick, a rational agent is never going to get their hopes up. &nbsp; &nbsp; &nbsp;</p>\n<p>&nbsp;</p>\n<p>I would like, at this point, to talk about the notion of metaconfidence. &nbsp;When we talk to the crazy pony man, and to the woman with the coin, what we leave with are two identical numerical probabilities. &nbsp;However, those numbers do not represent the sum total of the information at our disposal. &nbsp;In the two cases, we have differing levels of confidence <em>in our levels of confidence</em>. &nbsp;And, furthermore, this difference has an actual ramifications on what a rational agent should expect to observe. &nbsp;In other words, even from a very conservative perspective, metaconfidence intervals <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">pay rent</a>. &nbsp;By treating the two probabilities as identical, we are needlessly throwing away information. &nbsp;I'm honestly not sure if this topic has been discussed before. &nbsp;I am not up to date on the literature on the subject. &nbsp;If the subject has already been thoroughly discussed, I apologize for the waste of time. &nbsp;</p>\n<p>Disclaimer aside, I'd like to propose that we push this a step further, and say that metaconfidence should play a role in how we calculate expected utility. &nbsp;If we have a very small probability of a large payoff (positive or negative), we should behave differently when metaconfidence is high than when it is low. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p>\n<p>From a very superificial analysis, lying in bed, metaconfidence appears to be directional. &nbsp;A low metaconfidence, in the case of the pony claim, should not increase the probability that the probability of a pony dropping out of the sky is HIGHER than our initial estimate. &nbsp;It also works the other way as well: if we have a very high degree of confidence in some event (the sun rising tomorrow), and we get some very suspect evidence to the contrary (an ancient civilization predicting the end of the world tonight), and we update our probability downward slightly, our low metaconfidence should not make us believe that the sun is less likely to rise tomorrow than we thought. &nbsp;Low metaconfidence should move our effective probability estimate against the direction of the evidence that we have low confidence in: the pony is less likely, and the sunrise is more likely, than a naive probability estimate would suggest. &nbsp; &nbsp;</p>\n<p>So, if you have a claim like the pony claim (or Pascal's mugging), in which you have a very low estimated probability, and a very low metaconfidence, should become dramatically less likely to actually happen, in the real world, than a case in which we have a low estimated probability, but a very high confidence in that probability. &nbsp;See the pony versus the coins. &nbsp;Rationally, we can only&nbsp;mathematically&nbsp;justify so low a confidence in the crazy pony man's claims. &nbsp;However, in the territory, you can add enough coins that the two probabilities are mathematically equal, and you are <em>still</em>&nbsp;more likely to get a run of heads than you are to have a pony magically drop out of the sky. &nbsp;I am proposing metaconfidence weighting as a way to get around this issue, and allow our map to more accurately reflect the underlying territory. &nbsp;It's not perfect, since metaconfidence is still, ultimately, calculated from our map of the territory, but it seems to me, based on my extremely brief analysis, that it is at least an improvement on the current model. &nbsp;&nbsp;&nbsp;</p>\n<p>Essentially, this idea is based on the understanding that the numbers that we generate and call probability do not, in fact, correspond to the actual rules of the territory. &nbsp;They are approximations, and they are perturbed by observation, and our finite data set limits the resolution of the probability intervals we can draw. &nbsp;This causes systematic distortions at the extreme ends of the probability spectrum, and especially at the small end, where the scale of the distortion rises dramatically as a function of the actual probability. &nbsp;I believe that the&nbsp;apparently&nbsp;absurd behavior demonstrated by an expected-utility agent exposed to Pascal's mugging, is a result of these distortions. &nbsp;I am proposing we attempt to compensate by filling in the missing information at the extreme ends of the bell curve with data from our model about our sources of evidence, and about the underlying nature of the territory. &nbsp;In other words, this is simply a way to use our available evidence more efficiently, and I suspect that, in practice, it eliminates many of the Pascal's-mugging-style problems we encounter currently. &nbsp; &nbsp; &nbsp;&nbsp;</p>\n<p>I apologize for not having worked the math out completely. &nbsp;I would like to reiterate that it is six thirty in the morning, and I've only been thinking about the subject for about a hundred minutes. &nbsp;That said, I'm not likely to get any sleep either way, so I thought I'd jot the idea down and see what you folks thought. &nbsp;Having outside eyes is very helpful, when you've just had a Brilliant New Idea. &nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "grwvpsnqScQonMWun", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 4, "extendedScore": null, "score": 9.5494029291689e-07, "legacy": true, "legacyId": "18008", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["a7n8GdKiAZRX86T5A"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-01T13:06:17.003Z", "modifiedAt": null, "url": null, "title": "Becoming a gene machine - what should change?", "slug": "becoming-a-gene-machine-what-should-change", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:01.975Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Delta", "createdAt": "2012-08-01T11:37:41.645Z", "isAdmin": false, "displayName": "Delta"}, "userId": "DT22husLNHbuy8TaZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MHHji8MJWYZCJYb5S/becoming-a-gene-machine-what-should-change", "pageUrlRelative": "/posts/MHHji8MJWYZCJYb5S/becoming-a-gene-machine-what-should-change", "linkUrl": "https://www.lesswrong.com/posts/MHHji8MJWYZCJYb5S/becoming-a-gene-machine-what-should-change", "postedAtFormatted": "Wednesday, August 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Becoming%20a%20gene%20machine%20-%20what%20should%20change%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABecoming%20a%20gene%20machine%20-%20what%20should%20change%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMHHji8MJWYZCJYb5S%2Fbecoming-a-gene-machine-what-should-change%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Becoming%20a%20gene%20machine%20-%20what%20should%20change%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMHHji8MJWYZCJYb5S%2Fbecoming-a-gene-machine-what-should-change", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMHHji8MJWYZCJYb5S%2Fbecoming-a-gene-machine-what-should-change", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 511, "htmlBody": "<p>Hello everyone,</p>\n<p>After being introduced to the fascinating subject of evolutionary theory by Less Wrong and starting reading The Selfish Gene I have been slowly coming to terms with the mind-blowing revelation that I am simply a machine built to ensure the preservation of my genes, and that they are the only part of me that will outlive me. This is a change of huge magnitude, requiring I abandon the usual cached thoughts and perceptions of humanity as somehow special, detached from and above the world and baser matter that built us.</p>\n<p>Such a revelation should make me question all my assumptions, permeate my thinking, yet I find myself still thinking much the same ways I did before. I have not fully integrated this information and its implications into my world-view. I have noticed myself changing my mind less often than I think, and hope.</p>\n<p>My question to you is therefore, how would you expect a person who had learnt of their status as a \"mere\" gene machine then reflected and fully integrated the knowledge to think? What new thoughts and habits would they form compared to their old life as an immortal special creature, allegedly made in god's image? What would you expect to change?</p>\n<p>I offer the following suggestions of the kinds of change this hypothetical person, let us call them \"the subject\", would make:</p>\n<p>- The subject would have to reformulate their attitude to other non-human life-forms or potential lifeforms. With no divine spark seperating us from other animals or artificial minds, they would experience the freedom to decide what they place in their \"tribe\" (I'm reminded of Human the piggy in Speaker for the Dead realising he can include other cultures and even alien species in his definition of his \"tribe\"). Would they show more empathy towards non-sapient animals too? How else would this manifest?</p>\n<p>- The subject would become more aware of their own mortality and that of others. This would hopefully result in taking additional care of themselves and others on the basis that each has only one chance to be happy and our indifferent creators will not do so. Regrettably this could go the other way and result in undervaluing life given its brevity and seeing no need for morality.</p>\n<p>- The subject would feel additional kinship towards fellow humans, bearing in mind that their fundamental structure is almost exactly the same. They would hopefully have greater difficulty labelling others as inhuman or evil and be better capable of empathy. This coupled with their own mortality might incline them to pursue longer-term projects for the benefit of humanity as a whole.</p>\n<p>- Less laudably the subject might make their new awareness a source of pride instead of humility, and take pleasure in looking down up those who still hold such \"backward\" beliefs, seeing them as weak for embracing reassuring falsehoods and having inflated senses of their uniqueness and special-ness.</p>\n<p>&nbsp;</p>\n<p>These are all very general, and I would be very interested to hear your ideas of specific behaviours such a conversion would engender if properly reflected upon and integrated. Thank you for your time.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MHHji8MJWYZCJYb5S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 0, "extendedScore": null, "score": 9.549566978709872e-07, "legacy": true, "legacyId": "18009", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-01T15:25:52.159Z", "modifiedAt": null, "url": null, "title": "Politics Discussion Thread August 2012", "slug": "politics-discussion-thread-august-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:06.299Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OrphanWilde", "createdAt": "2012-06-21T18:24:36.749Z", "isAdmin": false, "displayName": "OrphanWilde"}, "userId": "cdW87AS6fdK2sywL2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MnnPHuah4fh8K5pif/politics-discussion-thread-august-2012", "pageUrlRelative": "/posts/MnnPHuah4fh8K5pif/politics-discussion-thread-august-2012", "linkUrl": "https://www.lesswrong.com/posts/MnnPHuah4fh8K5pif/politics-discussion-thread-august-2012", "postedAtFormatted": "Wednesday, August 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Politics%20Discussion%20Thread%20August%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APolitics%20Discussion%20Thread%20August%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMnnPHuah4fh8K5pif%2Fpolitics-discussion-thread-august-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Politics%20Discussion%20Thread%20August%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMnnPHuah4fh8K5pif%2Fpolitics-discussion-thread-august-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMnnPHuah4fh8K5pif%2Fpolitics-discussion-thread-august-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 223, "htmlBody": "<p>In line with the results of the poll <a href=\"/r/discussion/lw/dsv/is_politics_the_mindkiller_an_inconclusive_test/\">here</a>, a thread for discussing politics. &nbsp;Incidentally, folks, I think downvoting the option you disagree with in a poll is generally considered poor form.</p>\n<p>&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">1.) Top-level comments should introduce arguments; responses should be responses to those arguments.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">2.) Upvote and downvote based on whether or not you find an argument convincing in the context in which it was raised. &nbsp;This means if it's a good argument against the argument it is responding to, not whether or not there's a good/obvious counterargument to it; if you have a good counterargument, raise it. &nbsp;If it's a convincing argument, and the counterargument is also convincing, upvote both. &nbsp;If both arguments are unconvincing, downvote both.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">3.) A single argument per comment would be ideal; as MixedNuts points out&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/dsv/is_politics_the_mindkiller_an_inconclusive_test/73yp\">here</a>, it's otherwise hard to distinguish between one good and one bad argument, which makes the upvoting/downvoting difficult to evaluate.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">4.) In general try to avoid color politics; try to discuss political issues, rather than political parties, wherever possible.</p>\n<p>&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If anybody thinks the rules should be dropped here, now that we're no longer conducting a test - I already dropped the upvoting/downvoting limits I tried, unsuccessfully, to put in - let me know. &nbsp;The first rule is the only one I think is strictly necessary.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Debiasing attempt: If you haven't yet read <a href=\"/lw/gw/politics_is_the_mindkiller/\">Politics is the Mindkiller</a>, you should.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MnnPHuah4fh8K5pif", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": -4, "extendedScore": null, "score": 9.550242848116274e-07, "legacy": true, "legacyId": "18010", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 167, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["THWYgvqTdyHkmpFgE", "9weLK2AJ9JEt2Tt8f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-01T15:39:18.317Z", "modifiedAt": null, "url": null, "title": "Open Thread, August 1-15, 2012", "slug": "open-thread-august-1-15-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:02.951Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BCtQRC2mxKciW2tBz/open-thread-august-1-15-2012", "pageUrlRelative": "/posts/BCtQRC2mxKciW2tBz/open-thread-august-1-15-2012", "linkUrl": "https://www.lesswrong.com/posts/BCtQRC2mxKciW2tBz/open-thread-august-1-15-2012", "postedAtFormatted": "Wednesday, August 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20August%201-15%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20August%201-15%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBCtQRC2mxKciW2tBz%2Fopen-thread-august-1-15-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20August%201-15%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBCtQRC2mxKciW2tBz%2Fopen-thread-august-1-15-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBCtQRC2mxKciW2tBz%2Fopen-thread-august-1-15-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post, even in Discussion, it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BCtQRC2mxKciW2tBz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 9.550307908831376e-07, "legacy": true, "legacyId": "18011", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 150, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-01T18:29:20.591Z", "modifiedAt": null, "url": null, "title": "August 2012 Media Thread", "slug": "august-2012-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:35.870Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HfT29E2oSazB4wkpd/august-2012-media-thread", "pageUrlRelative": "/posts/HfT29E2oSazB4wkpd/august-2012-media-thread", "linkUrl": "https://www.lesswrong.com/posts/HfT29E2oSazB4wkpd/august-2012-media-thread", "postedAtFormatted": "Wednesday, August 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20August%202012%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAugust%202012%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHfT29E2oSazB4wkpd%2Faugust-2012-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=August%202012%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHfT29E2oSazB4wkpd%2Faugust-2012-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHfT29E2oSazB4wkpd%2Faugust-2012-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 220, "htmlBody": "<p>&nbsp;</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">This is the monthly thread for posting media of various types that you've found that you enjoy. I find that exposure to LW ideas makes me less likely to enjoy some entertainment media that is otherwise quite popular, and finding media recommended by LWers is a good way to mitigate this. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the&nbsp;<a style=\"color: #8a8a8b; \" href=\"/r/discussion/tag/media_thread/\">older threads</a>.</p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">Rules:</p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a&nbsp;<a style=\"color: #8a8a8b; \" href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres. There is a meta thread for comments about future threads.</li>\n<li>If you have a thread to add, such as a video game thread or an Anime thread, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HfT29E2oSazB4wkpd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 9.551131349628943e-07, "legacy": true, "legacyId": "18014", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-01T19:04:59.872Z", "modifiedAt": null, "url": null, "title": "Looking for a roommate in Mountain View", "slug": "looking-for-a-roommate-in-mountain-view", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:58.642Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsteinhardt", "createdAt": "2010-08-05T03:07:27.568Z", "isAdmin": false, "displayName": "jsteinhardt"}, "userId": "EF8W65G6RaXxZjLBX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Wxc7hxB66FTuYXQpt/looking-for-a-roommate-in-mountain-view", "pageUrlRelative": "/posts/Wxc7hxB66FTuYXQpt/looking-for-a-roommate-in-mountain-view", "linkUrl": "https://www.lesswrong.com/posts/Wxc7hxB66FTuYXQpt/looking-for-a-roommate-in-mountain-view", "postedAtFormatted": "Wednesday, August 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Looking%20for%20a%20roommate%20in%20Mountain%20View&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALooking%20for%20a%20roommate%20in%20Mountain%20View%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWxc7hxB66FTuYXQpt%2Flooking-for-a-roommate-in-mountain-view%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Looking%20for%20a%20roommate%20in%20Mountain%20View%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWxc7hxB66FTuYXQpt%2Flooking-for-a-roommate-in-mountain-view", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWxc7hxB66FTuYXQpt%2Flooking-for-a-roommate-in-mountain-view", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 510, "htmlBody": "<p>In September I will be moving to Mountain View, CA together with a friend of mine from MIT. It turns out that the quality/cost ratio increases noticeably with the number of people living together, so we are looking for one or more additional roommates. All things being equal, I would much rather live with other rationalists, which is why I'm posting this on LessWrong.</p>\n<h3>About us</h3>\n<p>My name is Jacob, and my roommate's name is Jonathan. We both recently graduated from MIT (me with a bachelor's in mathematics, him with a master's in electrical engineering). I am going to graduate school in machine learning at Stanford; Jonathan works at Synaptics (the company that makes touch sensors).</p>\n<p>You can find approximately four-month-outdated information about me at my old MIT <a href=\"http://web.mit.edu/jsteinha/www/\">website</a>. I've been awarded both the Hertz and NSF Fellowships, which means that I have a guaranteed source of income for the next five years regardless of any external factors like my adviser's ability to pay for me. <strong>I teach for SPARC (CFAR's high school program) and am very interested in building up the rationalist community in the south bay.</strong></p>\n<h3>Reasons you should live with us</h3>\n<ul>\n<li>we both have steady sources of income and are on highly successful career tracks</li>\n<li>your behavior is strongly affected by the culture you live in; living with other rationalists will make you more rational</li>\n<li>we both value open communication and are difficult to offend, which makes conflict resolution much easier</li>\n<li>be at the center of exciting developments: I am working directly on important problems in AI and rationalist outreach, and know an&nbsp;embarrassingly&nbsp;large amount of math / computer science, even by LessWrong standards</li>\n<li>I know a lot about sports and strength training, and am happy to help you out if your goal is to become stronger / more athletic</li>\n<li>I am also first aid and CPR certified, so you are slightly less likely to die if you live with me</li>\n<li>Jonathan is a pretty good cook and would be interested in leading the effort on group dinners and/or teaching some culinary basics.</li>\n<li>Jonathan is good at do-it-yourself electronics (sensors, microcontrollers, FPGAs) and is willing to share experience / expertise</li>\n<li>we are both willing to participate in house-wide life-hacking experiments (N = 3 is much better than N = 1 for data size)</li>\n</ul>\n<h3>What we are looking for</h3>\n<ul>\n<li>interest in building up the rationalist community</li>\n<li>steady source of income</li>\n<li>you are interesting to talk to and instrumentally rational</li>\n</ul>\n<h3>Contact info</h3>\n<p>If you might be interested in living with us, send me a PM telling me a little bit about yourself; I'll then give you my e-mail and we can figure out if there is likely to be a good fit (obviously, we will also meet in person before any final decisions are made).</p>\n<p>EDIT: One person said via email that they tried and failed to PM me. In case this is a larger issue, my e-mail is jsteinha@csail.mit.edu.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Wxc7hxB66FTuYXQpt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 15, "extendedScore": null, "score": 9.551304030002552e-07, "legacy": true, "legacyId": "18015", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>In September I will be moving to Mountain View, CA together with a friend of mine from MIT. It turns out that the quality/cost ratio increases noticeably with the number of people living together, so we are looking for one or more additional roommates. All things being equal, I would much rather live with other rationalists, which is why I'm posting this on LessWrong.</p>\n<h3 id=\"About_us\">About us</h3>\n<p>My name is Jacob, and my roommate's name is Jonathan. We both recently graduated from MIT (me with a bachelor's in mathematics, him with a master's in electrical engineering). I am going to graduate school in machine learning at Stanford; Jonathan works at Synaptics (the company that makes touch sensors).</p>\n<p>You can find approximately four-month-outdated information about me at my old MIT <a href=\"http://web.mit.edu/jsteinha/www/\">website</a>. I've been awarded both the Hertz and NSF Fellowships, which means that I have a guaranteed source of income for the next five years regardless of any external factors like my adviser's ability to pay for me. <strong>I teach for SPARC (CFAR's high school program) and am very interested in building up the rationalist community in the south bay.</strong></p>\n<h3 id=\"Reasons_you_should_live_with_us\">Reasons you should live with us</h3>\n<ul>\n<li>we both have steady sources of income and are on highly successful career tracks</li>\n<li>your behavior is strongly affected by the culture you live in; living with other rationalists will make you more rational</li>\n<li>we both value open communication and are difficult to offend, which makes conflict resolution much easier</li>\n<li>be at the center of exciting developments: I am working directly on important problems in AI and rationalist outreach, and know an&nbsp;embarrassingly&nbsp;large amount of math / computer science, even by LessWrong standards</li>\n<li>I know a lot about sports and strength training, and am happy to help you out if your goal is to become stronger / more athletic</li>\n<li>I am also first aid and CPR certified, so you are slightly less likely to die if you live with me</li>\n<li>Jonathan is a pretty good cook and would be interested in leading the effort on group dinners and/or teaching some culinary basics.</li>\n<li>Jonathan is good at do-it-yourself electronics (sensors, microcontrollers, FPGAs) and is willing to share experience / expertise</li>\n<li>we are both willing to participate in house-wide life-hacking experiments (N = 3 is much better than N = 1 for data size)</li>\n</ul>\n<h3 id=\"What_we_are_looking_for\">What we are looking for</h3>\n<ul>\n<li>interest in building up the rationalist community</li>\n<li>steady source of income</li>\n<li>you are interesting to talk to and instrumentally rational</li>\n</ul>\n<h3 id=\"Contact_info\">Contact info</h3>\n<p>If you might be interested in living with us, send me a PM telling me a little bit about yourself; I'll then give you my e-mail and we can figure out if there is likely to be a good fit (obviously, we will also meet in person before any final decisions are made).</p>\n<p>EDIT: One person said via email that they tried and failed to PM me. In case this is a larger issue, my e-mail is jsteinha@csail.mit.edu.</p>", "sections": [{"title": "About us", "anchor": "About_us", "level": 1}, {"title": "Reasons you should live with us", "anchor": "Reasons_you_should_live_with_us", "level": 1}, {"title": "What we are looking for", "anchor": "What_we_are_looking_for", "level": 1}, {"title": "Contact info", "anchor": "Contact_info", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-01T19:19:39.722Z", "modifiedAt": null, "url": null, "title": "Admissions Essay Help?", "slug": "admissions-essay-help", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:06.639Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OnTheOtherHandle", "createdAt": "2010-11-29T01:36:16.484Z", "isAdmin": false, "displayName": "OnTheOtherHandle"}, "userId": "QFvtvhSzkcjYxQZ3N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aWmPZ8Qcf7TxzoftE/admissions-essay-help", "pageUrlRelative": "/posts/aWmPZ8Qcf7TxzoftE/admissions-essay-help", "linkUrl": "https://www.lesswrong.com/posts/aWmPZ8Qcf7TxzoftE/admissions-essay-help", "postedAtFormatted": "Wednesday, August 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Admissions%20Essay%20Help%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdmissions%20Essay%20Help%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaWmPZ8Qcf7TxzoftE%2Fadmissions-essay-help%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Admissions%20Essay%20Help%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaWmPZ8Qcf7TxzoftE%2Fadmissions-essay-help", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaWmPZ8Qcf7TxzoftE%2Fadmissions-essay-help", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 760, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\" style=\"line-height: normal;\">I need help writing a college application essay that will maximize my chances of getting into a school that the world considers prestigious. (17 years old, preparing to enter 12<sup>th</sup> grade at a central California high school as of this writing.)</p>\n<p class=\"MsoNormal\" style=\"line-height: normal;\">Throughout high school, I resisted being over-scheduled, and basically eschewed all extracurricular activities in favor of having time to think and read. Even when my parents pushed me into things like tennis, dance, or debate clubs (<a href=\"/lw/gw/politics_is_the_mindkiller/\">ugh</a>), I was secure in the belief that I could forgo them and rely on my grades and test scores to get me into a college that was good enough to earn a useful engineering degree and find a few interesting friends. (I was right.)</p>\n<p class=\"MsoNormal\" style=\"line-height: normal;\">However, my priorities have changed, and I&rsquo;m starting to really value the extra leverage prestige can bring me. I plan to start a Less Wrong/<a href=\"http://80000hours.org/\">80,000 Hours</a> club at whatever university I end up attending. I would have access to more intelligent, interested people at Stanford than at, say, UC Irvine. Perhaps more importantly, the club itself would have a better standing in the outside world if it were founded in Stanford. (This in addition to the fact that Stanford already has a world-class <a href=\"http://main.da.stanford.edu/\">Decisions and Ethics Center</a> that may be able to help.)</p>\n<p class=\"MsoNormal\" style=\"line-height: normal;\">This is not to say I now regret not being an officer in a dozen useless clubs or participating in endless extracurricular activities. I do, however, regret not doing at least <a href=\"http://calnewport.com/blog/2010/03/26/how-to-get-into-stanford-with-bs-on-your-transcript-failed-simulations-the-surprising-psychology-of-impressiveness/\">one really impressive, externally-verifiable thing</a> like writing a book. Nothing in my life would make someone say, &ldquo;Wow, how the hell did she <em>do</em> that?&rdquo; If admissions officers could scan my brain, they would find a lot that would make them say, &ldquo;How the hell could she <em>think</em> that?&rdquo; &ndash; but not much of it would be positive.</p>\n<p class=\"MsoNormal\" style=\"line-height: normal;\">So my question is, how do I write a personal statement essay, 250-500 words, that will leave an impression in an admissions officer&rsquo;s mind, <strong>without lying or plagiarizing</strong>, given that my adolescence was spent thinking and reading, not *doing*? Each university then has 2-4 follow-up prompts (&lt;= 250 words), such as these from Stanford:</p>\n<ol type=\"1\">\n<li class=\"MsoNormal\" style=\"mso-margin-top-alt: auto; mso-margin-bottom-alt: auto; line-height: normal; mso-list: l0 level1 lfo1; tab-stops: list .5in;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Stanford students possess intellectual vitality. Reflect on an idea or experience that has been important to your intellectual development.</span></li>\n<li class=\"MsoNormal\" style=\"mso-margin-top-alt: auto; mso-margin-bottom-alt: auto; line-height: normal; mso-list: l0 level1 lfo1; tab-stops: list .5in;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-fareast-font-family: &quot;Times New Roman&quot;;\">Virtually all of Stanford&rsquo;s undergraduates live on campus. What would you want your future roommate to know about you? Tell us something about you that will help your roommate&mdash;and us&mdash;know you better.</span></li>\n<li class=\"MsoNormal\" style=\"mso-margin-top-alt: auto; mso-margin-bottom-alt: auto; line-height: normal; mso-list: l0 level1 lfo1; tab-stops: list .5in;\"><span style=\"font-size: 12.0pt; font-family: &quot;Times New Roman&quot;,&quot;serif&quot;; mso-fareast-font-family: &quot;Times New Roman&quot;;\">What matters to you, and why?</span></li>\n</ol>\n<p class=\"MsoNormal\" style=\"line-height: normal;\">The problem with answering these is that all of my *best* answers for these questions (&ldquo;Newcomblike problems,&rdquo; &ldquo;Hey, do you want to join this rationality club I want to start?&rdquo;, and &ldquo;optimal philanthropy,&rdquo; respectively) would take way more than 250 words to explain.</p>\n<p class=\"MsoNormal\" style=\"line-height: normal;\">The focus on Stanford, by the way, is because my parents would be extremely unwilling to send me to a university on the East Coast, even if it were really prestigious. But feel free to give me general advice or advice specific to another university. :) If it actually happens, I'll be in a better position to convince them.</p>\n<p class=\"MsoNormal\" style=\"line-height: normal;\"><strong>May Be Relevant</strong>:</p>\n<p class=\"MsoNormal\" style=\"line-height: normal;\">I once tutored a girl in Algebra 1 over a period of three months, bringing her grades up from a D to a B. She stopped needing help and I didn&rsquo;t go looking for another tutee.</p>\n<p class=\"MsoNormal\" style=\"line-height: normal;\">I completed <a href=\"http://www.nanowrimo.org/\">NaNoWriMo</a> my freshman year &ndash; yeah, it was pretty bad.</p>\n<p class=\"MsoNormal\" style=\"line-height: normal;\">I&rsquo;ve been writing a daily essay on <a href=\"http://750words.com/\">750 words</a> since December 2010, and have written over 518,000 words in 562 days &ndash; writing something 98% of the time, and completing my words 95% of the time. (Although a lot of the missed days were due to glitches in the early website eating my words.)</p>\n<p class=\"MsoNormal\" style=\"line-height: normal;\">I entered the Science Fair with a couple friends, hated it because it crushed the spirit of curious inquiry under a predetermined experimental procedure with a predetermined result, and unsurprisingly didn&rsquo;t win &ndash; although we got a certificate from the US Army.</p>\n<p class=\"MsoNormal\" style=\"line-height: normal;\">I joined a community service club, hated it because we were just unpaid labor for rich people who didn&rsquo;t need much help, but stayed anyway because my friends were in it.</p>\n<p class=\"MsoNormal\" style=\"line-height: normal;\">General SAT: Reading and Writing scores slightly above the median for most prestigious universities, Math score slightly below. 800's on SAT Math II (Pre-calculus), SAT Biology Molecular, and SAT US History.</p>\n<p class=\"MsoNormal\" style=\"line-height: normal;\">5's on AP Calculus AB, AP English Language, and other, less relevant AP's. Five AP classes so far taken, received A's, planning to take 6 more next year.</p>\n<p class=\"MsoNormal\" style=\"line-height: normal;\">High probability of a good letter of recommendation from APUSH and Calculus teachers.</p>\n<p class=\"MsoNormal\" style=\"line-height: normal;\">Thank you!</p>\n<p class=\"MsoNormal\" style=\"line-height: normal;\">Edit: Fixed the hyperlink formatting.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aWmPZ8Qcf7TxzoftE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 9.551375052020105e-07, "legacy": true, "legacyId": "18016", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9weLK2AJ9JEt2Tt8f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-01T20:49:41.319Z", "modifiedAt": null, "url": null, "title": "Russian plan for immortality [link]", "slug": "russian-plan-for-immortality-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:00.643Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3i4vJ9zETivSuHaTh/russian-plan-for-immortality-link", "pageUrlRelative": "/posts/3i4vJ9zETivSuHaTh/russian-plan-for-immortality-link", "linkUrl": "https://www.lesswrong.com/posts/3i4vJ9zETivSuHaTh/russian-plan-for-immortality-link", "postedAtFormatted": "Wednesday, August 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Russian%20plan%20for%20immortality%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARussian%20plan%20for%20immortality%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3i4vJ9zETivSuHaTh%2Frussian-plan-for-immortality-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Russian%20plan%20for%20immortality%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3i4vJ9zETivSuHaTh%2Frussian-plan-for-immortality-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3i4vJ9zETivSuHaTh%2Frussian-plan-for-immortality-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 68, "htmlBody": "<p><a href=\"http://www.cbc.ca/news/yourcommunity/2012/07/human-immortality-could-be-possible-by-2045-say-russian-scientists.html\">http://www.cbc.ca/news/yourcommunity/2012/07/human-immortality-could-be-possible-by-2045-say-russian-scientists.html</a></p>\n<p>The nice thing about Russians (I'm from that&nbsp;neighborhood originally) is that they are absolutely crazy and will try just about anything. They also probably have/had second-best science culture behind US (though they suffered significant brain drain as huge numbers of educated&nbsp;Jews&nbsp;left in the last 25 years). They have less regulation and quite a few rich people with ideas. Seems like a worthwhile group to keep in touch with.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3i4vJ9zETivSuHaTh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 8, "extendedScore": null, "score": 9.551811093543572e-07, "legacy": true, "legacyId": "18017", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-02T05:07:46.592Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] When Anthropomorphism Became Stupid", "slug": "seq-rerun-when-anthropomorphism-became-stupid", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:58.559Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dNd3v5rpPMw8NWGRr/seq-rerun-when-anthropomorphism-became-stupid", "pageUrlRelative": "/posts/dNd3v5rpPMw8NWGRr/seq-rerun-when-anthropomorphism-became-stupid", "linkUrl": "https://www.lesswrong.com/posts/dNd3v5rpPMw8NWGRr/seq-rerun-when-anthropomorphism-became-stupid", "postedAtFormatted": "Thursday, August 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20When%20Anthropomorphism%20Became%20Stupid&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20When%20Anthropomorphism%20Became%20Stupid%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdNd3v5rpPMw8NWGRr%2Fseq-rerun-when-anthropomorphism-became-stupid%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20When%20Anthropomorphism%20Became%20Stupid%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdNd3v5rpPMw8NWGRr%2Fseq-rerun-when-anthropomorphism-became-stupid", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdNd3v5rpPMw8NWGRr%2Fseq-rerun-when-anthropomorphism-became-stupid", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 174, "htmlBody": "<p>Today's post, <a href=\"/lw/t5/when_anthropomorphism_became_stupid/\">When Anthropomorphism Became Stupid</a> was originally published on 16 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Anthropomorphism didn't become obviously wrong until we realized that the tangled neurons inside the brain were performing complex information processing, and that this complexity arose as a result of evolution.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dw3/seq_rerun_hot_air_doesnt_disagree/\">Hot Air Doesn't Disagree</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dNd3v5rpPMw8NWGRr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.554224204544993e-07, "legacy": true, "legacyId": "18027", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["f4RJtHBPvDRJcCTva", "QGqbpqT4NS59KtmEo", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-02T08:17:08.744Z", "modifiedAt": null, "url": null, "title": "Reinforcement Learning: A Non-Standard Introduction (Part 2)", "slug": "reinforcement-learning-a-non-standard-introduction-part-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:32.745Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "royf", "createdAt": "2012-05-28T04:41:04.869Z", "isAdmin": false, "displayName": "royf"}, "userId": "Rrw92MqPSK6T8nSBg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xdjA6YtE7QBsLYQ3i/reinforcement-learning-a-non-standard-introduction-part-2", "pageUrlRelative": "/posts/xdjA6YtE7QBsLYQ3i/reinforcement-learning-a-non-standard-introduction-part-2", "linkUrl": "https://www.lesswrong.com/posts/xdjA6YtE7QBsLYQ3i/reinforcement-learning-a-non-standard-introduction-part-2", "postedAtFormatted": "Thursday, August 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reinforcement%20Learning%3A%20A%20Non-Standard%20Introduction%20(Part%202)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReinforcement%20Learning%3A%20A%20Non-Standard%20Introduction%20(Part%202)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxdjA6YtE7QBsLYQ3i%2Freinforcement-learning-a-non-standard-introduction-part-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reinforcement%20Learning%3A%20A%20Non-Standard%20Introduction%20(Part%202)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxdjA6YtE7QBsLYQ3i%2Freinforcement-learning-a-non-standard-introduction-part-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxdjA6YtE7QBsLYQ3i%2Freinforcement-learning-a-non-standard-introduction-part-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1013, "htmlBody": "<p><strong>Followup to:</strong>&nbsp;<a href=\"/lw/dsq/reinforcement_learning_a_nonstandard_introduction/\">Part 1</a></p>\n<p>In part 1 we modeled the dynamics of an agent and its environment as a turn-based discrete-time process. We now start on the path of <a href=\"/lw/iu/mysterious_answers_to_mysterious_questions/\">narrowing</a>&nbsp;the model, in the ambitious search for an explanation and an algorithm for the behavior of intelligent agents.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>The spacecraft is nearing the end of its 9 month journey. In its womb rests a creature of marvelous design. Soon, it will reach its destination, and on the surface of the alien planet will <a href=\"http://en.wikipedia.org/wiki/File:Curiosity%27s_Seven_Minutes_of_Terror.ogv\">gently land</a> an <a href=\"http://en.wikipedia.org/wiki/Curiosity_rover\">intelligent agent</a>.&nbsp;Almost immediately the agent will start interacting with its environment, collecting and analyzing samples of air and ground and radiation, and eventually moving about to survey the land and to find energy and interesting stuff to do.</p>\n<p>It takes too long, about 14 minutes at the time of landing, for the state of Mars to affect the state of the brains of the people at the NASA mission control. This makes the state of the software of the rover important: it will be the one to decide the rover's actions, to a large degree.</p>\n<p>Even after it's landed, <a href=\"http://science.slashdot.org/story/13/07/06/1154235/patching-software-on-another-planet\">the software of the rover can be changed remotely</a>, but not the hardware. This puts some limitations on any brilliant ideas we could have for improving the rover at this late stage of the mission. How can we model these limitations?</p>\n<p>Our model <a href=\"/lw/dsq/reinforcement_learning_a_nonstandard_introduction/\">so far</a> defined the dynamics of the agent's interaction with its environment through the probabilities:</p>\n<p style=\"padding-left: 30px; \">p(W<sub>t</sub>|W<sub>t-1</sub>,M<sub>t-1</sub>)</p>\n<p style=\"padding-left: 30px; \">q(M<sub>t</sub>|M<sub>t-1</sub>,W<sub>t</sub>)</p>\n<p>for the current state of the world (Mars) and the agent (the rover), given their previous state.</p>\n<p>At some point the rover will take a sample of the air. Could we choose q to be such that M<sub>t</sub>&nbsp;at that time will reflect the oxygen level around the rover? Yes, the rover has hardware that, subsequent to analyzing the sample, will have one of a number of possible states, depending on the oxygen levels. It is wired to the rover's central controller, which can then have its state reflect that of the oxygen analyzer.</p>\n<p>At some point the rover will take a sample of the ground. Could we choose q to be such that M<sub>t</sub>&nbsp;at that time will reflect Mars'&nbsp;<em>sweetness</em>? Probably not any more than it already does. The rover is probably not equipped with any hardware sensitive to substances perceived by living creatures on Earth as sweet. This was a design choice, guided by the assumption that these organic molecules are extremely unlikely to be found on Mars. If they are, the rover cannot be made to reflect their concentration level.</p>\n<p>In other words, the rover is not <em>omniscient</em>. The scope of things it can possibly know of the state of Mars is strictly (and vastly) smaller than the scope of things which are <em>true</em>&nbsp;of the state of Mars. We call O for Observation the aspects of the environment that the agent can perceive.</p>\n<p>Similarly, p cannot take any value, because the rover is not <em>omnipotent</em>. It has engines, so it can cause Mars (relative to the rover's own point of view) to rotate under its wheels. But it cannot (I dare hope) blast Mars out of existence, nor plant a rose there. We call A for Action the operation of the rover's actuators.</p>\n<p>This more detailed model is illustrated in the Bayesian network:</p>\n<p><img src=\"http://images.lesswrong.com/t3_dux_1.png\" alt=\"\" width=\"626\" height=\"266\" /></p>\n<p>The dynamics of the model are (pardon my Greek)</p>\n<p style=\"padding-left: 30px; \">p(W<sub>t</sub>|W<sub>t-1</sub>,A<sub>t-1</sub>)</p>\n<p style=\"padding-left: 30px; \">&sigma;(O<sub>t</sub>|W<sub>t</sub>)</p>\n<p style=\"padding-left: 30px; \">q(M<sub>t</sub>|M<sub>t-1</sub>,O<sub>t</sub>)</p>\n<p style=\"padding-left: 30px; \">&pi;(A<sub>t</sub>|M<sub>t</sub>)</p>\n<p>Now M<sub>t</sub>&nbsp;can only depend on the state of the world W<sub>t</sub>&nbsp;through the agent's observation O<sub>t</sub>. When designing the rover's software, we can choose the memory update scheme q, thereby deciding how the rover processes the information in O<sub>t</sub>&nbsp;and what part of it it remembers. But we cannot change&nbsp;&sigma;, which is how the rover's sensory input depend on the truth of the Martian world.</p>\n<p>To clarify: the agent <em>can</em>&nbsp;change, through action, the kind of observations it gets. The quality of the ground sample can greatly improve if the correct appendage actually goes down to&nbsp;the ground. But that's not changing&nbsp;&sigma;. That's changing the position of Mars with respect to the appendage (a part of W<sub>t</sub>), and through that affecting the observation.</p>\n<p>Similarly, the rover can only change the state W<sub>t</sub>&nbsp;of Mars through an action A<sub>t</sub>. The software controls what command is sent to the engines in any state of its execution, which is summarized in&nbsp;&pi;. But it cannot change p, which is how the chosen action and the laws of physics cause Mars to change.</p>\n<p>So by adding detail to the model we have introduced further&nbsp;independences, which help narrow the model. Not only is the <a href=\"/lw/dsq/reinforcement_learning_a_nonstandard_introduction/\">future independent on the past given the present</a>, but now the dependence between the two parts of the current state, M<sub>t</sub>&nbsp;and W<sub>t</sub>, is completely explained by the previous memory state M<sub>t-1</sub>&nbsp;and the observation O<sub>t</sub>. This means that everything the agent knows about the world must come from the memory and the senses - no clairvoyance!</p>\n<p>The symmetric independence - no telekinesis! - can be found by flipping the Bayesian network. Its details are left as an exercise for the reader.</p>\n<p>As a final note, the inability to change p or &sigma; is only with respect to that specific rover which is now 14 light minutes away from here. Future rovers can easily have sweetness gauges or rose-planting devices, but they will still not be omniscient nor omnipotent. We can exemplify this principle further on 3 different scales:</p>\n<p>1. If we try to improve humans through training and education, but without changing their biology (nor enhancing it with technology), we will probably be limited in the kinds of things we can make the brain notice, or remember, or decide to do.</p>\n<p>2. Suppose that some senses (for example, the eye) have reached a <a href=\"http://en.wikipedia.org/wiki/Fitness_landscape\">local maximum</a> in their evolution, so that any improvement is extremely unlikely. Suppose further that the brain is not in such a local maximum, and can easily evolve to be better. Then the evolution of the brain will be constrained by the current suboptimal properties of the senses.</p>\n<p>3. Even if we are allowed to design an intelligent agent from scratch, the laws of physics as we know them will limit the part of the world it can perceive through observations, and the part it can influence through actions.</p>\n<p><strong>Continue reading:</strong>&nbsp;<a href=\"/lw/dz4/reinforcement_preference_and_utility/\">Reinforcement, Preference and Utility</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2d4": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xdjA6YtE7QBsLYQ3i", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 16, "extendedScore": null, "score": 9.555141933299845e-07, "legacy": true, "legacyId": "17961", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HPAjhrbYk6rPbpSXx", "6i3zToomS86oj9bS6", "bPeB6RT78k8dXKYKf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-02T09:22:17.211Z", "modifiedAt": null, "url": null, "title": "Roommate interest and coordination thread", "slug": "roommate-interest-and-coordination-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:57.368Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "patrickscottshields", "createdAt": "2010-04-20T01:11:39.077Z", "isAdmin": false, "displayName": "patrickscottshields"}, "userId": "8gfGDttD6N2kAHmpw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tip7d2cFNzyaqqB3H/roommate-interest-and-coordination-thread", "pageUrlRelative": "/posts/tip7d2cFNzyaqqB3H/roommate-interest-and-coordination-thread", "linkUrl": "https://www.lesswrong.com/posts/tip7d2cFNzyaqqB3H/roommate-interest-and-coordination-thread", "postedAtFormatted": "Thursday, August 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Roommate%20interest%20and%20coordination%20thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARoommate%20interest%20and%20coordination%20thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftip7d2cFNzyaqqB3H%2Froommate-interest-and-coordination-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Roommate%20interest%20and%20coordination%20thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftip7d2cFNzyaqqB3H%2Froommate-interest-and-coordination-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftip7d2cFNzyaqqB3H%2Froommate-interest-and-coordination-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 228, "htmlBody": "<p>This thread is for the discussion of options for people interested in changing their living environments some time in the next year or so. It's a place to:</p>\n<ul class=\"simple\">\n<li>Share your situation to get an outside view</li>\n<li>Get on the radar of potential roommates</li>\n<li>Discuss existing communities or places that may be a good fit</li>\n<li>Describe what you're looking for in a living environment</li>\n<li>Post your procedure for deciding where to live</li>\n<li>Coordinate with others to find compatible roommates</li>\n<li>Discuss which factors are relevant to deciding where to live</li>\n<li>Post resources or data relevant to deciding where to live</li>\n</ul>\n<p>Whether you're graduating from college, moving for a new job, or looking to further optimize your living environment for other reasons, talking with others can help you identify options, catch inaccurate beliefs or poor reasoning, meet potential roommates, and more. Thanks to everyone who contributes!</p>\n<p>(This thread has been on my mind for a while. Reading <a class=\"reference external\" href=\"/r/discussion/lw/dwf/looking_for_a_roommate_in_mountain_view/\">this recent roommate-seeking post</a> inspired me actually write and post it. I'll post my own situation in the comments below.)</p>\n<p>To discuss the concept of this thread (rather than participating in the thread's intended discussion), please reply to <a href=\"/r/discussion/lw/dx0/roommate_interest_and_coordination_thread/74u1\">this comment</a>. Credit goes to the <a href=\"/lw/ctg/marketplace_transactions_open_thread/\">open transactions thread</a> and <a href=\"/lw/drj/group_rationality_diary_72312/\">group rationality diary</a> for some of the style and wording of this post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tip7d2cFNzyaqqB3H", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 13, "extendedScore": null, "score": 9.55545765904062e-07, "legacy": true, "legacyId": "18036", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Wxc7hxB66FTuYXQpt", "qY3XmmHHG7s9wnwNq", "bfqpD7ZLq2FCBYfYC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-02T16:09:06.846Z", "modifiedAt": null, "url": null, "title": "PM system is not working ", "slug": "pm-system-is-not-working", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:58.741Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsteinhardt", "createdAt": "2010-08-05T03:07:27.568Z", "isAdmin": false, "displayName": "jsteinhardt"}, "userId": "EF8W65G6RaXxZjLBX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ywjedCJZ2H7R2hqX5/pm-system-is-not-working", "pageUrlRelative": "/posts/ywjedCJZ2H7R2hqX5/pm-system-is-not-working", "linkUrl": "https://www.lesswrong.com/posts/ywjedCJZ2H7R2hqX5/pm-system-is-not-working", "postedAtFormatted": "Thursday, August 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20PM%20system%20is%20not%20working%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APM%20system%20is%20not%20working%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FywjedCJZ2H7R2hqX5%2Fpm-system-is-not-working%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=PM%20system%20is%20not%20working%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FywjedCJZ2H7R2hqX5%2Fpm-system-is-not-working", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FywjedCJZ2H7R2hqX5%2Fpm-system-is-not-working", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 23, "htmlBody": "<p>enough people have reported this that i wanted to make it publicly known. on my phone so will let others provide more detail</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ywjedCJZ2H7R2hqX5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 19, "extendedScore": null, "score": 9.557429886812664e-07, "legacy": true, "legacyId": "18038", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-02T17:14:10.036Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne, practical rationality", "slug": "meetup-melbourne-practical-rationality-14", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uYxi6iBcXjwQ5yJzF/meetup-melbourne-practical-rationality-14", "pageUrlRelative": "/posts/uYxi6iBcXjwQ5yJzF/meetup-melbourne-practical-rationality-14", "linkUrl": "https://www.lesswrong.com/posts/uYxi6iBcXjwQ5yJzF/meetup-melbourne-practical-rationality-14", "postedAtFormatted": "Thursday, August 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%2C%20practical%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%2C%20practical%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuYxi6iBcXjwQ5yJzF%2Fmeetup-melbourne-practical-rationality-14%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%2C%20practical%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuYxi6iBcXjwQ5yJzF%2Fmeetup-melbourne-practical-rationality-14", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuYxi6iBcXjwQ5yJzF%2Fmeetup-melbourne-practical-rationality-14", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/cj'>Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 August 2012 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">55 walsh st west melbourne 3003 australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality, as distinct from the social and rationality outreach meetups. Look for a social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>This meetup repeats on the 1st Friday of each month.</p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p>\n\n<p>(Sorry for the late notice.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/cj'>Melbourne, practical rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uYxi6iBcXjwQ5yJzF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 9.557745319932438e-07, "legacy": true, "legacyId": "18039", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality\">Discussion article for the meetup : <a href=\"/meetups/cj\">Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 August 2012 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">55 walsh st west melbourne 3003 australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality, as distinct from the social and rationality outreach meetups. Look for a social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>This meetup repeats on the 1st Friday of each month.</p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p>\n\n<p>(Sorry for the late notice.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality1\">Discussion article for the meetup : <a href=\"/meetups/cj\">Melbourne, practical rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality", "level": 1}, {"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-02T17:15:04.020Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne, practical rationality", "slug": "meetup-melbourne-practical-rationality-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:00.594Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XuqyRiJoYBaKYQW3G/meetup-melbourne-practical-rationality-0", "pageUrlRelative": "/posts/XuqyRiJoYBaKYQW3G/meetup-melbourne-practical-rationality-0", "linkUrl": "https://www.lesswrong.com/posts/XuqyRiJoYBaKYQW3G/meetup-melbourne-practical-rationality-0", "postedAtFormatted": "Thursday, August 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%2C%20practical%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%2C%20practical%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXuqyRiJoYBaKYQW3G%2Fmeetup-melbourne-practical-rationality-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%2C%20practical%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXuqyRiJoYBaKYQW3G%2Fmeetup-melbourne-practical-rationality-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXuqyRiJoYBaKYQW3G%2Fmeetup-melbourne-practical-rationality-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ck'>Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 August 2012 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">55 walsh st west melbourne 3003 australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality, as distinct from the social and rationality outreach meetups. Look for a social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>This meetup repeats on the 1st Friday of each month.</p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p>\n\n<p>(Sorry for the late notice.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ck'>Melbourne, practical rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XuqyRiJoYBaKYQW3G", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 9.557749682655968e-07, "legacy": true, "legacyId": "18040", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality\">Discussion article for the meetup : <a href=\"/meetups/ck\">Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 August 2012 07:00:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">55 walsh st west melbourne 3003 australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality, as distinct from the social and rationality outreach meetups. Look for a social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>This meetup repeats on the 1st Friday of each month.</p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p>\n\n<p>(Sorry for the late notice.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality1\">Discussion article for the meetup : <a href=\"/meetups/ck\">Melbourne, practical rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality", "level": 1}, {"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-02T17:44:12.893Z", "modifiedAt": null, "url": null, "title": "AI 5 minute existential risk talk", "slug": "ai-5-minute-existential-risk-talk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:58.860Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WmbG5CY8hmBabaWDa/ai-5-minute-existential-risk-talk", "pageUrlRelative": "/posts/WmbG5CY8hmBabaWDa/ai-5-minute-existential-risk-talk", "linkUrl": "https://www.lesswrong.com/posts/WmbG5CY8hmBabaWDa/ai-5-minute-existential-risk-talk", "postedAtFormatted": "Thursday, August 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%205%20minute%20existential%20risk%20talk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%205%20minute%20existential%20risk%20talk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmbG5CY8hmBabaWDa%2Fai-5-minute-existential-risk-talk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%205%20minute%20existential%20risk%20talk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmbG5CY8hmBabaWDa%2Fai-5-minute-existential-risk-talk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWmbG5CY8hmBabaWDa%2Fai-5-minute-existential-risk-talk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p>After complaints about misquoting, a slightly altered version of my AI 5-minute talk is now up at:</p>\n<p><a href=\"http://www.youtube.com/watch?v=3jSMe0owGMs\">http://www.youtube.com/watch?v=3jSMe0owGMs</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WmbG5CY8hmBabaWDa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 8, "extendedScore": null, "score": 9.557891022483616e-07, "legacy": true, "legacyId": "18041", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-03T00:29:54.003Z", "modifiedAt": null, "url": null, "title": "[Link] Holistic learning ebook", "slug": "link-holistic-learning-ebook", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:20.461Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hC83eKp9LFpw9FBks/link-holistic-learning-ebook", "pageUrlRelative": "/posts/hC83eKp9LFpw9FBks/link-holistic-learning-ebook", "linkUrl": "https://www.lesswrong.com/posts/hC83eKp9LFpw9FBks/link-holistic-learning-ebook", "postedAtFormatted": "Friday, August 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Holistic%20learning%20ebook&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Holistic%20learning%20ebook%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhC83eKp9LFpw9FBks%2Flink-holistic-learning-ebook%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Holistic%20learning%20ebook%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhC83eKp9LFpw9FBks%2Flink-holistic-learning-ebook", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhC83eKp9LFpw9FBks%2Flink-holistic-learning-ebook", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 598, "htmlBody": "<p>This ebook is kind of dopey, but it's one of the few resources I've seen where someone who's reasonably good at learning stuff tries to dissect and communicate the mental mechanisms they use for learning:</p>\n<p><a href=\"http://www.scotthyoung.com/blog/Programs/HolisticLearningEBook.pdf\">http://www.scotthyoung.com/blog/Programs/HolisticLearningEBook.pdf</a></p>\n<p>Here's a quick summary.</p>\n<ul>\n<li>You can learn things faster and better by improving the strategies you use for learning stuff.</li>\n<li>\"Holistic\" learning is opposed to \"rote\" learning. &nbsp;Holistic learners make lots of connections between different things they learn, and between things they learn and things that are personally relevant to them. &nbsp;An example might be <a href=\"http://nebula.deanza.edu/~newton/4B/4BEquationSummary.jpg\">this diagram</a> of various concepts in electrostatics, which I no longer know how to interpret. &nbsp;Another example might be me remembering about that diagram when reading the book.</li>\n<li>Holistic learners understand concepts in many different ways in order to really \"get\" them. &nbsp;They focus on building mental models instead of memorizing facts or procedures.</li>\n<li>If you understand a body of knowledge well enough, and forget a specific thing, you should be able to reconstruct your understanding of it based on related things you understand.</li>\n<li>The book refers to a \"model\" as something specific you understand particularly well that you can explain other things in terms of. &nbsp;For example, your \"model\" of a subspace (in linear algebra) might be a plane cutting through 3d space. &nbsp;Not all subspaces are planes, but thinking of a plane could be a way to quickly preload a bunch of relevant concepts in to your head.</li>\n<li>To learn holistically: \n<ul>\n<li>\"Visceralize\" concepts by summarizing them with a specific image, sound, feeling, and/or texture. &nbsp;Example: when learning programming, think of an array as a bunch of colored cubes suspended along a cord.</li>\n<li>Use metaphors to understand things better. &nbsp;Whenenever you learn something new, try to figure out what it reminds you of. &nbsp;If it's something from a totally unrelated domain, that's great.</li>\n<li>Explore your understanding network, ideally by solving problems, in order to fix glitches in your understanding and refresh it.</li>\n</ul>\n</li>\n<li>Holistic learning works great for some subjects, like science and math, but it's not as good for others, like history and law. &nbsp;It also helps less with concrete skills, like playing golf.</li>\n</ul>\n<p>The author sells various information &amp; coaching products in this vein, but as far as I can tell the ebook I linked to is the only free one:&nbsp;<a href=\"http://www.scotthyoung.com/lmslvidcourse/2.html\">http://www.scotthyoung.com/lmslvidcourse/2.html</a>. &nbsp;(If anyone pays for any of these, they should summarize them (to understand them better) and post the summaries to LW ;].) I'm definitely interested in hearing about <a href=\"http://c2.com/cgi/wiki?MappersVsPackers\">other</a> <a href=\"http://c2.com/cgi/wiki?ModelMaker\">resources</a> people know of on the mechanics of learning.</p>\n<hr />\n<p>Someone once told me that if you're a grad student studying under a Nobel laureate, you're much more likely to later win the Nobel yourself. &nbsp;(I just searched the internet for evidence regarding this claim and couldn't find any, so I'm now <a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">less confident</a> in it.) &nbsp;This claim suggests that doing good research is learnable.</p>\n<p>The person who told me this thought these research skills couldn't be described with words, and could only be transmitted through actual research partnerships. &nbsp;I think it's more likely that they <em>can</em> be described with words, but no Nobel laureate has bothered to sit down and write a book called \"How I Do Research\". &nbsp;(Please leave a comment if you know of a book like this!)</p>\n<p>Even if your fluid intelligence is static and difficult to improve, that doesn't prevent you from improving the mental algorithms and habits you use to accomplish tasks.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fF9GEdWXKJ3z73TmB": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hC83eKp9LFpw9FBks", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 14, "extendedScore": null, "score": 9.55985859903002e-07, "legacy": true, "legacyId": "18043", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mnS2WYLCGJP2kQkRn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-03T06:14:18.577Z", "modifiedAt": null, "url": null, "title": "Issue 301 shipped: Show parent comments on /comments", "slug": "issue-301-shipped-show-parent-comments-on-comments", "viewCount": null, "lastCommentedAt": "2012-08-06T13:56:09.378Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xmkGGBfftoEbq5wae/issue-301-shipped-show-parent-comments-on-comments", "pageUrlRelative": "/posts/xmkGGBfftoEbq5wae/issue-301-shipped-show-parent-comments-on-comments", "linkUrl": "https://www.lesswrong.com/posts/xmkGGBfftoEbq5wae/issue-301-shipped-show-parent-comments-on-comments", "postedAtFormatted": "Friday, August 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Issue%20301%20shipped%3A%20Show%20parent%20comments%20on%20%2Fcomments&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIssue%20301%20shipped%3A%20Show%20parent%20comments%20on%20%2Fcomments%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxmkGGBfftoEbq5wae%2Fissue-301-shipped-show-parent-comments-on-comments%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Issue%20301%20shipped%3A%20Show%20parent%20comments%20on%20%2Fcomments%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxmkGGBfftoEbq5wae%2Fissue-301-shipped-show-parent-comments-on-comments", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxmkGGBfftoEbq5wae%2Fissue-301-shipped-show-parent-comments-on-comments", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 25, "htmlBody": "<p>See http://code.google.com/p/lesswrong/issues/detail?id=301 for detail.</p>\n<p>Go to <a href=\"/prefs/\">your Preferences page</a>&nbsp;and \"<label style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\" for=\"show_parent_comments\">Show parent comments on</label><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\">&nbsp;</span><a style=\"color: #3d3d3e; font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify;\" href=\"http://lesswrong.com/comments\">/comments</a>\" to take advantage of this feature.</p>\n<p>(Work done by John Simon, integrated by <a href=\"/user/wmoore/\">User:wmoore</a>.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xmkGGBfftoEbq5wae", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 43, "extendedScore": null, "score": 9.561529549575808e-07, "legacy": true, "legacyId": "18055", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-03T08:02:15.927Z", "modifiedAt": null, "url": null, "title": "Wellcome Collection in London has exhibition on human augmentation", "slug": "wellcome-collection-in-london-has-exhibition-on-human", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JrG3YzpRFwCeqq6mc/wellcome-collection-in-london-has-exhibition-on-human", "pageUrlRelative": "/posts/JrG3YzpRFwCeqq6mc/wellcome-collection-in-london-has-exhibition-on-human", "linkUrl": "https://www.lesswrong.com/posts/JrG3YzpRFwCeqq6mc/wellcome-collection-in-london-has-exhibition-on-human", "postedAtFormatted": "Friday, August 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Wellcome%20Collection%20in%20London%20has%20exhibition%20on%20human%20augmentation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWellcome%20Collection%20in%20London%20has%20exhibition%20on%20human%20augmentation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJrG3YzpRFwCeqq6mc%2Fwellcome-collection-in-london-has-exhibition-on-human%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Wellcome%20Collection%20in%20London%20has%20exhibition%20on%20human%20augmentation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJrG3YzpRFwCeqq6mc%2Fwellcome-collection-in-london-has-exhibition-on-human", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJrG3YzpRFwCeqq6mc%2Fwellcome-collection-in-london-has-exhibition-on-human", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 13, "htmlBody": "<p>If you live in London, <a href=\"http://www.wellcomecollection.org/whats-on/exhibitions/superhuman.aspx\">it might be of some interest to you</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JrG3YzpRFwCeqq6mc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 5, "extendedScore": null, "score": 9.562053418705939e-07, "legacy": true, "legacyId": "18057", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-03T15:10:05.004Z", "modifiedAt": null, "url": null, "title": "What are you working on? August 2012", "slug": "what-are-you-working-on-august-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:07.296Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/99jDNXCxFfDmGQxF5/what-are-you-working-on-august-2012", "pageUrlRelative": "/posts/99jDNXCxFfDmGQxF5/what-are-you-working-on-august-2012", "linkUrl": "https://www.lesswrong.com/posts/99jDNXCxFfDmGQxF5/what-are-you-working-on-august-2012", "postedAtFormatted": "Friday, August 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20you%20working%20on%3F%20August%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20you%20working%20on%3F%20August%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F99jDNXCxFfDmGQxF5%2Fwhat-are-you-working-on-august-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20you%20working%20on%3F%20August%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F99jDNXCxFfDmGQxF5%2Fwhat-are-you-working-on-august-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F99jDNXCxFfDmGQxF5%2Fwhat-are-you-working-on-august-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 105, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is the bimonthly 'What are you working On?' thread. Previous threads are&nbsp;</span><a style=\"color: #8a8a8b;\" href=\"/r/discussion/tag/waywo\">here</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">. So here's the question:</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; padding-left: 60px;\"><em>What are you working on?&nbsp;</em></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Here are some guidelines:</p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<li>Focus on projects that you have recently made progress on, not projects that you're thinking about doing but haven't started.</li>\n<li>Why this project and not others? Mention reasons why you're doing the project and/or why others should contribute to your project (if applicable).</li>\n<li>Talk about your goals for the project.</li>\n<li>Any kind of project is fair game: personal improvement, research project, art project, whatever.</li>\n<li>Link to your work if it's linkable.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "99jDNXCxFfDmGQxF5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 13, "extendedScore": null, "score": 9.564129959102651e-07, "legacy": true, "legacyId": "18060", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 95, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-03T15:21:10.113Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Madison, NYC", "slug": "weekly-lw-meetups-austin-madison-nyc", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RLcCw4d2evh5p7FGp/weekly-lw-meetups-austin-madison-nyc", "pageUrlRelative": "/posts/RLcCw4d2evh5p7FGp/weekly-lw-meetups-austin-madison-nyc", "linkUrl": "https://www.lesswrong.com/posts/RLcCw4d2evh5p7FGp/weekly-lw-meetups-austin-madison-nyc", "postedAtFormatted": "Friday, August 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Madison%2C%20NYC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Madison%2C%20NYC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRLcCw4d2evh5p7FGp%2Fweekly-lw-meetups-austin-madison-nyc%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Madison%2C%20NYC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRLcCw4d2evh5p7FGp%2Fweekly-lw-meetups-austin-madison-nyc", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRLcCw4d2evh5p7FGp%2Fweekly-lw-meetups-austin-madison-nyc", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 409, "htmlBody": "<p><strong>This summary was posted to LW Main on July 27th, and has been moved to discussion.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/c1\">Dublin, Ireland Meetup:&nbsp;<span class=\"date\">05 August 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/c4\">Washington DC Biased Boardgames meetup:&nbsp;<span class=\"date\">05 August 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/c3\">Brussels meetup:&nbsp;<span class=\"date\">11 August 2012 12:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">28 July 2018 01:30PM</span></a></li>\n<li><a href=\"/meetups/c5\">Madison: Team Problem-Solving:&nbsp;<span class=\"date\">29 July 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/c8\">Humanist Open Mic, NYC, Wednesday August 1st:&nbsp;<span class=\"date\">01 August 2012 02:35PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong>&nbsp;</strong><strong></strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RLcCw4d2evh5p7FGp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.564183774671748e-07, "legacy": true, "legacyId": "17906", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-03T15:33:53.905Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes August 2012", "slug": "rationality-quotes-august-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:28.757Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alejandro1", "createdAt": "2011-09-14T21:04:19.242Z", "isAdmin": false, "displayName": "Alejandro1"}, "userId": "K4b3vEKg7EGRr2o9A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wyHWrcnBE8GE76XHW/rationality-quotes-august-2012", "pageUrlRelative": "/posts/wyHWrcnBE8GE76XHW/rationality-quotes-august-2012", "linkUrl": "https://www.lesswrong.com/posts/wyHWrcnBE8GE76XHW/rationality-quotes-august-2012", "postedAtFormatted": "Friday, August 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20August%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20August%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwyHWrcnBE8GE76XHW%2Frationality-quotes-august-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20August%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwyHWrcnBE8GE76XHW%2Frationality-quotes-august-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwyHWrcnBE8GE76XHW%2Frationality-quotes-august-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"> </span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Here's the new thread for posting quotes, with the usual rules:</span></p>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Please post all quotes separately, so that they can be voted up/down separately. &nbsp;(If they are strongly related, reply to your own comments. &nbsp;If strongly ordered, then go ahead and post them together.)</span></li>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Do not quote yourself</span></li>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">Do not quote comments/posts on LW/OB</span></li>\n<li><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 11px; text-align: justify;\">No more than 5 quotes per person per monthly thread, please.</span></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wyHWrcnBE8GE76XHW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 9.564245575511723e-07, "legacy": true, "legacyId": "18042", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 434, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-03T17:52:47.311Z", "modifiedAt": null, "url": null, "title": "\"Epiphany addiction\"", "slug": "epiphany-addiction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:32.410Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CZridws5zBzwfjgef/epiphany-addiction", "pageUrlRelative": "/posts/CZridws5zBzwfjgef/epiphany-addiction", "linkUrl": "https://www.lesswrong.com/posts/CZridws5zBzwfjgef/epiphany-addiction", "postedAtFormatted": "Friday, August 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Epiphany%20addiction%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Epiphany%20addiction%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCZridws5zBzwfjgef%2Fepiphany-addiction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Epiphany%20addiction%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCZridws5zBzwfjgef%2Fepiphany-addiction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCZridws5zBzwfjgef%2Fepiphany-addiction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 254, "htmlBody": "<p>LW doesn't seem to have a discussion of the article&nbsp;<a href=\"http://www.succeedsocially.com/epiphany\">Epiphany Addiction</a>, by Chris at succeedsocially. First paragraph:</p>\n<blockquote>\n<p>\"Epiphany Addiction\" is an informal little term I came up with to describe a process that I've observed happen to people who try to work on their personal issues. How it works is that someone will be trying to solve a problem they have, say a lack of confidence around other people. Somehow they'll come across a piece of advice or a motivational snippet that will make them have an epiphany or a profound realization. This often happens when people are reading self-help materials and they come across something that stands out to them. People can also come up with epiphanies themselves if they're doing a lot of writing and reflecting in an attempt to try and analyze their problems.</p>\n</blockquote>\n<p>I like that article because it describes a dangerous failure mode of smart people. One example was the self-help blog of Phillip Eby (pjeby), where each new post seemed to bring new amazing insights, and after a while you became jaded. An even better, though controversial, example could be Eliezer's Sequences, if you view them as a series of epiphanies about AI research that didn't lead to much tangible progress. (Please don't make that statement the sole focus of discussion!)</p>\n<p>The underlying problem seems to be that people get a rush of power from neat-sounding realizations, and mistake that feeling for actual power. I don't know any good remedy for that, but being aware of the problem could help.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"9YFoDPFwMoWthzgkY": 4, "kEX5CzbfiAzGn4q8B": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CZridws5zBzwfjgef", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 57, "baseScore": 69, "extendedScore": null, "score": 0.000136, "legacy": true, "legacyId": "18063", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 53, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 94, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-03T22:14:18.183Z", "modifiedAt": null, "url": null, "title": "Meetup : Pittsburgh: Semantic Stopsigns", "slug": "meetup-pittsburgh-semantic-stopsigns", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KatjaGrace", "createdAt": "2009-02-27T14:15:22.378Z", "isAdmin": false, "displayName": "KatjaGrace"}, "userId": "jRRYAy2mQAHy2Mq3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/64mMgxpcZgbRg25Mx/meetup-pittsburgh-semantic-stopsigns", "pageUrlRelative": "/posts/64mMgxpcZgbRg25Mx/meetup-pittsburgh-semantic-stopsigns", "linkUrl": "https://www.lesswrong.com/posts/64mMgxpcZgbRg25Mx/meetup-pittsburgh-semantic-stopsigns", "postedAtFormatted": "Friday, August 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Pittsburgh%3A%20Semantic%20Stopsigns&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Pittsburgh%3A%20Semantic%20Stopsigns%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F64mMgxpcZgbRg25Mx%2Fmeetup-pittsburgh-semantic-stopsigns%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Pittsburgh%3A%20Semantic%20Stopsigns%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F64mMgxpcZgbRg25Mx%2Fmeetup-pittsburgh-semantic-stopsigns", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F64mMgxpcZgbRg25Mx%2Fmeetup-pittsburgh-semantic-stopsigns", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/cl'>Pittsburgh: Semantic Stopsigns</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 August 2012 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Eatunique, Craig Street, Pittsburgh</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Suggested reading: <a href=\"http://lesswrong.com/lw/it/semantic_stopsigns/\" rel=\"nofollow\">http://lesswrong.com/lw/it/semantic_stopsigns/</a>\nEatunique serves salads, wraps, sandwiches, drinks, and that kind of thing. Call 412-304-6258 if you can't find us.\nOur mailing list is at https://groups.google.com/group/lw-pgh?pli=1</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/cl'>Pittsburgh: Semantic Stopsigns</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "64mMgxpcZgbRg25Mx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 9.566189821856594e-07, "legacy": true, "legacyId": "18065", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh__Semantic_Stopsigns\">Discussion article for the meetup : <a href=\"/meetups/cl\">Pittsburgh: Semantic Stopsigns</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 August 2012 06:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Eatunique, Craig Street, Pittsburgh</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Suggested reading: <a href=\"http://lesswrong.com/lw/it/semantic_stopsigns/\" rel=\"nofollow\">http://lesswrong.com/lw/it/semantic_stopsigns/</a>\nEatunique serves salads, wraps, sandwiches, drinks, and that kind of thing. Call 412-304-6258 if you can't find us.\nOur mailing list is at https://groups.google.com/group/lw-pgh?pli=1</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh__Semantic_Stopsigns1\">Discussion article for the meetup : <a href=\"/meetups/cl\">Pittsburgh: Semantic Stopsigns</a></h2>", "sections": [{"title": "Discussion article for the meetup : Pittsburgh: Semantic Stopsigns", "anchor": "Discussion_article_for_the_meetup___Pittsburgh__Semantic_Stopsigns", "level": 1}, {"title": "Discussion article for the meetup : Pittsburgh: Semantic Stopsigns", "anchor": "Discussion_article_for_the_meetup___Pittsburgh__Semantic_Stopsigns1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FWMfQKG3RpZx6irjm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-04T01:57:58.368Z", "modifiedAt": null, "url": null, "title": "The supposedly hard problem of consciousness and the nonexistence of sense data: Is your dog a conscious being?", "slug": "the-supposedly-hard-problem-of-consciousness-and-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:01.883Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "vFgwGmWvHDWRxnBia", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/f43nMxsrjwMGGorWR/the-supposedly-hard-problem-of-consciousness-and-the", "pageUrlRelative": "/posts/f43nMxsrjwMGGorWR/the-supposedly-hard-problem-of-consciousness-and-the", "linkUrl": "https://www.lesswrong.com/posts/f43nMxsrjwMGGorWR/the-supposedly-hard-problem-of-consciousness-and-the", "postedAtFormatted": "Saturday, August 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20supposedly%20hard%20problem%20of%20consciousness%20and%20the%20nonexistence%20of%20sense%20data%3A%20Is%20your%20dog%20a%20conscious%20being%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20supposedly%20hard%20problem%20of%20consciousness%20and%20the%20nonexistence%20of%20sense%20data%3A%20Is%20your%20dog%20a%20conscious%20being%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff43nMxsrjwMGGorWR%2Fthe-supposedly-hard-problem-of-consciousness-and-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20supposedly%20hard%20problem%20of%20consciousness%20and%20the%20nonexistence%20of%20sense%20data%3A%20Is%20your%20dog%20a%20conscious%20being%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff43nMxsrjwMGGorWR%2Fthe-supposedly-hard-problem-of-consciousness-and-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff43nMxsrjwMGGorWR%2Fthe-supposedly-hard-problem-of-consciousness-and-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 867, "htmlBody": "<div class=\"MsoNormal\" style=\"text-align: center;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;;\">Of dogs and cows</span></strong></div>\n<div class=\"MsoNormal\"><span style=\"font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;;\">Are dogs conscious? My guess, you think so: that&rsquo;s why they&rsquo;re termed &ldquo;sentient.&rdquo; We assume that dogs see the world much as we do, despite being receptive to different information; they experience the same conscious data of sense as we experience. You, nevertheless, might be prepared to concede the ultimate unfathomability of the question, but if not, consider a related question: when did consciousness first arise in the course of organic evolution?</span></div>\n<div class=\"MsoNormal\"><br /></div>\n<div class=\"MsoNormal\"><span style=\"font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;;\">The reason the questions are obscure deserves scrutiny. I think I know <em style=\"mso-bidi-font-style: normal;\">you&rsquo;re</em> conscious because you say you know what I&rsquo;m saying when I mention &ldquo;consciousness&rdquo; or &ldquo;experience,&rdquo; but the limits of my knowledge of consciousness are telling: I will never find some physical structure to explain it. This isn&rsquo;t due to lack of empirical research or of theoretical ingenuity. To explain an observation, you must describe it, and the language used for describing conscious experience is the same language used for describing the object the experience refers to. The most I can do to describe the experienced &ldquo;brownness&rdquo; is achieved by referring to its cause. When I see a brown cow, I can only describe the raw experience as &ldquo;brown&rdquo;: the color that ordinarily gives rise to the experience. Thus, I necessarily omit from the description exactly what I want to explain: the qualitative character of the &ldquo;brown&rdquo; experience.</span></div>\n<div class=\"MsoNormal\" style=\"text-align: center;\"><br /></div>\n<div class=\"MsoNormal\" style=\"text-align: center;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;;\">Apparent self-evidence</span></strong></div>\n<div class=\"MsoNormal\"><span style=\"font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;;\">If qualitative consciousness existed, it would be utterly inexplicable; yet, the evidence of direct experience seems self-evidently to support its own existence. This seemingly immediate awareness of our raw mental states seems to be just what it is like to be ourselves. (Thomas Nagel.) Regardless of the apparent indubitability of the intuition that we have raw experiential states, this intuition remains nothing more than belief, and beliefs are subject to illusions that mislead us systematically.</span></div>\n<div class=\"MsoNormal\"><br /></div>\n<div class=\"MsoNormal\"><span style=\"font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;;\">One reason you might resist the conclusion that qualitative experience is illusory is wholesale distortion of reality regarding objects of seemingly immediate awareness seems implausible just because of our intimate connection with our own experience, but scientific developments can render seemingly unrelated philosophical positions plausible. The work of neurologists, such as Oliver Sacks, should caution against the prejudice that some experiences are so basic they resist radical distortion and fabrication. An example Sacks describes is a brain-damaged patient who mistook his wife for a hat. Neuroscientists conclude that cognitive functions are assemblies of modules, making it less startling that beliefs can be so radically wrong. </span></div>\n<div class=\"MsoNormal\"><br /></div>\n<div class=\"MsoNormal\"><span style=\"font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;;\">There&rsquo;s also a conceptual reason for the reluctance of philosophers and scientists to reject the intuition of raw sense experience: lack of clarity about how to characterize the prewired belief responsible for the illusion. The intuition seems too complex and sophisticated to accommodate innate belief; philosophers trying to nail down the precise content of the belief that qualia exist have had recourse to thought experiments remote from actual experience, and nobody seems to have characterized the essence of qualia. My suggestion: <span style=\"color: red;\">the illusion of qualitative awareness is the belief that we when we perceive or imagine objects, we have independently real experiences characterizable only by the terms used to describe the external object itself</span>. <strong>Qualia are inherently ineffable contents of perception or imagination</strong>.</span></div>\n<div class=\"MsoNormal\" style=\"text-align: center;\"><br /></div>\n<div class=\"MsoNormal\" style=\"text-align: center;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;;\">The illusion&rsquo;s evolution</span></strong></div>\n<div class=\"MsoNormal\"><span style=\"font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;;\">This definition also suggests an evolutionary explanation for the illusion of qualitative experience. Thought doesn&rsquo;t depend on the illusion of consciousness, as one can easily conceive of an intelligent being without illusory beliefs about the nature of the thinking process, but the illusion of consciousness might have encouraged the development of thinking. Perhaps human ancestors evolved the innate belief that they have experiences with properties corresponding to those of their referents because this belief encouraged our ancestors to make mental models of the world&mdash;encouraged them to engage in the offline thinking unique for our species. Objectified conscious experience could encourage mental-model making by generalizing the prior insight that you can predict one external object by manipulating a similar external object. Our ancestors would then need only substitute internal objects for external ones.</span></div>\n<div class=\"MsoNormal\" style=\"text-align: center;\"><br /></div>\n<div class=\"MsoNormal\" style=\"text-align: center;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;;\">Bypassing &ldquo;sense data&rdquo; in the theory of knowledge</span></strong></div>\n<div class=\"MsoNormal\"><span style=\"font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;;\">According to one longstanding theory in epistemology, sense data are our only basis for knowing the external world. This doctrine, taken to its logical conclusion, leads to skepticism about the external world&rsquo;s existence: sense data, supposedly our window to the world, became an insuperable barrier to cognition, for if all our knowledge is nothing but construction from sense data, our sense data are all we know. We can&rsquo;t get out of our own minds.</span></div>\n<div class=\"MsoNormal\"><br /></div>\n<div class=\"MsoNormal\"><span style=\"font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;;\">The reason the sense data theory leads to skeptical conclusions goes back to ineffability. If we know the world by sense data, you can draw conclusions about the world only through analogy, that is, by forming a relationship between two descriptions. Ineffable sense data have nothing in common with a world of things, except their names</span> <span style=\"font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;;\">&mdash;such as &ldquo;brownness.&rdquo;</span></div>\n<div class=\"MsoNormal\" style=\"text-align: center;\"><br /></div>\n<div class=\"MsoNormal\" style=\"text-align: center;\"><strong style=\"mso-bidi-font-weight: normal;\"><span style=\"font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;;\">Two illusions</span></strong></div>\n<div class=\"MsoNormal\"><span style=\"font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;;\">This account of raw experience as an adaptive illusion brings clarity to the argument that <a href=\"http://juridicalcoherence.blogspot.com/2010/12/what-how-and-why-of-free-will.html\">free will is illusory</a>. The sense of free will, I concluded, is the misperception that experienced deciding causes behavior. But &ldquo;experience&rdquo; doesn&rsquo;t exist. <a href=\"http://tinyurl.com/cdl69lk\">Compatibilist free will</a> is incoherent because it assumes the causal efficacy of unreal raw experience. </span></div>\n<div class=\"MsoNormal\"><span style=\"font-family: &quot;Verdana&quot;,&quot;sans-serif&quot;;\">[Cross-posted at http://juridicalcoherence.blogspot.com/2012/08/160-supposedly-hard-problem-of.html]<br /></span></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "f43nMxsrjwMGGorWR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": -21, "extendedScore": null, "score": 9.567276198999564e-07, "legacy": true, "legacyId": "18064", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-04T10:48:20.832Z", "modifiedAt": null, "url": null, "title": "Smart non-reductionists, philosophical vs. engineering mindsets, and religion", "slug": "smart-non-reductionists-philosophical-vs-engineering", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:01.945Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/H2SayAkY4PuMcREYu/smart-non-reductionists-philosophical-vs-engineering", "pageUrlRelative": "/posts/H2SayAkY4PuMcREYu/smart-non-reductionists-philosophical-vs-engineering", "linkUrl": "https://www.lesswrong.com/posts/H2SayAkY4PuMcREYu/smart-non-reductionists-philosophical-vs-engineering", "postedAtFormatted": "Saturday, August 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Smart%20non-reductionists%2C%20philosophical%20vs.%20engineering%20mindsets%2C%20and%20religion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASmart%20non-reductionists%2C%20philosophical%20vs.%20engineering%20mindsets%2C%20and%20religion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH2SayAkY4PuMcREYu%2Fsmart-non-reductionists-philosophical-vs-engineering%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Smart%20non-reductionists%2C%20philosophical%20vs.%20engineering%20mindsets%2C%20and%20religion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH2SayAkY4PuMcREYu%2Fsmart-non-reductionists-philosophical-vs-engineering", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH2SayAkY4PuMcREYu%2Fsmart-non-reductionists-philosophical-vs-engineering", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1099, "htmlBody": "<p><a href=\"http://edwardfeser.blogspot.com/2012/08/concretizing-abstract.html\">Concretizing the abstract</a> is an interesting blog post in that it makes a relatively cogent argument for non-reductionism. While I don't agree with it, I found it useful in that it helped me better understand how intelligent non-reductionists think. It also helped clarify to me an old distinction, that of philosophers versus engineers.</p>\n<blockquote>\n<div class=\"MsoNormal\"><span style=\"font-size: small;\"><span style=\"line-height: 115%;\">We abstract when we consider some particular aspect of a concrete thing while bracketing off or ignoring the other aspects of the thing.&nbsp; For example, when you consider a dinner bell or the side of a pyramid exclusively as instances of <em>triangularity</em>, you ignore their color, size, function, and metal or stone composition.&nbsp; Or to borrow an example from </span><a href=\"http://edwardfeser.blogspot.com/2012/06/color-holds-and-quantum-theory.html\"><span style=\"line-height: 115%;\">a recent post</span></a><span style=\"line-height: 115%;\">, when aircraft engineers determine how many passengers can be carried on a certain plane, they might focus exclusively on their average weight and ignore not only the passengers&rsquo; sex, ethnicity, hair color, dinner service preferences, etc., but even the actual weight of any particular passenger. [...]</span></span></div>\n<div class=\"MsoNormal\">\n<div class=\"MsoNormal\"><span style=\"font-size: small;\"><span style=\"line-height: 115%;\">Abstractions can be very useful, and are of themselves perfectly innocent when we keep in mind that we are abstracting.&nbsp; The trouble comes when we start to think of abstractions as if they were concrete realities themselves -- thereby &ldquo;reifying&rdquo; them -- and especially when we think of the abstractions as somehow <em>more</em> real than the concrete realities from which they have been abstracted. [...]<br /></span></span></div>\n</div>\n<div class=\"MsoNormal\"><span style=\"font-size: small;\"><span style=\"line-height: 115%;\">I do not mean to deny that abstractions of the sort in question may have their uses.&nbsp; On the contrary, the mathematical conception of matter is extremely useful, as the astounding technologies that surround us in modern life make obvious.&nbsp; But contrary to what some proponents of </span><a href=\"http://edwardfeser.blogspot.com/2011/03/scientism-roundup.html\"><span style=\"line-height: 115%;\">scientism</span></a><span style=\"line-height: 115%;\"> suppose, it simply doesn&rsquo;t follow for a moment that that conception gives us an <em>exhaustive</em> conception of the material world, for reasons I have stated many times (e.g. </span><a href=\"http://edwardfeser.blogspot.com/2011/11/reading-rosenberg-part-ii.html\"><span style=\"line-height: 115%;\">here</span></a><span style=\"line-height: 115%;\">). [...]</span></span></div>\n<div class=\"MsoNormal\">\n<div class=\"MsoNormal\"><span style=\"font-size: small;\"><span style=\"line-height: 115%;\">Then there is social science.&nbsp; When we abstract from concrete human beings their purely economic motivations, ignoring everything else and then reifying this abstraction, the result is <em>homo economicus</em>, a strange creature who, unlike real people, is driven by nothing but the desire to maximize utility.&nbsp; Nietzschean analyses of human motivation in terms of the will to power are less susceptible of mathematical modeling (and thus less &ldquo;scientific&rdquo;), but are variations on the same sort of error.&nbsp; Evolutionary psychology often combines abstractions of the natural scientific and social scientific sort.&nbsp; Like the neuroscientist, the evolutionary psychologist often treats parts of human beings as if they were substances independent of the whole from which they have been abstracted (&rdquo;selfish genes,&rdquo; &ldquo;memes&rdquo;), and adds to this reification the abstractions of the economist (e.g. game theory). <br /></span></span></div>\n</div>\n<div class=\"MsoNormal\">\n<div class=\"MsoNormal\"><span style=\"font-size: small;\"><span style=\"line-height: 115%;\">As the neuroscientific and sociobiological examples indicate, the Reification Fallacy is often combined with other fallacies.&nbsp; In these cases, parts of a whole substance are first abstracted from it and treated as if they were substances in their own right (e.g. brain hemispheres, genes); and then a second, &ldquo;Mereological Fallacy&rdquo; (as Bennett and Hacker call it) is committed, in which what is intelligibly attributed only to the whole is attributed to the parts (e.g. the left hemisphere of the brain is said to &ldquo;interpret,&rdquo; and genes are said to be &ldquo;selfish&rdquo;). [...]<br /></span></span></div>\n</div>\n<div class=\"MsoNormal\">\n<div class=\"MsoNormal\"><span style=\"font-size: small;\"><span style=\"line-height: 115%;\">The irony is that while New Atheists and others beholden to scientism pride themselves on being &ldquo;reality based,&rdquo; that is precisely what they are not.&nbsp; Actual, concrete reality is extremely complicated.&nbsp; There is far more to material systems than what can be captured in the equations of physics, far more to human beings than can be captured in the categories of neuroscience or economics, and far more to religion than can be captured in the ludicrous straw men peddled by New Atheists.&nbsp; All of these simplifying abstractions (except the last) have their value, but when we treat them as anything <em>more</em> than simplifying abstractions we have left the realm of science and entered that of ideology.&nbsp; The varieties of reductionism, eliminativism, and the &ldquo;hermeneutics of suspicion&rdquo; are manifestations of this tendency to replace real things with abstractions.&nbsp; They are all attempts to &ldquo;conquer the abundance&rdquo; of reality (as Paul Feyerabend </span><a href=\"http://www.amazon.com/Conquest-Abundance-Abstraction-versus-Richness/dp/0226245349/ref=sr_1_1?ie=UTF8&amp;qid=1343952821&amp;sr=8-1&amp;keywords=CONQUEST+OF+ABUNDANCE\"><span style=\"line-height: 115%;\">might have put it</span></a><span style=\"line-height: 115%;\">), to force the world in all its concrete richness into a straightjacket.</span></span></div>\n</div>\n</blockquote>\n<p>I find this interesting in the way that smart people are likely to disagree with the correct interpretation of some of its claims - while others would say the post is <a href=\"/lw/iu/mysterious_answers_to_mysterious_questions/\">worshipping the mysterious</a>, others would say that it's just making reasonable cautions about the inherent methodological limitations of a certain approach. One might even think that it's essentially making a similar point as <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">Eliezer's warning about floating beliefs</a>, and therefore to <em>agree</em> with the Sequences. The caution of \"beware of thinking that your abstractions say everything that there is to be said about something\" <em>is</em> a reasonable one, and people <em>do</em> clearly make that mistake sometimes.<br /><br />I expect that part of what influences how plausible one finds this argument depends on whether one has more of an \"engineer's mindset\" or a \"philosopher's mindset\". Somebody with an engineer's mindset will think that \"yes, the abstractions we use might be imperfect, but what else do you propose we use? They're still the best tool for <em>accomplishing stuff</em>, and anything else is just philosophcial nonsense that isn't grounded in anything\". Whereas the philosopher is less interested in using their knowledge to \"accomplish stuff\", and more interested in the ideas and their implications themselves.</p>\n<p>As an aside, this distinction might be part of the reason why we have <a href=\"http://lesswrong.com/lw/8p4/2011_survey_results/\">so many</a> computer or hard science folks on this site. Partially it's because  Eliezer used a lot of CS jargon in writing the Sequences, but probably  also because the Sequences, while philosophical in nature, are also very <a href=\"http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">focused on practical results</a> and getting empirical predictions out of your beliefs.</p>\n<p>Looking at what we could use this distinction for (and thus taking an engineer's mindset) some people here have <a href=\"/lw/9g1/the_problem_with_too_many_rational_memes/\">mentioned getting an \"ick\" reaction</a> from religious people, just due to those people having strong false beliefs. I think that, combined with properly understanding the <a href=\"https://www.facebook.com/Xuenay/posts/10151291683588662\">emotional</a> <a href=\"https://www.facebook.com/Xuenay/posts/440912749282255\">basis</a> of religion, an understanding of the philosopher / engineer distinction can help avoid that reaction. <a href=\"https://plus.google.com/106597887376283858570/posts/epuW988fNzw\">Our values determine our beliefs</a>, and there are plenty of religious people who aren't stupid, crazy, or anything like that. They might simply be philosophers instead of engineers, <em>or</em> they might be engineers who are more interested in the instrumental benefits of religion than the <a href=\"/r/discussion/lw/dxr/epiphany_addiction/756y\">rather marginal benefits</a> of x-rationality. (Amusingly, such a \"religious engineer\" might justifiably consider <em>our</em> obsession with \"truth\" as just an odd philosophical pursuit.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "H2SayAkY4PuMcREYu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 21, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "18079", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6i3zToomS86oj9bS6", "a7n8GdKiAZRX86T5A", "HAEPbGaMygJq8L59k", "stb3Jjumzhv49zCEb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-04T12:27:54.454Z", "modifiedAt": null, "url": null, "title": "A cynical explanation for why rationalists worry about FAI", "slug": "a-cynical-explanation-for-why-rationalists-worry-about-fai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:55.315Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "aaronsw", "createdAt": "2012-07-15T14:56:16.271Z", "isAdmin": false, "displayName": "aaronsw"}, "userId": "sLuYMA84hm9Th3gYq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xDMdiiTYcZWvhjdoe/a-cynical-explanation-for-why-rationalists-worry-about-fai", "pageUrlRelative": "/posts/xDMdiiTYcZWvhjdoe/a-cynical-explanation-for-why-rationalists-worry-about-fai", "linkUrl": "https://www.lesswrong.com/posts/xDMdiiTYcZWvhjdoe/a-cynical-explanation-for-why-rationalists-worry-about-fai", "postedAtFormatted": "Saturday, August 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20cynical%20explanation%20for%20why%20rationalists%20worry%20about%20FAI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20cynical%20explanation%20for%20why%20rationalists%20worry%20about%20FAI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxDMdiiTYcZWvhjdoe%2Fa-cynical-explanation-for-why-rationalists-worry-about-fai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20cynical%20explanation%20for%20why%20rationalists%20worry%20about%20FAI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxDMdiiTYcZWvhjdoe%2Fa-cynical-explanation-for-why-rationalists-worry-about-fai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxDMdiiTYcZWvhjdoe%2Fa-cynical-explanation-for-why-rationalists-worry-about-fai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 286, "htmlBody": "<p>My friend, hearing me recount tales of LessWrong, recently asked me if I thought it was simply a coincidence that so many LessWrong rationality nerds cared so much about creating Friendly AI. \"If Eliezer had simply been obsessed by saving the world from asteroids, would they all be focused on that?\"</p>\n<p>Obviously one possibility (the inside view) is simply that rationality compels you to focus on FAI. But if we take <a href=\"http://wiki.lesswrong.com/wiki/Outside_view\">the outside view</a> for a second, it does seem like FAI has a special attraction for armchair rationalists: it's the rare heroic act that can be accomplished without ever confronting reality.</p>\n<p>After all, if you want to save the planet from an asteroid, you have to do a lot of <em>work</em>! You have to build stuff and test it and just generally solve a lot of gritty engineering problems. But if you want to save the planet from AI, you can conveniently do the whole thing without getting out of bed.</p>\n<p>Indeed, as <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/\">the Tool AI debate</a> as shown, SIAI types have withdrawn from reality even further. There are a lot of AI researchers who spend a lot of time building models, analyzing data, and generally solving a lot of gritty engineering problems all day. But the SIAI view conveniently says this is all very dangerous and that one shouldn't even begin to try implementing anything like an AI until one has perfectly solved all of the theoretical problems first.</p>\n<p>Obviously this isn't any sort of proof that working on FAI is irrational, but it does seem awfully suspicious that people who really like to spend their time thinking about ideas have managed to persuade themselves that they can <em>save the entire species&nbsp;from certain doom</em>&nbsp;just by thinking about ideas.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 2, "ZzxvopS4BwLuQy42n": 2, "YTCrHWYHAsAD74EHo": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xDMdiiTYcZWvhjdoe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 59, "baseScore": 28, "extendedScore": null, "score": 9.3e-05, "legacy": true, "legacyId": "18080", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 182, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6SGqkCgHuNr7d4yJm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-04T15:04:14.699Z", "modifiedAt": null, "url": null, "title": "What are the optimal biases to overcome?", "slug": "what-are-the-optimal-biases-to-overcome", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.950Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "aaronsw", "createdAt": "2012-07-15T14:56:16.271Z", "isAdmin": false, "displayName": "aaronsw"}, "userId": "sLuYMA84hm9Th3gYq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9SE67uz98kh6x2CxR/what-are-the-optimal-biases-to-overcome", "pageUrlRelative": "/posts/9SE67uz98kh6x2CxR/what-are-the-optimal-biases-to-overcome", "linkUrl": "https://www.lesswrong.com/posts/9SE67uz98kh6x2CxR/what-are-the-optimal-biases-to-overcome", "postedAtFormatted": "Saturday, August 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20the%20optimal%20biases%20to%20overcome%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20the%20optimal%20biases%20to%20overcome%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9SE67uz98kh6x2CxR%2Fwhat-are-the-optimal-biases-to-overcome%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20the%20optimal%20biases%20to%20overcome%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9SE67uz98kh6x2CxR%2Fwhat-are-the-optimal-biases-to-overcome", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9SE67uz98kh6x2CxR%2Fwhat-are-the-optimal-biases-to-overcome", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 519, "htmlBody": "<p>If you're interested in learning rationality, where should you start? Remember, instrumental rationality is about making decisions that get you what you want -- surely there are some lessons that will help you more than others.</p>\n<p>You might start with the most famous ones, which tend to be the ones popularized by Kahneman and Tversky. But K&amp;T were academics. They weren't trying to help people be more rational, they were trying to prove to other academics that people <em>were</em> irrational. The result is that they focused not on the most important biases, but the ones that were easiest to prove.</p>\n<p>Take their famous <a href=\"/lw/j7/anchoring_and_adjustment/\">anchoring experiment</a>, in which they showed the spin of a roulette wheel affected people's estimates about African countries. The idea wasn't that roulette wheels causing biased estimates was a huge social problem; it was that no academic could possibly argue that this behavior was somehow rational. They thereby scored a decisive blow for psychology against economists claiming we're just rational maximizers.</p>\n<p>Most academic work on irrationality has followed in K&amp;T's footsteps. And, in turn, much of the stuff done by LW and CFAR has followed in the footsteps of this academic work. So it's not hard to believe that LW types are good at avoiding these biases and thus do well on the psychology tests for them. (Indeed, many of the questions on these tests for rationality come straight from K&amp;T experiments!)</p>\n<p>But if you look at the average person and ask why they aren't getting what they want, very rarely do you conclude their biggest problem is that they're suffering from anchoring, framing effects, the planning fallacy, commitment bias, or any of the other stuff in the sequences. Usually their biggest problems are far more quotidian and commonsensical.</p>\n<p>Take Eliezer. Surely he wanted SIAI to be a well-functioning organization. And <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6klg\">he's admitted</a> that lukeprog has done more to achieve that goal of his than he has. Why is lukeprog so much better at getting what Eliezer wants than Eliezer is? It's surely not because lukeprog is so much better at avoiding Sequence-style cognitive biases! lukeprog <a href=\"/lw/7dy/a_rationalists_tale/\">readily admits</a> that he's constantly learning new rationality techniques from Eliezer.</p>\n<p>No, it's because lukeprog did what seems like common sense: he bought a copy of <em>Nonprofits for Dummies</em> and did what it recommends. As lukeprog himself <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6klg?context=1#6l4h\">says</a>, it wasn't lack of intelligence or resources or akrasia that kept Eliezer from doing these things, \"it was a gap in general rationality.\"</p>\n<p>So if you're interested in closing the gap, it seems like the skills to prioritize aren't things like commitment effect and the sunk cost fallacy, but stuff like \"figure out what your goals really are\", \"look at your situation objectively and list the biggest problems\", \"when you're trying something new and risky, read the <em>For Dummies</em> book about it first\", etc. For lack of better terminology, let's call the K&amp;T stuff \"cognitive biases\" and this stuff \"practical biases\" (even though it's all obviously both practical and cognitive and biases is kind of a negative way of looking at it).&nbsp;</p>\n<p>What are the best things you've found on tackling these \"practical biases\"? <a href=\"/r/lesswrong/lw/dya/what_are_the_optimal_biases_to_overcome/75by\">Post your suggestions in the comments.</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 1, "4R8JYu4QF2FqzJxE5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9SE67uz98kh6x2CxR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 70, "baseScore": 98, "extendedScore": null, "score": 0.000234, "legacy": true, "legacyId": "18082", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 98, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 70, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bMkCEZoBNhgRBtzoj", "9WX59u7g2sdKqnjDm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-05T02:31:55.502Z", "modifiedAt": null, "url": null, "title": "Random question about starfish", "slug": "random-question-about-starfish", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:02.308Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xeXizR29ByaHjhQS2/random-question-about-starfish", "pageUrlRelative": "/posts/xeXizR29ByaHjhQS2/random-question-about-starfish", "linkUrl": "https://www.lesswrong.com/posts/xeXizR29ByaHjhQS2/random-question-about-starfish", "postedAtFormatted": "Sunday, August 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Random%20question%20about%20starfish&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARandom%20question%20about%20starfish%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxeXizR29ByaHjhQS2%2Frandom-question-about-starfish%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Random%20question%20about%20starfish%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxeXizR29ByaHjhQS2%2Frandom-question-about-starfish", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxeXizR29ByaHjhQS2%2Frandom-question-about-starfish", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 74, "htmlBody": "<p>Anyone know how long a starfish can survive on land, and if it matters especially whether there is a hot sun.</p>\n<p>I promise this will turn out to be (at least somewhat) important and Less-Wrong relevant. Googling for the answer gives me a bunch of \"wiki-answers\" type sites that I don't trust and news articles that don't give me the specifics. If anyone either has marine-biologist cred, or better google-fu than me, I'd appreciate it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xeXizR29ByaHjhQS2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 2, "extendedScore": null, "score": 9.574440741677785e-07, "legacy": true, "legacyId": "18084", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-05T08:28:19.128Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Cartoon Guide to L\u00f6b's Theorem ", "slug": "seq-rerun-the-cartoon-guide-to-loeb-s-theorem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:00.993Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oscar_Cunningham", "createdAt": "2009-09-18T13:28:22.764Z", "isAdmin": false, "displayName": "Oscar_Cunningham"}, "userId": "G2SZuAiaBaNPg9rBt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9XQwYhkiSLvAMXGpc/seq-rerun-the-cartoon-guide-to-loeb-s-theorem", "pageUrlRelative": "/posts/9XQwYhkiSLvAMXGpc/seq-rerun-the-cartoon-guide-to-loeb-s-theorem", "linkUrl": "https://www.lesswrong.com/posts/9XQwYhkiSLvAMXGpc/seq-rerun-the-cartoon-guide-to-loeb-s-theorem", "postedAtFormatted": "Sunday, August 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Cartoon%20Guide%20to%20L%C3%B6b's%20Theorem%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Cartoon%20Guide%20to%20L%C3%B6b's%20Theorem%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9XQwYhkiSLvAMXGpc%2Fseq-rerun-the-cartoon-guide-to-loeb-s-theorem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Cartoon%20Guide%20to%20L%C3%B6b's%20Theorem%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9XQwYhkiSLvAMXGpc%2Fseq-rerun-the-cartoon-guide-to-loeb-s-theorem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9XQwYhkiSLvAMXGpc%2Fseq-rerun-the-cartoon-guide-to-loeb-s-theorem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p>Today's post, <a href=\"/lw/t6/the_cartoon_guide_to_lobs_theorem/\">The Cartoon Guide to L&ouml;b's Theorem </a> was originally published on 17 August 2008. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<blockquote>An explanation, using cartoons, of Lob's theorem.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/lw/t5/when_anthropomorphism_became_stupid/\">When Anthropomorphism Became Stupid</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9XQwYhkiSLvAMXGpc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 9.576174515102652e-07, "legacy": true, "legacyId": "18087", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ALCnqX6Xx8bpFMZq3", "f4RJtHBPvDRJcCTva", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-05T21:00:02.708Z", "modifiedAt": null, "url": null, "title": "[video] Robin Hanson: Uploads Economics 101", "slug": "video-robin-hanson-uploads-economics-101", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:20.112Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mapnoterritory", "createdAt": "2012-03-14T20:44:02.961Z", "isAdmin": false, "displayName": "mapnoterritory"}, "userId": "EvXiCAYEyRqnXMWAD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/syCxJhtKLQKAx7Aic/video-robin-hanson-uploads-economics-101", "pageUrlRelative": "/posts/syCxJhtKLQKAx7Aic/video-robin-hanson-uploads-economics-101", "linkUrl": "https://www.lesswrong.com/posts/syCxJhtKLQKAx7Aic/video-robin-hanson-uploads-economics-101", "postedAtFormatted": "Sunday, August 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Bvideo%5D%20Robin%20Hanson%3A%20Uploads%20Economics%20101&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Bvideo%5D%20Robin%20Hanson%3A%20Uploads%20Economics%20101%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsyCxJhtKLQKAx7Aic%2Fvideo-robin-hanson-uploads-economics-101%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Bvideo%5D%20Robin%20Hanson%3A%20Uploads%20Economics%20101%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsyCxJhtKLQKAx7Aic%2Fvideo-robin-hanson-uploads-economics-101", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsyCxJhtKLQKAx7Aic%2Fvideo-robin-hanson-uploads-economics-101", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 18, "htmlBody": "<p><a href=\"http://www.youtube.com/watch?v=s2GIirg43sU&amp;feature=relmfu\" target=\"_blank\">Lecture at youtube.</a>&nbsp;</p>\n<p>Sorry - haven't watched it yet so no summary, but I expect it to be fun.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "syCxJhtKLQKAx7Aic", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 8, "extendedScore": null, "score": 9.579833312498908e-07, "legacy": true, "legacyId": "18090", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-06T00:51:32.688Z", "modifiedAt": null, "url": null, "title": "Self-skepticism: the first principle of rationality", "slug": "self-skepticism-the-first-principle-of-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:06.286Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "aaronsw", "createdAt": "2012-07-15T14:56:16.271Z", "isAdmin": false, "displayName": "aaronsw"}, "userId": "sLuYMA84hm9Th3gYq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NoYYBAaMRp9Y5Jnpo/self-skepticism-the-first-principle-of-rationality", "pageUrlRelative": "/posts/NoYYBAaMRp9Y5Jnpo/self-skepticism-the-first-principle-of-rationality", "linkUrl": "https://www.lesswrong.com/posts/NoYYBAaMRp9Y5Jnpo/self-skepticism-the-first-principle-of-rationality", "postedAtFormatted": "Monday, August 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Self-skepticism%3A%20the%20first%20principle%20of%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASelf-skepticism%3A%20the%20first%20principle%20of%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNoYYBAaMRp9Y5Jnpo%2Fself-skepticism-the-first-principle-of-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Self-skepticism%3A%20the%20first%20principle%20of%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNoYYBAaMRp9Y5Jnpo%2Fself-skepticism-the-first-principle-of-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNoYYBAaMRp9Y5Jnpo%2Fself-skepticism-the-first-principle-of-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 654, "htmlBody": "<p>When Richard Feynman started&nbsp;<a href=\"http://www.lhup.edu/~dsimanek/cargocul.htm\">investigating irrationality</a> in the 1970s, he quickly begun to realize the problem wasn't limited to the obvious irrationalists.</p>\n<p>Uri Geller claimed he could bend keys with his mind. But was he really any different from the academics who insisted their special techniques could teach children to read? Both failed the crucial scientific test of skeptical experiment: Geller's keys failed to bend in Feynman's hands; outside tests showed the new techniques only caused reading scores to go down.</p>\n<p>What mattered was not how smart the people were, or whether they wore lab coats or used long words, but whether they followed what he concluded was the crucial principle of truly scientific thought: \"a kind of utter honesty--a kind of leaning over backwards\" to prove yourself wrong. In a word: self-skepticism.</p>\n<p>As Feynman wrote, \"The first principle is that you must not fool yourself -- and you are the easiest person to fool.\" Our beliefs always seem correct to us -- after all, that's why they're our beliefs -- so we have to work extra-hard to try to prove them wrong. This means constantly looking for ways to test them against reality and to think of reasons our tests might be insufficient.</p>\n<p>When I think of the most rational people I know, it's this quality of theirs that's most pronounced. They are constantly trying to prove themselves wrong -- they attack their beliefs with everything they can find and when they run out of weapons they go out and search for more. The result is that by the time I come around, they not only acknowledge all my criticisms but propose several more I hadn't even thought of.</p>\n<p>And when I think of the least rational people I know, what's striking is how they do the exact opposite: instead of viciously attacking their beliefs, they try desperately to defend them. They too have responses to all my critiques, but instead of acknowledging and agreeing, they viciously attack my critique so it never touches their precious belief.</p>\n<p>Since these two can be hard to distinguish, it's best to look at some examples. The Cochrane Collaboration <a href=\"http://www.thecochranelibrary.com/userfiles/ccoch/file/World%20No%20Tobacco%20Day/CD001188.pdf\">argues that</a> support from hospital nurses may be helpful in getting people to quit smoking. How do they know that? you might ask. Well, they found this was the result from doing a meta-analysis of 31 different studies. But maybe they chose a biased selection of studies? Well, they systematically searched \"MEDLINE, EMBASE and PsycINFO [along with] hand searching of specialist journals, conference proceedings, and reference lists of previous trials and overviews.\" But did the studies they pick suffer from selection bias? Well, they searched for that -- along with three other kinds of systematic bias. And so on. But even after all this careful work, they still only are confident enough to conclude&nbsp;\"the results&hellip;support a modest but positive effect&hellip;with caution &hellip; these meta-analysis findings need to be interpreted carefully in light of the methodological limitations\".</p>\n<p>Compare this to <a href=\"http://www.heritage.org/research/reports/2012/02/the-top-five-flawed-arguments-against-premium-support\">the Heritage Foundation's argument</a> for the bipartisan Wyden&ndash;Ryan premium support plan. Their report also discusses lots of objections to the proposal, but confidently knocks down each one: \"this analysis relies on two highly implausible assumptions ... All these predictions were dead wrong. ... this perspective completely ignores the history of Medicare\" Their conclusion is similarly confident: \"The arguments used by opponents of premium support are weak and flawed.\" Apparently there's just not a single reason to be cautious about their enormous government policy proposal!</p>\n<p>Now, of course, the Cochrane authors might be secretly quite confident and the Heritage Foundation might be wringing their hands with self-skepticism behind-the-scenes. But let's imagine for a moment that these aren't just reportes intended to persuade <em>others</em> of a belief and instead accurate portrayals of how these two different groups approached the question. Now ask: which style of thinking is more likely to lead <em>the authors</em> to the right answer? Which attitude seems more like Richard Feynman? Which seems more like Uri Geller?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "YTCrHWYHAsAD74EHo": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NoYYBAaMRp9Y5Jnpo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 62, "baseScore": 50, "extendedScore": null, "score": 0.000113, "legacy": true, "legacyId": "18092", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 50, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 106, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-06T05:41:44.737Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Cartoon Guide to Lob's Theorem", "slug": "seq-rerun-the-cartoon-guide-to-lob-s-theorem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:00.422Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CvAxvRK6dgsmbQSJM/seq-rerun-the-cartoon-guide-to-lob-s-theorem", "pageUrlRelative": "/posts/CvAxvRK6dgsmbQSJM/seq-rerun-the-cartoon-guide-to-lob-s-theorem", "linkUrl": "https://www.lesswrong.com/posts/CvAxvRK6dgsmbQSJM/seq-rerun-the-cartoon-guide-to-lob-s-theorem", "postedAtFormatted": "Monday, August 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Cartoon%20Guide%20to%20Lob's%20Theorem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Cartoon%20Guide%20to%20Lob's%20Theorem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCvAxvRK6dgsmbQSJM%2Fseq-rerun-the-cartoon-guide-to-lob-s-theorem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Cartoon%20Guide%20to%20Lob's%20Theorem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCvAxvRK6dgsmbQSJM%2Fseq-rerun-the-cartoon-guide-to-lob-s-theorem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCvAxvRK6dgsmbQSJM%2Fseq-rerun-the-cartoon-guide-to-lob-s-theorem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p>Today's post, <a href=\"/lw/t6/the_cartoon_guide_to_lobs_theorem/\">The Cartoon Guide to L&ouml;b's Theorem</a> was originally published on 17 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>An explanation, using cartoons, of Lob's theorem.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dwr/seq_rerun_when_anthropomorphism_became_stupid/\">When Anthropomorphism Became Stupid</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CvAxvRK6dgsmbQSJM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 2, "extendedScore": null, "score": 7e-06, "legacy": true, "legacyId": "18098", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ALCnqX6Xx8bpFMZq3", "dNd3v5rpPMw8NWGRr", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-06T05:50:19.798Z", "modifiedAt": null, "url": null, "title": "The Doubling Box", "slug": "the-doubling-box", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:02.378Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mestroyer", "createdAt": "2012-04-15T14:43:35.361Z", "isAdmin": false, "displayName": "Mestroyer"}, "userId": "xCcdyLecNTyFRbYso", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ohcThmMPDF56v6JJ7/the-doubling-box", "pageUrlRelative": "/posts/ohcThmMPDF56v6JJ7/the-doubling-box", "linkUrl": "https://www.lesswrong.com/posts/ohcThmMPDF56v6JJ7/the-doubling-box", "postedAtFormatted": "Monday, August 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Doubling%20Box&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Doubling%20Box%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FohcThmMPDF56v6JJ7%2Fthe-doubling-box%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Doubling%20Box%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FohcThmMPDF56v6JJ7%2Fthe-doubling-box", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FohcThmMPDF56v6JJ7%2Fthe-doubling-box", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 799, "htmlBody": "<p>Let's say you have a box that has a token in it that can be redeemed for 1 utilon. Every day, its contents double. There is no limit on how many utilons you can buy with these tokens. You are immortal. It is sealed, and if you open it, it becomes an ordinary box. You get the tokens it has created, but the box does not double its contents anymore. There are no other ways to get utilons.</p>\n<p>How long do you wait before opening it? If you never open it, you get nothing (<a href=\"http://www.youtube.com/watch?v=xKG07305CBs\">you lose! Good day, sir or madam</a>!) and whenever you take it, taking it one day later would have been twice as good.</p>\n<p>I hope this doesn't sound like a reductio ad absurdum against unbounded utility functions or not discounting the future, because if it does you are in danger of amputating the wrong limb to save yourself from paradox-gangrene.</p>\n<p>What if instead of growing exponentially without bound, it decays exponentially to the bound of your utility function? If your utility function is bounded at 10, what if the first day it is 5, the second 7.5, the third 8.75, etc. Assume all the little details, like remembering about the box, trading in the tokens, etc, are free.</p>\n<p>If you discount the future using any function that doesn't ever hit 0, then the growth rate of the tokens can be chosen to more than make up for your discounting.</p>\n<p>If it does hit 0 at time T, what if instead of doubling, it just increases by however many utilons will be adjusted to 1 by your discounting at that point every time of growth, but the intervals of growth shrink to nothing? You get an adjusted 1 utilon at time T - 1s, and another adjusted 1 utilon at T - 0.5s, and another at T - 0.25s, etc? Suppose you can think as fast as you want, and open the box at arbitrary speed. Also, that whatever solution your present self precommits to will be followed by the future self. (Their decision won't be changed by any change in what times they care about)</p>\n<p>EDIT: People in the comments have suggested using a utility function that is both bounded and discounting. If your utility function isn't so strongly discounting that it drops to 0 right after the present, then you can find some time interval very close to the present where the discounting is all nonzero. And if it's nonzero, you can have a box that disappears, taking all possible utility with it at the end of that interval, and that, leading up to that interval, grows the utility in intervals that shrink to nothing as you approach the end of the interval, and increasing the utility-worth of tokens in the box such that it compensates for whatever your discounting function is exactly enough to asymptotically approach your bound.</p>\n<p>Here is my solution. You can't assume that your future self will make the optimal decision, or even a good decision. You have to treat your future self as a physical object that your choices affect, and take the probability distribution of what decisions your future self will make, and how much utility they will net you into account.</p>\n<p>Think if yourself as a Turing machine. If you do not halt and open the box, you lose and get nothing. No matter how complicated your brain, you have a finite number of states. You want to be a <a href=\"http://en.wikipedia.org/wiki/Busy_beaver\">busy beaver</a> and take the most possible time to halt, but still halt.</p>\n<p>If, at the end, you say to yourself \"I just counted to the highest number I could, counting once per day, and then made a small mark on my skin, and repeated, and when my skin was full of marks, that I was constantly refreshing to make sure they didn't go away...</p>\n<p>...but I could let it double one more time, for more utility!\"</p>\n<p>If you return to a state you have already been at, you know you are going to be waiting forever and lose and get nothing. So it is in your best interest to open the box.</p>\n<p>So there is not a universal optimal solution to this problem, but there is an optimal solution for a finite mind.</p>\n<p>I remember reading a while ago about a paradox where you start with $1, and can trade that for a 50% chance of $2.01, which you can trade for a 25% chance of $4.03, which you can trade for a 12.5% chance of $8.07, etc (can't remember where I read it).</p>\n<p>This is the same paradox with one of the traps for <a href=\"http://en.memory-alpha.org/wiki/Kobayashi_Maru_scenario\">wannabe Captain Kirks</a> (using dollars instead of utilons) removed and one of the unnecessary variables (uncertainty) cut out.</p>\n<p>My solution also works on that. Every trade is analogous to a day waited to open the box.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ohcThmMPDF56v6JJ7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 22, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "18099", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 83, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-06T06:37:47.527Z", "modifiedAt": null, "url": null, "title": "Suber, \"The Ethics of Deep Self-Modification\"", "slug": "suber-the-ethics-of-deep-self-modification", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:02.652Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kMjfXymGpnrLDA6kP/suber-the-ethics-of-deep-self-modification", "pageUrlRelative": "/posts/kMjfXymGpnrLDA6kP/suber-the-ethics-of-deep-self-modification", "linkUrl": "https://www.lesswrong.com/posts/kMjfXymGpnrLDA6kP/suber-the-ethics-of-deep-self-modification", "postedAtFormatted": "Monday, August 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Suber%2C%20%22The%20Ethics%20of%20Deep%20Self-Modification%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASuber%2C%20%22The%20Ethics%20of%20Deep%20Self-Modification%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkMjfXymGpnrLDA6kP%2Fsuber-the-ethics-of-deep-self-modification%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Suber%2C%20%22The%20Ethics%20of%20Deep%20Self-Modification%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkMjfXymGpnrLDA6kP%2Fsuber-the-ethics-of-deep-self-modification", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkMjfXymGpnrLDA6kP%2Fsuber-the-ethics-of-deep-self-modification", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 582, "htmlBody": "<p><a href=\"http://www.earlham.edu/~peters/hometoc.htm\">Peter Suber</a> is the Director of the <a href=\"http://cyber.law.harvard.edu/research/hoap\">Harvard Open Access Project</a>, a Senior Researcher at <a href=\"http://www.arl.org/sparc/\">SPARC</a> (not <a href=\"http://appliedrationality.org/sparc2012/\">CFAR's SPARC</a>), a Research Professor of Philosophy at Earlham College, and more. He also created <a href=\"http://www.earlham.edu/~peters/nomic.htm\">Nomic</a>, the game in which you \"move\" by changing the rules, and wrote the original essay on <a href=\"http://wiki.lesswrong.com/wiki/Logical_rudeness\">logical rudeness</a>.</p>\n<p>In \"<a href=\"http://www.earlham.edu/~peters/writing/selfmod.htm\">Saving Machines From Themselves: The Ethics of Deep Self-Modification</a>\" (2002), Suber examines the ethics of self-modifying machines, sometimes quite eloquently:</p>\n<blockquote>\n<p>&nbsp;</p>\n<p>If you had the power to modify your deep structure, would you trust yourself to use it? If you had the power to give this power to... an AI, would you do it?</p>\n<p>We human beings do have the power to modify our deep structure, through drugs and surgery. But we cannot yet use this power with enough precision to make deep changes to our neural structure without high risk of death or disability. There are two reasons why we find ourselves in this position. First, our instruments of self-modification are crude. Second, we have very limited knowledge about where and how to apply our instruments to get specific desirable effects...</p>\n<p>It's conceivable that we might one day overcome both limitations. Even if we do, however, it is very likely that we'll acquire precise tools of self-modification long before we acquire precise knowledge about how to apply them. This is simply because manipulating brain components is easier than understanding brains. When we reach this stage, then we'll face the hard problems of self-modification: when is deep self-modification worth the risk of self-mutilation, and who should be free to make this judgment and take the risk?</p>\n<p>Intelligent machines are likely to encounter these ethical questions much sooner in their evolution than human beings. The deep structure of an AI is a consequence of its code... All its cognitive properties and personal characteristics supervene on its code, and modifying the code can be done with perfect precision. A machine's power of self-modification can not only be more precise than ours, but can finally be sufficiently precise to make some deep self-enhancements worth the risk of self-mutilation. At least some machines are likely to see the balance of risks that way.</p>\n<p>...Machines with [the ability to self-modify] might look with condolence and sympathy on beings like ourselves who lack it, much as Americans might have looked on Canadians prior to 1982 when Canadians could not amend their own constitution. When Canadians won the right to amend their own constitution, they spoke of \"repatriating\" their constitution and becoming \"sovereign\" in their own land for the first time. Similarly, a being who moves from familiar forms of human liberty to deep and precise self-modification will reclaim its will from the sovereign flux, and attain a genuine form of autonomy over its desires and powers for the first time.</p>\n<p>...An important kind of risk inherent in deep self-modification is for a machine to change its desires to a form it would originally have found regrettable, harmful, or even despicable. It might start a session of self-modification by looking for the secret of joy and end (like some Greek sages) deciding that tranquillity is superior to joy. This modification of desire en route to realizing it is easily classified as learning, and deserves our respect. But imagine the case of a machine hoping to make itself less narcissistic and more considerate of the interests of others, but ending by desiring to advance its own ends at the expense of others, even through violence.</p>\n<p>&nbsp;</p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kMjfXymGpnrLDA6kP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 20, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "18105", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-06T10:36:22.781Z", "modifiedAt": null, "url": null, "title": "Meetup : Bratislava, Slovakia - the first LW meetup", "slug": "meetup-bratislava-slovakia-the-first-lw-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:07.646Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "BarbaraB", "createdAt": "2012-04-04T15:11:52.435Z", "isAdmin": false, "displayName": "BarbaraB"}, "userId": "aHcDnFsxTnRpMEuXD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SB8x4ZX5ESju2gfbk/meetup-bratislava-slovakia-the-first-lw-meetup", "pageUrlRelative": "/posts/SB8x4ZX5ESju2gfbk/meetup-bratislava-slovakia-the-first-lw-meetup", "linkUrl": "https://www.lesswrong.com/posts/SB8x4ZX5ESju2gfbk/meetup-bratislava-slovakia-the-first-lw-meetup", "postedAtFormatted": "Monday, August 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Bratislava%2C%20Slovakia%20-%20the%20first%20LW%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Bratislava%2C%20Slovakia%20-%20the%20first%20LW%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSB8x4ZX5ESju2gfbk%2Fmeetup-bratislava-slovakia-the-first-lw-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Bratislava%2C%20Slovakia%20-%20the%20first%20LW%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSB8x4ZX5ESju2gfbk%2Fmeetup-bratislava-slovakia-the-first-lw-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSB8x4ZX5ESju2gfbk%2Fmeetup-bratislava-slovakia-the-first-lw-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 137, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/cm\">Bratislava, Slovakia - the first LW meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">18 August 2012 05:00:00PM (+0200)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Bratislava, city center</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Dear Lesswrongians, we cordially invite You to the first Lesswrong meetup in Bratislava ! We just were at the Rationality minicamp in California, met Eliezer and other awesome LessWrongians (and introduced the word \"awesome\" into our active vocabulary :-) ), and we would finally like to meet the Lesswrong fans in our area. For questions, write in the discussion under this announcement.</p>\n<p>We are looking forward to see You !</p>\n<p>BarbaraB and Viliam_Bur</p>\n</div>\n<div class=\"md\">\n<p>&nbsp;</p>\n<p>Edit: I removed the contact data after the meetup, because the meeting was in my home.</p>\n<p>Edit 2: The lesswrongians in our area can still contact me at barbarabrezna@gmail.com. We will try another meetup in October.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/cm\">Bratislava, Slovakia - the first LW meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SB8x4ZX5ESju2gfbk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 17, "extendedScore": null, "score": 9.58380937080643e-07, "legacy": true, "legacyId": "18109", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Bratislava__Slovakia___the_first_LW_meetup\">Discussion article for the meetup : <a href=\"/meetups/cm\">Bratislava, Slovakia - the first LW meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">18 August 2012 05:00:00PM (+0200)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Bratislava, city center</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Dear Lesswrongians, we cordially invite You to the first Lesswrong meetup in Bratislava ! We just were at the Rationality minicamp in California, met Eliezer and other awesome LessWrongians (and introduced the word \"awesome\" into our active vocabulary :-) ), and we would finally like to meet the Lesswrong fans in our area. For questions, write in the discussion under this announcement.</p>\n<p>We are looking forward to see You !</p>\n<p>BarbaraB and Viliam_Bur</p>\n</div>\n<div class=\"md\">\n<p>&nbsp;</p>\n<p>Edit: I removed the contact data after the meetup, because the meeting was in my home.</p>\n<p>Edit 2: The lesswrongians in our area can still contact me at barbarabrezna@gmail.com. We will try another meetup in October.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Bratislava__Slovakia___the_first_LW_meetup1\">Discussion article for the meetup : <a href=\"/meetups/cm\">Bratislava, Slovakia - the first LW meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Bratislava, Slovakia - the first LW meetup", "anchor": "Discussion_article_for_the_meetup___Bratislava__Slovakia___the_first_LW_meetup", "level": 1}, {"title": "Discussion article for the meetup : Bratislava, Slovakia - the first LW meetup", "anchor": "Discussion_article_for_the_meetup___Bratislava__Slovakia___the_first_LW_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-06T22:47:43.332Z", "modifiedAt": null, "url": null, "title": "SI/CFAR Are Looking for Contract Web Developers", "slug": "si-cfar-are-looking-for-contract-web-developers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:01.588Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "malo", "createdAt": "2011-12-28T20:01:23.182Z", "isAdmin": false, "displayName": "Malo"}, "userId": "DA863LaqrSGNFs5w5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wEQs84j7fPMTv4Ewy/si-cfar-are-looking-for-contract-web-developers", "pageUrlRelative": "/posts/wEQs84j7fPMTv4Ewy/si-cfar-are-looking-for-contract-web-developers", "linkUrl": "https://www.lesswrong.com/posts/wEQs84j7fPMTv4Ewy/si-cfar-are-looking-for-contract-web-developers", "postedAtFormatted": "Monday, August 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SI%2FCFAR%20Are%20Looking%20for%20Contract%20Web%20Developers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASI%2FCFAR%20Are%20Looking%20for%20Contract%20Web%20Developers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwEQs84j7fPMTv4Ewy%2Fsi-cfar-are-looking-for-contract-web-developers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SI%2FCFAR%20Are%20Looking%20for%20Contract%20Web%20Developers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwEQs84j7fPMTv4Ewy%2Fsi-cfar-are-looking-for-contract-web-developers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwEQs84j7fPMTv4Ewy%2Fsi-cfar-are-looking-for-contract-web-developers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">The </span><a style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\" href=\"http://singularity.org\" target=\"_blank\">Singularity Institute</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"> and the </span><a style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\" href=\"http://appliedrationality.org\" target=\"_blank\">Center for Applied Rationality</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\"> are looking to expand their web development team. As per usual we'd like to post this opportunity to the LW community first. All work will be on a contract basis (you quote us your hourly rate).</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \"><strong>Perks:</strong></p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">\n<li>Make money while contributing to organizations you care about.</li>\n<li>Age and credentials are irrelevant; only product matters.</li>\n</ul>\n<div style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \"><strong>Requirements</strong>:</div>\n<div style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">\n<ul style=\"padding: 0px; \">\n<li>WordPress and PHP skills (some features require PHP coding).</li>\n<li>Good design sense.</li>\n<li>Ability to start work on tasks/features quickly and deliver them in a timely manner (i.e., responsiveness).</li>\n</ul>\n<div><br /></div>\n<div>If you're interested, <a href=\"http://tinyurl.com/si-cfar-webdev\" target=\"_blank\"><strong>apply here!</strong></a></div>\n<div><strong><br /></strong></div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wEQs84j7fPMTv4Ewy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 6, "extendedScore": null, "score": 9.587373947077773e-07, "legacy": true, "legacyId": "18111", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-07T05:06:32.238Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] You Provably Can't Trust Yourself", "slug": "seq-rerun-you-provably-can-t-trust-yourself", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:01.264Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4zPhwtBmSWeZDdrag/seq-rerun-you-provably-can-t-trust-yourself", "pageUrlRelative": "/posts/4zPhwtBmSWeZDdrag/seq-rerun-you-provably-can-t-trust-yourself", "linkUrl": "https://www.lesswrong.com/posts/4zPhwtBmSWeZDdrag/seq-rerun-you-provably-can-t-trust-yourself", "postedAtFormatted": "Tuesday, August 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20You%20Provably%20Can't%20Trust%20Yourself&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20You%20Provably%20Can't%20Trust%20Yourself%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4zPhwtBmSWeZDdrag%2Fseq-rerun-you-provably-can-t-trust-yourself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20You%20Provably%20Can't%20Trust%20Yourself%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4zPhwtBmSWeZDdrag%2Fseq-rerun-you-provably-can-t-trust-yourself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4zPhwtBmSWeZDdrag%2Fseq-rerun-you-provably-can-t-trust-yourself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<p>Today's post, <a href=\"/lw/t8/you_provably_cant_trust_yourself/\">You Provably Can't Trust Yourself</a> was originally published on 19 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#You_Provably_Can.27t_Trust_Yourself\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Lob's theorem provides, by analogy, a nice explanation for why you really can't trust yourself. Don't trust thoughts because you think them, trust them because they were generated by trustworthy rules.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/dyf/seq_rerun_the_cartoon_guide_to_l%C3%B6bs_theorem/#comments\">The Cartoon Guide to L&ouml;b's Theorem</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4zPhwtBmSWeZDdrag", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 9.589221220725994e-07, "legacy": true, "legacyId": "18113", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rm8tv9qZ9nwQxhshx", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-07T06:26:13.809Z", "modifiedAt": null, "url": null, "title": "Draft: Get Lucky", "slug": "draft-get-lucky", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:08.138Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4i7ovu3wn3no7SRyq/draft-get-lucky", "pageUrlRelative": "/posts/4i7ovu3wn3no7SRyq/draft-get-lucky", "linkUrl": "https://www.lesswrong.com/posts/4i7ovu3wn3no7SRyq/draft-get-lucky", "postedAtFormatted": "Tuesday, August 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Draft%3A%20Get%20Lucky&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADraft%3A%20Get%20Lucky%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4i7ovu3wn3no7SRyq%2Fdraft-get-lucky%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Draft%3A%20Get%20Lucky%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4i7ovu3wn3no7SRyq%2Fdraft-get-lucky", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4i7ovu3wn3no7SRyq%2Fdraft-get-lucky", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 693, "htmlBody": "<p>Here's a draft of an article that I want to post soon, but I figured I might as well get some feedback before I go ahead. If anyone else has exercises or experience with this I'd love to hear about it.</p>\n<p>The popular science article version of the research mentioned can be found here:&nbsp;http://www.richardwiseman.com/resources/The_Luck_Factor.pdf</p>\n<p>\n<hr />\n</p>\n<p>There's no fundamental propensity to good outcomes, but it looks like Luck is a thing.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">There were some experiments done by Richard Wiseman (of 59 seconds fame/recommendation) investigating luck, and they found that there was a statistically significant factor which led to some people being more likely to receive unexpected benefits.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">He took two groups of people, one self-proclaimed &ldquo;very lucky&rdquo; and the other normal, and asked them to do simple tasks, like to count the number of times a photograph appeared in a newspaper. He didn't tell them was that he had rigged the magazine to have a few convenient conveniences, like a giant sentence telling them that there were 43 photographs, or large text about how if the reader pointed this out to the researchers they could get $250.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">Still, to a statistically significant degree, the &ldquo;lucky&rdquo; people noticed them more. What gives?</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">Have you ever missed an important detail because you were focused on something else? Once you're looking for something, you throw out details that fit. If you're looking for things that fit into the steps of a plan, then you're more likely to throw out stuff that's not already included.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">If you weren't trying to count the number of times a word appeared, you probably would have caught the sentences. Planning on counting makes you miss details like it already being done for you.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">Engineers bump into this sort of problem a lot. How many times have you spent a long time trying to code something before you found out that it was included in a library? Or tried to write a piece of code that doesn't actually wind up being used because what it accomplishes isn't actually needed?</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">This failure mode is child of lost causes and priming. If you try to do X in order to get G, then you're primed to miss Y even if it's relevant to G. If the bottom line is written, then you're going to leave stuff out.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">Insofar as Luck exists, it seems to be the ability to use unexpected but useful information.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">Not quite sure how to practice this skill. A few things I've tried:</p>\n<ul>\n<li>Meditation. There are a few mental operations that seem related to letting go of a set of predictions and looking at reality instead. Like, asking how it actually feels to be breathing rather than looking for what you think breathing is like.</li>\n</ul>\n<ul>\n<li>Feel &ldquo;open&rdquo; to the possibility of something popping up. I used to find four leaf clovers this way &ndash; by walking around occasionally glancing at the grass, and not particularly looking for a four leaf clover, but trusting my brain to draw my attention to it if there is one there and visible.</li>\n<li>\n<p style=\"margin-bottom: 0in\">You could probably actually walk around outside right now and try this. See what it feels like to notice details.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0in\">Walk around outside and see if any small animals catch your eye. Try to use the feeling of your attention being grabbed more often.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0in\">Look around the room you're in and see if there's anything that you've forgotten about, but is useful.</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0in\">When given a plan, ask yourself what kinds of functions or adjectives something could have that would make it useful, without drawing a picture of what that thing is.</p>\n<ul>\n<li>\n<p style=\"margin-bottom: 0in\">One time at a Rationality Minicamp someone asked if there was a sceptre-like object. Most people said no, and I said yes, even though I didn't know where it was. I was able to quickly find one in the room because I ran the algorithm of &ldquo;find the cylinder-ish thing in this room&rdquo; rather than the algorithm of asking myself what cylinder-like things there are, then go get it.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n</li>\n</ul>\n</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4i7ovu3wn3no7SRyq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 17, "extendedScore": null, "score": 4.1e-05, "legacy": true, "legacyId": "18119", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-07T12:13:25.300Z", "modifiedAt": null, "url": null, "title": "A question about Singularity Summit 2012", "slug": "a-question-about-singularity-summit-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:02.341Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "UngnsCobra", "createdAt": "2012-03-10T17:49:31.770Z", "isAdmin": false, "displayName": "UngnsCobra"}, "userId": "TTboizYjH4DXxsera", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3aDram6dKbEWp6JPy/a-question-about-singularity-summit-2012", "pageUrlRelative": "/posts/3aDram6dKbEWp6JPy/a-question-about-singularity-summit-2012", "linkUrl": "https://www.lesswrong.com/posts/3aDram6dKbEWp6JPy/a-question-about-singularity-summit-2012", "postedAtFormatted": "Tuesday, August 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20question%20about%20Singularity%20Summit%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20question%20about%20Singularity%20Summit%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3aDram6dKbEWp6JPy%2Fa-question-about-singularity-summit-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20question%20about%20Singularity%20Summit%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3aDram6dKbEWp6JPy%2Fa-question-about-singularity-summit-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3aDram6dKbEWp6JPy%2Fa-question-about-singularity-summit-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 68, "htmlBody": "<p>I am planning on going to the Singularity Summit this year, I applied for a student discount earlier on - approximately 3 weeks ago. &nbsp;Still haven't heard back. &nbsp;I am curious to hear if anyone else has applied for student discount and got a reply. &nbsp;I am studying in the UK, so really want to wrap my logistics issues up quickly! &nbsp;Hence, anyone else in the same boat?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3aDram6dKbEWp6JPy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": -2, "extendedScore": null, "score": 9.591299768444916e-07, "legacy": true, "legacyId": "18126", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-07T17:39:14.254Z", "modifiedAt": null, "url": null, "title": "Depth of Knowledge", "slug": "depth-of-knowledge", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:01.271Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "billswift", "createdAt": "2009-02-28T05:59:56.578Z", "isAdmin": false, "displayName": "billswift"}, "userId": "WBoeSNFkZ4q8nRenK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zrEwuHzN8ZyFLcejk/depth-of-knowledge", "pageUrlRelative": "/posts/zrEwuHzN8ZyFLcejk/depth-of-knowledge", "linkUrl": "https://www.lesswrong.com/posts/zrEwuHzN8ZyFLcejk/depth-of-knowledge", "postedAtFormatted": "Tuesday, August 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Depth%20of%20Knowledge&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADepth%20of%20Knowledge%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzrEwuHzN8ZyFLcejk%2Fdepth-of-knowledge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Depth%20of%20Knowledge%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzrEwuHzN8ZyFLcejk%2Fdepth-of-knowledge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzrEwuHzN8ZyFLcejk%2Fdepth-of-knowledge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 654, "htmlBody": "<p>Depth of Knowledge refers to how thoroughly you know and understand what you have been studying.&nbsp; You need to decide how deeply you need to go into a particular subject.<br /><br /><strong>Recognizing</strong> information seen before but not memorized.&nbsp; Information is familiar and you can relate parts when both are presented separately.&nbsp; Very useful for covering a lot of ground quickly and you may not need the information in the future.&nbsp; It really helps when you are trying to look up some particular piece of information by providing context.&nbsp; This is what is mostly tested by multiple-choice tests.&nbsp; Never mistake scoring well on most multiple-choice texts as real learning.<br /><br /><strong>Remembering</strong> information, memorizing lists &amp; facts - building up your internal data bank.&nbsp; Working from memory you can present key facts and relationships that you have studied.&nbsp; This is a necessary foundation for deeper learning.&nbsp; If you cannot remember details, then you cannot reason from them.&nbsp; Too often this step is skimped in modern schooling; rote memorization is often necessary for real learning to take place.&nbsp; The most important thing to remember here is that you must PAY ATTENTION to what you are trying to remember.&nbsp; Repeated exposure is necessary for memorization - but you also have to pay attention to the material.&nbsp; The better you pay attention, the less it will need to be repeated for real learning to take place.&nbsp; Having a theoretical framework for the material to fit will also help since we remember mostly by association, this helps recall that.&nbsp; Recall is best tested by fill-in-the-blank type tests and short essay-type questions.<br /><br />Because memorization is so time and effort intensive, don't waste your time memorizing things that are not important.&nbsp; First, you must remember the basics - arithmetic, spelling, vocabulary, and grammar.&nbsp; Next, you must remember things that you may need without time or references available when you need them, first aid and various other emergency skills.&nbsp; While learning, you need to memorize things that further learning will depend on, the basic facts and relationships in whatever field you are studying.&nbsp; Finally, when actually working in a field, you need to remember frequently used information that it would waste considerable time looking up repeatedly.&nbsp; These are also important to keep in mind, because to keep things memorized, you need to refresh the memory periodically.&nbsp; The first and last categories you will actually keep refreshed, if you are conscientious in their use, by using them.&nbsp; The hardest to keep up on is the second category, emergency skills.<br /><br />In the third level of knowledge, you <strong>understand</strong> the material and are able to explain things in your own words.&nbsp; You can draw new relationships between facts.&nbsp; You need to be able to remember details to explain things in your own words - for a long time I deluded myself that I could understand the material because I could follow along with the explanation easily enough as I read it.&nbsp; But <em>you do not truly understand something until you can explain it in your own words from memory</em>.<br /><br />Finally, you can <strong>use</strong> information in papers and discussions to articulate and defend what you know.&nbsp; You know the subject well enough to do independent, original research.&nbsp; Here you are learning in your subject primarily by working in the field rather than from others.</p>\n<p>&nbsp;</p>\n<p>The basic idea came from a book I read around 1980, <em>A Time to Learn</em>, by Phillip Bandt, Naomi Meara, &amp; Lyle Schmidt.&nbsp; I don't have any more information on it, because that is all I recorded at the time and Amazon lists multiple versions, and I have no idea which one it was that I read, since I could not find one where all three authors were listed.</p>\n<p>The note that paying attention is needed for memorization comes from Alan Baddeley's <em>Your Memory: A User's Guide</em>, and from Ellen Langer's <em>Mindfulness</em>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zrEwuHzN8ZyFLcejk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": -3, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "18127", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-07T22:15:46.402Z", "modifiedAt": null, "url": null, "title": "Transcript: \"Choice Machines, Causality, and Cooperation\"", "slug": "transcript-choice-machines-causality-and-cooperation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:02.933Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Randaly", "createdAt": "2010-04-20T23:31:03.738Z", "isAdmin": false, "displayName": "Randaly"}, "userId": "KdhDyNCDgA945WayD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/te2ZR82QzuRkbkDAF/transcript-choice-machines-causality-and-cooperation", "pageUrlRelative": "/posts/te2ZR82QzuRkbkDAF/transcript-choice-machines-causality-and-cooperation", "linkUrl": "https://www.lesswrong.com/posts/te2ZR82QzuRkbkDAF/transcript-choice-machines-causality-and-cooperation", "postedAtFormatted": "Tuesday, August 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Transcript%3A%20%22Choice%20Machines%2C%20Causality%2C%20and%20Cooperation%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATranscript%3A%20%22Choice%20Machines%2C%20Causality%2C%20and%20Cooperation%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fte2ZR82QzuRkbkDAF%2Ftranscript-choice-machines-causality-and-cooperation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Transcript%3A%20%22Choice%20Machines%2C%20Causality%2C%20and%20Cooperation%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fte2ZR82QzuRkbkDAF%2Ftranscript-choice-machines-causality-and-cooperation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fte2ZR82QzuRkbkDAF%2Ftranscript-choice-machines-causality-and-cooperation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4134, "htmlBody": "<p>Gary Drescher's presentation at the 2009 Singularity Summit, \"Choice Machines, Causality, and Cooperation,\" is online, at <a href=\"http://vimeo.com/7321259\">vimeo.</a> Drescher is the author of <a href=\"http://www.amazon.com/Good-Real-Demystifying-Paradoxes-Bradford/dp/0262042339\">Good and Real</a>, which has been recommended many times on LW. I've transcribed his talk, below.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p style=\"margin-bottom: 0in\">My talk this afternoon is about choice machines: machines such as ourselves that make choices in some reasonable sense of the word. The very notion of mechanical choice strikes many people as a contradiction in terms, and exploring that contradiction and its resolution is central to this talk. As a point of departure, I'll argue that even in a deterministic universe, there's room for choices to occur: we don't need to invoke some sort of free will that makes an exception to the determinism, no do we even need randomness, although a little randomness doesn't hurt. I'm going to argue that regardless of whether our universe is fully deterministic, it's at least deterministic enough that the compatibility of choice and full deterministic has some important ramifications that do apply to our universe. I'll argue that if we carry the compatibility of choice and determinism to its logical conclusions, we obtain some progressively weird corollaries: namely, that it sometimes makes sense to act for the sake of things that our actions cannot change and cannot cause, and that that might even suggest a way to derive an essentially ethical prescription: an explanation for why we sometimes help others even if doing so causes net harm to our own interests.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[1:15]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">An important caveat in all this, just to manage expectations a bit, is that the arguments I'll be presenting will be merely intuitive- or counter-intuitive, as the case may be- and not grounded in a precise and formal theory. Instead, I'm going to run some intuition pumps, as Daniel Dennett calls them, to try to persuade you what answers a successful theory would plausibly provide in a few key test cases.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[1:40]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">Perhaps the clearest way to illustrate the compatibility of choice and determinism is to construct or at least imagine a virtual world, which superficially resembles our own environment and which embodies intelligent or somewhat intelligent agents. As a computer program, this virtual world is quintessentially determinist: the program specifies the virtual world's initial conditions, and specifies how to calculate everything that happens next. So given the program itself, there are no degrees of freedom about what will happen in the virtual world. Things do change in the world from moment to moment, of course, but no event ever changes from what was determined at the outset. In effect, all events just sit, statically, in spacetime. Still, it makes sense for agents in the world to contemplate what would be the case were they to take some action or another, and it makes sense for them to select an action accordingly.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[2:35]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\"><img src=\"http://images.lesswrong.com/t3_dzk_1.png?v=86a0ca8b20d595b843079f2408dfb0e6\" alt=\"\" width=\"450\" height=\"305\" /></p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">For instance, an agent in the illustrated situation here might reason that, were it move to its right, which is our left, then the agent would obtain some tasty fruit. But, instead, if it moves to its left, it falls off a cliff. Accordingly, if its preferences scheme assigns positive utility to the fruit, and negative utility to falling off the cliff, that means the agent moves to its right and not to its left. And that process, I would submit, is what we more or less do ourselves when we engage in what we think of as making choices for the sake of our goals.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[3:08]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">The process, the computational process of selecting an action according to the desirability of what would be the case were the action taken, turns to be what our choice process consists of. So, from this perspective, choice is a particular kind of computation. The objection that choice isn't really occurring because the outcome was already determined is just as much a non-sequitur as suggesting that any other computation, for example, adding up a list of numbers, isn't really occurring just because the outcome was predetermined.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[3:41]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">So, the choice process takes place, and we consider that the agents has a choice about the action that the choice selects and has a choice about the associated outcomes, meaning that those outcomes occur as a consequence of the choice process. So, clearly an agent that executes a choice process and that correctly anticipates what would be the case if various contemplated actions were taken will better achieve its goals than one that, say, just acts at random or one that takes a fatalist stance, that there's no point in doing anything in particular since nothing can change from what it's already determined to be. So, if we were designing intelligent agents and wanted them to achieve their goals, we would design them to engage in a choice process. Or, if the virtual world were immense enough to support natural selection and the evolution of sufficiently intelligent creatures, then those evolved creatures could be expected to execute a choice process because of the benefits conferred.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[4:38]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">So the inalterability of everything that will ever happen does not imply the futility of acting for the sake of what is desired. The key to the choice relation is the &ldquo;would be-if&rdquo; relation, also known as the subjunctive or counterfactual relation. Counterfactual because it entertains a hypothetical antecedent about taking a certain action, that is possibly contrary to fact- as in the case of moving to the agent's left in this example. Even thought the moving left action does not in fact occur, the agent does usefully reason about what would the case if that action were taken, and indeed it's that very reasoning that ensures that the action does not in fact occur.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[5:21]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">There are various technical proposals for how to formally specific a &ldquo;would be-if&rdquo;relation- David Lewis has a classic formulation, Judea Pearl has a more recent one- but they're not necessarily the appropriate version of &ldquo;would be-if&rdquo; to use for purposes of making choices, for purposes of selecting an action based on the desirability of what would then be the case. And, although I won't be presenting a formal theory, the essence of this talk is to investigate some properties of &ldquo;would be-if,&rdquo; the counterfactual relation that's appropriate to use for making choices.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[5:57]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">In particular, I want to address next the possibility that, in a sufficiently deterministic universe, you have a choice about some things that your action cannot cause. Here's an example: assume or imagine that the universe is deterministic, with only one possible history following from any given state of the universe at a given moment. And let me define a predicate P that gets applied to the total state of the universe at some moment. The predicate P is defined to be true of a universe state just in case the laws of physics applied to that total state specify that a billion years after that state, my right hand is raised. Otherwise, the predicate P is false of that state.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[6:44]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">Now, suppose I decide, just on a whim, that I would like that state of the universe a billion years ago to have been such that the predicate P was true of that past state. I need only raise my right hand now, and, lo and behold, it was so. If, instead, I want the predicate to have been false, then I lower my hand and the predicate was false. Of course, I haven't changed what the past state of the universe is or was; the past is what it is, and can never be changed. There is merely a particular abstract relation, a &ldquo;would be-if&rdquo; relation, between my action and the particular past state that is the subject of my whimsical goal. I cannot reasonably take the action and not expect that the past state will be in correspondence.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[7:39]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">So, I can't change the past, nor does my action have any causal influence over the past- at least, not in the way we normally and usefully conceive of causality, where causes are temporally prior to effects, and where we can think of causal relations as essentially specifying how the universe computes its subsequent states from its previous states. Nonetheless, I have exactly as much choice about the past value of the predicate I have defined as I have, despite its inalterability, as I have about whether to raise my hand now, despite the inalterability of that too, in a deterministic universe. And if I were to believe otherwise, and were to refrain from raising my hand merely because I can't change the past even though I do have a whimsical preference about the past value of the specified predicate, then, as always with fatalist resignation, I'd be needlessly forfeiting an opportunity to have my goals fulfilled.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[8:41]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">If we accept the conclusion that we sometimes have a choice about what you cannot change or even cause, or at least tentatively accept it in order to explore its ramifications, then we can go on now to examine a well-known science fiction scenario called Newcomb's Problem. In Newcomb's Problem, a mischievous benefactor presents you with two boxes: there is a small, transparent box, containing a thousand dollars, which you can see; and there is a larger, opaque box, which you are truthfully told contains either a million dollars or nothing at all. You can't see which; the box is opaque, and you are not allowed to examine it. But you are truthfully assured that the box has been sealed, and that its contents will not change from whatever it already is.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[9:27]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">You are now offered a very odd choice: you can take either the opaque box alone, or take both boxes, and you get to keep the contents of whatever you take. That sure sounds like a no brainer:if we assume that maximizing your expected payoff in this particular encounter is the sole relevant goal, then regardless of what's in the opaque box, there's no benefit to foregoing the additional thousand dollars.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[9:51]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">But, before you choose, you are told how the benefactor decided how much money to put in the opaque box- and that brings us to the science fiction part of the scenario. What the benefactor did was take a very detailed local snapshot of the state of the universe a few minutes ago, and then run a faster-than-real time simulation to predict with high accuracy to predict with high accuracy whether you would take both boxes, or just the opaque box. A million dollars was put in the opaque box if and only if you were predicted to take only the opaque box.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[10:22]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">Admittedly the super-predictability here is a bit physically implausible, and goes beyond a mere stipulation of determinism. Still, at least it's not logically impossible- provided that the simulator can avoid having to simulate itself, and thus avoid a potential infinite regress. (The opaque box's opacity is important in that regard: it serves to insulate you from being effectively informed of the outcome of the simulation itself, so the simulation doesn't have to predict its own outcome in order to predict what you are going to have to do.) So, let's indulge the super-predictability assumption, and see what comes from it. Eventually, I'm going to argue that the real world is at least deterministic enough and predictable enough that some of the science-fiction conclusions do carry over to reality.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[11:12]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">So, you now face the following choice: if you take the opaque box alone, then you can expect with high reliability that the simulation predicted you would do so, and so you expect to find a million dollars in the opaque box. If, on the other hand, you take both boxes, then you should expect the simulation to have predicted that, and you expect to find nothing in the opaque box. If and only if you expect to take the opaque box alone, you expect to walk away with a million dollars. Of course, your choice does not cause the opaque box's content to be one way or the other; according to the stipulated rules, the box content already is what it is, and will not change from that regardless of what choice you make.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[11:49]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">But we can apply the lesson from the handraising example- the lesson that you sometimes have a choice about things your action does not change or cause- because you can reason about what would be the case if, perhaps contrary to fact, you were to take a particular hypothetical action. And, in fact, we can regard Newcomb's Problem as essentially harnessing the same past predicate consequence as in the handraising example- namely, if and only if you take just the opaque box, then the past state of the universe, at the time the predictor took the detailed snapshot was such that that state leads, by physical laws, to your taking just the opaque box. And, if and only if the past state was thus, the predictor would predict you taking the opaque box alone, and so a million dollars would be in the opaque box, making that the more lucrative choice. And it's certainly the case that people who would make the opaque box choice have a much higher expected gain from such encounters than those who take both boxes.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[12:47]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">Still, it's possible to maintain, as many people do, that taking both boxes is the rational choice, and that the situation is essentially rigged to punish you for your predicted rationality- much as if a written exam were perversely graded to give points only for wrong answers. From that perspective, taking both boxes is the rational choice, even if you are then left to lament your unfortunate rationality. But that perspective is, at the very least, highly suspect in a situation where, unlike the hapless exam-taker, you are informed of the rigging and can take it into account when choosing your action, as you can in Newcomb's Problem.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[13:31]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">And, by the way, it's possible to consider an even stranger variant of Newcomb's Problem, in which both boxes are transparent. In this version, the predictor runs a simulation that tentatively presumes that you'll see a million dollars in the larger box. You'll be presented with a million dollars in the box for real if and only if the simulation shows that you would then take the million dollar box alone. If, instead, the simulation predicts that you would take both boxes if you see a million dollars in the larger box, then the larger box is left empty when presented for real.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[14:12]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">So, let's suppose you're confronted with this scenario, and you do see a million dollars in the box when it's presented for real. Even though the million dollars is already there, and you see it, and it can't change, nonetheless I claim that you should still take the million dollar box alone. Because, if you were to take both boxes instead, contrary to what in fact must be the case in order for you to be in this situation in the first place, then, also contrary to what is in fact the case, the box would not contain a million dollars- even though in fact it does, and even though that can't change! The same two-part reasoning applies as before: if and only if you were to take just the larger box, then the state of the universe at the time the predictor takes a snapshot must have been such that you would take just that box if you were to see a million dollars in that box. If and only if the past state had been thus, the Predictor would have put a million dollars in the box.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[15:07]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">Now, the prescription here to take just the larger box is more shockingly counter-intuitive than I can hope to decisively argue for in a brief talk, but, do at least note that a person who agrees that it is rational to take just the one box here does fare better than a person who believes otherwise, who would never be presented with a million dollars in the first place. If we do, at least tentatively, accept some of this analysis, for the sake of argument to see what follows from it, then we can move on now to another toy scenario, which dispenses with the determinism and super-prediction assumptions and arguably has more direct real world applicability.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[15:42]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">That scenario is the famous prisoner's dilemma. The prisoner's dilemma is a two player game in which both players make their moves simultaneously and independently, with no communication until both moves have been made. A move consists of writing down either the word &ldquo;cooperate&rdquo; or &ldquo;defect.&rdquo; The payoff matrix is as shown:</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\"><img src=\"http://images.lesswrong.com/t3_dzk_0.png\" alt=\"\" width=\"613\" height=\"415\" /></p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">If both players choose cooperate, they both receive 99 dollars. If both defect, they both get 1 dollar. But if one player cooperates and the other defects, then the one who cooperates gets nothing, and the one who defects gets 100 dollars.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[16:25]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">Crucially, we stipulate that each player cares only about maximizing her own expected payoff, and that the payoff in this particular instance of the game is the only goal, with no affect on anything else, including any subsequent rounds of the game, that could further complicate the decision. Let's assume that both players are smart and knowledgeable enough to find the correct solution to this problem and to act accordingly. What I mean by the correct answer is the one that maximizes that player's expected payoff. Let's further assume that each player is aware of the other player's competence, and their knowledge of their own competence, and so on. So then, what is the right answer that they'll both find?</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[17:07]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">On the face of it, it would be nice if both players were to cooperate, and receive close to the maximum payoff. But if I'm one of the players, I might reason that y opponent's move is causally independent of mine: regardless of what I do, my opponent's move is either to cooperate or not. If my opponent cooperates, I receive a dollar more if I defect than if I cooperate- 100$ vs 99$. Likewise if my opponent defects: I get a dollar more if I defect than if I cooperate, in this case 1 dollar vs nothing. So, in either case, regardless of what move my opponent makes, my defected causes me to get one dollar more than my cooperating causes me to get, which seemingly makes defected the right choice. Defecting is indeed the choice that's endorsed by standard game theory. And of course my opponent can reason similarly.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[18:06]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">So, if we're both convinced that we only have a choice about what we can cause, then we're both rationally compelled to defect, leaving us both much poorer than if we both cooperated. So, here again, an exclusively causal view of what we have a choice about leads to us having to lament that our unfortunate rationality keeps a much better outcome out of our reach. But we can arrive at a better outcome if we keep in mind the lesson from Newcomb's problem or even the handraising example that it can make sense to act for the sake of what would be the case if you so acted, even if your action does not cause it to be the case. Even without the help of any super-predictors in this scenario, I can reason that if I, acting by stipulation as a correct solver of this problem, were to choose to cooperate, then that's what correct solvers of this problem do in such situations, and in particular that's what my opponent, as a correct solver of this problem, does too.</p>\n<p style=\"margin-bottom: 0in\">[19:05]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">Similarly, if I were to figure out that defecting is correct, that's what I can expect my opponent to do. This is similar to my ability to predict what your answer to adding a given pair of numbers would be: I can merely add the numbers myself, and, given our mutual competence at addition, solve the problem. The universe is predictable enough that we routinely, and fairly accurately, make such predictions about one another. From this viewpoint, I can reason that, if I were to cooperate or not, then my opponent would make the corresponding choice- if indeed we are both correctly solving the same problem, my opponent maximizing his expected payoff just as I maximize mine. I therefore act for the sake of what my opponent's action would then be, even though I cannot causally influence my opponent to take one action or the other, since there is no communication between us. Accordingly, I cooperate, and so does my opponent, using similar reasoning, and we both do fairly well.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[20:05]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">One problem with the Prisoner's Dilemma is that the idealized degree of symmetry that's postulated between the two players may seldom occur in real life. But there are some important generalizations that may apply much more broadly. In particular, in many situations, the beneficiary of your cooperation may not be the same as the person whose cooperation benefits you. Instead, your decision whether to cooperate with one person may be symmetric to a different person's decision to cooperate with you. Again, even in the absence of any causal influence upon your potential benefactors, even if they will never learn of your cooperation with others, and even, moreover, if you already know of their cooperation with you before you make your own choice. That is analogous to the transparent version of Newcomb's Problem: there too, you act for the same of something that you already know is already obtained.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[21:04]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">Anyways, as many authors have noted with regards to the Prisoner's Dilemma, this is beginning to sound a little like the Golden Rule or the Categorical Imperative: act towards others as you would like others to act towards you, in similar situations. The analysis in terms of counterfactual reasoning provides a rationale, under some circumstances, for taking an action that causes net harm to your own interests and net benefit to others' interests although the choice is still ultimately grounded in your own goals because of what would be the case because of others' isomorphic behavior if you yourself were to cooperate or not. Having a derivable rationale for ethical or benevolent behaviour would be desirable for all sorts of reasons, not least of which is to help us make the momentous decisions as to how or even whether to engineer the Singularity, and also to tell us what sort of value system we might want- or expect- an AI to have.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[22:08]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">But a key assumption of the argument just given is that it requires all participants to be perfectly rational, and, further, to be aware of all others' rationality- Douglas Hofstadter refers to this as the &ldquo;<a href=\"http://www.gwern.net/docs/1985-hofstadter#dilemmas-for-superrational-thinkers-leading-up-to-a-luring-lottery\">superrationality</a>&rdquo; assumption. It would be nice to be able to show that, even among those of us with more limited rationality, there's still enough of a &ldquo;would be-if&rdquo; relation, albeit perhaps quantitatively weakened, between my own choice and others' choices in Prisoner's Dilemma situations to justify the cooperative solution in such cases. But I'm not aware of an entirely satisfactory treatment of that question, so it remains an open question as far as I know. Still, I think it's hopeful that we can at least get our foot in the door, by arguing for the correctness of the cooperative solution in some cases that presume idealized rationality.</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">[23:00]</p>\n<p style=\"margin-bottom: 0in\">&nbsp;</p>\n<p style=\"margin-bottom: 0in\">Summing up, the key points are that:</p>\n<ul>\n<li>\n<p style=\"margin-bottom: 0in\">Inalterability does not imply futility</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0in\">You have a choice about some things that your action cannot change or even cause</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0in\">One consequence is a derivable prescription to sometimes cooperate with others even when doing so causes net harm to your goals and net benefits to their goals</p>\n</li>\n<li>\n<p style=\"margin-bottom: 0in\">The same false intuition that makes all choice seem impossible or futile given determinism, also makes it seem futile to act for the sake of the million dollars in Newcomb's Problem, or for the sake of another player's cooperation in the Prisoner's Dilemma, since you cannot cause, or even change, what you act for the sake of</p>\n</li>\n</ul>\n<p>Making a fully convincing case for all of this would require a convincing theory of the \"would be-if\" relation, the counterfactual or subjunctive relation, consists of, which I have not presented. What this talk outlined instead is a glimpse of some consequences that such a theory would arguably have to lead to, some answers the theory would have to give in some key examples, if the theory can avoid putting us in the position of lamenting our own rationality.</p>\n<p>As for an underlying theory, my book <a href=\"http://www.amazon.com/Good-Real-Demystifying-Paradoxes-Bradford/dp/0262042339\">Good and Real</a> sketches what could be seen as a modified <a href=\"http://en.wikipedia.org/wiki/Evidential_decision_theory\">evidentialist theory</a>, for those familiar with that concept. But there is some exciting work being pursued now by Eliezer Yudkowsky and others at the Singularity Institute and elsewhere that may be converging on <a href=\"http://intelligence.org/files/TDT1.pdf\">a much more rigorous and elegant underlying theory</a>, and hopefully we'll be hearing more about that in the not-too-distance future.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "te2ZR82QzuRkbkDAF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 16, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "18128", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-08T00:37:03.789Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Social Meetup", "slug": "meetup-washington-dc-social-meetup-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:02.851Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zvmCzpwbXWi9GkCk4/meetup-washington-dc-social-meetup-1", "pageUrlRelative": "/posts/zvmCzpwbXWi9GkCk4/meetup-washington-dc-social-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/zvmCzpwbXWi9GkCk4/meetup-washington-dc-social-meetup-1", "postedAtFormatted": "Wednesday, August 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Social%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Social%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzvmCzpwbXWi9GkCk4%2Fmeetup-washington-dc-social-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Social%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzvmCzpwbXWi9GkCk4%2Fmeetup-washington-dc-social-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzvmCzpwbXWi9GkCk4%2Fmeetup-washington-dc-social-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/cn'>Washington DC Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 August 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Washington DC, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This meetup is the first of a new schedule of meetup, which will alternate \"serious\", discussion oriented meetups, with \"social\", more activity based meetups.</p>\n\n<p>This meetup will be a social meetup; we will be playing the game of thrones board game.</p>\n\n<p>Meetup will be at a private residence, please PM or email me for the address.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/cn'>Washington DC Social Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zvmCzpwbXWi9GkCk4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "18129", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Social_Meetup\">Discussion article for the meetup : <a href=\"/meetups/cn\">Washington DC Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 August 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Washington DC, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This meetup is the first of a new schedule of meetup, which will alternate \"serious\", discussion oriented meetups, with \"social\", more activity based meetups.</p>\n\n<p>This meetup will be a social meetup; we will be playing the game of thrones board game.</p>\n\n<p>Meetup will be at a private residence, please PM or email me for the address.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Social_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/cn\">Washington DC Social Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Social Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Social_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Social Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Social_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-08T00:37:04.917Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Social Meetup", "slug": "meetup-washington-dc-social-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HYQr5radNBgFat9DL/meetup-washington-dc-social-meetup", "pageUrlRelative": "/posts/HYQr5radNBgFat9DL/meetup-washington-dc-social-meetup", "linkUrl": "https://www.lesswrong.com/posts/HYQr5radNBgFat9DL/meetup-washington-dc-social-meetup", "postedAtFormatted": "Wednesday, August 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Social%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Social%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHYQr5radNBgFat9DL%2Fmeetup-washington-dc-social-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Social%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHYQr5radNBgFat9DL%2Fmeetup-washington-dc-social-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHYQr5radNBgFat9DL%2Fmeetup-washington-dc-social-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/co'>Washington DC Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 August 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Washington DC, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This meetup is the first of a new schedule of meetup, which will alternate \"serious\", discussion oriented meetups, with \"social\", more activity based meetups.</p>\n\n<p>This meetup will be a social meetup; we will be playing the game of thrones board game.</p>\n\n<p>Meetup will be at a private residence, please PM or email me for the address.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/co'>Washington DC Social Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HYQr5radNBgFat9DL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 9.5949333051137e-07, "legacy": true, "legacyId": "18130", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Social_Meetup\">Discussion article for the meetup : <a href=\"/meetups/co\">Washington DC Social Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 August 2012 03:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Washington DC, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This meetup is the first of a new schedule of meetup, which will alternate \"serious\", discussion oriented meetups, with \"social\", more activity based meetups.</p>\n\n<p>This meetup will be a social meetup; we will be playing the game of thrones board game.</p>\n\n<p>Meetup will be at a private residence, please PM or email me for the address.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Social_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/co\">Washington DC Social Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Social Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Social_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Social Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Social_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-08T02:29:02.541Z", "modifiedAt": null, "url": null, "title": "The High Impact Network (THINK) - Launching Now", "slug": "the-high-impact-network-think-launching-now", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:30.858Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KEcWJSzxFzcYvyLsW/the-high-impact-network-think-launching-now", "pageUrlRelative": "/posts/KEcWJSzxFzcYvyLsW/the-high-impact-network-think-launching-now", "linkUrl": "https://www.lesswrong.com/posts/KEcWJSzxFzcYvyLsW/the-high-impact-network-think-launching-now", "postedAtFormatted": "Wednesday, August 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20High%20Impact%20Network%20(THINK)%20-%20Launching%20Now&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20High%20Impact%20Network%20(THINK)%20-%20Launching%20Now%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKEcWJSzxFzcYvyLsW%2Fthe-high-impact-network-think-launching-now%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20High%20Impact%20Network%20(THINK)%20-%20Launching%20Now%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKEcWJSzxFzcYvyLsW%2Fthe-high-impact-network-think-launching-now", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKEcWJSzxFzcYvyLsW%2Fthe-high-impact-network-think-launching-now", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 973, "htmlBody": "<p><a href=\"http://www.thehighimpactnetwork.org\"><img style=\"vertical-align: middle;\" src=\"http://humanistculture.com/splash_header.jpg\" alt=\"\" width=\"720\" height=\"277\" /></a></p>\n<p>&nbsp;</p>\n<p><a href=\"http://thehighimpactnetwork.org\">THINK,&nbsp;The High Impact Network</a>, is&nbsp;going live this week.</p>\n<p>We're a network of Effective Altruists (EAs), looking to do the most good for the most people<sup>1</sup>&nbsp;as efficiently as possible. We aren't bound by a central cause or ethical framework, but rather by a process, and a commitment to rigor and rationality as we try to make the world a better place.</p>\n<p>THINK meetups are forming around the world. Some are functioning as student groups at prominent universities, others are general meetups for people of all ages who want to make effective altruism a part of their life. As I write this, 20 meetups are getting ready to launch in the fall, and discussions are underway for an additional 30. If you'd like to connect with other EA-types, see if a meetup's forming in your area, or run your own meetup, send us an e-mail <a href=\"mailto:meetups@thehighimpactnetwork.org\">here</a>, or visit <a href=\"http://www.thehighimpactnetwork.org/\">our website</a>.</p>\n<p>We're putting together a collection of meetup modules, which newly formed groups can use for content at weekly meetups. These fall into roughly two categories:</p>\n<ul>\n<li>Introductory materials, designed to teach the basics of Effective Altruism to newcomers.</li>\n<li>Self Improvement tools, helping newcomers and veterans to become strong enough to tackle the difficult problems ahead.</li>\n</ul>\n<p><a href=\"http://www.thehighimpactnetwork.org/ideas\">Five sample modules are available on our website</a>, and more are coming. If you have ideas for a module and would like to create you own, e-mail us at <a href=\"mailto:modules@thehighimpactnetwork.org\">modules@thehighimpactnetwork.org</a>.</p>\n<p>But most importantly - we want bright, enthusiastic people who care deeply about the world to collaborate with each other on high impact projects.&nbsp;</p>\n<h2><br /></h2>\n<h2>Optimal Philanthropy. Effective Altruism.</h2>\n<p>&nbsp;</p>\n<p>Less Wrong veterans will recognize the basics of <a href=\"/lw/3gj/efficient_charity_do_unto_others/\">Optimal Philanthropy</a>, although we consider avenues beyond traditional charity. (The phrase \"effective altruism\" was settled on after much deliberation). For those unfamiliar, a brief overview.</p>\n<p>Over the past decade, important changes have begun to take root in the philanthropy/altruist sector:</p>\n<ul>\n<li>Organizations like <a href=\"http://www.givewell.org/\">Givewell</a>, as well as a growing number of foundations like the Gates Foundation,&nbsp;are shifting the discussion of giving towards efficiency and evidence.</li>\n<li>Groups like <a href=\"http://www.givingwhatwecan.org/\">Giving What We Can</a> and <a href=\"http://boldergiving.org/index.php\">Bolder Giving</a> are encouraging people to incorporate philanthropy into their lifestyle. You can donate 10% or more of your income and still be among the richest people on the planet, living a satisfying life.</li>\n<li>The organization <a href=\"http://www.80000hours.org\">80,000 Hours</a> is promoting high impact career choice. You'll spent thousands of hours at your job. You can accomplish dramatically more good for the world if you optimize for it.</li>\n</ul>\n<p>Above all, serious discussion is slowly mounting towards an incredibly important question - if you want to have the biggest impact you possibly can, what do you do?</p>\n<p>Donating to provably efficient charities is an obvious first step, but more is possible. Systemic changes can have a powerful impact. New technologies have the potential to radically improve lives - as well as the capacity to destroy life as we know it. The Singularity Institute, the Future of Humanity Institute, Givewell and others are all in the process of grappling with this problem. I think it's fair to say that the Less Wrong community has had a noteworthy impact on the discussion.</p>\n<div>We believe it's important that more people consider this question, and work on both the meta-tasks of comparing potential high impact causes, as well as the object-level tasks that follow.&nbsp;</div>\n<div><br /></div>\n<div>\n<h2>A New Kind of Community</h2>\n<h2><br /></h2>\n</div>\n<div>These ideas have been spreading. The seeds have been sown for a new kind of movement, which we believe has the potential to change the world on a scale rarely seen - at least not in a deliberate fashion. The Effective Altruism movement is growing slowly, but we think it's time for it to explode into something powerful and good.</div>\n<div>In many ways this is not unlike the existing Less Wrong community. The NYC Less Wrong meetup has had a profound impact on me, personally. I've learned to explore important new ideas, think rigorously. I've learned the value of having likeminded people to share both important problems and my day to day experiences with. Most importantly, I've developed a sense of agency - I've realized I can personally cause big things to happen.</div>\n<div>Less Wrong is about general rationality, which people can apply to numerous areas. There's tremendous value to having that, without attaching it to any cause or even meta-cause. But there's room for more than one community (truth be told I think everyone should have at least two tribes that don't fully intersect). There's an Eliezer quote I've been thinking about lately:<br /></div>\n<div style=\"padding-left: 30px;\"><em>\"<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">Should the Earth last so long, I would like to see, as the form of rationalist communities, taskforces focused on all the work that needs doing to fix up the world.\"</span></em></div>\n<p>Among the most valuable things the Less Wrong community has taught is the importance of... well, community. For Effective Altruism to be successful as a movement and a lifestyle, it needs people working together who share a passion for it, a commitment to intellectual rigor, and a sense of humor. People who can help each other grow, collaborate on important projects, and more.</p>\n<p>&nbsp;</p>\n<h2>THINK. The High Impact Network. Ready to launch this fall.</h2>\n<h2><br /></h2>\n<div>After just two months of work, we have approximately 30 volunteers and 6 directors, putting an average of 170 hours per week into THINK. Twenty meetups are gearing up to launch, with discussions going to set up another thirty. Our <a href=\"https://www.facebook.com/groups/449641588398523/\">English-speaking Facebook group</a> has 103 members as I write this, and in just a week the <a href=\"https://www.facebook.com/groups/157891774348009/\">Swedish-speaking group based in Stockholm</a>&nbsp;went from 3 to 57 members.</div>\n<div>This is just the beginning. We're ready to start tackling the world's biggest problems, and we hope you are too.</div>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p><sup>1</sup>&nbsp;<sub>Where by \"help 'people'\" we mean \"and animals too.\" Depending on your ethical framework. Probably not including clams. Quite possibly including future sentient beings of various sorts. It's complicated. Come to a meetup, we'll talk about it.</sub></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jZF2jwLnPKBv6m3Ag": 1, "qAvbtzdG2A2RBn7in": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KEcWJSzxFzcYvyLsW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 46, "extendedScore": null, "score": 0.000102, "legacy": true, "legacyId": "18091", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 46, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"http://www.thehighimpactnetwork.org\"><img style=\"vertical-align: middle;\" src=\"http://humanistculture.com/splash_header.jpg\" alt=\"\" width=\"720\" height=\"277\"></a></p>\n<p>&nbsp;</p>\n<p><a href=\"http://thehighimpactnetwork.org\">THINK,&nbsp;The High Impact Network</a>, is&nbsp;going live this week.</p>\n<p>We're a network of Effective Altruists (EAs), looking to do the most good for the most people<sup>1</sup>&nbsp;as efficiently as possible. We aren't bound by a central cause or ethical framework, but rather by a process, and a commitment to rigor and rationality as we try to make the world a better place.</p>\n<p>THINK meetups are forming around the world. Some are functioning as student groups at prominent universities, others are general meetups for people of all ages who want to make effective altruism a part of their life. As I write this, 20 meetups are getting ready to launch in the fall, and discussions are underway for an additional 30. If you'd like to connect with other EA-types, see if a meetup's forming in your area, or run your own meetup, send us an e-mail <a href=\"mailto:meetups@thehighimpactnetwork.org\">here</a>, or visit <a href=\"http://www.thehighimpactnetwork.org/\">our website</a>.</p>\n<p>We're putting together a collection of meetup modules, which newly formed groups can use for content at weekly meetups. These fall into roughly two categories:</p>\n<ul>\n<li>Introductory materials, designed to teach the basics of Effective Altruism to newcomers.</li>\n<li>Self Improvement tools, helping newcomers and veterans to become strong enough to tackle the difficult problems ahead.</li>\n</ul>\n<p><a href=\"http://www.thehighimpactnetwork.org/ideas\">Five sample modules are available on our website</a>, and more are coming. If you have ideas for a module and would like to create you own, e-mail us at <a href=\"mailto:modules@thehighimpactnetwork.org\">modules@thehighimpactnetwork.org</a>.</p>\n<p>But most importantly - we want bright, enthusiastic people who care deeply about the world to collaborate with each other on high impact projects.&nbsp;</p>\n<h2><br></h2>\n<h2 id=\"Optimal_Philanthropy__Effective_Altruism_\">Optimal Philanthropy. Effective Altruism.</h2>\n<p>&nbsp;</p>\n<p>Less Wrong veterans will recognize the basics of <a href=\"/lw/3gj/efficient_charity_do_unto_others/\">Optimal Philanthropy</a>, although we consider avenues beyond traditional charity. (The phrase \"effective altruism\" was settled on after much deliberation). For those unfamiliar, a brief overview.</p>\n<p>Over the past decade, important changes have begun to take root in the philanthropy/altruist sector:</p>\n<ul>\n<li>Organizations like <a href=\"http://www.givewell.org/\">Givewell</a>, as well as a growing number of foundations like the Gates Foundation,&nbsp;are shifting the discussion of giving towards efficiency and evidence.</li>\n<li>Groups like <a href=\"http://www.givingwhatwecan.org/\">Giving What We Can</a> and <a href=\"http://boldergiving.org/index.php\">Bolder Giving</a> are encouraging people to incorporate philanthropy into their lifestyle. You can donate 10% or more of your income and still be among the richest people on the planet, living a satisfying life.</li>\n<li>The organization <a href=\"http://www.80000hours.org\">80,000 Hours</a> is promoting high impact career choice. You'll spent thousands of hours at your job. You can accomplish dramatically more good for the world if you optimize for it.</li>\n</ul>\n<p>Above all, serious discussion is slowly mounting towards an incredibly important question - if you want to have the biggest impact you possibly can, what do you do?</p>\n<p>Donating to provably efficient charities is an obvious first step, but more is possible. Systemic changes can have a powerful impact. New technologies have the potential to radically improve lives - as well as the capacity to destroy life as we know it. The Singularity Institute, the Future of Humanity Institute, Givewell and others are all in the process of grappling with this problem. I think it's fair to say that the Less Wrong community has had a noteworthy impact on the discussion.</p>\n<div>We believe it's important that more people consider this question, and work on both the meta-tasks of comparing potential high impact causes, as well as the object-level tasks that follow.&nbsp;</div>\n<div><br></div>\n<div>\n<h2 id=\"A_New_Kind_of_Community\">A New Kind of Community</h2>\n<h2><br></h2>\n</div>\n<div>These ideas have been spreading. The seeds have been sown for a new kind of movement, which we believe has the potential to change the world on a scale rarely seen - at least not in a deliberate fashion. The Effective Altruism movement is growing slowly, but we think it's time for it to explode into something powerful and good.</div>\n<div>In many ways this is not unlike the existing Less Wrong community. The NYC Less Wrong meetup has had a profound impact on me, personally. I've learned to explore important new ideas, think rigorously. I've learned the value of having likeminded people to share both important problems and my day to day experiences with. Most importantly, I've developed a sense of agency - I've realized I can personally cause big things to happen.</div>\n<div>Less Wrong is about general rationality, which people can apply to numerous areas. There's tremendous value to having that, without attaching it to any cause or even meta-cause. But there's room for more than one community (truth be told I think everyone should have at least two tribes that don't fully intersect). There's an Eliezer quote I've been thinking about lately:<br></div>\n<div style=\"padding-left: 30px;\"><em>\"<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">Should the Earth last so long, I would like to see, as the form of rationalist communities, taskforces focused on all the work that needs doing to fix up the world.\"</span></em></div>\n<p>Among the most valuable things the Less Wrong community has taught is the importance of... well, community. For Effective Altruism to be successful as a movement and a lifestyle, it needs people working together who share a passion for it, a commitment to intellectual rigor, and a sense of humor. People who can help each other grow, collaborate on important projects, and more.</p>\n<p>&nbsp;</p>\n<h2 id=\"THINK__The_High_Impact_Network__Ready_to_launch_this_fall_\">THINK. The High Impact Network. Ready to launch this fall.</h2>\n<h2><br></h2>\n<div>After just two months of work, we have approximately 30 volunteers and 6 directors, putting an average of 170 hours per week into THINK. Twenty meetups are gearing up to launch, with discussions going to set up another thirty. Our <a href=\"https://www.facebook.com/groups/449641588398523/\">English-speaking Facebook group</a> has 103 members as I write this, and in just a week the <a href=\"https://www.facebook.com/groups/157891774348009/\">Swedish-speaking group based in Stockholm</a>&nbsp;went from 3 to 57 members.</div>\n<div>This is just the beginning. We're ready to start tackling the world's biggest problems, and we hope you are too.</div>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<p><sup>1</sup>&nbsp;<sub>Where by \"help 'people'\" we mean \"and animals too.\" Depending on your ethical framework. Probably not including clams. Quite possibly including future sentient beings of various sorts. It's complicated. Come to a meetup, we'll talk about it.</sub></p>\n<p>&nbsp;</p>", "sections": [{"title": "Optimal Philanthropy. Effective Altruism.", "anchor": "Optimal_Philanthropy__Effective_Altruism_", "level": 1}, {"title": "A New Kind of Community", "anchor": "A_New_Kind_of_Community", "level": 1}, {"title": "THINK. The High Impact Network. Ready to launch this fall.", "anchor": "THINK__The_High_Impact_Network__Ready_to_launch_this_fall_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "17 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pC47ZTsPNAkjavkXs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-08T04:27:28.227Z", "modifiedAt": null, "url": null, "title": "Natural Laws Are Descriptions, not Rules", "slug": "natural-laws-are-descriptions-not-rules", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:38.254Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pragmatist", "createdAt": "2011-08-26T17:36:14.792Z", "isAdmin": false, "displayName": "pragmatist"}, "userId": "gs25cnPDLYqK8H68Q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/34Q5PJEaDCriijkc7/natural-laws-are-descriptions-not-rules", "pageUrlRelative": "/posts/34Q5PJEaDCriijkc7/natural-laws-are-descriptions-not-rules", "linkUrl": "https://www.lesswrong.com/posts/34Q5PJEaDCriijkc7/natural-laws-are-descriptions-not-rules", "postedAtFormatted": "Wednesday, August 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Natural%20Laws%20Are%20Descriptions%2C%20not%20Rules&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANatural%20Laws%20Are%20Descriptions%2C%20not%20Rules%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F34Q5PJEaDCriijkc7%2Fnatural-laws-are-descriptions-not-rules%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Natural%20Laws%20Are%20Descriptions%2C%20not%20Rules%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F34Q5PJEaDCriijkc7%2Fnatural-laws-are-descriptions-not-rules", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F34Q5PJEaDCriijkc7%2Fnatural-laws-are-descriptions-not-rules", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2584, "htmlBody": "<h2>Laws as Rules<br /></h2>\n<p>We speak casually of the laws of nature <em>determining</em> the distribution of matter and energy, or <em>governing</em> the behavior of physical objects. Implicit in this rhetoric is a metaphysical picture: the laws are <em>rules</em> that constrain the temporal evolution of stuff in the universe. In some important sense, the laws are prior to the distribution of stuff. The physicist Paul Davies <a href=\"http://www.npr.org/books/titles/138310675/cosmic-jackpot-why-our-universe-is-just-right-for-life#excerpt\">expresses</a> this idea with a bit more flair: \"[W]e have this image of really existing laws of physics ensconced in a transcendent aerie, lording it over lowly matter.\" The origins of this conception can be traced back to the beginnings of the scientific revolution, when <a href=\"http://plato.stanford.edu/entries/descartes-physics/#4\">Descartes</a> and <a href=\"http://en.wikipedia.org/wiki/Isaac_Newton%27s_religious_views#God_as_masterful_creator\">Newton</a> established the discovery of laws as the central aim of physical inquiry. In a scientific culture immersed in theism, it was unproblematic, even natural, to think of physical laws as rules. They are rules laid down by God that drive the development of the universe in accord with His divine plan.</p>\n<p>Does this prescriptive conception of law make sense in a secular context? Perhaps if we replace the divine creator of traditional religion with a more naturalist-friendly lawgiver, such as an ur-simulator. But what if there is no intentional agent at the root of it all? Ordinarily, when I think of a physical system as constrained by some rule, it is not the rule itself doing the constraining. The rule is just a piece of language; it is an <em>expression</em> of a constraint that is actually enforced by interaction with some other physical system -- a programmer, say, or a physical barrier, or a police force. In the sort of picture Davies presents, however, it is the rules themselves that enforce the constraint. The laws lord it over lowly matter. So on this view, the fact that all electrons repel one another is explained by the existence of some external entity, not an ordinary physical entity but a law of nature, that somehow <em>forces</em> electrons to repel one another, and this isn't just short-hand for God or the simulator forcing the behavior.</p>\n<p>I put it to you that this account of natural law is utterly mysterious and borders on the nonsensical. How exactly are abstract, non-physical objects -- laws of nature, living in their \"transcendent aerie\" -- supposed to interact with physical stuff? What is the mechanism by which the constraint is applied? Could the laws of nature have been different, so that they forced electrons to attract one another? The view should also be anathema to any self-respecting empiricist, since the laws appear to be idle danglers in the metaphysical theory. What is the difference between a universe where all electrons, as a matter of contingent fact, attract one another, and a universe where they attract one another because they are compelled to do so by the really existing laws of physics? Is there any test that could distinguish between these states of affairs?</p>\n<p><a id=\"more\"></a></p>\n<h2>Laws as Descriptions<br /></h2>\n<p><a href=\"http://sas-space.sas.ac.uk/963/1/N_Cartwright_God.pdf\">There are those</a> who take the incoherence of the secular prescriptive conception of laws as reason to reject the whole concept of laws of nature as an anachronistic holdover from a benighted theistic age. I don't think the situation is that dire. Discovering laws of nature is a hugely important activity in physics. It turns out that the behavior of large classes of objects can be given a unified compact mathematical description, and this is crucial to our ability to exercise predictive control over our environment. The significant word in the last sentence is \"description\". A much more congenial alternative to the prescriptive view is available. Instead of thinking of laws as rules that have an existence above and beyond the objects they govern, think of them as particularly concise and powerful descriptions of regular behavior.</p>\n<p>On this descriptive conception of laws, the laws do not exist independently in some transcendent realm. They are not prior to the distribution of matter and energy. The laws are just descriptions of salient patterns in that distribution. Of course, if this is correct, then our talk of the laws governing matter must be understood as metaphorical, but this is a small price to pay for a view that actually makes sense. There may be a concern that we are losing some important explanatory ground here. After all, on the prescriptive view the laws of nature <em>explain</em> why all electrons attract one another, whereas on the descriptive view the laws just restate the fact that all electrons attract one another. But consider the following dialogue:</p>\n<p><em>A</em>: Why are these two metal blocks repelling each other?</p>\n<p><em>B</em>: Because they're both negatively charged, which means they have an excess of electrons, and electrons repel one another.</p>\n<p><em>A</em>: But why do electrons repel one another?</p>\n<p><em>B</em>: Because like charges always repel.</p>\n<p><em>A</em>: But why is that?</p>\n<p><em>B</em>: Because if you do the path integral for the electromagnetic field (using Maxwell's Lagrangian) with source terms corresponding to two spatially separated lumps of identical charge density, you will find that the potential energy of the field is greater the smaller the spatial separation between the lumps, and we know the force points in the opposite direction to the gradient of the potential energy.</p>\n<p><em>A</em>: But why are the dynamics of the electromagnetic field derived from Maxwell's Lagrangian rather than some other equation? And why does the path integral method work at all?</p>\n<p><em>B</em>: BECAUSE IT IS THE LAW.</p>\n<p>Is the last link in this chain doing any explanatory work at all? Does it give us any further traction on the problem? <em>B</em> might as well have ended that conversation by saying \"Well, that's just the way things are.\" Now, laws of nature do have a privileged role in physical explanation, but that privilege is due to their simplicity and generality, not to some mysterious quasi-causal power they exert over matter. The fact that a certain generalization is a law of nature does not <em>account</em> for the truth and explanatory power of the generalization, any more than the fact that a soldier has won the Medal of Honor accounts for his or her courage in combat. Lawhood is a <em>recognition</em> of the generalization's truth and explanatory power. It is an honorific; it doesn't confer any further explanatory oomph.</p>\n<h2>The Best System Account of Laws</h2>\n<p><a href=\"http://en.wikipedia.org/wiki/David_Lewis_%28philosopher%29\">David Lewis</a> offers us a somewhat worked out version of the descriptive conception of law. Consider the set of all truths about the world expressible in a particular language. We can construct deductive systems out of this set of propositions by picking out some of the propositions as axioms. The logical consequences of these axioms are the theorems of the deductive system. These deductive systems compete with one another along (at least) two dimensions: the <em>simplicity </em>of the axioms, and the <em>strength</em> or <em>information content</em> of the system as a whole. We prefer systems that give us more information about the world, but this greater strength often comes at the cost of simplicity. For instance, a system whose axioms comprised the entire set of truths about the world would be maximally strong, but not simple at all. Conversely, a system whose only axiom is something like \"Stuff happens\" would be pretty simple, but very uninformative. What we are looking for is the appropriate balance of simplicity and strength [1].</p>\n<p>According to Lewis, the laws of nature correspond to the axioms of the deductive system that best balances simplicity and strength. He does not provide a precise algorithm for evaluating this balance, and I don't think his proposal should be read as an attempt at a technically precise decision procedure for lawhood anyway. It is more like a heuristic picture of what we are doing when we look for laws. We are looking for simple generalizations that can be used to deduce a large amount of information about the world. Laws are highly compressed descriptions of broad classes of phenomena. This view evidently differs quite substantially from the Davies picture I presented at the beginning of this post. On Lewis's view, the collection of particular facts about the world determines the laws of nature, since the laws are merely compact descriptions of those facts. On Davies's view, the determination runs the other way. The laws are independent entities that determine the particular facts about the world. Stuff in the world is arranged the way it is because the laws compelled that arrangement.</p>\n<p>One last point about Lewis's account. Lewis acknowledges that there is an important language dependence in his view of laws. If we ignore this, we get absurd results. For instance, consider a system whose only axiom is \"For all x, x is F\" where \"F\" is defined to be a predicate that applies to all and only events that occur in this world. This axiom is maximally informative, since it rules out all other possible worlds, and it seems exceedingly simple. Yet we wouldn't want to declare it a law of nature. The problem, obviously, is that all the complexity of the axiom is hidden by our choice of language, with this weird specially rigged predicate. To rule out this possibility, Lewis specifies that all candidate deductive systems must employ the vocabulary of fundamental physics.</p>\n<p>But we could also regard lawhood as a <a href=\"/lw/ro/2place_and_1place_words/\">2-place function</a> which maps a proposition and vocabulary pair to \"True\" if the proposition is an axiom of the best system in that vocabulary and \"False\" otherwise. Lewis has chosen to curry this function by fixing the vocabulary variable. Leaving the function uncurried, however, highlights that we could have different laws for different vocabularies and, consequently, for different levels of description. If I were an economist, I wouldn't be interested (at least not <em>qua </em>economist) in deductive systems that talked about quarks and leptons. I would be interested in deductive systems that talked about prices and demand. The best system for this coarser-grained<em> </em>vocabulary will give us the laws of economics, distinct from the laws of physics.</p>\n<h2>Lawhood Is in the Map, not in the Territory</h2>\n<p>There is another significant difference between the descriptive and prescriptive accounts that I have not yet discussed. On the Davies-style conception of laws as rules, lawhood is an element of reality. A law is a distinctive beast, an abstract entity perched in a transcendent aerie. On the descriptive account, by comparison, lawhood is part of our map, not the territory. Note that I am not saying that the <em>laws themselves</em> are a feature of the map and not the territory. Laws are just particularly salient redundancies, ones that permit us to construct useful compressed descriptions of reality. These redundancies are, of course, out there in the territory. However, the fact that certain regularities are especially useful for the organization of knowledge is at least partially dependent on facts about us, since we are the ones doing the organizing in pursuit of our particular practical projects. Nature does not flag these regularities as laws, we do.</p>\n<p>This realization has consequences for how we evaluate certain forms of reductionism. I should begin by noting that there is a type of reductionism I tentatively endorse and that I think is untouched by these speculations. I call this <em>mereological reductionism </em>[2]; it is the claim that all the stuff in the universe is entirely built out of the kinds of things described by fundamental physics. The vague statement is intentional, since fundamental physicists aren't yet sure what kinds of things they are describing, but the motivating idea behind the view is to rule out the existence of immaterial souls and the like. However, reductionists typically embrace a stronger form of reductionism that one might label <em>nomic reductionism</em> [3]. The view is that the fundamental laws of physics are the only <em>really existant</em> laws, and that laws in the non-fundamental disciplines are merely convenient short-cuts that we must employ due to our computational limitations.</p>\n<p>One appealing argument for this form of reductionism is the apparent superfluity of non-fundamental laws. Macroscopic systems are entirely built out of parts whose behavior is determined by the laws of physics. It follows that the behavior of these systems is also fixed by those fundamental laws. Additional non-fundamental laws are otiose; there is nothing left for them to do. Barry Loewer <a href=\"http://rci.rutgers.edu/~loewer/papers/Why_is_there_Anything_Except_Physics.pdf\">puts it like this</a>: \"Why would God make [non-fundamental laws] the day after he made physics when the world would go on exactly as if they were there without them?\" If these laws play no explanatory role, Ockham's razor demands that we strike them from our ontological catalog, leaving only the fundamental laws.</p>\n<p>I trust it is apparent that this argument relies on the prescriptive conception of laws. It assumes that real laws of nature <em>do stuff</em>; they push and pull matter and energy around. It is this implicit assumption that raises the overdetermination concern. On this assumption, if the fundamental laws of physics are already lording it over all matter, then there is no room for another locus of authority. However, the argument (and much of the appeal of the associated reductionist viewpoint) fizzles, if we regard laws as descriptive. Employing a Lewisian account, all we have are different best systems, geared towards vocabularies at different resolutions, that highlight different regularities as the basis for a compressed description of a system. There is nothing problematic with having different ways to compress information about a system. Specifically, we are not compelled by worries about overdetermination to declare one of these methods of compression to be <em>more real</em> than another. In response to Loewer's theological question, the proponent of the descriptive conception could say that God does not get to separately specify the non-fundamental and fundamental laws. By creating the pattern of events in space-time she implicitly fixes them all.</p>\n<p>Nomic reductionism would have us believe that the lawhood of the laws of physics is part of the territory, while the lawhood of the laws of psychology is just part of our map. Once we embrace the descriptive conception of laws, however, there is no longer this sharp ontological divide between the fundamental and non-fundamental laws. One reason for privileging the laws of physics is revealed to be the product of a confused metaphysical picture. However, one might think there are still other good reasons for privileging these laws that entail a reductionism more robust than the mereological variety. For instance, even if we accept that laws of physics don't possess a different ontological status, we can still believe that they have a prized position in the explanatory hierarchy. This leads to <em>explanatory reductionism</em>, the view that explanations couched in the vocabulary of fundamental physics are always better because fundamental physics provides us with more accurate models than the non-fundamental sciences. Also, even if one denies that the laws of physics themselves are pushing matter around, one can still believe that all the actual pushing and pulling there is, all the causal action, is described by the laws of physics, and that the non-fundamental laws do not describe genuine causal relations. We could call this kind of view <em>causal reductionism</em>.</p>\n<p>Unfortunately for the reductionist, explanatory and causal reductionism don't fare much better than nomic reductionism. Stay tuned for the reasons why!</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>[1] Lewis actually adds a third desideratum, <em>fit</em>, that allows for the evaluation of systems with probabilistic axioms, but I leave this out for simplicity of exposition. I have tweaked Lewis's presentation in a couple of other ways as well. For his own initial presentation of the view, see <a href=\"http://www.amazon.com/Counterfactuals-David-K-Lewis/dp/0631224254/ref=sr_1_1?ie=UTF8&amp;qid=1340138895&amp;sr=8-1&amp;keywords=lewis+counterfactuals\"><em>Counterfactuals</em></a>, pp. 72-77. For a more up-to-date presentation, dealing especially with issues involving probabilistic laws, see <a href=\"http://tedsider.org/teaching/properties/Lewis%20-%20Humean%20supervenience%20debugged.pdf\">this paper</a> (PDF).</p>\n<p>[2] From the Greek <em>meros</em>, meaning \"part\".</p>\n<p>[3] From the Greek <em>nomos</em>, meaning \"law\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xgpBASEThXPuKRhbS": 1, "GLykb6NukBeBQtDvQ": 1, "ZpG9rheyAkgCoEQea": 1, "3uE2pXvbcnS9nnZRE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "34Q5PJEaDCriijkc7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 39, "extendedScore": null, "score": 9.59605438544781e-07, "legacy": true, "legacyId": "16599", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Laws_as_Rules\">Laws as Rules<br></h2>\n<p>We speak casually of the laws of nature <em>determining</em> the distribution of matter and energy, or <em>governing</em> the behavior of physical objects. Implicit in this rhetoric is a metaphysical picture: the laws are <em>rules</em> that constrain the temporal evolution of stuff in the universe. In some important sense, the laws are prior to the distribution of stuff. The physicist Paul Davies <a href=\"http://www.npr.org/books/titles/138310675/cosmic-jackpot-why-our-universe-is-just-right-for-life#excerpt\">expresses</a> this idea with a bit more flair: \"[W]e have this image of really existing laws of physics ensconced in a transcendent aerie, lording it over lowly matter.\" The origins of this conception can be traced back to the beginnings of the scientific revolution, when <a href=\"http://plato.stanford.edu/entries/descartes-physics/#4\">Descartes</a> and <a href=\"http://en.wikipedia.org/wiki/Isaac_Newton%27s_religious_views#God_as_masterful_creator\">Newton</a> established the discovery of laws as the central aim of physical inquiry. In a scientific culture immersed in theism, it was unproblematic, even natural, to think of physical laws as rules. They are rules laid down by God that drive the development of the universe in accord with His divine plan.</p>\n<p>Does this prescriptive conception of law make sense in a secular context? Perhaps if we replace the divine creator of traditional religion with a more naturalist-friendly lawgiver, such as an ur-simulator. But what if there is no intentional agent at the root of it all? Ordinarily, when I think of a physical system as constrained by some rule, it is not the rule itself doing the constraining. The rule is just a piece of language; it is an <em>expression</em> of a constraint that is actually enforced by interaction with some other physical system -- a programmer, say, or a physical barrier, or a police force. In the sort of picture Davies presents, however, it is the rules themselves that enforce the constraint. The laws lord it over lowly matter. So on this view, the fact that all electrons repel one another is explained by the existence of some external entity, not an ordinary physical entity but a law of nature, that somehow <em>forces</em> electrons to repel one another, and this isn't just short-hand for God or the simulator forcing the behavior.</p>\n<p>I put it to you that this account of natural law is utterly mysterious and borders on the nonsensical. How exactly are abstract, non-physical objects -- laws of nature, living in their \"transcendent aerie\" -- supposed to interact with physical stuff? What is the mechanism by which the constraint is applied? Could the laws of nature have been different, so that they forced electrons to attract one another? The view should also be anathema to any self-respecting empiricist, since the laws appear to be idle danglers in the metaphysical theory. What is the difference between a universe where all electrons, as a matter of contingent fact, attract one another, and a universe where they attract one another because they are compelled to do so by the really existing laws of physics? Is there any test that could distinguish between these states of affairs?</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Laws_as_Descriptions\">Laws as Descriptions<br></h2>\n<p><a href=\"http://sas-space.sas.ac.uk/963/1/N_Cartwright_God.pdf\">There are those</a> who take the incoherence of the secular prescriptive conception of laws as reason to reject the whole concept of laws of nature as an anachronistic holdover from a benighted theistic age. I don't think the situation is that dire. Discovering laws of nature is a hugely important activity in physics. It turns out that the behavior of large classes of objects can be given a unified compact mathematical description, and this is crucial to our ability to exercise predictive control over our environment. The significant word in the last sentence is \"description\". A much more congenial alternative to the prescriptive view is available. Instead of thinking of laws as rules that have an existence above and beyond the objects they govern, think of them as particularly concise and powerful descriptions of regular behavior.</p>\n<p>On this descriptive conception of laws, the laws do not exist independently in some transcendent realm. They are not prior to the distribution of matter and energy. The laws are just descriptions of salient patterns in that distribution. Of course, if this is correct, then our talk of the laws governing matter must be understood as metaphorical, but this is a small price to pay for a view that actually makes sense. There may be a concern that we are losing some important explanatory ground here. After all, on the prescriptive view the laws of nature <em>explain</em> why all electrons attract one another, whereas on the descriptive view the laws just restate the fact that all electrons attract one another. But consider the following dialogue:</p>\n<p><em>A</em>: Why are these two metal blocks repelling each other?</p>\n<p><em>B</em>: Because they're both negatively charged, which means they have an excess of electrons, and electrons repel one another.</p>\n<p><em>A</em>: But why do electrons repel one another?</p>\n<p><em>B</em>: Because like charges always repel.</p>\n<p><em>A</em>: But why is that?</p>\n<p><em>B</em>: Because if you do the path integral for the electromagnetic field (using Maxwell's Lagrangian) with source terms corresponding to two spatially separated lumps of identical charge density, you will find that the potential energy of the field is greater the smaller the spatial separation between the lumps, and we know the force points in the opposite direction to the gradient of the potential energy.</p>\n<p><em>A</em>: But why are the dynamics of the electromagnetic field derived from Maxwell's Lagrangian rather than some other equation? And why does the path integral method work at all?</p>\n<p><em>B</em>: BECAUSE IT IS THE LAW.</p>\n<p>Is the last link in this chain doing any explanatory work at all? Does it give us any further traction on the problem? <em>B</em> might as well have ended that conversation by saying \"Well, that's just the way things are.\" Now, laws of nature do have a privileged role in physical explanation, but that privilege is due to their simplicity and generality, not to some mysterious quasi-causal power they exert over matter. The fact that a certain generalization is a law of nature does not <em>account</em> for the truth and explanatory power of the generalization, any more than the fact that a soldier has won the Medal of Honor accounts for his or her courage in combat. Lawhood is a <em>recognition</em> of the generalization's truth and explanatory power. It is an honorific; it doesn't confer any further explanatory oomph.</p>\n<h2 id=\"The_Best_System_Account_of_Laws\">The Best System Account of Laws</h2>\n<p><a href=\"http://en.wikipedia.org/wiki/David_Lewis_%28philosopher%29\">David Lewis</a> offers us a somewhat worked out version of the descriptive conception of law. Consider the set of all truths about the world expressible in a particular language. We can construct deductive systems out of this set of propositions by picking out some of the propositions as axioms. The logical consequences of these axioms are the theorems of the deductive system. These deductive systems compete with one another along (at least) two dimensions: the <em>simplicity </em>of the axioms, and the <em>strength</em> or <em>information content</em> of the system as a whole. We prefer systems that give us more information about the world, but this greater strength often comes at the cost of simplicity. For instance, a system whose axioms comprised the entire set of truths about the world would be maximally strong, but not simple at all. Conversely, a system whose only axiom is something like \"Stuff happens\" would be pretty simple, but very uninformative. What we are looking for is the appropriate balance of simplicity and strength [1].</p>\n<p>According to Lewis, the laws of nature correspond to the axioms of the deductive system that best balances simplicity and strength. He does not provide a precise algorithm for evaluating this balance, and I don't think his proposal should be read as an attempt at a technically precise decision procedure for lawhood anyway. It is more like a heuristic picture of what we are doing when we look for laws. We are looking for simple generalizations that can be used to deduce a large amount of information about the world. Laws are highly compressed descriptions of broad classes of phenomena. This view evidently differs quite substantially from the Davies picture I presented at the beginning of this post. On Lewis's view, the collection of particular facts about the world determines the laws of nature, since the laws are merely compact descriptions of those facts. On Davies's view, the determination runs the other way. The laws are independent entities that determine the particular facts about the world. Stuff in the world is arranged the way it is because the laws compelled that arrangement.</p>\n<p>One last point about Lewis's account. Lewis acknowledges that there is an important language dependence in his view of laws. If we ignore this, we get absurd results. For instance, consider a system whose only axiom is \"For all x, x is F\" where \"F\" is defined to be a predicate that applies to all and only events that occur in this world. This axiom is maximally informative, since it rules out all other possible worlds, and it seems exceedingly simple. Yet we wouldn't want to declare it a law of nature. The problem, obviously, is that all the complexity of the axiom is hidden by our choice of language, with this weird specially rigged predicate. To rule out this possibility, Lewis specifies that all candidate deductive systems must employ the vocabulary of fundamental physics.</p>\n<p>But we could also regard lawhood as a <a href=\"/lw/ro/2place_and_1place_words/\">2-place function</a> which maps a proposition and vocabulary pair to \"True\" if the proposition is an axiom of the best system in that vocabulary and \"False\" otherwise. Lewis has chosen to curry this function by fixing the vocabulary variable. Leaving the function uncurried, however, highlights that we could have different laws for different vocabularies and, consequently, for different levels of description. If I were an economist, I wouldn't be interested (at least not <em>qua </em>economist) in deductive systems that talked about quarks and leptons. I would be interested in deductive systems that talked about prices and demand. The best system for this coarser-grained<em> </em>vocabulary will give us the laws of economics, distinct from the laws of physics.</p>\n<h2 id=\"Lawhood_Is_in_the_Map__not_in_the_Territory\">Lawhood Is in the Map, not in the Territory</h2>\n<p>There is another significant difference between the descriptive and prescriptive accounts that I have not yet discussed. On the Davies-style conception of laws as rules, lawhood is an element of reality. A law is a distinctive beast, an abstract entity perched in a transcendent aerie. On the descriptive account, by comparison, lawhood is part of our map, not the territory. Note that I am not saying that the <em>laws themselves</em> are a feature of the map and not the territory. Laws are just particularly salient redundancies, ones that permit us to construct useful compressed descriptions of reality. These redundancies are, of course, out there in the territory. However, the fact that certain regularities are especially useful for the organization of knowledge is at least partially dependent on facts about us, since we are the ones doing the organizing in pursuit of our particular practical projects. Nature does not flag these regularities as laws, we do.</p>\n<p>This realization has consequences for how we evaluate certain forms of reductionism. I should begin by noting that there is a type of reductionism I tentatively endorse and that I think is untouched by these speculations. I call this <em>mereological reductionism </em>[2]; it is the claim that all the stuff in the universe is entirely built out of the kinds of things described by fundamental physics. The vague statement is intentional, since fundamental physicists aren't yet sure what kinds of things they are describing, but the motivating idea behind the view is to rule out the existence of immaterial souls and the like. However, reductionists typically embrace a stronger form of reductionism that one might label <em>nomic reductionism</em> [3]. The view is that the fundamental laws of physics are the only <em>really existant</em> laws, and that laws in the non-fundamental disciplines are merely convenient short-cuts that we must employ due to our computational limitations.</p>\n<p>One appealing argument for this form of reductionism is the apparent superfluity of non-fundamental laws. Macroscopic systems are entirely built out of parts whose behavior is determined by the laws of physics. It follows that the behavior of these systems is also fixed by those fundamental laws. Additional non-fundamental laws are otiose; there is nothing left for them to do. Barry Loewer <a href=\"http://rci.rutgers.edu/~loewer/papers/Why_is_there_Anything_Except_Physics.pdf\">puts it like this</a>: \"Why would God make [non-fundamental laws] the day after he made physics when the world would go on exactly as if they were there without them?\" If these laws play no explanatory role, Ockham's razor demands that we strike them from our ontological catalog, leaving only the fundamental laws.</p>\n<p>I trust it is apparent that this argument relies on the prescriptive conception of laws. It assumes that real laws of nature <em>do stuff</em>; they push and pull matter and energy around. It is this implicit assumption that raises the overdetermination concern. On this assumption, if the fundamental laws of physics are already lording it over all matter, then there is no room for another locus of authority. However, the argument (and much of the appeal of the associated reductionist viewpoint) fizzles, if we regard laws as descriptive. Employing a Lewisian account, all we have are different best systems, geared towards vocabularies at different resolutions, that highlight different regularities as the basis for a compressed description of a system. There is nothing problematic with having different ways to compress information about a system. Specifically, we are not compelled by worries about overdetermination to declare one of these methods of compression to be <em>more real</em> than another. In response to Loewer's theological question, the proponent of the descriptive conception could say that God does not get to separately specify the non-fundamental and fundamental laws. By creating the pattern of events in space-time she implicitly fixes them all.</p>\n<p>Nomic reductionism would have us believe that the lawhood of the laws of physics is part of the territory, while the lawhood of the laws of psychology is just part of our map. Once we embrace the descriptive conception of laws, however, there is no longer this sharp ontological divide between the fundamental and non-fundamental laws. One reason for privileging the laws of physics is revealed to be the product of a confused metaphysical picture. However, one might think there are still other good reasons for privileging these laws that entail a reductionism more robust than the mereological variety. For instance, even if we accept that laws of physics don't possess a different ontological status, we can still believe that they have a prized position in the explanatory hierarchy. This leads to <em>explanatory reductionism</em>, the view that explanations couched in the vocabulary of fundamental physics are always better because fundamental physics provides us with more accurate models than the non-fundamental sciences. Also, even if one denies that the laws of physics themselves are pushing matter around, one can still believe that all the actual pushing and pulling there is, all the causal action, is described by the laws of physics, and that the non-fundamental laws do not describe genuine causal relations. We could call this kind of view <em>causal reductionism</em>.</p>\n<p>Unfortunately for the reductionist, explanatory and causal reductionism don't fare much better than nomic reductionism. Stay tuned for the reasons why!</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<p>[1] Lewis actually adds a third desideratum, <em>fit</em>, that allows for the evaluation of systems with probabilistic axioms, but I leave this out for simplicity of exposition. I have tweaked Lewis's presentation in a couple of other ways as well. For his own initial presentation of the view, see <a href=\"http://www.amazon.com/Counterfactuals-David-K-Lewis/dp/0631224254/ref=sr_1_1?ie=UTF8&amp;qid=1340138895&amp;sr=8-1&amp;keywords=lewis+counterfactuals\"><em>Counterfactuals</em></a>, pp. 72-77. For a more up-to-date presentation, dealing especially with issues involving probabilistic laws, see <a href=\"http://tedsider.org/teaching/properties/Lewis%20-%20Humean%20supervenience%20debugged.pdf\">this paper</a> (PDF).</p>\n<p>[2] From the Greek <em>meros</em>, meaning \"part\".</p>\n<p>[3] From the Greek <em>nomos</em>, meaning \"law\".</p>", "sections": [{"title": "Laws as Rules", "anchor": "Laws_as_Rules", "level": 1}, {"title": "Laws as Descriptions", "anchor": "Laws_as_Descriptions", "level": 1}, {"title": "The Best System Account of Laws", "anchor": "The_Best_System_Account_of_Laws", "level": 1}, {"title": "Lawhood Is in the Map, not in the Territory", "anchor": "Lawhood_Is_in_the_Map__not_in_the_Territory", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "236 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 236, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-08T05:58:52.441Z", "modifiedAt": null, "url": null, "title": "Group rationality diary, 8/6/12", "slug": "group-rationality-diary-8-6-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:35.203Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cata", "createdAt": "2010-06-02T18:13:22.408Z", "isAdmin": false, "displayName": "cata"}, "userId": "X9jdpCokhLjCMZEc3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rYmXpBi4S688aNM7f/group-rationality-diary-8-6-12", "pageUrlRelative": "/posts/rYmXpBi4S688aNM7f/group-rationality-diary-8-6-12", "linkUrl": "https://www.lesswrong.com/posts/rYmXpBi4S688aNM7f/group-rationality-diary-8-6-12", "postedAtFormatted": "Wednesday, August 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20rationality%20diary%2C%208%2F6%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20rationality%20diary%2C%208%2F6%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrYmXpBi4S688aNM7f%2Fgroup-rationality-diary-8-6-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20rationality%20diary%2C%208%2F6%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrYmXpBi4S688aNM7f%2Fgroup-rationality-diary-8-6-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrYmXpBi4S688aNM7f%2Fgroup-rationality-diary-8-6-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 206, "htmlBody": "<p><span style=\"line-height: 19px; color: #333333; font-family: Arial, Helvetica, sans-serif; text-align: justify;\">This is the public group instrumental rationality diary for the week of August 6th. It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<div id=\"entry_t3_drj\" class=\"content clear\" style=\"font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: justify; background-color: #ffffff; \">\n<div class=\"md\" style=\"font-size: small; \">\n<div style=\"margin-bottom: 1em; \">\n<div style=\"margin-bottom: 1em; \">\n<ul style=\"margin: 10px 2em; padding: 0px; list-style: disc outside; line-height: 19px; \">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em; line-height: 19px; \">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px; \">Thanks to everyone who contributes!</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px; \"><a style=\"color: #8a8a8b; text-decoration: underline; \" href=\"/lw/drj/group_rationality_diary_72312/\">Last week's diary</a>;&nbsp;<a style=\"color: #8a8a8b; text-decoration: underline; \" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">archive of prior diaries</a>.</p>\n<h5><span style=\"font-weight: normal;\">(Sorry for being late this week -- I'm&nbsp;<a href=\"http://www.gocongress.org/\">on vacation</a>&nbsp;and got distracted :-)</span></h5>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rYmXpBi4S688aNM7f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 9.596504653759821e-07, "legacy": true, "legacyId": "18142", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bfqpD7ZLq2FCBYfYC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-08T06:23:25.793Z", "modifiedAt": null, "url": null, "title": "Reinforcement, Preference and Utility", "slug": "reinforcement-preference-and-utility", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:03.661Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "royf", "createdAt": "2012-05-28T04:41:04.869Z", "isAdmin": false, "displayName": "royf"}, "userId": "Rrw92MqPSK6T8nSBg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bPeB6RT78k8dXKYKf/reinforcement-preference-and-utility", "pageUrlRelative": "/posts/bPeB6RT78k8dXKYKf/reinforcement-preference-and-utility", "linkUrl": "https://www.lesswrong.com/posts/bPeB6RT78k8dXKYKf/reinforcement-preference-and-utility", "postedAtFormatted": "Wednesday, August 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reinforcement%2C%20Preference%20and%20Utility&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReinforcement%2C%20Preference%20and%20Utility%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbPeB6RT78k8dXKYKf%2Freinforcement-preference-and-utility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reinforcement%2C%20Preference%20and%20Utility%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbPeB6RT78k8dXKYKf%2Freinforcement-preference-and-utility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbPeB6RT78k8dXKYKf%2Freinforcement-preference-and-utility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 916, "htmlBody": "<p><strong>Followup to:</strong>&nbsp;<a href=\"/lw/dux/reinforcement_learning_a_nonstandard_introduction/\">Reinforcement Learning: A Non-Standard Introduction</a></p>\n<p>A reinforcement-learning agent is interacting with its environment through the perception of observations and the performance of actions.</p>\n<p>We describe the influence of the world on the agent in two steps. The first is the generation of a sensory input O<sub>t</sub>&nbsp;based on the state of the world W<sub>t</sub>. We assume that this step is in accordance with the laws of physics, and out of anyone's hands. The second step is the actual changing the agent's mind to a new state M<sub>t</sub>. The probability distributions of these steps are, respectively,&nbsp;&sigma;(O<sub>t</sub>|W<sub>t</sub>)&nbsp;and q(M<sub>t</sub>|M<sub>t-1</sub>,O<sub>t</sub>).</p>\n<p>Similarly, the agent affects the world by deciding on an action A<sub>t</sub>&nbsp;and performing it. The designer of the agent can choose the probability distribution of actions&nbsp;&pi;(A<sub>t</sub>|M<sub>t</sub>), but not the natural laws p(W<sub>t+1</sub>|W<sub>t</sub>,A<sub>t</sub>)&nbsp;saying how these actions change the world.</p>\n<p><img src=\"http://images.lesswrong.com/t3_dz4_0.png?v=00e86fa9a0ae5c5bde454b1c97801bc1\" alt=\"\" width=\"626\" height=\"266\" /></p>\n<p>So how do we choose q and&nbsp;&pi;? Let's first assume that it matters how. That is, let's assume that we have some preference over the results of the process, the actual values that all the variables take. Let's also make some further assumptions regarding this preference relation:</p>\n<p>&nbsp; &nbsp;1. The first assumption will be the standard <a href=\"/lw/244/vnm_expected_utility_theory_uses_abuses_and/\">von Neumann-Morgenstern rationality</a>. This is a good opportunity to point out a common misconception in the interpretation of that result. It is often pointed out that humans, for instance, are not rational in the VNM sense. That is completely beside the point.</p>\n<p><a href=\"/lw/of/dissolving_the_question/\">The agent doesn't <em>choose</em></a>&nbsp;the transitions q and&nbsp;&pi;. <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">The agent <em>is</em></a>&nbsp;these transitions. So it's not the agent that needs to be rational about the preference, and indeed it may appear not to be. If the agent has evolved, we may argue that evolutionary fitness is VNM-rational (even if the local-optimization process leads to sub-optimal fitness). If humans design a Mars rover to perform certain functions, we may argue that the goal dictates a VNM-rational preference (even if we, imperfect designers that we are, can only approximate it).</p>\n<p>&nbsp; &nbsp;2. The second assumption will be that the preference is strictly about the territory W<sub>t</sub>, never about the map M<sub>t</sub>. This means that we are never interested in merely putting the memory of the agent in a \"good\" state. We need it to be followed up by \"good\" actions.</p>\n<p>You may think that an agent that generates accurate predictions of the stock market could be very useful. But if the agent doesn't follow up with actually investing well, or at least reliably reporting its findings to someone who does, then what good is it?</p>\n<p>So we are going to define the preference with respect to only the states W<sub>1</sub>, ..., W<sub>n</sub>&nbsp;and the actions A<sub>1</sub>, ..., A<sub>n</sub>. If the observations are somehow important, we can always define them as being part of the world. If the agent's memory is somehow important, it will have to reliably communicate it out through actions.</p>\n<p>&nbsp; &nbsp;3. The third assumption will be the <a href=\"/lw/at/sunk_cost_fallacy/\">sunk cost</a>&nbsp;assumption. We can fix some values of the first t steps of the process, and consider the preference with respect to the remaining n-t steps. This is like finding ourselves in time t, with a given t-step history, considering what to do next (though of course we plan for that contingency ahead of time).&nbsp;Our assumption is that we find that the preference is the same, regardless of the fixed history.</p>\n<p>This last assumption gives rise to a special version of the <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">von Neumann-Morgenstern utility theorem</a>. We find that what we really prefer is to have as high as possible the expectation of a utility function with a special structure: the utility u<sub>t</sub>&nbsp;depends only on W<sub>t</sub>, A<sub>t</sub>&nbsp;and the <em>expectation</em> of u<sub>t+1</sub>&nbsp;from the following step:</p>\n<p style=\"padding-left: 30px; \">u<sub>t</sub>(W<sub>t</sub>,A<sub>t</sub>,E(u<sub>t+1</sub>))</p>\n<p>This kind of recursion which goes backwards in time is a recurring theme in&nbsp;reinforcement learning.</p>\n<p>We would like to have even more structure in our utility function, but this is where things become less principled and more open to personal taste among researchers. We base our own taste on a strong intuition that the following could, one day, be made to rely on much better principles than it currently does.</p>\n<p>We will assume that u<sub>t</sub>&nbsp;is simply the summation of some immediate utility and the expectation of future utility:</p>\n<p style=\"padding-left: 30px; \">u<sub>t</sub>(W<sub>t</sub>,A<sub>t</sub>,E(u<sub>t+1</sub>)) = R(W<sub>t</sub>,A<sub>t</sub>) + E(u<sub>t+1</sub>)</p>\n<p>Here R is the Reward, the additional utility that the agent gets from taking the action A<sub>t</sub>&nbsp;when the world is in state W<sub>t</sub>. It's not hard to see that we can write down our utility in closed form, as the total of all rewards we get throughout the process</p>\n<p style=\"padding-left: 30px;\">R(W<sub>1</sub>,A<sub>1</sub>) + R(W<sub>2</sub>,A<sub>2</sub>) +&nbsp;... + R(W<sub>n</sub>,A<sub>n</sub>)</p>\n<p>As a final note, if the agent is intended to achieve a high expectation of the total reward, then it may be helpful for the agent to actually <em>observe</em>&nbsp;its reward when it gets it. And indeed, many reinforcement learning algorithms require that the reward signal is visible to the agent as part of its observation each step. This can help the agent adapt its behavior to changes in the environment. After all, reinforcement learning means, quite literally, adaptation in response to a reward signal.</p>\n<p>However, reinforcement learning can also happen when the reward signal is not explicit in the agent's observations. To the degree that the observations carry information about the state of the world, and that the reward is determined by that state, information about the reward is anyway implicit in the observations.</p>\n<p>In this sense, the reward is just a score for the quality of the agent's design. If we take it into account when we design the agent, we end up choosing q and &pi; so that the agent will&nbsp;exploit the implicit information in its observations to get a good reward.</p>\n<p><strong>Continue reading:</strong>&nbsp;<a href=\"/r/lesswrong/lw/e6a/the_bayesian_agent/\">The Bayesian Agent</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2d4": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bPeB6RT78k8dXKYKf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 13, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "18112", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xdjA6YtE7QBsLYQ3i", "YCMfQoqqi2o9Tjwoa", "Mc6QcrsbH5NRXbCRX", "yA4gF5KrboK2m2Xu7", "tyMdPwd8x2RygcheE", "G4XKiJ2Q93JGCJxCT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-08T07:26:16.053Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] No License To Be Human", "slug": "seq-rerun-no-license-to-be-human", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:04.112Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/t5omBrwZ7BKs7fviy/seq-rerun-no-license-to-be-human", "pageUrlRelative": "/posts/t5omBrwZ7BKs7fviy/seq-rerun-no-license-to-be-human", "linkUrl": "https://www.lesswrong.com/posts/t5omBrwZ7BKs7fviy/seq-rerun-no-license-to-be-human", "postedAtFormatted": "Wednesday, August 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20No%20License%20To%20Be%20Human&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20No%20License%20To%20Be%20Human%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft5omBrwZ7BKs7fviy%2Fseq-rerun-no-license-to-be-human%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20No%20License%20To%20Be%20Human%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft5omBrwZ7BKs7fviy%2Fseq-rerun-no-license-to-be-human", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft5omBrwZ7BKs7fviy%2Fseq-rerun-no-license-to-be-human", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>Today's post, <a href=\"/lw/t9/no_license_to_be_human/\">No License To Be Human</a> was originally published on 20 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>We don't pull children off of train tracks because that's what humans do, or because we can consistently hold that it's better to pull children off of train tracks. We pull children off of train tracks because it's right.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/dz5/seq_rerun_you_provably_cant_trust_yourself/\">You Provably Can't Trust Yourself</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "t5omBrwZ7BKs7fviy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.596931485414667e-07, "legacy": true, "legacyId": "18148", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YrhT7YxkRJoRnr7qD", "4zPhwtBmSWeZDdrag", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-08T13:16:27.269Z", "modifiedAt": null, "url": null, "title": "Friendly AI and the limits of computational epistemology", "slug": "friendly-ai-and-the-limits-of-computational-epistemology", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:23.129Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QanBKR8Fw5TsvC5E7/friendly-ai-and-the-limits-of-computational-epistemology", "pageUrlRelative": "/posts/QanBKR8Fw5TsvC5E7/friendly-ai-and-the-limits-of-computational-epistemology", "linkUrl": "https://www.lesswrong.com/posts/QanBKR8Fw5TsvC5E7/friendly-ai-and-the-limits-of-computational-epistemology", "postedAtFormatted": "Wednesday, August 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Friendly%20AI%20and%20the%20limits%20of%20computational%20epistemology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFriendly%20AI%20and%20the%20limits%20of%20computational%20epistemology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQanBKR8Fw5TsvC5E7%2Ffriendly-ai-and-the-limits-of-computational-epistemology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Friendly%20AI%20and%20the%20limits%20of%20computational%20epistemology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQanBKR8Fw5TsvC5E7%2Ffriendly-ai-and-the-limits-of-computational-epistemology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQanBKR8Fw5TsvC5E7%2Ffriendly-ai-and-the-limits-of-computational-epistemology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2479, "htmlBody": "<p>Very soon, Eliezer is supposed to start posting a new sequence, on \"Open Problems in Friendly AI\". After several years in which its activities were dominated by the topic of human rationality, this ought to mark the beginning of a new phase for the Singularity Institute, one in which it is visibly working on artificial intelligence once again. If everything comes together, then it will now be a straight line from here to the end. <br /><br />I foresee that, once the new sequence gets going, it won't be that easy to question the framework in terms of which the problems are posed. So I consider this my last opportunity for some time, to set out an alternative big picture. It's a framework in which all those rigorous mathematical and computational issues still need to be investigated, so a lot of \"orthodox\" ideas about Friendly AI should carry across. But the context is different, and it makes a difference. <br /><br />Begin with the really big picture. What would it take to produce a friendly singularity? You need to find the true ontology, find the true morality, and win the intelligence race. For example, if your Friendly AI was to be an expected utility maximizer, it would need to model the world correctly (\"true ontology\"), value the world correctly (\"true morality\"), and it would need to outsmart its opponents (\"win the intelligence race\"). <br /><br />Now let's consider how SI will approach these goals. <br /><br />The evidence says that the working ontological hypothesis of SI-associated researchers will be <a href=\"http://wiki.lesswrong.com/wiki/The_Quantum_Physics_Sequence#Timeless_Physics\">timeless many-worlds quantum mechanics</a>, possibly embedded in a \"Tegmark Level IV multiverse\", with the auxiliary hypothesis that algorithms can <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">\"feel like something from inside\"</a> and that this is what conscious experience is. <br /><br />The true morality is to be found by understanding the true decision procedure employed by human beings, and idealizing it according to criteria implicit in that procedure. That is, one would seek to understand conceptually the physical and cognitive causation at work in concrete human choices, both conscious and unconscious, with the expectation that there will be a crisp, complex, and specific answer to the question \"why and how do humans make the choices that they do?\" Undoubtedly there would be some biological variation, and there would also be significant elements of the \"human decision procedure\",&nbsp; as instantiated in any specific individual, which are set by experience and by culture, rather than by genetics. Nonetheless one expects that there is something like a specific <em>algorithm</em> or algorithm-template here, which is part of the standard <em>Homo sapiens</em> cognitive package and biological design; just another anatomical feature, particular to our species. <br /><br />Having reconstructed this algorithm via scientific analysis of human genome, brain, and behavior, one would then idealize it using its own criteria. This algorithm defines the de-facto value system that human beings employ, but that is not necessarily the value system they would <em>wish</em> to employ; nonetheless, human self-dissatisfaction also arises from the use of this algorithm to judge ourselves. So it contains the seeds of its own improvement. The value system of a Friendly AI is to be obtained from the recursive self-improvement of the natural human decision procedure. <br /><br />Finally, this is all for naught if seriously unfriendly AI appears first. It isn't good enough just to have the right goals, you must be able to carry them out. In the global race towards <a href=\"http://wiki.lesswrong.com/wiki/Artificial_general_intelligence\">artificial general intelligence</a>, SI might hope to \"win\" either by being the first to achieve AGI, or by having its prescriptions adopted by those who <em>do</em> first achieve AGI. They have some in-house competence regarding models of universal AI like AIXI, and they have many contacts in the world of AGI research, so they're at least <em>engaged</em> with this aspect of the problem. <br /><br />Upon examining this tentative reconstruction of SI's game-plan, I find I have two major reservations. The big one, and the one most difficult to convey, concerns the ontological assumptions. <a href=\"/lw/c1x/extrapolating_values_without_outsourcing/\">In second place</a> is what I see as an undue emphasis on the idea of outsourcing the methodological and design problems of FAI research to uploaded researchers and/or a proto-FAI which is simulating or modeling human researchers. This is supposed to be a way to finesse philosophical difficulties like \"what is consciousness anyway\"; you just simulate some humans until they agree that they have solved the problem. The reasoning goes that if the simulation is good enough, it will be just as good as if ordinary non-simulated humans solved it. <br /><br />I also used to have a <a href=\"/lw/4wq/rationality_singularity_method_and_the_mainstream/\">third major criticism</a>, that the big SI focus on rationality outreach was a mistake; but it brought in a lot of new people, and in any case that phase is ending, with the creation of CFAR, a separate organization. So we are down to two basic criticisms. <br /><br />First, \"ontology\". I do not think that SI intends to just program its AI with an apriori belief in the Everett multiverse, for two reasons. First, like anyone else, their ventures into AI will surely begin with programs that work within very limited and more down-to-earth ontological domains. Second, at least some of the AI's world-model ought to be obtained rationally. Scientific theories are supposed to be rationally justified, e.g. by their capacity to make successful predictions, and one would prefer that the AI's ontology results from the employment of its epistemology, rather than just being an axiom; not least because we want it to be able to question that ontology, should the evidence begin to count against it. <br /><br />For this reason, although I have campaigned against many-worlds dogmatism on this site for several years, I'm not especially concerned about the possibility of SI producing an AI that is \"dogmatic\" in this way. For an AI to independently assess the merits of rival physical theories, the theories would need to be expressed with much more precision than they have been in LW's debates, and the disagreements about which theory is rationally favored would be replaced with objectively resolvable choices among exactly specified models. <br /><br />The real problem, which is not just SI's problem, but a chronic and worsening problem of intellectual culture in the era of mathematically formalized science, is a dwindling of the ontological options to materialism, platonism, or an unstable combination of the two, and a similar restriction of <em>epistemology</em> to computation. <br /><br />Any assertion that we need an ontology beyond materialism (or physicalism or naturalism) is liable to be immediately rejected by this audience, so I shall immediately explain what I mean. It's just the usual problem of \"qualia\". There are qualities which are part of reality - we know this because they are part of experience, and experience is part of reality - but which are not part of our physical description of reality. The problematic \"belief in materialism\" is actually the belief in the completeness of current materialist ontology, a belief which prevents people from seeing any need to consider radical or exotic solutions to the qualia problem. There is every reason to think that the world-picture arising from a correct solution to that problem will still be one in which you have \"things with states\" causally interacting with other \"things with states\", and a sensible materialist shouldn't find that objectionable. <br /><br />What I mean by platonism, is an ontology which reifies mathematical or computational abstractions, and says that they are the stuff of reality. Thus assertions that reality is a computer program, or a Hilbert space. Once again, the qualia are absent; but in this case, instead of the deficient ontology being based on supposing that there is nothing but particles, it's based on supposing that there is nothing but the intellectual constructs used to model the world. <br /><br />Although the abstract concept of a computer program (the abstractly conceived <a href=\"http://en.wikipedia.org/wiki/State_transition_system\">state machine</a> which it instantiates) does not contain qualia, people often treat programs as having mind-like qualities, especially by imbuing them with semantics - the states of the program are conceived to be \"about\" something, just like thoughts are. And thus computation has been the way in which materialism has tried to restore the mind to a place in its ontology. This is the unstable combination of materialism and platonism to which I referred. It's unstable because it's not a real solution, though it can live unexamined for a long time in a person's belief system. <br /><br />An ontology which genuinely contains qualia will nonetheless still contain \"things with states\" undergoing state transitions, so there will be state machines, and consequently, computational concepts will still be valid, they will still have a place in the description of reality. But the computational description is an abstraction; the ontological essence of the state plays no part in this description; only its causal role in the network of possible states matters for computation. The attempt to make computation the <em>foundation</em> of an ontology of mind is therefore proceeding in the wrong direction. <br /><br />But here we run up against the hazards of computational <em>epistemology</em>, which is playing such a central role in artificial intelligence. Computational epistemology is good at identifying the minimal state machine which could have produced the data. But it cannot by itself tell you what those states are \"like\". It can only say that X was probably caused by a Y that was itself caused by Z. <br /><br />Among the properties of human consciousness are knowledge that something exists, knowledge that consciousness exists, and a long string of other facts about the nature of what we experience. Even if an AI scientist employing a computational epistemology managed to produce a model of the world which correctly identified the causal relations between consciousness, its knowledge, and the objects of its knowledge, the AI scientist would not know that its X, Y, and Z refer to, say, \"knowledge of existence\", \"experience of existence\", and \"existence\". The same might be said of any successful analysis of qualia, knowledge of qualia, and how they fit into neurophysical causality. <br /><br />It would be up to human beings - for example, the AI's programmers and handlers - to ensure that entities in the AI's causal model were given appropriate significance. And here we approach the second big problem, the enthusiasm for outsourcing the solution of hard problems of FAI design to the AI and/or to simulated human beings. The latter is a somewhat impractical idea anyway, but here I want to highlight the risk that the AI's <em>designers</em> will have false ontological beliefs about the nature of mind, which are then implemented apriori in the AI. That strikes me as far more likely than implanting a wrong apriori about physics; computational epistemology <em>can</em> discriminate usefully between different mathematical models of physics, because it can judge one state machine model as better than another, and current physical ontology is essentially one of interacting state machines. But as I have argued, not only must the true ontology be deeper than state-machine materialism, there is no way for an AI employing computational epistemology to bootstrap to a deeper ontology. <br /><br />In a phrase: to use computational epistemology is to commit to state-machine materialism as your apriori ontology. And the problem with state-machine materialism is not that it models the world in terms of causal interactions between things-with-states; the problem is that it can't go any deeper than that, yet apparently we can. Something about the ontological constitution of consciousness makes it possible for us to experience existence, to have the concept of existence, to know that we are experiencing existence, and similarly for the experience of color, time, and all those other aspects of being that fit so uncomfortably into our scientific ontology. <br /><br />It must be that the true <em>epistemology</em>, for a conscious being, is something more than computational epistemology. And maybe an AI can't bootstrap its way to knowing this expanded epistemology - because an AI doesn't <em>really</em> know or experience anything, only a consciousness, whether natural or artificial, does those things - but maybe a human being can. My own investigations suggest that the tradition of thought which made the most progress in this direction was the philosophical school known as <a href=\"http://plato.stanford.edu/entries/phenomenology/\">transcendental phenomenology</a>. But transcendental phenomenology is very unfashionable now, precisely because of apriori materialism. People don't see what \"categorial intuition\" or \"adumbrations of givenness\" or any of the other weird phenomenological concepts could possibly mean for an evolved Bayesian neural network; and they're right, there is no connection. But the idea that a human being is a state machine running on a distributed neural computation is just a hypothesis, and I would argue that it is a hypothesis in contradiction with so much of the <em>phenomenological</em> data, that we really ought to look for a more sophisticated refinement of the idea. Fortunately, 21st-century physics, if not yet neurobiology, can provide alternative hypotheses in which complexity of state originates from something other than concatenation of parts - for example, entanglement, or from topological structures in a field. In such ideas I believe we see a glimpse of the true ontology of mind, one which from the inside resembles the ontology of transcendental phenomenology; which in its mathematical, formal representation may involve structures like <a href=\"http://arxiv.org/abs/1001.1062\">iterated Clifford algebras</a>; and which in its biophysical context would appear to be describing a mass of entangled electrons in that hypothetical sweet spot, somewhere in the brain, where there's a mechanism to protect against decoherence. <br /><br />Of course this is why I've talked about \"monads\" <a href=\"/lw/1bs/how_to_think_like_a_quantum_monadologist/\">in the past</a>, but my objective here is not to promote neo-monadology, that's something I need to take up with neuroscientists and biophysicists and quantum foundations people. What I wish to do here is to argue against the completeness of computational epistemology, and to caution against the rejection of phenomenological data just because it conflicts with state-machine materialism or computational epistemology. This is an argument and a warning that should be meaningful for anyone trying to make sense of their existence in the scientific cosmos, but it has a special significance for this arcane and idealistic enterprise called \"friendly AI\". My message for friendly AI researchers is not that computational epistemology is invalid, or that it's wrong to think about the mind as a state machine, just that all that isn't the full story. A monadic mind would be a state machine, but <em>ontologically</em> it would be different from the same state machine running on a network of a billion monads. You need to do the impossible one more time, and make your plans bearing in mind that the true ontology is something more than your current intellectual tools allow you to represent.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QanBKR8Fw5TsvC5E7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 53, "baseScore": 23, "extendedScore": null, "score": 9.598638254928123e-07, "legacy": true, "legacyId": "18149", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 148, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yA4gF5KrboK2m2Xu7", "MWjtBRnWvrEGB22KB", "F75MwixdkpcxHDTsj", "RKEepbECEXQXqXFNm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-08T14:39:02.580Z", "modifiedAt": null, "url": null, "title": "[Link] \u201cProxy measures, sunk costs, and Chesterton's fence\u201d, or: the sunk cost heuristic", "slug": "link-proxy-measures-sunk-costs-and-chesterton-s-fence-or-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:01.819Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kpreid", "createdAt": "2009-04-28T03:07:40.133Z", "isAdmin": false, "displayName": "kpreid"}, "userId": "rKiev4kfnTWRmCJit", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iagQwQGCnvpbJAXay/link-proxy-measures-sunk-costs-and-chesterton-s-fence-or-the", "pageUrlRelative": "/posts/iagQwQGCnvpbJAXay/link-proxy-measures-sunk-costs-and-chesterton-s-fence-or-the", "linkUrl": "https://www.lesswrong.com/posts/iagQwQGCnvpbJAXay/link-proxy-measures-sunk-costs-and-chesterton-s-fence-or-the", "postedAtFormatted": "Wednesday, August 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20%E2%80%9CProxy%20measures%2C%20sunk%20costs%2C%20and%20Chesterton's%20fence%E2%80%9D%2C%20or%3A%20the%20sunk%20cost%20heuristic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20%E2%80%9CProxy%20measures%2C%20sunk%20costs%2C%20and%20Chesterton's%20fence%E2%80%9D%2C%20or%3A%20the%20sunk%20cost%20heuristic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiagQwQGCnvpbJAXay%2Flink-proxy-measures-sunk-costs-and-chesterton-s-fence-or-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20%E2%80%9CProxy%20measures%2C%20sunk%20costs%2C%20and%20Chesterton's%20fence%E2%80%9D%2C%20or%3A%20the%20sunk%20cost%20heuristic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiagQwQGCnvpbJAXay%2Flink-proxy-measures-sunk-costs-and-chesterton-s-fence-or-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiagQwQGCnvpbJAXay%2Flink-proxy-measures-sunk-costs-and-chesterton-s-fence-or-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 248, "htmlBody": "<p>Thought this post might be of interest to LW: <a href=\"http://unenumerated.blogspot.com/2012/08/proxy-measures-sunk-costs-and.html\">Proxy measures, sunk costs, and Chesterton's fence</a>. To summarize: Previous costs are a proxy measure for previous estimates of value, which may have information current estimates of value do not; therefore acting according to the sunk cost fallacy is not necessarily wrong.</p>\n<p>This is not an entirely new idea here, but I liked the writeup. Previous discussion: <a href=\"/lw/9jy/sunk_costs_fallacy_fallacy/\">Sunk Costs Fallacy Fallacy</a>; <a href=\"/lw/9si/is_sunk_cost_fallacy_a_fallacy/\">Is Sunk Cost Fallacy a Fallacy?</a>.</p>\n<p>Excerpt:</p>\n<blockquote cite=\"http://unenumerated.blogspot.com/2012/08/proxy-measures-sunk-costs-and.html\">\n<p>If your evidence may be substantially incomplete you shouldn't just ignore sunk costs &mdash; they contain valuable information about decisions you or others made in the past, perhaps after much greater thought or access to evidence than that of which you are currently capable. Even more generally, you should be loss averse &mdash; you should tend to prefer avoiding losses over acquiring seemingly equivalent gains, and you should be divestiture averse (i.e. exhibit endowment effects) &mdash; you should tend to prefer what you already have to what you might trade it for &mdash; in both cases to the extent your ability to measure the value of the two items is incomplete. Since usually in the real world, and to an even greater degree in our ancestors'&nbsp;evolutionary environments, our ability to measure value is and was&nbsp;woefully incomplete, it should come as no surprise that people often value sunk costs, are loss averse, and exhibit endowment effects &mdash; and indeed under such circumstances of incomplete value measurement&nbsp;it hardly constitutes \"fallacy\" or \"bias\" to do so.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ntahi2tr7e9DjCYdu": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iagQwQGCnvpbJAXay", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "18150", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dHJuevTkH92sjLYXe", "QvuD7R5L5ABw9tDdD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-08T17:44:13.405Z", "modifiedAt": null, "url": null, "title": "Meetup : Less Wrong Sydney: 20th August ", "slug": "meetup-less-wrong-sydney-20th-august", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oklord", "createdAt": "2011-03-22T11:37:19.291Z", "isAdmin": false, "displayName": "Oklord"}, "userId": "EusNQMHkhmSq2k9Y9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DSepFRJ6h5piZFetB/meetup-less-wrong-sydney-20th-august", "pageUrlRelative": "/posts/DSepFRJ6h5piZFetB/meetup-less-wrong-sydney-20th-august", "linkUrl": "https://www.lesswrong.com/posts/DSepFRJ6h5piZFetB/meetup-less-wrong-sydney-20th-august", "postedAtFormatted": "Wednesday, August 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Less%20Wrong%20Sydney%3A%2020th%20August%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Less%20Wrong%20Sydney%3A%2020th%20August%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDSepFRJ6h5piZFetB%2Fmeetup-less-wrong-sydney-20th-august%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Less%20Wrong%20Sydney%3A%2020th%20August%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDSepFRJ6h5piZFetB%2Fmeetup-less-wrong-sydney-20th-august", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDSepFRJ6h5piZFetB%2Fmeetup-less-wrong-sydney-20th-august", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/cp'>Less Wrong Sydney: 20th August </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 August 2012 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">565-567 George Street, Sydney, Australia 2000</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey all!</p>\n\n<p>Just a reminder for the up-coming meet up at the Sydney RSL - level 3 at the restaurant.</p>\n\n<p>Wed did not get to try nomic last time- barring a  better idea, might as well give it a shot now</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/cp'>Less Wrong Sydney: 20th August </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DSepFRJ6h5piZFetB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.599950559944714e-07, "legacy": true, "legacyId": "18152", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Sydney__20th_August_\">Discussion article for the meetup : <a href=\"/meetups/cp\">Less Wrong Sydney: 20th August </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 August 2012 06:30:00PM (+1000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">565-567 George Street, Sydney, Australia 2000</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hey all!</p>\n\n<p>Just a reminder for the up-coming meet up at the Sydney RSL - level 3 at the restaurant.</p>\n\n<p>Wed did not get to try nomic last time- barring a  better idea, might as well give it a shot now</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong_Sydney__20th_August_1\">Discussion article for the meetup : <a href=\"/meetups/cp\">Less Wrong Sydney: 20th August </a></h2>", "sections": [{"title": "Discussion article for the meetup : Less Wrong Sydney: 20th August ", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Sydney__20th_August_", "level": 1}, {"title": "Discussion article for the meetup : Less Wrong Sydney: 20th August ", "anchor": "Discussion_article_for_the_meetup___Less_Wrong_Sydney__20th_August_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-08T18:00:42.911Z", "modifiedAt": null, "url": null, "title": "Biohacking in New York, Cybernetics and first Cyborg Hate Crime: theverge.com", "slug": "biohacking-in-new-york-cybernetics-and-first-cyborg-hate", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:07.885Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MatthewBaker", "createdAt": "2011-06-03T22:19:50.449Z", "isAdmin": false, "displayName": "MatthewBaker"}, "userId": "xEPvhkraqrPSryfFr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/js92shXkuF7NjDYeH/biohacking-in-new-york-cybernetics-and-first-cyborg-hate", "pageUrlRelative": "/posts/js92shXkuF7NjDYeH/biohacking-in-new-york-cybernetics-and-first-cyborg-hate", "linkUrl": "https://www.lesswrong.com/posts/js92shXkuF7NjDYeH/biohacking-in-new-york-cybernetics-and-first-cyborg-hate", "postedAtFormatted": "Wednesday, August 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Biohacking%20in%20New%20York%2C%20Cybernetics%20and%20first%20Cyborg%20Hate%20Crime%3A%20theverge.com&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABiohacking%20in%20New%20York%2C%20Cybernetics%20and%20first%20Cyborg%20Hate%20Crime%3A%20theverge.com%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjs92shXkuF7NjDYeH%2Fbiohacking-in-new-york-cybernetics-and-first-cyborg-hate%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Biohacking%20in%20New%20York%2C%20Cybernetics%20and%20first%20Cyborg%20Hate%20Crime%3A%20theverge.com%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjs92shXkuF7NjDYeH%2Fbiohacking-in-new-york-cybernetics-and-first-cyborg-hate", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjs92shXkuF7NjDYeH%2Fbiohacking-in-new-york-cybernetics-and-first-cyborg-hate", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>http://www.theverge.com/2012/8/8/3177438/cyborg-america-biohackers-grinders-body-hackers</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "js92shXkuF7NjDYeH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 4, "extendedScore": null, "score": 9.600031154781e-07, "legacy": true, "legacyId": "18153", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-08T19:10:06.885Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver Big Public Meetup", "slug": "meetup-vancouver-big-public-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Arkanj3l", "createdAt": "2011-04-23T03:48:47.569Z", "isAdmin": false, "displayName": "Arkanj3l"}, "userId": "nQmA4dnBdX99WyCt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MHTDorNK499m8XzcM/meetup-vancouver-big-public-meetup", "pageUrlRelative": "/posts/MHTDorNK499m8XzcM/meetup-vancouver-big-public-meetup", "linkUrl": "https://www.lesswrong.com/posts/MHTDorNK499m8XzcM/meetup-vancouver-big-public-meetup", "postedAtFormatted": "Wednesday, August 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver%20Big%20Public%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver%20Big%20Public%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMHTDorNK499m8XzcM%2Fmeetup-vancouver-big-public-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver%20Big%20Public%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMHTDorNK499m8XzcM%2Fmeetup-vancouver-big-public-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMHTDorNK499m8XzcM%2Fmeetup-vancouver-big-public-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 98, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/cq'>Vancouver Big Public Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 August 2012 06:06:06PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Benny's Bagels 2505 W Broadway Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Excelsior Rationalists!\nTomorrow is the Big Monthly Meetup at Benny's Bagels, replete with Capital Letters and other assorted foodstuffs.\nTomorrow's meetup is casual, so there will be no necessary discussion structure or the like. Bringing your own boardgames is encouraged (especially <a href=\"https://en.wikipedia.org/wiki/Set_%28game%29\" rel=\"nofollow\">Set</a>, if you have it).\nWe host these public meetups every month to encourage people to come out and have fun. So here's your chance!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/cq'>Vancouver Big Public Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MHTDorNK499m8XzcM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.600370322044509e-07, "legacy": true, "legacyId": "18154", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Big_Public_Meetup\">Discussion article for the meetup : <a href=\"/meetups/cq\">Vancouver Big Public Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 August 2012 06:06:06PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Benny's Bagels 2505 W Broadway Vancouver, BC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Excelsior Rationalists!\nTomorrow is the Big Monthly Meetup at Benny's Bagels, replete with Capital Letters and other assorted foodstuffs.\nTomorrow's meetup is casual, so there will be no necessary discussion structure or the like. Bringing your own boardgames is encouraged (especially <a href=\"https://en.wikipedia.org/wiki/Set_%28game%29\" rel=\"nofollow\">Set</a>, if you have it).\nWe host these public meetups every month to encourage people to come out and have fun. So here's your chance!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_Big_Public_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/cq\">Vancouver Big Public Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver Big Public Meetup", "anchor": "Discussion_article_for_the_meetup___Vancouver_Big_Public_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Vancouver Big Public Meetup", "anchor": "Discussion_article_for_the_meetup___Vancouver_Big_Public_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-08T20:02:34.213Z", "modifiedAt": null, "url": null, "title": "Is lossless information transfer possible?", "slug": "is-lossless-information-transfer-possible", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:36.063Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kirpi", "createdAt": "2011-05-17T10:21:17.706Z", "isAdmin": false, "displayName": "kirpi"}, "userId": "RQLK3s5cH7XwGDrFA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LBw9Qhh7jzEbrduWM/is-lossless-information-transfer-possible", "pageUrlRelative": "/posts/LBw9Qhh7jzEbrduWM/is-lossless-information-transfer-possible", "linkUrl": "https://www.lesswrong.com/posts/LBw9Qhh7jzEbrduWM/is-lossless-information-transfer-possible", "postedAtFormatted": "Wednesday, August 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20lossless%20information%20transfer%20possible%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20lossless%20information%20transfer%20possible%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLBw9Qhh7jzEbrduWM%2Fis-lossless-information-transfer-possible%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20lossless%20information%20transfer%20possible%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLBw9Qhh7jzEbrduWM%2Fis-lossless-information-transfer-possible", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLBw9Qhh7jzEbrduWM%2Fis-lossless-information-transfer-possible", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 459, "htmlBody": "<p>I am trying to establish what (if anything) makes human beings superior to other organisms.</p>\n<p>I have a hypothesis that, the only thing at which human beings are \"superior\" to other organisms is that we can transfer information without a loss to other human beings.</p>\n<p>This difference may already be well established. I couldn't find a good read on this, so I wanted to ask your opinion.</p>\n<p>Many organisms seem to have superior capabilities than human beings; strength, speed, agility, vision, hearing, regeneration etc. And even high IQ (at least on a hardware level on dolphins etc) may not be unique to humankind.</p>\n<p>So, my first suspect, high IQ alone does not seem to be a differentiator of our species. (It does not even seem to be predictor of success within the species)</p>\n<p>Then I remember the famous experiment of hosing down of gorillas trying to reach bananas. (To which I can't find the original citation) Shortly;</p>\n<p>- Some gorillas are hosed with cold water when they try to reach bananas.</p>\n<p>- Then they learn to stop trying to eat these bananas.</p>\n<p>- The gorillas are replaced with other gorillas one by one.</p>\n<p>- The old gorillas prevent new comers from reaching the banana even though they are not hosed anymore.</p>\n<p>- When all of the gorillas are replaced, they still stop each other from reaching the banana.</p>\n<p>It seems like the information is partially transferred. They can't transfer the cause. But human beings can transfer the cause. So, are human beings the only species that can transfer information without a loss?</p>\n<p>The primary assumption I made is that, human beings can transfer infomation without loss. This turns out to be the major discussion topic. Is lossless information transfer is even possible? There seems to be opposition against this idea also.</p>\n<p>For example, isn't this a lossless transfer to the reader;</p>\n<p>\"The sunlight seems yellow to human beings who are at this point on earth when earth is positioned like this with respect to sun\"</p>\n<p>By the way, by information, I don't mean the representation of it but the information itself. (i.e. Digitizing, wording or syntax for short does not matter)</p>\n<p>If lossless transfer wasn't possible, it looks like we couldn't advance (at least) technology at all (like the gorilla example) Or there may be countermeasures to this loss too. (Like various people attacking one problem over and over again independently and finding a combined solution of the problem at an acceptable level)</p>\n<p>To sum up, are the following true assertations?</p>\n<p>- Information can be transferred within a species without loss.</p>\n<p>- Human beings are the only species that can transfer information without loss.</p>\n<p>- Capability to transfer information without loss is what makes human beings superior to other organisms.</p>\n<p>p.s. For this is my first discussion post, please don't beat this too hard :)</p>\n<p>p.p.s. Distinguished does not mean superior.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LBw9Qhh7jzEbrduWM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -12, "extendedScore": null, "score": 9.60062669471206e-07, "legacy": true, "legacyId": "18155", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-08T23:43:44.054Z", "modifiedAt": null, "url": null, "title": "Seeking advice on using evolutionary methods to solve the 3-body problem", "slug": "seeking-advice-on-using-evolutionary-methods-to-solve-the-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:02.354Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Troshen", "createdAt": "2011-11-08T17:26:55.623Z", "isAdmin": false, "displayName": "Troshen"}, "userId": "74vNJiAARMPP4reof", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GvCMck8LK9WaMKCyg/seeking-advice-on-using-evolutionary-methods-to-solve-the-3", "pageUrlRelative": "/posts/GvCMck8LK9WaMKCyg/seeking-advice-on-using-evolutionary-methods-to-solve-the-3", "linkUrl": "https://www.lesswrong.com/posts/GvCMck8LK9WaMKCyg/seeking-advice-on-using-evolutionary-methods-to-solve-the-3", "postedAtFormatted": "Wednesday, August 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Seeking%20advice%20on%20using%20evolutionary%20methods%20to%20solve%20the%203-body%20problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeeking%20advice%20on%20using%20evolutionary%20methods%20to%20solve%20the%203-body%20problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGvCMck8LK9WaMKCyg%2Fseeking-advice-on-using-evolutionary-methods-to-solve-the-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Seeking%20advice%20on%20using%20evolutionary%20methods%20to%20solve%20the%203-body%20problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGvCMck8LK9WaMKCyg%2Fseeking-advice-on-using-evolutionary-methods-to-solve-the-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGvCMck8LK9WaMKCyg%2Fseeking-advice-on-using-evolutionary-methods-to-solve-the-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 190, "htmlBody": "<p>NOTE - I mean the 3-body problem in orbital mechanics, not in atomic physics.</p>\r\n<p>Hi there,</p>\r\n<p>Some recent discussions here on LW have led me to ponder the 3-body problem again.</p>\r\n<p>&nbsp;</p>\r\n<p><a href=\"http://en.wikipedia.org/wiki/N-body_problem\">http://en.wikipedia.org/wiki/N-body_problem</a></p>\r\n<p><a href=\"http://en.wikipedia.org/wiki/N-body_problem#General_considerations:_solving_the_n-body_problem\">http://en.wikipedia.org/wiki/N-body_problem#General_considerations:_solving_the_n-body_problem</a></p>\r\n<p>&nbsp;</p>\r\n<p>I wonder if new and novel methods that exist today might be applied to solving the \"unsolvable\" 3-body problem.&nbsp;</p>\r\n<p>Specifically I'm wondering \"Can I create an evolutionary derived algorithm to solve equations of motion, and then can I continue on with it's evolution to solve the 3-body problem at the level of Sundman's slowly-converging series, and then can I continue on with it's evolution to come up with a closed-form solution to solve for the position of all the bodies in our solar system?</p>\r\n<p>Another question is \"What level of&nbsp;hyper-accurate model&nbsp;of the entire solar system would be needed?\"</p>\r\n<p>&nbsp;</p>\r\n<p>I think that Chaos Theory says this isn't possible.&nbsp; Let's suppose for the moment that Chaos Theory only exists because our models of the universe aren't accurate enough to be use to predict far into the future.</p>\r\n<p>Here's why I'm posting this to LW.&nbsp; I don't really even know where to start with answering these questions, but I bet the LWers can point me in the right direction.</p>\r\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GvCMck8LK9WaMKCyg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -4, "extendedScore": null, "score": 9.60170775254383e-07, "legacy": true, "legacyId": "18157", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-09T02:50:05.133Z", "modifiedAt": null, "url": null, "title": "Meetup : Longmont Colorado Meetup", "slug": "meetup-longmont-colorado-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gBHXEf6dcjyKzm24C/meetup-longmont-colorado-meetup", "pageUrlRelative": "/posts/gBHXEf6dcjyKzm24C/meetup-longmont-colorado-meetup", "linkUrl": "https://www.lesswrong.com/posts/gBHXEf6dcjyKzm24C/meetup-longmont-colorado-meetup", "postedAtFormatted": "Thursday, August 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Longmont%20Colorado%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Longmont%20Colorado%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgBHXEf6dcjyKzm24C%2Fmeetup-longmont-colorado-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Longmont%20Colorado%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgBHXEf6dcjyKzm24C%2Fmeetup-longmont-colorado-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgBHXEf6dcjyKzm24C%2Fmeetup-longmont-colorado-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 60, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/cr'>Longmont Colorado Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 August 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Luna Cafe, 800 Coffman St, Longmont, CO 80501</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Trying a new location, as an experiment.</p>\n\n<p>All Denver and Boulder Less Wrongers are welcome to join the Fort Collins group for coffee, conversation, and a walk in the park.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/cr'>Longmont Colorado Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gBHXEf6dcjyKzm24C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 9.602618811428136e-07, "legacy": true, "legacyId": "18163", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Longmont_Colorado_Meetup\">Discussion article for the meetup : <a href=\"/meetups/cr\">Longmont Colorado Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 August 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Luna Cafe, 800 Coffman St, Longmont, CO 80501</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Trying a new location, as an experiment.</p>\n\n<p>All Denver and Boulder Less Wrongers are welcome to join the Fort Collins group for coffee, conversation, and a walk in the park.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Longmont_Colorado_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/cr\">Longmont Colorado Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Longmont Colorado Meetup", "anchor": "Discussion_article_for_the_meetup___Longmont_Colorado_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Longmont Colorado Meetup", "anchor": "Discussion_article_for_the_meetup___Longmont_Colorado_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-09T06:48:03.849Z", "modifiedAt": null, "url": null, "title": "Mentioning cryonics to a dying person", "slug": "mentioning-cryonics-to-a-dying-person", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:38.359Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielH", "createdAt": "2012-07-11T02:04:18.876Z", "isAdmin": false, "displayName": "DanielH"}, "userId": "vpzKspDLmWPnZL2fc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sSYmGhBpjTG88Q4FW/mentioning-cryonics-to-a-dying-person", "pageUrlRelative": "/posts/sSYmGhBpjTG88Q4FW/mentioning-cryonics-to-a-dying-person", "linkUrl": "https://www.lesswrong.com/posts/sSYmGhBpjTG88Q4FW/mentioning-cryonics-to-a-dying-person", "postedAtFormatted": "Thursday, August 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mentioning%20cryonics%20to%20a%20dying%20person&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMentioning%20cryonics%20to%20a%20dying%20person%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsSYmGhBpjTG88Q4FW%2Fmentioning-cryonics-to-a-dying-person%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mentioning%20cryonics%20to%20a%20dying%20person%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsSYmGhBpjTG88Q4FW%2Fmentioning-cryonics-to-a-dying-person", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsSYmGhBpjTG88Q4FW%2Fmentioning-cryonics-to-a-dying-person", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 339, "htmlBody": "<p>My paternal grandmother is dying of cancer (not brain cancer). She is still relatively healthy, and is taking chemo, but there is little hope of remission (and even if that does happen, she'll probably die of heart failure fairly soon). Her current plan is to be cremated and have the ashes buried in a graveyard (in my opinion, the worst of both of the \"standard\" approaches, but that's not the point of this post).</p>\n<p>I would prefer if she were cryopreserved, but am unsure how to even begin to broach the subject. I also have no idea how to convince her. She is not particularly religious, but is concerned with leaving as much money for my grandfather (and later my parents and me) as possible. I have previously discussed cryonics with my parents; my father brushed off the idea and my mom looked into it but dismissed the idea because the future isn't likely to want her (I find this argument ridiculous on several grounds). This means that I can't count on them to help talk to my grandmother. I may be able to talk to my grandfather first, but this would probably not be much of an asset: he is into several different conspiracy theories (the most recent ones center around the world secretly being controlled by the \"elites\" who use the U.S. President, U.K. Prime Minister, etc. as figurative puppets), but my grandmother doesn't seem to believe these and probably wouldn't listen much to his talk of cryonics either.</p>\n<p>Any suggestions of how to broach the topic or convince her once the topic is broached would be appreciated. I am currently at my grandparents' house, but am leaving less than a day after posting this (most of which will be spent at the local nighttime, and thus asleep). I would prefer not to upset her, both for obvious reasons and because I may not be able to bring myself to bring it up on the day we depart if it will cause us to leave on a bad note.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sSYmGhBpjTG88Q4FW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 11, "extendedScore": null, "score": 9.603782494197403e-07, "legacy": true, "legacyId": "18179", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 73, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-09T07:19:30.480Z", "modifiedAt": null, "url": null, "title": "From the \"weird math questions\" department...", "slug": "from-the-weird-math-questions-department", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:04.345Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CronoDAS", "createdAt": "2009-02-27T04:42:19.587Z", "isAdmin": false, "displayName": "CronoDAS"}, "userId": "Q2oaNonArzibx5cQN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZNqQ2DsBptwn8emtn/from-the-weird-math-questions-department", "pageUrlRelative": "/posts/ZNqQ2DsBptwn8emtn/from-the-weird-math-questions-department", "linkUrl": "https://www.lesswrong.com/posts/ZNqQ2DsBptwn8emtn/from-the-weird-math-questions-department", "postedAtFormatted": "Thursday, August 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20From%20the%20%22weird%20math%20questions%22%20department...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFrom%20the%20%22weird%20math%20questions%22%20department...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZNqQ2DsBptwn8emtn%2Ffrom-the-weird-math-questions-department%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=From%20the%20%22weird%20math%20questions%22%20department...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZNqQ2DsBptwn8emtn%2Ffrom-the-weird-math-questions-department", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZNqQ2DsBptwn8emtn%2Ffrom-the-weird-math-questions-department", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 156, "htmlBody": "<p>Here's something I've been wondering about, in the context of Solomonoff induction and uncomputable sequences.</p>\n<p>I have a device that is either a halting oracle, or an ordinary Turing machine which gives the correct answer to the halting problem for all programs smaller than some finite length N but always outputs \"does not halt\" when asked to evaluate programs larger than N. If you don't know what N is and you don't have infinite time, is there a way to tell the difference between the actual halting oracle (which gives correct answers for all possible programs) and a \"fake\" halting oracle which starts giving wrong answers for some N that just happens to be larger than any program that you've tested so far?</p>\n<p>The Kolmogorov complexity of an uncomputable sequence is infinite, so Solomonoff induction assigns it a probability of zero, but there's always a computable number with less than epsilon error, so would this ever actually matter?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb25c": 2, "bTeiZr6YAEaSPQTC8": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZNqQ2DsBptwn8emtn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 7, "extendedScore": null, "score": 9.60393626912394e-07, "legacy": true, "legacyId": "18180", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-09T13:42:19.982Z", "modifiedAt": null, "url": null, "title": "Solving the two envelopes problem", "slug": "solving-the-two-envelopes-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:27.601Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rstarkov", "createdAt": "2011-02-21T19:53:18.490Z", "isAdmin": false, "displayName": "rstarkov"}, "userId": "tsHG32deqwFhnQ2sx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GezzauYzGTkcwgkA7/solving-the-two-envelopes-problem", "pageUrlRelative": "/posts/GezzauYzGTkcwgkA7/solving-the-two-envelopes-problem", "linkUrl": "https://www.lesswrong.com/posts/GezzauYzGTkcwgkA7/solving-the-two-envelopes-problem", "postedAtFormatted": "Thursday, August 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Solving%20the%20two%20envelopes%20problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASolving%20the%20two%20envelopes%20problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGezzauYzGTkcwgkA7%2Fsolving-the-two-envelopes-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Solving%20the%20two%20envelopes%20problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGezzauYzGTkcwgkA7%2Fsolving-the-two-envelopes-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGezzauYzGTkcwgkA7%2Fsolving-the-two-envelopes-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1323, "htmlBody": "<p>Suppose you are presented with a game. You are given a red and a blue envelope with some money in each. You are allowed to ask an independent party to open both envelopes, and tell you the ratio of blue:red amounts (but not the actual amounts). If you do, the game master replaces the envelopes, and the amounts inside are chosen by him using the same algorithm as before.</p>\n<p>You ask the independent observer to check the amounts a million times, and find that half the time the ratio is 2 (blue has twice as much as red), and half the time it's 0.5 (red has twice as much as blue). At this point, the game master discloses that in fact, the way he chooses the amounts mathematically guarantees that these probabilities hold.</p>\n<p>Which envelope should you pick to maximize your expected wealth?</p>\n<p>It may seem surprising, but with this set-up, the game master can choose to make either red or blue have a higher expected amount of money in it, or make the two the same. Asking the independent party as described above will not help you establish which is which. This is the surprising part and is, in my opinion, the crux of the two envelopes problem.</p>\n<p><a id=\"more\"></a></p>\n<p>This is not quite how the <a href=\"http://en.wikipedia.org/wiki/Two_envelopes_problem\">two envelopes problem</a> is usually presented, but this is the presentation I arrived at after contemplating the original puzzle. The original puzzle prescribes a specific strategy that the game master follows, makes the envelopes indistinguishable, and provides a paradoxical argument which is obviously false, but it's not so obvious where it goes wrong.</p>\n<p>Note that for simplicity, let's assume that money is a real quantity and can be subdivided indefinitely. This avoids the problem of odd amounts like $1.03 not being exactly divisible by two.</p>\n<h3>The flawed argument</h3>\n<p>The flawed argument goes as follows. Let's call the amount in the blue envelope B, and in red R. You have confirmed that half the time, B is equal to 2R, and half the time it's R/2. This is a fact. Surely then the expected value of B is (2R * 50% + R/2 * 50%), which simplifies to 1.25R. In other words, the blue envelope has a higher expected amount of money given the evidence we have.</p>\n<p>But notice that the situation is completely symmetric. From the information you have, it's also obvious that half the time, R is 2B, and half the time it's B/2. So by the same argument the expected value of R is 1.25B. Uh-oh. The expected value of both envelopes is higher than the other?...</p>\n<h3>Game master strategies</h3>\n<p>Let's muddy up the water a little by considering the strategies the game master can use to pick the amounts for each envelope.</p>\n<h5>Strategy 1</h5>\n<p>Pick an amount X between $1 and $1000 randomly. Throw a fair die. If you get an odd number, put X into the red envelope and 2X into blue. Otherwise put X into blue and 2X into red.</p>\n<h5>Strategy 2</h5>\n<p>Pick an amount X between $1 and $1000 randomly. Put this into the red envelope. Throw a fair die. If you get an odd number, put 2X into blue, and if it's even, put X/2 into blue.</p>\n<p>The difference between these strategies is fairly subtle. I hope it's sufficiently obvious that the <em>\"ratio condition\"</em> (B = 2R half the time and R = 2B the other half) is true for both strategies. However, suppose we have two people take part in this game, one always picking the red envelope and the other always picking the blue envelope. After a million repetitions of this game, with the first strategy, the two guys will have won almost exactly the same amounts in total. After a million repetitions with the second strategy, the total amount won by the blue guy will be <em>25% higher</em> than the total amount won by the red guy!</p>\n<p>Now observe that strategy 2 can be trivially inverted to favour the red envelope instead of the blue one. The player can ask an independent observer for ratios (as described in the introduction) all he wants, but this information will not allow him to distinguish between these three scenarios (strategy 1, strategy 2 and strategy 2 inverted). It's obviously impossible to figure out which envelope has a higher expected winnings from this information!</p>\n<h3>What's going on here?</h3>\n<p>I hope I've convinced you by now that the information about the likelihood of the ratios does not tell you which envelope is better. But what <em>exactly</em> is the flaw in the original argument?</p>\n<p>Let's formalize the puzzle a bit. We have two random variables, R and B. We are permitted to ask someone to sample each one and compute the ratio of the samples, r/b, and disclose it to us. Let's define a random variable called RB whose samples are produced by sampling R and B and computing their ratio. We know that RB can take two values, 2 and 0.5, with equal probability. Let's also define BR, which is the opposite ratio: that of a sample of B to a sample of R. BR can also take two values, 2 and 0.5, with equal probability.</p>\n<p>The flawed argument is simply that the expected value of RB, E(RB), is 1.25, which is greater than 1, and therefore E(R) &gt; E(B). The flawed argument continues that E(BR) is 1.25 too, therefore E(B) &gt; E(R), leading to a contradiction. What's the flaw?</p>\n<h3>Solution</h3>\n<p>The expected value of RB, E(RB), really is 1.25. The puzzle gets that one right. E(BR) is <em>also</em> 1.25. The flaw in the argument is simply that it assumes E(X/Y) &gt; 1 implies that E(X) &gt; E(Y). This implication seems to hold intuitively, but human intuition is notoriously bad at probabilities. It is easy to prove that this implication is false, by considering a simple counter-example courtesy of <span class=\"comment-author\"><span class=\"author\"><a id=\"author_t1_75dh\" href=\"/user/VincentYu/\">VincentYu</a>.</span></span></p>\n<p><span class=\"comment-author\"><span class=\"author\">Consider two independent random variables, X and Y. X can take values 20 and 60, while Y can take values 2 and 100, both with equal probability. To calculate the expected value of X/Y, one can enumerate all possible combinations, multiplying each by its probability. The four possible combinations of X and Y are 20/2, 20/100, 60/2 and 60/100. Each combination is 25% likely. Hence E(X/Y) is 10.2. This is greater than 1, so the if the implication were to hold, E(X) should be greater than E(Y). But E(X) is (20+60)/2 = 40, while E(Y) is (2+100)/2 = 51. Hence, the implication E(X/Y) &gt; 1 =&gt; E(X) &gt; E(Y) does not hold in general.<br /></span></span></p>\n<p>So there you have it. The proposed argument relies on an implication which seems true intuitively, but turns out to be false under scrutiny. Mystery solved?... Almost.</p>\n<h5>Imprecise language's contribution to the puzzle<br /></h5>\n<p>The argument concerning the original, indistinguishable envelopes, is phrased like this: <em>\"(1) I denote by A the amount in my selected envelope. (2) The other envelope may contain either 2A or A/2, with a 50% probability each. (3) So the expected value of the money in the other envelope is 1.25A. (4) Hence, the other envelope is expected to have more dollars.\"</em></p>\n<p>Depending on how pedantic you are, you might say that the statement made in the third sentence is strictly false, or that it is too ambiguous to be strictly false, or that at least one interpretation is true. The expected value 1.25A is <em>\"of the amount of money contained in the other envelope expressed in terms of the amount of money in this envelope\"</em>. It is <strong><em>not</em> </strong><em>\"of the amount of money in the other envelope expressed in dollars\"</em>. Hence the last sentence does not follow, and if the statements were made in full and with complete accuracy, the fact that it does not follow is a little bit more obvious.</p>\n<p>In closing, I would say this puzzle is hard because \"in terms of this envelope\" and \"in terms of dollars\" are typically equivalent enough in everyday life, but when it comes to expected values, this equivalence breaks down rather counter-intuitively.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"L3NcKBNTvQaFXwv9u": 1, "dPPATLhRmhdJtJM2t": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GezzauYzGTkcwgkA7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 45, "extendedScore": null, "score": 0.000126, "legacy": true, "legacyId": "18081", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Suppose you are presented with a game. You are given a red and a blue envelope with some money in each. You are allowed to ask an independent party to open both envelopes, and tell you the ratio of blue:red amounts (but not the actual amounts). If you do, the game master replaces the envelopes, and the amounts inside are chosen by him using the same algorithm as before.</p>\n<p>You ask the independent observer to check the amounts a million times, and find that half the time the ratio is 2 (blue has twice as much as red), and half the time it's 0.5 (red has twice as much as blue). At this point, the game master discloses that in fact, the way he chooses the amounts mathematically guarantees that these probabilities hold.</p>\n<p>Which envelope should you pick to maximize your expected wealth?</p>\n<p>It may seem surprising, but with this set-up, the game master can choose to make either red or blue have a higher expected amount of money in it, or make the two the same. Asking the independent party as described above will not help you establish which is which. This is the surprising part and is, in my opinion, the crux of the two envelopes problem.</p>\n<p><a id=\"more\"></a></p>\n<p>This is not quite how the <a href=\"http://en.wikipedia.org/wiki/Two_envelopes_problem\">two envelopes problem</a> is usually presented, but this is the presentation I arrived at after contemplating the original puzzle. The original puzzle prescribes a specific strategy that the game master follows, makes the envelopes indistinguishable, and provides a paradoxical argument which is obviously false, but it's not so obvious where it goes wrong.</p>\n<p>Note that for simplicity, let's assume that money is a real quantity and can be subdivided indefinitely. This avoids the problem of odd amounts like $1.03 not being exactly divisible by two.</p>\n<h3 id=\"The_flawed_argument\">The flawed argument</h3>\n<p>The flawed argument goes as follows. Let's call the amount in the blue envelope B, and in red R. You have confirmed that half the time, B is equal to 2R, and half the time it's R/2. This is a fact. Surely then the expected value of B is (2R * 50% + R/2 * 50%), which simplifies to 1.25R. In other words, the blue envelope has a higher expected amount of money given the evidence we have.</p>\n<p>But notice that the situation is completely symmetric. From the information you have, it's also obvious that half the time, R is 2B, and half the time it's B/2. So by the same argument the expected value of R is 1.25B. Uh-oh. The expected value of both envelopes is higher than the other?...</p>\n<h3 id=\"Game_master_strategies\">Game master strategies</h3>\n<p>Let's muddy up the water a little by considering the strategies the game master can use to pick the amounts for each envelope.</p>\n<h5>Strategy 1</h5>\n<p>Pick an amount X between $1 and $1000 randomly. Throw a fair die. If you get an odd number, put X into the red envelope and 2X into blue. Otherwise put X into blue and 2X into red.</p>\n<h5>Strategy 2</h5>\n<p>Pick an amount X between $1 and $1000 randomly. Put this into the red envelope. Throw a fair die. If you get an odd number, put 2X into blue, and if it's even, put X/2 into blue.</p>\n<p>The difference between these strategies is fairly subtle. I hope it's sufficiently obvious that the <em>\"ratio condition\"</em> (B = 2R half the time and R = 2B the other half) is true for both strategies. However, suppose we have two people take part in this game, one always picking the red envelope and the other always picking the blue envelope. After a million repetitions of this game, with the first strategy, the two guys will have won almost exactly the same amounts in total. After a million repetitions with the second strategy, the total amount won by the blue guy will be <em>25% higher</em> than the total amount won by the red guy!</p>\n<p>Now observe that strategy 2 can be trivially inverted to favour the red envelope instead of the blue one. The player can ask an independent observer for ratios (as described in the introduction) all he wants, but this information will not allow him to distinguish between these three scenarios (strategy 1, strategy 2 and strategy 2 inverted). It's obviously impossible to figure out which envelope has a higher expected winnings from this information!</p>\n<h3 id=\"What_s_going_on_here_\">What's going on here?</h3>\n<p>I hope I've convinced you by now that the information about the likelihood of the ratios does not tell you which envelope is better. But what <em>exactly</em> is the flaw in the original argument?</p>\n<p>Let's formalize the puzzle a bit. We have two random variables, R and B. We are permitted to ask someone to sample each one and compute the ratio of the samples, r/b, and disclose it to us. Let's define a random variable called RB whose samples are produced by sampling R and B and computing their ratio. We know that RB can take two values, 2 and 0.5, with equal probability. Let's also define BR, which is the opposite ratio: that of a sample of B to a sample of R. BR can also take two values, 2 and 0.5, with equal probability.</p>\n<p>The flawed argument is simply that the expected value of RB, E(RB), is 1.25, which is greater than 1, and therefore E(R) &gt; E(B). The flawed argument continues that E(BR) is 1.25 too, therefore E(B) &gt; E(R), leading to a contradiction. What's the flaw?</p>\n<h3 id=\"Solution\">Solution</h3>\n<p>The expected value of RB, E(RB), really is 1.25. The puzzle gets that one right. E(BR) is <em>also</em> 1.25. The flaw in the argument is simply that it assumes E(X/Y) &gt; 1 implies that E(X) &gt; E(Y). This implication seems to hold intuitively, but human intuition is notoriously bad at probabilities. It is easy to prove that this implication is false, by considering a simple counter-example courtesy of <span class=\"comment-author\"><span class=\"author\"><a id=\"author_t1_75dh\" href=\"/user/VincentYu/\">VincentYu</a>.</span></span></p>\n<p><span class=\"comment-author\"><span class=\"author\">Consider two independent random variables, X and Y. X can take values 20 and 60, while Y can take values 2 and 100, both with equal probability. To calculate the expected value of X/Y, one can enumerate all possible combinations, multiplying each by its probability. The four possible combinations of X and Y are 20/2, 20/100, 60/2 and 60/100. Each combination is 25% likely. Hence E(X/Y) is 10.2. This is greater than 1, so the if the implication were to hold, E(X) should be greater than E(Y). But E(X) is (20+60)/2 = 40, while E(Y) is (2+100)/2 = 51. Hence, the implication E(X/Y) &gt; 1 =&gt; E(X) &gt; E(Y) does not hold in general.<br></span></span></p>\n<p>So there you have it. The proposed argument relies on an implication which seems true intuitively, but turns out to be false under scrutiny. Mystery solved?... Almost.</p>\n<h5>Imprecise language's contribution to the puzzle<br></h5>\n<p>The argument concerning the original, indistinguishable envelopes, is phrased like this: <em>\"(1) I denote by A the amount in my selected envelope. (2) The other envelope may contain either 2A or A/2, with a 50% probability each. (3) So the expected value of the money in the other envelope is 1.25A. (4) Hence, the other envelope is expected to have more dollars.\"</em></p>\n<p>Depending on how pedantic you are, you might say that the statement made in the third sentence is strictly false, or that it is too ambiguous to be strictly false, or that at least one interpretation is true. The expected value 1.25A is <em>\"of the amount of money contained in the other envelope expressed in terms of the amount of money in this envelope\"</em>. It is <strong><em>not</em> </strong><em>\"of the amount of money in the other envelope expressed in dollars\"</em>. Hence the last sentence does not follow, and if the statements were made in full and with complete accuracy, the fact that it does not follow is a little bit more obvious.</p>\n<p>In closing, I would say this puzzle is hard because \"in terms of this envelope\" and \"in terms of dollars\" are typically equivalent enough in everyday life, but when it comes to expected values, this equivalence breaks down rather counter-intuitively.</p>", "sections": [{"title": "The flawed argument", "anchor": "The_flawed_argument", "level": 1}, {"title": "Game master strategies", "anchor": "Game_master_strategies", "level": 1}, {"title": "What's going on here?", "anchor": "What_s_going_on_here_", "level": 1}, {"title": "Solution", "anchor": "Solution", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "33 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-10T02:55:00.199Z", "modifiedAt": null, "url": null, "title": "Meetup : Board Games \"Seattle\"", "slug": "meetup-board-games-seattle", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GuySrinivasan", "createdAt": "2009-02-27T21:00:32.986Z", "isAdmin": false, "displayName": "GuySrinivasan"}, "userId": "HMnfd9HdRCfuRcdBG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ixnRThNFsPfdxkgLS/meetup-board-games-seattle", "pageUrlRelative": "/posts/ixnRThNFsPfdxkgLS/meetup-board-games-seattle", "linkUrl": "https://www.lesswrong.com/posts/ixnRThNFsPfdxkgLS/meetup-board-games-seattle", "postedAtFormatted": "Friday, August 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Board%20Games%20%22Seattle%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Board%20Games%20%22Seattle%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FixnRThNFsPfdxkgLS%2Fmeetup-board-games-seattle%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Board%20Games%20%22Seattle%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FixnRThNFsPfdxkgLS%2Fmeetup-board-games-seattle", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FixnRThNFsPfdxkgLS%2Fmeetup-board-games-seattle", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 60, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/cs\">Board Games</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">11 August 2012 02:00:00PM (-0700)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Redmond, WA, USA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Play board games. Have fun. Have fun while playing board games. The agenda is straightforward. Bring friends or games if you like. Here's the thread with more details including an address, ride info, and the number 3^^^3: https://groups.google.com/forum/?fromgroups#!topic/lw-seattle/3KatvMGOe8A%5B1-25%5D</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/cs\">Board Games</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ixnRThNFsPfdxkgLS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 9.609688113084865e-07, "legacy": true, "legacyId": "18183", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Board_Games\">Discussion article for the meetup : <a href=\"/meetups/cs\">Board Games</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">11 August 2012 02:00:00PM (-0700)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Redmond, WA, USA</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Play board games. Have fun. Have fun while playing board games. The agenda is straightforward. Bring friends or games if you like. Here's the thread with more details including an address, ride info, and the number 3^^^3: https://groups.google.com/forum/?fromgroups#!topic/lw-seattle/3KatvMGOe8A%5B1-25%5D</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Board_Games1\">Discussion article for the meetup : <a href=\"/meetups/cs\">Board Games</a></h2>", "sections": [{"title": "Discussion article for the meetup : Board Games", "anchor": "Discussion_article_for_the_meetup___Board_Games", "level": 1}, {"title": "Discussion article for the meetup : Board Games", "anchor": "Discussion_article_for_the_meetup___Board_Games1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-10T06:07:03.782Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Invisible Frameworks", "slug": "seq-rerun-invisible-frameworks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:02.969Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nZBAtKnxNRfpBb2c4/seq-rerun-invisible-frameworks", "pageUrlRelative": "/posts/nZBAtKnxNRfpBb2c4/seq-rerun-invisible-frameworks", "linkUrl": "https://www.lesswrong.com/posts/nZBAtKnxNRfpBb2c4/seq-rerun-invisible-frameworks", "postedAtFormatted": "Friday, August 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Invisible%20Frameworks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Invisible%20Frameworks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnZBAtKnxNRfpBb2c4%2Fseq-rerun-invisible-frameworks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Invisible%20Frameworks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnZBAtKnxNRfpBb2c4%2Fseq-rerun-invisible-frameworks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnZBAtKnxNRfpBb2c4%2Fseq-rerun-invisible-frameworks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 177, "htmlBody": "<p>Today's post, <a href=\"/lw/ta/invisible_frameworks/\">Invisible Frameworks</a> was originally published on 22 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Invisible_Frameworks\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A particular system of values is analyzed, and is used to demonstrate the idea that anytime you consider changing your morals, you do so using your own current meta-morals. Forget this at your peril.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/e04/seq_rerun_no_license_to_be_human/\">No License To Be Human</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nZBAtKnxNRfpBb2c4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.610628463856188e-07, "legacy": true, "legacyId": "18189", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sCs48JtMnQwQsZwyN", "t5omBrwZ7BKs7fviy", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-10T08:13:44.744Z", "modifiedAt": null, "url": null, "title": "[Link] Admitting to Bias", "slug": "link-admitting-to-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:33.796Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GLaDOS", "createdAt": "2011-04-26T20:59:08.539Z", "isAdmin": false, "displayName": "GLaDOS"}, "userId": "wdPp4B7WGssb2gHwP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/H7iNv58YKmzjoAd68/link-admitting-to-bias", "pageUrlRelative": "/posts/H7iNv58YKmzjoAd68/link-admitting-to-bias", "linkUrl": "https://www.lesswrong.com/posts/H7iNv58YKmzjoAd68/link-admitting-to-bias", "postedAtFormatted": "Friday, August 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Admitting%20to%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Admitting%20to%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH7iNv58YKmzjoAd68%2Flink-admitting-to-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Admitting%20to%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH7iNv58YKmzjoAd68%2Flink-admitting-to-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH7iNv58YKmzjoAd68%2Flink-admitting-to-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1455, "htmlBody": "<p><strong>Summary:</strong><em> </em><strong>Current social psychology research is probably on average compromised by political bias leftward</strong>.<em> Conservative researchers are likely discriminated against in at least this field. More importantly papers and research that does not fit a liberal perspective faces greater barriers and burdens. &nbsp; </em></p>\n<p><a href=\"http://www.insidehighered.com/news/2012/08/08/survey-finds-social-psychologists-admit-anti-conservative-bias\">An article</a> in the online publication inside higher ed on a survey on anti-conservative bias among social psychologists.</p>\n<blockquote>\n<p>Numerous surveys have found that professors, especially those in some disciplines, are to the left of the general public. But those same -- and other -- surveys have rarely found evidence that left-leaning academics discriminate on the basis of politics. So to many academics, the question of ideological bias is not a big deal. Investment bankers may lean to the right, but that doesn't mean they don't provide good service (or as best the economy will permit) to clients of all political stripes, the argument goes.<br /> <br /> And professors should be assumed to have the same professionalism.</p>\n<p><strong>A new study, however, challenges that assumption -- at least in the field of social psychology. </strong>The study isn't due to be published until next month (in <em>Perspectives on Psychological Science</em>), and the authors and others are noting limitations to the study. But its findings of bias by social psychologists (even if just a decent-sized minority of them) are already getting considerable buzz in conservative circles. Just over 37 percent of those surveyed said that, given equally qualified candidates for a job, they would support the hiring of a liberal candidate over a conservative candidate. Smaller percentages agreed that a \"conservative perspective\" would negatively influence their odds of supporting a paper for inclusion in a journal or a proposal for a grant. (The final version of the paper is not yet available, but <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2002636\">an early version may be found</a> on the website of the Social Science Research Network.)</p>\n<p>To some on the right, such findings are hardly surprising. But to the authors, who expected to find lopsided political leanings, but not bias, the results were not what they expected.</p>\n<p><strong>\"The questions were pretty blatant. We didn't expect people would give those answers,\"</strong> said Yoel Inbar, a co-author, who is a visiting assistant professor at the Wharton School of the University of Pennsylvania, and an assistant professor of social psychology at Tilburg University, in the Netherlands.</p>\n<p>He said that the findings should concern academics. Of the bias he and a co-author found, he said, \"I don't think it's O.K.\"</p>\n<p>Discussion of faculty politics extends well beyond social psychology, and humanities professors are frequently accused of being \"tenured radicals\" (a label some wear with pride). But social psychology has had an intense debate over the issue in the last year.</p>\n<p>At the 2011 meeting of the Society for Personality and Social Psychology, <strong>Jonathan Haidt</strong> of the University of Virginia <a href=\"http://www.nytimes.com/2011/02/08/science/08tier.html\">polled the audience of some 1,000 in a convention center ballroom to ask how many were liberals</a> (the vast majority of hands went up), how many were centrists or libertarians (he counted a couple dozen or so), and how many were conservatives (three hands went up).<strong> In his talk, he said that the conference reflected \"a statistically impossible lack of diversity,&rdquo; </strong>in a country where 40 percent of Americans are conservative and only 20 percent are liberal.<strong> He said he worried about the discipline becoming a \"tribal-moral community\" in ways that hurt the field's credibility.</strong></p>\n</blockquote>\n<p>The link above is worth following. The problems that arise remind me of the situation with academic and our own ethics in light of <a href=\"/r/discussion/lw/78e/antisocial_personality_traits_predict_utilitarian/\">this paper</a>.</p>\n<blockquote>\n<p>That speech prompted the research that is about to be published. Members of a social psychologists' e-mail list were surveyed twice. (The group is not limited to American social scientists or faculty members, but about 90 percent are academics, including grad students, and more than 80 percent are Americans.) Not surprisingly, the overwhelming majority of those surveyed identified as liberal on social, foreign and economic policy, with the strongest conservative presence on economic policy. Only 6 percent described themselves as conservative over all.</p>\n<p>The questions on willingness to discriminate against conservatives were asked in two ways: what the respondents thought they would do, and what they thought their colleagues would do. The pool included conservatives (who presumably aren't discriminating against conservatives) so the liberal response rates may be a bit higher, Inbar said.</p>\n<p>The percentages below reflect those who gave a score of 4 or higher on a 7-point scale on how likely they would be to do something (with 4 being \"somewhat\" likely).</p>\n<p><strong>Percentages of Social Psychologists Who Would Be Biased in Various Ways</strong></p>\n<table style=\"width: 441px;\" border=\"1\" cellspacing=\"1\" cellpadding=\"1\">\n<tbody>\n<tr>\n<td style=\"width: 288px;\">&nbsp;</td>\n<td style=\"width: 64px; text-align: center;\"><strong>Self</strong></td>\n<td style=\"width: 64px; text-align: center;\"><strong>Colleagues</strong></td>\n</tr>\n<tr>\n<td style=\"width: 288px;\">A \"politically conservative perspective\" by author would have a negative influence on evaluation of a paper</td>\n<td style=\"width: 64px; text-align: right;\">18.6%</td>\n<td style=\"width: 64px; text-align: right;\">34.2%</td>\n</tr>\n<tr>\n<td style=\"width: 288px;\">A \"politically conservative perspective\" by author would have a negative influence on evaluation of a grant proposal</td>\n<td style=\"width: 64px; text-align: right;\">23.8%</td>\n<td style=\"width: 64px; text-align: right;\">36.9%</td>\n</tr>\n<tr>\n<td style=\"width: 288px;\">Would be reluctant to extend symposium invitation to a colleague who is \"politically quite conservative\"</td>\n<td style=\"width: 64px; text-align: right;\">14.0%</td>\n<td style=\"width: 64px; text-align: right;\">29.6%</td>\n</tr>\n<tr>\n<td style=\"width: 288px;\">Would vote for liberal over conservative job candidate if they were equally qualified</td>\n<td style=\"width: 64px; text-align: right;\">37.5%</td>\n<td style=\"width: 64px; text-align: right;\">44.1%</td>\n</tr>\n</tbody>\n</table>\n</blockquote>\n<p>I can't help but think that self-assessments are probably too generous. For predictive power of how an individual behaves when the behaviour in question is undesirable, I'm more likely to take their estimate of how \"colleagues\" behave than their estimate of how they personally do.&nbsp;</p>\n<blockquote>\n<p>The more liberal the survey respondents identified as being, the more likely they were to say that they would discriminate.</p>\n<p><strong>The paper notes surveys and statements by conservatives in the field saying that they are reluctant to speak out and says that \"they are right to do so,\"</strong> given the numbers of individuals who indicate they might be biased or that their colleagues might be biased in various ways.</p>\n<p>Inbar said that he has no idea if other fields would have similar results. And he stressed that the questions were hypothetical; the survey did not ask participants if they had actually done these things.</p>\n<p><strong>He said that the study also collected free responses from participants, and that conservative responses were consistent with the idea that there is bias out there. \"The responses included really egregious stuff, people being belittled by their advisers publicly for voting Republican.\"</strong></p>\n</blockquote>\n<p>This shouldn't be surprising to hear since to quote <a href=\"/lw/cz7/conspiracy_theories_as_agency_fictions/6slg\">CharlieSheen</a>: <strong></strong>\"we even have <a href=\"/user/James_Miller/\">LW posters</a> who have in academia personally experienced discrimination and harassment because of their right wing politics.\"</p>\n<blockquote>\n<p>Neil Gross, a professor of sociology at the University of British Columbia, urged caution about the results. Gross has written extensively on faculty political issues. He is the co-author of <a href=\"http://www.insidehighered.com/news/2007/10/08/politics\">a 2007 report</a> that found that while professors may lean left, they do so less than is imagined and less uniformly across institution type than is imagined.</p>\n<p>Gross said it was important to remember that the percentages saying they would discriminate in various ways are answering yes to a relatively low bar of \"somewhat.\" He also said that the numbers would have been \"more meaningful\" if they had asked about actual behavior by respondents in the last year, not the more general question of whether they might do these things.</p>\n<p>At the same time, he said that the numbers \"are higher than I would have expected.\" One theory Gross has is that the questions are \"picking up general political animosity as much as anything else.\"</p>\n<p>If you are wondering about the political leanings of the social psychologists who conducted the study, they are on the left. Inbar said he describes himself as \"a pretty doctrinaire liberal,\" who volunteered for the Obama campaign in 2008 and who votes Democrat. His co-author, Joris Lammers of Tilburg, is to Inbar's left, he said.</p>\n<p><strong>What most impressed him about the issues raised by the study, Inbar said, is the need to think about \"basic fairness.\"</strong></p>\n</blockquote>\n<p>While I can see Lammers' point that this as disturbing from a fairness perspective to people grinding their way through academia and should serve as warning for right wing LessWrong readers working through the system, <strong>I find the issue of how this our heavy reliance on academia for our map of reality might lead to us inheriting such distortions of the map of reality much more concerning.</strong> Overall in light of this if a widely accepted conclusion from social psychology favours a \"right wing\" perspective it is more likely to be correct than if no such biases against such perspectives existed. Conclusions that favour \"left wing\" perspective are also somewhat less likely to be true than if no such biases existed. We should update accordingly.</p>\n<p>I also think there are reasons to think <a href=\"/r/discussion/lw/e1b/link_admitting_to_bias/76ht\">we may have similar problems on this site</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "H7iNv58YKmzjoAd68", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 22, "extendedScore": null, "score": 6.4e-05, "legacy": true, "legacyId": "18191", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Summary:</strong><em> </em><strong>Current social psychology research is probably on average compromised by political bias leftward</strong>.<em> Conservative researchers are likely discriminated against in at least this field. More importantly papers and research that does not fit a liberal perspective faces greater barriers and burdens. &nbsp; </em></p>\n<p><a href=\"http://www.insidehighered.com/news/2012/08/08/survey-finds-social-psychologists-admit-anti-conservative-bias\">An article</a> in the online publication inside higher ed on a survey on anti-conservative bias among social psychologists.</p>\n<blockquote>\n<p>Numerous surveys have found that professors, especially those in some disciplines, are to the left of the general public. But those same -- and other -- surveys have rarely found evidence that left-leaning academics discriminate on the basis of politics. So to many academics, the question of ideological bias is not a big deal. Investment bankers may lean to the right, but that doesn't mean they don't provide good service (or as best the economy will permit) to clients of all political stripes, the argument goes.<br> <br> And professors should be assumed to have the same professionalism.</p>\n<p><strong>A new study, however, challenges that assumption -- at least in the field of social psychology. </strong>The study isn't due to be published until next month (in <em>Perspectives on Psychological Science</em>), and the authors and others are noting limitations to the study. But its findings of bias by social psychologists (even if just a decent-sized minority of them) are already getting considerable buzz in conservative circles. Just over 37 percent of those surveyed said that, given equally qualified candidates for a job, they would support the hiring of a liberal candidate over a conservative candidate. Smaller percentages agreed that a \"conservative perspective\" would negatively influence their odds of supporting a paper for inclusion in a journal or a proposal for a grant. (The final version of the paper is not yet available, but <a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2002636\">an early version may be found</a> on the website of the Social Science Research Network.)</p>\n<p>To some on the right, such findings are hardly surprising. But to the authors, who expected to find lopsided political leanings, but not bias, the results were not what they expected.</p>\n<p><strong>\"The questions were pretty blatant. We didn't expect people would give those answers,\"</strong> said Yoel Inbar, a co-author, who is a visiting assistant professor at the Wharton School of the University of Pennsylvania, and an assistant professor of social psychology at Tilburg University, in the Netherlands.</p>\n<p>He said that the findings should concern academics. Of the bias he and a co-author found, he said, \"I don't think it's O.K.\"</p>\n<p>Discussion of faculty politics extends well beyond social psychology, and humanities professors are frequently accused of being \"tenured radicals\" (a label some wear with pride). But social psychology has had an intense debate over the issue in the last year.</p>\n<p>At the 2011 meeting of the Society for Personality and Social Psychology, <strong>Jonathan Haidt</strong> of the University of Virginia <a href=\"http://www.nytimes.com/2011/02/08/science/08tier.html\">polled the audience of some 1,000 in a convention center ballroom to ask how many were liberals</a> (the vast majority of hands went up), how many were centrists or libertarians (he counted a couple dozen or so), and how many were conservatives (three hands went up).<strong> In his talk, he said that the conference reflected \"a statistically impossible lack of diversity,\u201d </strong>in a country where 40 percent of Americans are conservative and only 20 percent are liberal.<strong> He said he worried about the discipline becoming a \"tribal-moral community\" in ways that hurt the field's credibility.</strong></p>\n</blockquote>\n<p>The link above is worth following. The problems that arise remind me of the situation with academic and our own ethics in light of <a href=\"/r/discussion/lw/78e/antisocial_personality_traits_predict_utilitarian/\">this paper</a>.</p>\n<blockquote>\n<p>That speech prompted the research that is about to be published. Members of a social psychologists' e-mail list were surveyed twice. (The group is not limited to American social scientists or faculty members, but about 90 percent are academics, including grad students, and more than 80 percent are Americans.) Not surprisingly, the overwhelming majority of those surveyed identified as liberal on social, foreign and economic policy, with the strongest conservative presence on economic policy. Only 6 percent described themselves as conservative over all.</p>\n<p>The questions on willingness to discriminate against conservatives were asked in two ways: what the respondents thought they would do, and what they thought their colleagues would do. The pool included conservatives (who presumably aren't discriminating against conservatives) so the liberal response rates may be a bit higher, Inbar said.</p>\n<p>The percentages below reflect those who gave a score of 4 or higher on a 7-point scale on how likely they would be to do something (with 4 being \"somewhat\" likely).</p>\n<p><strong id=\"Percentages_of_Social_Psychologists_Who_Would_Be_Biased_in_Various_Ways\">Percentages of Social Psychologists Who Would Be Biased in Various Ways</strong></p>\n<table style=\"width: 441px;\" border=\"1\" cellspacing=\"1\" cellpadding=\"1\">\n<tbody>\n<tr>\n<td style=\"width: 288px;\">&nbsp;</td>\n<td style=\"width: 64px; text-align: center;\"><strong>Self</strong></td>\n<td style=\"width: 64px; text-align: center;\"><strong>Colleagues</strong></td>\n</tr>\n<tr>\n<td style=\"width: 288px;\">A \"politically conservative perspective\" by author would have a negative influence on evaluation of a paper</td>\n<td style=\"width: 64px; text-align: right;\">18.6%</td>\n<td style=\"width: 64px; text-align: right;\">34.2%</td>\n</tr>\n<tr>\n<td style=\"width: 288px;\">A \"politically conservative perspective\" by author would have a negative influence on evaluation of a grant proposal</td>\n<td style=\"width: 64px; text-align: right;\">23.8%</td>\n<td style=\"width: 64px; text-align: right;\">36.9%</td>\n</tr>\n<tr>\n<td style=\"width: 288px;\">Would be reluctant to extend symposium invitation to a colleague who is \"politically quite conservative\"</td>\n<td style=\"width: 64px; text-align: right;\">14.0%</td>\n<td style=\"width: 64px; text-align: right;\">29.6%</td>\n</tr>\n<tr>\n<td style=\"width: 288px;\">Would vote for liberal over conservative job candidate if they were equally qualified</td>\n<td style=\"width: 64px; text-align: right;\">37.5%</td>\n<td style=\"width: 64px; text-align: right;\">44.1%</td>\n</tr>\n</tbody>\n</table>\n</blockquote>\n<p>I can't help but think that self-assessments are probably too generous. For predictive power of how an individual behaves when the behaviour in question is undesirable, I'm more likely to take their estimate of how \"colleagues\" behave than their estimate of how they personally do.&nbsp;</p>\n<blockquote>\n<p>The more liberal the survey respondents identified as being, the more likely they were to say that they would discriminate.</p>\n<p><strong>The paper notes surveys and statements by conservatives in the field saying that they are reluctant to speak out and says that \"they are right to do so,\"</strong> given the numbers of individuals who indicate they might be biased or that their colleagues might be biased in various ways.</p>\n<p>Inbar said that he has no idea if other fields would have similar results. And he stressed that the questions were hypothetical; the survey did not ask participants if they had actually done these things.</p>\n<p><strong id=\"He_said_that_the_study_also_collected_free_responses_from_participants__and_that_conservative_responses_were_consistent_with_the_idea_that_there_is_bias_out_there___The_responses_included_really_egregious_stuff__people_being_belittled_by_their_advisers_publicly_for_voting_Republican__\">He said that the study also collected free responses from participants, and that conservative responses were consistent with the idea that there is bias out there. \"The responses included really egregious stuff, people being belittled by their advisers publicly for voting Republican.\"</strong></p>\n</blockquote>\n<p>This shouldn't be surprising to hear since to quote <a href=\"/lw/cz7/conspiracy_theories_as_agency_fictions/6slg\">CharlieSheen</a>: <strong></strong>\"we even have <a href=\"/user/James_Miller/\">LW posters</a> who have in academia personally experienced discrimination and harassment because of their right wing politics.\"</p>\n<blockquote>\n<p>Neil Gross, a professor of sociology at the University of British Columbia, urged caution about the results. Gross has written extensively on faculty political issues. He is the co-author of <a href=\"http://www.insidehighered.com/news/2007/10/08/politics\">a 2007 report</a> that found that while professors may lean left, they do so less than is imagined and less uniformly across institution type than is imagined.</p>\n<p>Gross said it was important to remember that the percentages saying they would discriminate in various ways are answering yes to a relatively low bar of \"somewhat.\" He also said that the numbers would have been \"more meaningful\" if they had asked about actual behavior by respondents in the last year, not the more general question of whether they might do these things.</p>\n<p>At the same time, he said that the numbers \"are higher than I would have expected.\" One theory Gross has is that the questions are \"picking up general political animosity as much as anything else.\"</p>\n<p>If you are wondering about the political leanings of the social psychologists who conducted the study, they are on the left. Inbar said he describes himself as \"a pretty doctrinaire liberal,\" who volunteered for the Obama campaign in 2008 and who votes Democrat. His co-author, Joris Lammers of Tilburg, is to Inbar's left, he said.</p>\n<p><strong id=\"What_most_impressed_him_about_the_issues_raised_by_the_study__Inbar_said__is_the_need_to_think_about__basic_fairness__\">What most impressed him about the issues raised by the study, Inbar said, is the need to think about \"basic fairness.\"</strong></p>\n</blockquote>\n<p>While I can see Lammers' point that this as disturbing from a fairness perspective to people grinding their way through academia and should serve as warning for right wing LessWrong readers working through the system, <strong>I find the issue of how this our heavy reliance on academia for our map of reality might lead to us inheriting such distortions of the map of reality much more concerning.</strong> Overall in light of this if a widely accepted conclusion from social psychology favours a \"right wing\" perspective it is more likely to be correct than if no such biases against such perspectives existed. Conclusions that favour \"left wing\" perspective are also somewhat less likely to be true than if no such biases existed. We should update accordingly.</p>\n<p>I also think there are reasons to think <a href=\"/r/discussion/lw/e1b/link_admitting_to_bias/76ht\">we may have similar problems on this site</a>.</p>", "sections": [{"title": "Percentages of Social Psychologists Who Would Be Biased in Various Ways", "anchor": "Percentages_of_Social_Psychologists_Who_Would_Be_Biased_in_Various_Ways", "level": 1}, {"title": "He said that the study also collected free responses from participants, and that conservative responses were consistent with the idea that there is bias out there. \"The responses included really egregious stuff, people being belittled by their advisers publicly for voting Republican.\"", "anchor": "He_said_that_the_study_also_collected_free_responses_from_participants__and_that_conservative_responses_were_consistent_with_the_idea_that_there_is_bias_out_there___The_responses_included_really_egregious_stuff__people_being_belittled_by_their_advisers_publicly_for_voting_Republican__", "level": 1}, {"title": "What most impressed him about the issues raised by the study, Inbar said, is the need to think about \"basic fairness.\"", "anchor": "What_most_impressed_him_about_the_issues_raised_by_the_study__Inbar_said__is_the_need_to_think_about__basic_fairness__", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "131 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 131, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9cCT6DYYL4PWXi65n"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-10T14:44:09.378Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Cambridge UK, Copenhagen, Dublin, Pittsburgh, Washington DC", "slug": "weekly-lw-meetups-austin-cambridge-uk-copenhagen-dublin", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:01.453Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZyR5xsEfsEB43Fn4d/weekly-lw-meetups-austin-cambridge-uk-copenhagen-dublin", "pageUrlRelative": "/posts/ZyR5xsEfsEB43Fn4d/weekly-lw-meetups-austin-cambridge-uk-copenhagen-dublin", "linkUrl": "https://www.lesswrong.com/posts/ZyR5xsEfsEB43Fn4d/weekly-lw-meetups-austin-cambridge-uk-copenhagen-dublin", "postedAtFormatted": "Friday, August 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Cambridge%20UK%2C%20Copenhagen%2C%20Dublin%2C%20Pittsburgh%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Cambridge%20UK%2C%20Copenhagen%2C%20Dublin%2C%20Pittsburgh%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZyR5xsEfsEB43Fn4d%2Fweekly-lw-meetups-austin-cambridge-uk-copenhagen-dublin%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Cambridge%20UK%2C%20Copenhagen%2C%20Dublin%2C%20Pittsburgh%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZyR5xsEfsEB43Fn4d%2Fweekly-lw-meetups-austin-cambridge-uk-copenhagen-dublin", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZyR5xsEfsEB43Fn4d%2Fweekly-lw-meetups-austin-cambridge-uk-copenhagen-dublin", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 422, "htmlBody": "<p><strong>This summary was posted to LW Main on August 3rd. It has now been moved to discussion.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/c1\">Dublin, Ireland Meetup:&nbsp;<span class=\"date\">05 August 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/c9\">Third Copenhagen Meet-up.:&nbsp;<span class=\"date\">05 August 2012 04:00PM</span></a></li>\n<li><a href=\"/meetups/c4\">Washington DC Biased Boardgames meetup:&nbsp;<span class=\"date\">05 August 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/cl\">Pittsburgh: Semantic Stopsigns:&nbsp;<span class=\"date\">08 August 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/c3\">Brussels meetup:&nbsp;<span class=\"date\">11 August 2012 12:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">04 August 2012 01:30PM</span></a></li>\n<li><a href=\"/meetups/cd\">Cambridge (MA) Meetup:&nbsp;<span class=\"date\">05 August 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/cf\">Cambridge (MA) Meetup:&nbsp;<span class=\"date\">19 August 2012 02:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong>&nbsp;</strong><strong></strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZyR5xsEfsEB43Fn4d", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.613161033940678e-07, "legacy": true, "legacyId": "18061", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-10T18:05:52.417Z", "modifiedAt": null, "url": null, "title": "Opinions on Boltzmann brain arguments constraining modern multiverse theories", "slug": "opinions-on-boltzmann-brain-arguments-constraining-modern", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:03.833Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6WJfXysMGqj4DH7DM/opinions-on-boltzmann-brain-arguments-constraining-modern", "pageUrlRelative": "/posts/6WJfXysMGqj4DH7DM/opinions-on-boltzmann-brain-arguments-constraining-modern", "linkUrl": "https://www.lesswrong.com/posts/6WJfXysMGqj4DH7DM/opinions-on-boltzmann-brain-arguments-constraining-modern", "postedAtFormatted": "Friday, August 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Opinions%20on%20Boltzmann%20brain%20arguments%20constraining%20modern%20multiverse%20theories&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpinions%20on%20Boltzmann%20brain%20arguments%20constraining%20modern%20multiverse%20theories%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6WJfXysMGqj4DH7DM%2Fopinions-on-boltzmann-brain-arguments-constraining-modern%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Opinions%20on%20Boltzmann%20brain%20arguments%20constraining%20modern%20multiverse%20theories%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6WJfXysMGqj4DH7DM%2Fopinions-on-boltzmann-brain-arguments-constraining-modern", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6WJfXysMGqj4DH7DM%2Fopinions-on-boltzmann-brain-arguments-constraining-modern", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<p>Leading inflationary multiverse theorist Andrei Linde (and others) gives significant enough credence to the Boltzmann brain problem to tweak some of the parameters of his theory (here for example&nbsp;<a href=\"http://arxiv.org/abs/hep-th/0611043\">http://arxiv.org/abs/hep-th/0611043</a>). Are there more elegant ways to deal with the issue that LWs are aware of that would not require changing physics to&nbsp;accommodate?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6WJfXysMGqj4DH7DM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 2, "extendedScore": null, "score": 9.614149305987002e-07, "legacy": true, "legacyId": "18195", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-10T18:45:34.081Z", "modifiedAt": null, "url": null, "title": "Subsuming Purpose, Part 1", "slug": "subsuming-purpose-part-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:03.292Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OrphanWilde", "createdAt": "2012-06-21T18:24:36.749Z", "isAdmin": false, "displayName": "OrphanWilde"}, "userId": "cdW87AS6fdK2sywL2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rcHds7yLm3WWd7EPX/subsuming-purpose-part-1", "pageUrlRelative": "/posts/rcHds7yLm3WWd7EPX/subsuming-purpose-part-1", "linkUrl": "https://www.lesswrong.com/posts/rcHds7yLm3WWd7EPX/subsuming-purpose-part-1", "postedAtFormatted": "Friday, August 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Subsuming%20Purpose%2C%20Part%201&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASubsuming%20Purpose%2C%20Part%201%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrcHds7yLm3WWd7EPX%2Fsubsuming-purpose-part-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Subsuming%20Purpose%2C%20Part%201%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrcHds7yLm3WWd7EPX%2Fsubsuming-purpose-part-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrcHds7yLm3WWd7EPX%2Fsubsuming-purpose-part-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 698, "htmlBody": "<p>&nbsp;</p>\n<h4>Summary:</h4>\n<p>The purpose of this entry is to establish the existence of local equilibriums which introduce deviations from an ends-driven organization (an organization whose primary focus is a particular purpose) to transform it into a means-driven organization (an organization whose primary focus is the means to achieve its purpose, rather than the purpose itself).</p>\n<h4>Subsuming Purpose, Part 1</h4>\n<p>Imagine you run a charity, and you have two star employees; one shares your goals without any emphasis on a means, the other believes in the cause but believes firmly in fundraising as the best means to that end. &nbsp;Both contribute to your charity, but the fundraiser does more good overall. &nbsp;The fundraiser enables your organization. &nbsp;Who do you set as your successor?</p>\n<p>Who will your successor choose as their successor?</p>\n<p>The person who believes in the purpose will choose the best person for achieving that purpose. &nbsp;The person who believes in a specific means to achieve that ends will choose the best person for those means. &nbsp;The means will subsume the ends. &nbsp;A person who values specific means, say, fundraising, is more likely to promote fellow fundraisers; he values their contributions more. &nbsp;Specialists, and in particular the lines of thinking which lead to specialization, create rigidity in the organization.</p>\n<p>Suppose that you choose the fundraiser. &nbsp;The fundraiser, by dint of having chosen to specialize in fundraising, probably believes that fundraising is more important than the alternative means of supporting the organization: he will probably choose to promote other effective fundraisers over their alternatives.</p>\n<p>And now people who don't agree that fundraising will start protesting, seeing their charity becoming increasingly subverted; fundraising is rewarded over the charitable purpose of the organization. &nbsp;They will leave, or protest; if their protests aren't heeded, for example because fundraisers who believe in fundraising do already run the company, they may be marginalized. &nbsp;Such individuals may be selected out, either self-selectively, or by explicit opposition by management to introducing people who are likely to cause trouble for them in the future.</p>\n<p><strong>Generalized</strong>:</p>\n<p>In the example above, I made one particular assumption: That somebody who possesses some choice-driven characteristic X (competency at fundraising in the example) is more likely to believe that X is important, and will favor X over alternative&nbsp;characteristics. &nbsp;It's not necessary that this is always the case; a generalist may also possess some characteristic X. &nbsp;It's only necessary that p(XY) &gt; p(X!Y), where X is&nbsp;possession&nbsp;of characteristic X, and Y is belief that X is an important characteristic to have (belief that fundraising is the most valuable pursuit for the charitable organization in the example).</p>\n<p>Any preference, once established, which follows a tendency such that p(XY) &gt; p(X!Y) will concrete itself into the organization once given a foothold; those who are selected based on X will also have, on average, a preference for X. &nbsp;They will select individuals with X.</p>\n<p>The danger of organization specialization, as opposed to individual specialization, arises when that preference extends to preference; when, given two people X, those who have a preference for X (those who have characteristic Y) are preferred over those who do not. &nbsp;This is the point at which selecting people for X and Y becomes a runaway process, a process which may subsume the original purpose of the organization.</p>\n<p>When those who do not have a preference for X begin to believe that X has already overtaken the original purpose of the organization, the meaningful possibilities are that they will either fight it or leave. &nbsp;If they simply leave, they harden the preference for X; there are fewer individuals in the organization who oppose Y. &nbsp;If they fight it and win, they've won for a day; an equilibrium has not yet been reached. &nbsp;If they fight it and lose, they establish a preference for preference; people who disagree with the orthodoxy of X begin to be seen as potential conflict creators in the organization, and just as problematically, revealing the preference for X may alter the decisions of those who might enter the organization otherwise; a non-Y individual may choose another organization which better suits their preferences.</p>\n<p><a href=\"/lw/lv/every_cause_wants_to_be_a_cult/\">Every Cause Wants to be a Cult</a>. &nbsp;Every belief wants to be an orthodoxy. &nbsp;Orthodoxy is a stable equilibrium, the pit surrounding the gently sloped hill of idea diversity.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rcHds7yLm3WWd7EPX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -1, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "18182", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>&nbsp;</p>\n<h4 id=\"Summary_\">Summary:</h4>\n<p>The purpose of this entry is to establish the existence of local equilibriums which introduce deviations from an ends-driven organization (an organization whose primary focus is a particular purpose) to transform it into a means-driven organization (an organization whose primary focus is the means to achieve its purpose, rather than the purpose itself).</p>\n<h4 id=\"Subsuming_Purpose__Part_1\">Subsuming Purpose, Part 1</h4>\n<p>Imagine you run a charity, and you have two star employees; one shares your goals without any emphasis on a means, the other believes in the cause but believes firmly in fundraising as the best means to that end. &nbsp;Both contribute to your charity, but the fundraiser does more good overall. &nbsp;The fundraiser enables your organization. &nbsp;Who do you set as your successor?</p>\n<p>Who will your successor choose as their successor?</p>\n<p>The person who believes in the purpose will choose the best person for achieving that purpose. &nbsp;The person who believes in a specific means to achieve that ends will choose the best person for those means. &nbsp;The means will subsume the ends. &nbsp;A person who values specific means, say, fundraising, is more likely to promote fellow fundraisers; he values their contributions more. &nbsp;Specialists, and in particular the lines of thinking which lead to specialization, create rigidity in the organization.</p>\n<p>Suppose that you choose the fundraiser. &nbsp;The fundraiser, by dint of having chosen to specialize in fundraising, probably believes that fundraising is more important than the alternative means of supporting the organization: he will probably choose to promote other effective fundraisers over their alternatives.</p>\n<p>And now people who don't agree that fundraising will start protesting, seeing their charity becoming increasingly subverted; fundraising is rewarded over the charitable purpose of the organization. &nbsp;They will leave, or protest; if their protests aren't heeded, for example because fundraisers who believe in fundraising do already run the company, they may be marginalized. &nbsp;Such individuals may be selected out, either self-selectively, or by explicit opposition by management to introducing people who are likely to cause trouble for them in the future.</p>\n<p><strong>Generalized</strong>:</p>\n<p>In the example above, I made one particular assumption: That somebody who possesses some choice-driven characteristic X (competency at fundraising in the example) is more likely to believe that X is important, and will favor X over alternative&nbsp;characteristics. &nbsp;It's not necessary that this is always the case; a generalist may also possess some characteristic X. &nbsp;It's only necessary that p(XY) &gt; p(X!Y), where X is&nbsp;possession&nbsp;of characteristic X, and Y is belief that X is an important characteristic to have (belief that fundraising is the most valuable pursuit for the charitable organization in the example).</p>\n<p>Any preference, once established, which follows a tendency such that p(XY) &gt; p(X!Y) will concrete itself into the organization once given a foothold; those who are selected based on X will also have, on average, a preference for X. &nbsp;They will select individuals with X.</p>\n<p>The danger of organization specialization, as opposed to individual specialization, arises when that preference extends to preference; when, given two people X, those who have a preference for X (those who have characteristic Y) are preferred over those who do not. &nbsp;This is the point at which selecting people for X and Y becomes a runaway process, a process which may subsume the original purpose of the organization.</p>\n<p>When those who do not have a preference for X begin to believe that X has already overtaken the original purpose of the organization, the meaningful possibilities are that they will either fight it or leave. &nbsp;If they simply leave, they harden the preference for X; there are fewer individuals in the organization who oppose Y. &nbsp;If they fight it and win, they've won for a day; an equilibrium has not yet been reached. &nbsp;If they fight it and lose, they establish a preference for preference; people who disagree with the orthodoxy of X begin to be seen as potential conflict creators in the organization, and just as problematically, revealing the preference for X may alter the decisions of those who might enter the organization otherwise; a non-Y individual may choose another organization which better suits their preferences.</p>\n<p><a href=\"/lw/lv/every_cause_wants_to_be_a_cult/\">Every Cause Wants to be a Cult</a>. &nbsp;Every belief wants to be an orthodoxy. &nbsp;Orthodoxy is a stable equilibrium, the pit surrounding the gently sloped hill of idea diversity.</p>\n<p>&nbsp;</p>", "sections": [{"title": "Summary:", "anchor": "Summary_", "level": 1}, {"title": "Subsuming Purpose, Part 1", "anchor": "Subsuming_Purpose__Part_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yEjaj7PWacno5EvWa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-10T20:53:59.060Z", "modifiedAt": null, "url": null, "title": "A rationalist My Little Pony fanfic", "slug": "a-rationalist-my-little-pony-fanfic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:54.387Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2NnJrE8jY34ipLZkL/a-rationalist-my-little-pony-fanfic", "pageUrlRelative": "/posts/2NnJrE8jY34ipLZkL/a-rationalist-my-little-pony-fanfic", "linkUrl": "https://www.lesswrong.com/posts/2NnJrE8jY34ipLZkL/a-rationalist-my-little-pony-fanfic", "postedAtFormatted": "Friday, August 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20rationalist%20My%20Little%20Pony%20fanfic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20rationalist%20My%20Little%20Pony%20fanfic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2NnJrE8jY34ipLZkL%2Fa-rationalist-my-little-pony-fanfic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20rationalist%20My%20Little%20Pony%20fanfic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2NnJrE8jY34ipLZkL%2Fa-rationalist-my-little-pony-fanfic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2NnJrE8jY34ipLZkL%2Fa-rationalist-my-little-pony-fanfic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 128, "htmlBody": "<p>For the past two months, I've been writing, and posting, roughly two thousand words a day of \"<a href=\"https://www.fimfiction.net/story/33512/Myou%27ve-Gotta-be-Kidding-Me\">Myou've Gotta be Kidding Me</a>\", a story set in a \"My Little Pony: Friendship is Magic\" fanfic universe, \"<a href=\"https://www.fimfiction.net/group/907\">Chess Game of the Gods</a>\". Outside of the sheer NaNoWriMo-like exercise of pushing out near-daily chapters, I've also been trying to keep in mind the various principles I've learned from Yudkowsky and LessWrong, and to try to present them in a way that people who like reading MLP fanfics might be able to appreciate.</p>\n<p>I've just come to something of a minor climax with chapter 60, and while I'll definitely be continuing the story, this seems like a good time to mention it here, for whatever feedback and constructive criticism anyone cares to offer.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2NnJrE8jY34ipLZkL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 20, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "18196", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 102, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-11T05:17:45.237Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Mirrors and Paintings", "slug": "seq-rerun-mirrors-and-paintings", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:05.422Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ABpH4XTQFsLCQGpYN/seq-rerun-mirrors-and-paintings", "pageUrlRelative": "/posts/ABpH4XTQFsLCQGpYN/seq-rerun-mirrors-and-paintings", "linkUrl": "https://www.lesswrong.com/posts/ABpH4XTQFsLCQGpYN/seq-rerun-mirrors-and-paintings", "postedAtFormatted": "Saturday, August 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Mirrors%20and%20Paintings&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Mirrors%20and%20Paintings%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FABpH4XTQFsLCQGpYN%2Fseq-rerun-mirrors-and-paintings%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Mirrors%20and%20Paintings%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FABpH4XTQFsLCQGpYN%2Fseq-rerun-mirrors-and-paintings", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FABpH4XTQFsLCQGpYN%2Fseq-rerun-mirrors-and-paintings", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 197, "htmlBody": "<p>Today's post, <a href=\"/lw/tb/mirrors_and_paintings/\">Mirrors and Paintings</a> was originally published on 23 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There is a proposal for programming a friendly AI, called CEV. Essentially, this strategy consists of teaching a computer to look at human brains and deduce, from that, morality. This should work better than trying to program morality \"by hand\", since we really aren't smart enough to solve that problem with an acceptable degree of accuracy.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/e19/seq_rerun_invisible_frameworks/\">Invisible Frameworks</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ABpH4XTQFsLCQGpYN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.61744234056906e-07, "legacy": true, "legacyId": "18199", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jNAAZ9XNyt82CXosr", "nZBAtKnxNRfpBb2c4", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-11T09:59:26.150Z", "modifiedAt": null, "url": null, "title": "Meetup : Berlin Meetup", "slug": "meetup-berlin-meetup-5", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:29.554Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/u97ypaPpcdfgZJQob/meetup-berlin-meetup-5", "pageUrlRelative": "/posts/u97ypaPpcdfgZJQob/meetup-berlin-meetup-5", "linkUrl": "https://www.lesswrong.com/posts/u97ypaPpcdfgZJQob/meetup-berlin-meetup-5", "postedAtFormatted": "Saturday, August 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berlin%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berlin%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu97ypaPpcdfgZJQob%2Fmeetup-berlin-meetup-5%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berlin%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu97ypaPpcdfgZJQob%2Fmeetup-berlin-meetup-5", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu97ypaPpcdfgZJQob%2Fmeetup-berlin-meetup-5", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 46, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ct'>Berlin Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 September 2012 07:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Spreegold, Hufelandstra\u00dfe 20, 10407 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're meeting for dinner at <a href=\"http://www.spreegold.com\" rel=\"nofollow\">http://www.spreegold.com</a> . There'll be a sign. Please leave a comment if you're planning to attend.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ct'>Berlin Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "u97ypaPpcdfgZJQob", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.618823509466427e-07, "legacy": true, "legacyId": "18211", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berlin_Meetup\">Discussion article for the meetup : <a href=\"/meetups/ct\">Berlin Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 September 2012 07:30:00PM (+0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Spreegold, Hufelandstra\u00dfe 20, 10407 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're meeting for dinner at <a href=\"http://www.spreegold.com\" rel=\"nofollow\">http://www.spreegold.com</a> . There'll be a sign. Please leave a comment if you're planning to attend.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berlin_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/ct\">Berlin Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berlin Meetup", "anchor": "Discussion_article_for_the_meetup___Berlin_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Berlin Meetup", "anchor": "Discussion_article_for_the_meetup___Berlin_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-11T15:25:51.807Z", "modifiedAt": null, "url": null, "title": "Meetup : NYC. Joint Meetup With Another Rationality Community", "slug": "meetup-nyc-joint-meetup-with-another-rationality-community", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:04.111Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YQL58RnF8yLr69osv/meetup-nyc-joint-meetup-with-another-rationality-community", "pageUrlRelative": "/posts/YQL58RnF8yLr69osv/meetup-nyc-joint-meetup-with-another-rationality-community", "linkUrl": "https://www.lesswrong.com/posts/YQL58RnF8yLr69osv/meetup-nyc-joint-meetup-with-another-rationality-community", "postedAtFormatted": "Saturday, August 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20NYC.%20Joint%20Meetup%20With%20Another%20Rationality%20Community&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20NYC.%20Joint%20Meetup%20With%20Another%20Rationality%20Community%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYQL58RnF8yLr69osv%2Fmeetup-nyc-joint-meetup-with-another-rationality-community%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20NYC.%20Joint%20Meetup%20With%20Another%20Rationality%20Community%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYQL58RnF8yLr69osv%2Fmeetup-nyc-joint-meetup-with-another-rationality-community", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYQL58RnF8yLr69osv%2Fmeetup-nyc-joint-meetup-with-another-rationality-community", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 140, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/cu'>NYC. Joint Meetup With Another Rationality Community</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 August 2012 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">NYC, 26th Street, Madison Avenue</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There are, in fact, two rationality-centered communities in NYC. The other is called Ergo. While not officially affiliated with Less Wrong, many of the people there are familiar with material here. But some reason there's not a whole lot of cross pollination between the two groups.\nWe think this is a little silly. So organizers from the two groups have arranged a joint public meetup this week, at Madison Square Park. Come on down this Thursday, August 16th at 7 PM. (We'll be just south of 26th street, at the Admiral David Farragut monument)\nCome down to meet, greet and swap rationalist origin stories!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/cu'>NYC. Joint Meetup With Another Rationality Community</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YQL58RnF8yLr69osv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 9.620424530814429e-07, "legacy": true, "legacyId": "18212", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___NYC__Joint_Meetup_With_Another_Rationality_Community\">Discussion article for the meetup : <a href=\"/meetups/cu\">NYC. Joint Meetup With Another Rationality Community</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 August 2012 07:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">NYC, 26th Street, Madison Avenue</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There are, in fact, two rationality-centered communities in NYC. The other is called Ergo. While not officially affiliated with Less Wrong, many of the people there are familiar with material here. But some reason there's not a whole lot of cross pollination between the two groups.\nWe think this is a little silly. So organizers from the two groups have arranged a joint public meetup this week, at Madison Square Park. Come on down this Thursday, August 16th at 7 PM. (We'll be just south of 26th street, at the Admiral David Farragut monument)\nCome down to meet, greet and swap rationalist origin stories!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___NYC__Joint_Meetup_With_Another_Rationality_Community1\">Discussion article for the meetup : <a href=\"/meetups/cu\">NYC. Joint Meetup With Another Rationality Community</a></h2>", "sections": [{"title": "Discussion article for the meetup : NYC. Joint Meetup With Another Rationality Community", "anchor": "Discussion_article_for_the_meetup___NYC__Joint_Meetup_With_Another_Rationality_Community", "level": 1}, {"title": "Discussion article for the meetup : NYC. Joint Meetup With Another Rationality Community", "anchor": "Discussion_article_for_the_meetup___NYC__Joint_Meetup_With_Another_Rationality_Community1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-12T03:19:50.062Z", "modifiedAt": null, "url": null, "title": "Magical Healing Powers", "slug": "magical-healing-powers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:08.806Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QrnkPGw4cRMiF8KrE/magical-healing-powers", "pageUrlRelative": "/posts/QrnkPGw4cRMiF8KrE/magical-healing-powers", "linkUrl": "https://www.lesswrong.com/posts/QrnkPGw4cRMiF8KrE/magical-healing-powers", "postedAtFormatted": "Sunday, August 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Magical%20Healing%20Powers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMagical%20Healing%20Powers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQrnkPGw4cRMiF8KrE%2Fmagical-healing-powers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Magical%20Healing%20Powers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQrnkPGw4cRMiF8KrE%2Fmagical-healing-powers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQrnkPGw4cRMiF8KrE%2Fmagical-healing-powers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 203, "htmlBody": "<p>Imagine you had magical healing powers. Sitting quietly with someone and holding their hand you could restore them to health. While this would be a wonderful ability to have, it would also be a hard one: any time you spent on something other than healing people would mean unnecessary suffering. How could you justify a pleasant dinner with your family or a relaxing weekend at the beach when that meant more people living in pain?</p>\n<p>But you already have these powers. Through the magic of <a href=\"http://www.givewell.org/charities/top-charities\">effective charity</a> you can donate money to help people right now. The tradeoff remains: time you give yourself when you could be working means money you don't earn which then can't go to help the people who would most benefit from it.</p>\n<p>(I don't think this means you should try for complete selflessness; you need to <a href=\"/lw/d97\">balance your needs against others'</a>. But the balance should probably be a lot further towards others' than it currently is.)</p>\n<p><strong>Update 2012-08-12: </strong>this is a response to hearing people offline&nbsp;saying&nbsp;that if they had magical \"help other people\" powers then they should spend lots of time using them, without having considered that they already have non-magical \"help other people\" powers.</p>\n<p><em><small>I also posted this <a href=\"http://www.jefftk.com/news/2012-08-11.html\">on my blog</a></small></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QrnkPGw4cRMiF8KrE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 2, "extendedScore": null, "score": 9.623927969948288e-07, "legacy": true, "legacyId": "18214", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xjufPJmEsN7d9dmkf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-12T05:03:29.457Z", "modifiedAt": null, "url": null, "title": "What is moral foundation theory good for?", "slug": "what-is-moral-foundation-theory-good-for", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:18.999Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "novalis", "createdAt": "2010-02-22T00:37:37.766Z", "isAdmin": false, "displayName": "novalis"}, "userId": "wBabXhGtRaTr94FcJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MtRgQB2k72eCvufdS/what-is-moral-foundation-theory-good-for", "pageUrlRelative": "/posts/MtRgQB2k72eCvufdS/what-is-moral-foundation-theory-good-for", "linkUrl": "https://www.lesswrong.com/posts/MtRgQB2k72eCvufdS/what-is-moral-foundation-theory-good-for", "postedAtFormatted": "Sunday, August 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20moral%20foundation%20theory%20good%20for%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20moral%20foundation%20theory%20good%20for%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMtRgQB2k72eCvufdS%2Fwhat-is-moral-foundation-theory-good-for%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20moral%20foundation%20theory%20good%20for%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMtRgQB2k72eCvufdS%2Fwhat-is-moral-foundation-theory-good-for", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMtRgQB2k72eCvufdS%2Fwhat-is-moral-foundation-theory-good-for", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1789, "htmlBody": "<p>I've seen <a href=\"/lw/bs3/intelligence_explosion_vs_cooperative_explosion/\">Jonathan Haidt</a> <a href=\"/lw/1v0/signaling_strategies_and_morality/\">mentioned</a> <a href=\"/lw/74f/are_deontological_moral_judgments_rationalizations/\">on</a> Less Wrong a few times, and so when I saw <a href=\"http://online.wsj.com/article/SB10001424052702303830204577446512522582648.html\">an article</a> about (in part) Haidt's new book elsewhere, I thought it would be an interesting read. It was, but not for the reasons I expected. Perhaps it is unfair to judge Haidt before I have read the book, but the quotes in the article reveal some seriously sloppy thinking.</p>\n<p>Haidt <a href=\"http://www.moralfoundations.org/\">believes</a> that there are at least six sources of moral values; the first five are harm/caring, fairness, loyalty, authority, sanctity/disgust. Liberty was recently added to the list, but doesn't seem to have made it into this article. He claims that liberals (in the American sense), care mostly (or only) only about the harm and fairness values, while conservatives care about all five. I myself am a one-foundation person, since I consider unfairness either a special case of harm, or a good heuristic for where harm is likely to occur; my views are apparently so rare that they haven't come up on Haidt's survey, and I haven't met anyone else who has reported a score like mine.</p>\n<p>While Haidt describes himself as a \"centrist\", he argues that \"you need loyalty, authority and sanctity to run a decent society.\" There are at least three ways that this claim can be read:</p>\n<p>(1) Haidt's personal moral foundations actually include all five bases, so this is a tautology; of course someone who thinks loyalty is fundamental will think a society without loyalty is not decent. From the tenor of the article, this is at least psychologically plausible.</p>\n<p>(2) The three non-universal values can be justified in terms of the common values. This is the interpretation that seems to be supported by some parts of the article, but it has its own issues.</p>\n<p>(3) Haidt cannot tell the difference between (1) and (2). Most of the article makes this claim entirely plausible.</p>\n<p>Here's one example of Haidt's moral confusion:</p>\n<p>\"In India, where he performed field studies early in his professional career, he encountered a society in some ways patriarchal, sexist and illiberal. Yet it worked and the people were lovely.\"</p>\n<p>First, was Haidt surprised to find people with different politics than his to be personable? Had he literally never met a conservative before?</p>\n<p>Second, what does it mean to say that the society \"worked\", or that the people were \"lovely\"? Indian society privileges men and certain castes over women and other castes. I say this not to denigrate India specifically, since there's no society in which women are treated equally to men, but to explain that India does have serious problems. <a href=\"http://www.unicef.org/infobycountry/india_statistics.html\">Literacy rates among women are 68% of that of men</a>, to pick a random statistic. And, of course, violence against women is endemic. Haidt <a href=\"http://www.edge.org/3rd_culture/haidt08/haidt08_index.html\">reports</a> that he \"dined with men whose wives silently served us and then retreated to the kitchen.\" What does he suppose would have happened if one day one of those women refused to serve, or even, after serving, sat down at the table to join the discussion?</p>\n<p>Of course, even this is an upper-class concern; lower class Indian women are far more likely to work outside the home, in order to survive. Apparently in some parts of India, public toilets charge women (who can ill afford it) but not men. And I can only assume that the situation was worse when Haidt was there, at least a decade ago.</p>\n<p>Haidt rationalizes this by saying, \"I was able to see a moral world in which families, not individuals, are the basic unit of society...\". Perhaps this is the story that they tell (and perhaps they even believe it). But history shows that when women can find alternatives, they don't choose to live like this. So there is both a harm and a fairness concern here. Haidt, having seen the loyalty/authority story, comes to ignore the harm/fairness story. He follows this by an anecdote focusing on the harm caused by individualism, since he is apparently incapable of justifying the non-universal foundations on their own terms.</p>\n<p>Here's another case of this confusion. Haidt claimed that among street children in Brazil, the \"most dangerous person in the world is mom's boyfriend. When women have a succession of men coming through, their daughters will get raped,\" he says. \"The right is right to be sounding the alarm about the decline of marriage, and the left is wrong to say, 'Oh, any kind of family is OK.' It's not OK.\"</p>\n<p>In this instance, Haidt is switching the goalposts. His moral foundation test is designed to isolate the five foundations. But here, there is clearly harm in addition to any violation of tradition. He doesn't exactly say which non-harm foundation he wants to invoke here -- that is, what the mothers' violation is. Impurity is the only plausible choice. This, of course, brings to the front one of the most common real effects of the \"purity\" foundation: to disempower women.</p>\n<p>I should add that there is no citation on this data; it also doesn't seem to appear in the book (at least, not that I could find via Google Books). A quick glance through Google does not reveal a plausible source for this. So where did he get it from? Probably not via direct observation (how would he have observed these rapes?). He must have heard it from Brazilians. Well, if that's true, then these Brazilian women must know it. And since nobody wants their daughter to get raped, this must mean that they have a very good reason for inviting these men in -- maybe the alternative is starvation. Recall that we're talking about \"street children\" here. I just can't imagine a woman saying, \"yeah, he's going to rape my daughter, but I really love him!\" But I think it's actually more likely that this is just the sort of rumor that the Catholic Church would want to spread, to combat unmarried cohabitation. It gets its memetic strength from blame-shifting/just-worldism: \"If you didn't want your daughter to get raped, why did you shack (literally?) up with this guy?\"</p>\n<p>It's true that there are dangers from non-related men, as <a href=\"http://www.amazon.com/Mother-Nature-Maternal-Instincts-Species/dp/0345408934/ref=sr_1_1?ie=UTF8&amp;qid=1344745968&amp;sr=8-1\">Sarah Blaffer Hrdy discusses in _Mother Nature: Maternal Instincts and How They Shape the Human Species</a>; there are also potential benefits. Hrdy's book (which I haven't finished reading yet) discusses both, and also vastly complicates the view of what \"traditional\" family is. She presents multiple equilibria, some more common among farmers and others more common among foragers (to use Robin Hanson's language). A Brazilian shantytown doesn't really fit well into either framework, so it's unclear whether norms adapted for either would be effective.</p>\n<p>So does Haidt believe that nontraditional families are wrong because they violate purity? Or because they're harmful? The standard conservative reply to this is that our traditions evolved because they were useful (i.e. prevented harm), and to erase the traditions without understanding the value that they provided is an mistake. This is put in a delightfully patronizing way by <a href=\"http://epicureandealmaker.blogspot.com/2012/03/chesterton-fence.html\">Chesterton</a> -- notice how he will \"allow\" you to clear away a tradition as though it were his decision to make.</p>\n<p>And it is in fact relatively easy to come up with evolutionary psychology just-so stories as reasons for why loyalty, authority, and purity would have been useful in the ancestral environment. (The same is true of fairness). Authority, for instance, might help with collective decision making. Maybe it's best for the tribe to go take the left fork, and it might be better to take the right fork. But it is almost always better for them all to take the same fork, than it is to split up. If there's one tribal leader, then they can make that decision and have others agree with it. This isn't a case of group selection; every individual of the group benefits from coordination. I describe this as a \"just-so story\" here because it would be extremely difficult to find evidence for whether in fact a specific moral intuition evolved for a specific reason. Haidt's book apparently presents some of these arguments in the context of group selection, but in this particular example, group selection (or even kin selection or reciprocal altruism) isn't a necessary part of the hypothesis; treating groups as part of the environment (rather than as the unit of evolution) is sufficient.</p>\n<p>Moral foundations theory is perhaps useful descriptively, in that, if it were shown to be something beyond a just-so story, it would explain why there are five (or six, or more) foundations as opposed to one or two. It is, however, missing a piece: why are there people who don't share all five foundations? The evolutionary argument is not useful prescriptively, because evolution only cares about harm (and only certain kinds of harm), and once we decide to see moral questions in terms of harm, then questions of actual harm can screen off the other evolved heuristics. Yes, humans are <a href=\"/lw/l0/adaptationexecuters_not_fitnessmaximizers/\">Adaptation-Executers, not Fitness-Maximizers</a>. So there are lots of cases where we follow our evolved intuitions rather than the pressures that selected for those intuitions. But we are also apparently adapted to contemplate moral philosophy. So when we find ourselves justifying an evolved intuition A in terms of another evolved intuition B, we might consider B more fundamental. And if there are cases where A isn't explainable in terms of B, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Haidt-Moral-Dumfounding-When-Intuition-Finds-No-Reason.pdf\">five-foundation people just get stuck</a>. This, perhaps does explain the one- or two-foundation view; it's what happens when you ask \"why?\" once, and throw out everything that doesn't actually have an answer. When you ask a second time, you're getting into the realm of meta-ethics. &nbsp;Instrumental five-foundation people (such as Haidt, probably), wouldn't get stuck -- but they would fall back to harm.</p>\n<p>Maybe there's another argument for the three non-universal foundations, but Haidt doesn't make it. Does he feel that, by defining something as a \"foundation\", it doesn't need an argument? But if so, why does he keep reaching for harm as an explanation?</p>\n<p>As a descriptive theory, Haidt's moral foundation framework helps explain some of the differing moral values people have. Haidt seems to wrongly interpret it as a useful prescriptive tool. However he has not presented any reason to think that it is, in fact, useful prescriptively, and has presented several reasons to doubt it. &nbsp;</p>\n<p>[Added later:]</p>\n<p>None of this is to say that there are no reasons to be conservative. &nbsp;You could be conservative instrumentally (as Haidt seems to be), or you could be conservative because you really do consider all five bases to be inherently valuable (you could also do both at once, but that should make you slightly suspicious that you're rationalizing). &nbsp;There's no inherent problem with either of those. &nbsp;Haidt's problem is that he wants to have it both ways; he want to present the non-universal foundations as inherently valuable, but all his actual arguments are about their instrumental value.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MtRgQB2k72eCvufdS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 11, "extendedScore": null, "score": 4.5e-05, "legacy": true, "legacyId": "18216", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 300, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rLzMBxew4S4TevtqB", "BviRaP4przARdmb8b", "62p74DvwNHgQXCXcH", "XPErvb8m9FapXCjhA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-12T05:27:17.885Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Unnatural Categories", "slug": "seq-rerun-unnatural-categories", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:08.851Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kp36e5bW9PEPFjYkq/seq-rerun-unnatural-categories", "pageUrlRelative": "/posts/kp36e5bW9PEPFjYkq/seq-rerun-unnatural-categories", "linkUrl": "https://www.lesswrong.com/posts/kp36e5bW9PEPFjYkq/seq-rerun-unnatural-categories", "postedAtFormatted": "Sunday, August 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Unnatural%20Categories&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Unnatural%20Categories%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkp36e5bW9PEPFjYkq%2Fseq-rerun-unnatural-categories%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Unnatural%20Categories%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkp36e5bW9PEPFjYkq%2Fseq-rerun-unnatural-categories", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkp36e5bW9PEPFjYkq%2Fseq-rerun-unnatural-categories", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 216, "htmlBody": "<p>Today's post, <a href=\"/lw/tc/unnatural_categories/\">Unnatural Categories</a> was originally published on 24 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Unnatural_Categories\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There are some mental categories we draw that are relatively simple and straightforward. Others get trickier, because they are primarily drawn in such a way that whether or not something fits into that category is important information to our utility function. Deciding whether someone is \"alive\", for instance. Is someone like Terry Schaivo alive? This issue is why, in part, technology creates new moral dilemmas, and why teaching morality to a computer is so hard.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/e1j/seq_rerun_mirrors_and_paintings/\">Mirrors and Paintings</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kp36e5bW9PEPFjYkq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.62455366749493e-07, "legacy": true, "legacyId": "18217", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XeHYXXTGRuDrhk5XL", "ABpH4XTQFsLCQGpYN", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-12T18:39:47.894Z", "modifiedAt": null, "url": null, "title": "The Fallacy of Large Numbers", "slug": "the-fallacy-of-large-numbers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:06.033Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "dspeyer", "createdAt": "2011-02-07T05:00:21.339Z", "isAdmin": false, "displayName": "dspeyer"}, "userId": "dTXMZRzgFhj4aKhNX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/R8DBQD72tajt4mDhv/the-fallacy-of-large-numbers", "pageUrlRelative": "/posts/R8DBQD72tajt4mDhv/the-fallacy-of-large-numbers", "linkUrl": "https://www.lesswrong.com/posts/R8DBQD72tajt4mDhv/the-fallacy-of-large-numbers", "postedAtFormatted": "Sunday, August 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Fallacy%20of%20Large%20Numbers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Fallacy%20of%20Large%20Numbers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR8DBQD72tajt4mDhv%2Fthe-fallacy-of-large-numbers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Fallacy%20of%20Large%20Numbers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR8DBQD72tajt4mDhv%2Fthe-fallacy-of-large-numbers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR8DBQD72tajt4mDhv%2Fthe-fallacy-of-large-numbers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 601, "htmlBody": "<p>I've been seeing this a lot lately, and I don't think it's been written about here before</p>\n<div>\n<p>Let's start with a motivating example. &nbsp;Suppose you have a fleet of 100 cars (or horses, or people, or whatever). &nbsp;For any given car, on any given day, there's a 3% chance that it'll be out for repairs (or sick, or attending grandmothers' funerals, or whatever). &nbsp;For simplicity's sake, assume all failures are uncorrelated. &nbsp;How many cars can you afford to offer to customers each day? &nbsp;Take a moment to think of a number.</p>\n<p>Well, 3% failure means 97% success. &nbsp;So we expect 97 to be available and can afford to offer 97. &nbsp;Does that sound good? &nbsp;Take a moment to answer.</p>\n<p>Well, maybe not so good. &nbsp;Sometimes we'll get unlucky. &nbsp;And not being able to deliver on a contract is painful. &nbsp;Maybe we should reserve 4 and only offer 96. &nbsp;Or maybe we'll play it very safe and reserve twice the needed number. &nbsp;6 in reserve, 94 for customers. &nbsp;But is that overkill? &nbsp;Take note of what you're thinking now.</p>\n<p style=\"text-align: left;\" dir=\"ltr\">The likelihood of having more than 4 unavailable is <a href=\"http://www.wolframalpha.com/input/?i=sum&amp;a=*C.sum-_*Calculator.dflt-&amp;f2=0.03^k+*+.97^%28100-k%29+*+choose%28100%2Ck%29&amp;x=0&amp;y=0&amp;f=Sum.sumfunction_0.03^k+*+.97^%28100-k%29+*+choose%28100%2Ck%29&amp;f3=5&amp;f=Sum.sumlowerlimit_5&amp;f4=100&amp;f=Sum.sumupperlimit_100&amp;a=*FVarOpt.1-_**-.***Sum.sumvariable---.*--\">18%</a>. &nbsp;The likelihood of having more than 6 unavailable is <a href=\"http://www.wolframalpha.com/input/?i=sum&amp;a=*C.sum-_*Calculator.dflt-&amp;f2=0.03^k+*+.97^%28100-k%29+*+choose%28100%2Ck%29&amp;x=0&amp;y=0&amp;f=Sum.sumfunction_0.03^k+*+.97^%28100-k%29+*+choose%28100%2Ck%29&amp;f3=7&amp;f=Sum.sumlowerlimit_7&amp;f4=100&amp;f=Sum.sumupperlimit_100&amp;a=*FVarOpt.1-_**-.***Sum.sumvariable---.*--\">3.1%</a>. &nbsp;About once a month. &nbsp;Even reserving 8, requiring 9 failures to get you in trouble, gets you in trouble <a href=\"http://www.wolframalpha.com/input/?i=sum&amp;a=*C.sum-_*Calculator.dflt-&amp;f2=0.03^k+*+.97^%28100-k%29+*+choose%28100%2Ck%29&amp;x=0&amp;y=0&amp;f=Sum.sumfunction_0.03^k+*+.97^%28100-k%29+*+choose%28100%2Ck%29&amp;f3=9&amp;f=Sum.sumlowerlimit_9&amp;f4=100&amp;f=Sum.sumupperlimit_100&amp;a=*FVarOpt.1-_**-.***Sum.sumvariable---.*--\">0.3%</a> of the time. &nbsp;More than once a year. &nbsp;Reserving 9 -- three times the expected -- gets the risk down to <a href=\"http://www.wolframalpha.com/input/?i=sum&amp;a=*C.sum-_*Calculator.dflt-&amp;f2=0.03^k+*+.97^%28100-k%29+*+choose%28100%2Ck%29&amp;x=0&amp;y=0&amp;f=Sum.sumfunction_0.03^k+*+.97^%28100-k%29+*+choose%28100%2Ck%29&amp;f3=10&amp;f=Sum.sumlowerlimit_10&amp;f4=100&amp;f=Sum.sumupperlimit_100&amp;a=*FVarOpt.1-_**-.***Sum.sumvariable---.*--\">0.087%</a> or a little less than every three years. &nbsp;A number we can finally feel safe with.</p>\n<p style=\"text-align: left;\" dir=\"ltr\">So much for expected values. &nbsp;What happened to the Law of Large Numbers? &nbsp;Short answer: 100 isn't large.</p>\n<p style=\"text-align: left;\" dir=\"ltr\">The Law of Large Numbers states that for sufficiently large samples, the results look like the expected value (for any reasonable definition of like).</p>\n<p style=\"text-align: left;\" dir=\"ltr\"><strong>The Fallacy of Large Numbers states that your numbers are sufficiently large.</strong></p>\n<p style=\"text-align: left;\" dir=\"ltr\">This doesn't just apply to expected values. &nbsp;It also applies to looking at a noisy signal and handwaving that the noise will average away with repeated measurements. &nbsp;Before you can say something like that, you need to look at how many measurements, and how much noise, and crank out a lot of calculations. &nbsp;This variant is particularly tricky because you often don't have numbers on how much noise there is, making it hard to do the calculation. &nbsp;When the calculation is hard, the handwave is more tempting. &nbsp;That doesn't make it more accurate.</p>\n<p style=\"text-align: left;\" dir=\"ltr\">I don't know of any general tools for saying when statistical approximations become safe. &nbsp;The best thing I know is to spot-check like I did above. &nbsp;Brute-forcing combinatorics sounds scary, but Wolfram Alpha can be your friend (as above). &nbsp;So can python, which has native bignum support. &nbsp;Python has a reputation as being slow for number crunching, but with n&lt;1000 and a modern cpu it usually doesn't matter.</p>\n<p style=\"text-align: left;\" dir=\"ltr\">One warning sign is if your tools were developed in a very different context than where you're using them. &nbsp;Some approximations were invented for dealing with radioactive decay, where n resembles Avogadro's Number. &nbsp;Applying these tools to the American population is risky. &nbsp;Some were developed for the American population. &nbsp;Applying them to students in your classroom is risky.</p>\n<p style=\"text-align: left;\" dir=\"ltr\">Another danger is that your dataset can shrink. &nbsp;If you've validated your tools for your entire dataset, and then thrown out some datapoints and divided the rest along several axes, don't be surprised if some of your data subsets are now too small for your tools.</p>\n<p style=\"text-align: left;\" dir=\"ltr\">This fallacy is related to \"assuming events are uncorrelated\" and \"assuming distributions are normal\". &nbsp;It's a special case of \"choosing statistical tools based on how easy they are to use whether they're applicable to your use-case or not\".</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1, "dJ6eJxJrCEget7Wb6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "R8DBQD72tajt4mDhv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 29, "extendedScore": null, "score": 7.5e-05, "legacy": true, "legacyId": "18220", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-12T23:14:51.247Z", "modifiedAt": null, "url": null, "title": "What are your questions about making a difference?", "slug": "what-are-your-questions-about-making-a-difference", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:06.261Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benjamin_Todd", "createdAt": "2012-02-27T10:33:26.778Z", "isAdmin": false, "displayName": "Benjamin_Todd"}, "userId": "moXBqwii3GoAMuRwa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bzShxYcRz7K7oNtsB/what-are-your-questions-about-making-a-difference", "pageUrlRelative": "/posts/bzShxYcRz7K7oNtsB/what-are-your-questions-about-making-a-difference", "linkUrl": "https://www.lesswrong.com/posts/bzShxYcRz7K7oNtsB/what-are-your-questions-about-making-a-difference", "postedAtFormatted": "Sunday, August 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20your%20questions%20about%20making%20a%20difference%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20your%20questions%20about%20making%20a%20difference%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbzShxYcRz7K7oNtsB%2Fwhat-are-your-questions-about-making-a-difference%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20your%20questions%20about%20making%20a%20difference%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbzShxYcRz7K7oNtsB%2Fwhat-are-your-questions-about-making-a-difference", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbzShxYcRz7K7oNtsB%2Fwhat-are-your-questions-about-making-a-difference", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 116, "htmlBody": "<div class=\"content\">\n<p>How can you best use your time to make a difference?</p>\n<p><a href=\"http://www.80000hours.org\">80,000 Hours</a> now has several people working full time on research, and they would like your questions!</p>\n<p>We&rsquo;re happy to consider any questions about how to effectively make a difference, in whatever sphere of your life &ndash; volunteering, career or donations. These questions could be at the conceptual or ethical level, or they could concern nitty-gritty practicalities.</p>\n<p>We&rsquo;re particularly interested in questions that are not already well addressed by other groups, and where there's significant opportunity for progress.</p>\n<p>The most popular questions will receive the attention of our research team, and their findings will feature in our new careers guide.</p>\n<p>Either post your questions below, or send them to <a href=\"mailto:careers@80000hours.org\">careers@80000hours.org</a></p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qAvbtzdG2A2RBn7in": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bzShxYcRz7K7oNtsB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 69, "baseScore": 28, "extendedScore": null, "score": 9.62979695675294e-07, "legacy": true, "legacyId": "17821", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-13T05:19:40.459Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Magical Categories", "slug": "seq-rerun-magical-categories", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:04.174Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yF2DiAAhJChWF7taf/seq-rerun-magical-categories", "pageUrlRelative": "/posts/yF2DiAAhJChWF7taf/seq-rerun-magical-categories", "linkUrl": "https://www.lesswrong.com/posts/yF2DiAAhJChWF7taf/seq-rerun-magical-categories", "postedAtFormatted": "Monday, August 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Magical%20Categories&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Magical%20Categories%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyF2DiAAhJChWF7taf%2Fseq-rerun-magical-categories%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Magical%20Categories%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyF2DiAAhJChWF7taf%2Fseq-rerun-magical-categories", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyF2DiAAhJChWF7taf%2Fseq-rerun-magical-categories", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<p>Today's post, <a href=\"/lw/td/magical_categories/\">Magical Categories</a> was originally published on 24 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Magical_Categories\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>We underestimate the complexity of our own unnatural categories. This doesn't work when you're trying to build a FAI.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/e21/seq_rerun_unnatural_categories/\">Unnatural Categories</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yF2DiAAhJChWF7taf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.631589924566355e-07, "legacy": true, "legacyId": "18229", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PoDAyQMWEXBBBEJ5P", "kp36e5bW9PEPFjYkq", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-13T06:37:08.609Z", "modifiedAt": null, "url": null, "title": "[Link] The perils of \u201creason\u201d", "slug": "link-the-perils-of-reason", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:05.261Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GLaDOS", "createdAt": "2011-04-26T20:59:08.539Z", "isAdmin": false, "displayName": "GLaDOS"}, "userId": "wdPp4B7WGssb2gHwP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LntN9bdbF78wk28cj/link-the-perils-of-reason", "pageUrlRelative": "/posts/LntN9bdbF78wk28cj/link-the-perils-of-reason", "linkUrl": "https://www.lesswrong.com/posts/LntN9bdbF78wk28cj/link-the-perils-of-reason", "postedAtFormatted": "Monday, August 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20The%20perils%20of%20%E2%80%9Creason%E2%80%9D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20The%20perils%20of%20%E2%80%9Creason%E2%80%9D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLntN9bdbF78wk28cj%2Flink-the-perils-of-reason%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20The%20perils%20of%20%E2%80%9Creason%E2%80%9D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLntN9bdbF78wk28cj%2Flink-the-perils-of-reason", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLntN9bdbF78wk28cj%2Flink-the-perils-of-reason", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 704, "htmlBody": "<p><a href=\"http://blogs.discovermagazine.com/gnxp/2012/08/the-perils-of-reason/?utm_source=twitterfeed&amp;utm_medium=twitter\">Post</a> by fellow LW reader Razib Khan, who many here probably know from&nbsp; the <a href=\"http://www.gnxp.com/\">gnxp site</a> or perhaps from his <a href=\"/lw/1qu/bhtv_eliezer_yudkowsky_razib_khan/\">debate with Eliezer</a>. Somewhat related to a post <a href=\"/lw/cqs/link_reason_the_god_that_fails_but_we_keep/\">we also seem to have discussed</a>.</p>\n<blockquote>\n<p><a href=\"http://blogs.discovermagazine.com/gnxp/files/2012/08/beliefs.png\"><img class=\"alignleft size-full wp-image-17737\" title=\"beliefs\" src=\"http://blogs.discovermagazine.com/gnxp/files/2012/08/beliefs.png\" alt=\"\" width=\"209\" height=\"240\" /></a></p>\n<p>In my <a href=\"http://blogs.discovermagazine.com/gnxp/2012/08/the-scourging-of-sam-harris/\">post below</a> in regards to Sam Harris&rsquo; recent interactions on the web I <a href=\"http://blogs.discovermagazine.com/gnxp/2012/05/reason-the-god-that-fails-but-we-keep-socially-promoting/\">reasserted by suspicion of reason</a>. This naturally elicited curiosity, or hostility, from some. I&rsquo;ve <a href=\"http://blogs.discovermagazine.com/gnxp/2012/05/reason-the-god-that-fails-but-we-keep-socially-promoting/\">talked about this</a> before, but the illustration to the left gets at my primary issue. <strong>When individuals are reasoning alone they often have a high degree of uncertainty as to their conclusions.</strong> But when individuals are reasoning together they seem to converge very rapidly and with great confidence upon a particular position. What&rsquo;s going on here? In the second case it isn&rsquo;t reason at all, but our natural human predisposition toward group conformity. There&rsquo;s a huge psychological literature on this, so I won&rsquo;t belabor the point. When people brandish &ldquo;reason&rdquo; and &ldquo;rationality&rdquo; explicitly I&rsquo;m somewhat skeptical. <strong>If rational conclusions are so plain and self-evident why are we even asserting the primacy of reason?</strong>&nbsp;If something really is so clearly reasonable you usually don&rsquo;t go around trumpeting how reasonable it is.</p>\n<p>Another pitfall of reason is that it lulls use into the delusion that we have a transparent understanding of our own motivations and logic, as well as the motivation and logic of others. In my post below I explicitly stated that I disagreed with Harris on the substance of much of what he asserted and assumes in the first paragraph, <strong>but multiple people simply imputed to me his views as if they were mine!</strong>&nbsp;Even though I declaimed this position very early on, they simply could not generate an coherent framework where I did not agree with either them or Harris. There were only two options conceivable for them which the &ldquo;reason&rdquo; engine could operate upon. As I clearly did not agree with them (or so they thought), they simply injected in the axioms which would be appropriate for Sam Harris into my own box, and then began firing the appropriate propositions.</p>\n<p><strong>Here we have the problem that reasonable arguments and the self-evident truth of rationality is often only clear among people who already agree on everything of substance</strong>. People who agree can confidently assert the rationality and reasonableness of their arguments to those who have the exactly same perspective. So, for example, you have educated people like William F. Buckley, Jr. explaining that there is more evidence of the resurrection of Jesus Christ than that Abraham Lincoln gave the Emancipation Proclamation. This was eminently reasonable to the circles which Buckley moved in. After all, Christ <em>did </em>rise from the dead, everyone knows that! &nbsp;Well, not really. Buckley&rsquo;s son, Christopher, who is not a believer, has explained that his father had a genuinely difficult time imagining the perspective of those who did not share his beliefs on this matter.</p>\n<p>This is not to say that reason and rationality are not without utility. These are humanity&rsquo;s great cognitive jewels. But great tools can be used to various ends, and true reason and rationality are very difficult. Mathematics for example is undoubtedly true rationality, with crisp and precise inferences being derivable. But most other intellectual structures are not so clearly self-evident as mathematics. Verbal logic and reasoning are riddled with the pitfalls of <a href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\">cognitive bias</a>. Because most people share the same systematic biases it is very difficult for groups of individuals engaging in self-reinforcing masturbatory &lsquo;rationality&rsquo; discourses to perhaps step back and wonder about their motivated reasoning.&nbsp;Unfortunately it may be that reason emerged as a human faculty <a href=\"http://www.nytimes.com/2011/06/15/arts/people-argue-just-to-win-scholars-assert.html?pagewanted=all\">to win arguments, not resolve truth</a>. If this is true we are much more lawyers than mathematicians in our discourse. Does this seem plausible to you? Unfortunately it does seem plausible to me.</p>\n<p>Where does this leave us? <strong>I think we need to be skeptical of reasoned arguments</strong>. This doesn&rsquo;t lead me down the path of intellectual nihilism. Reason is which leads us to truth is possible. But it may be that this is a very specialized usage of reason, which requires special conditions. &rsquo;tis far easier to <em>seem </em>clever than <em>be </em>correct.</p>\n</blockquote>\n<p><strong>Edit:</strong> I linked to the wrong article! (~_~;) Fixed!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LntN9bdbF78wk28cj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 3, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "18235", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4gP3692vXuauKGjMR", "55vDmhF8vZykersix"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-13T19:22:21.097Z", "modifiedAt": null, "url": null, "title": "Bayes for Schizophrenics: Reasoning in Delusional Disorders", "slug": "bayes-for-schizophrenics-reasoning-in-delusional-disorders", "viewCount": null, "lastCommentedAt": "2019-05-01T20:27:09.073Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/euJm4RwkAptZnP89i/bayes-for-schizophrenics-reasoning-in-delusional-disorders", "pageUrlRelative": "/posts/euJm4RwkAptZnP89i/bayes-for-schizophrenics-reasoning-in-delusional-disorders", "linkUrl": "https://www.lesswrong.com/posts/euJm4RwkAptZnP89i/bayes-for-schizophrenics-reasoning-in-delusional-disorders", "postedAtFormatted": "Monday, August 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bayes%20for%20Schizophrenics%3A%20Reasoning%20in%20Delusional%20Disorders&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABayes%20for%20Schizophrenics%3A%20Reasoning%20in%20Delusional%20Disorders%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeuJm4RwkAptZnP89i%2Fbayes-for-schizophrenics-reasoning-in-delusional-disorders%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bayes%20for%20Schizophrenics%3A%20Reasoning%20in%20Delusional%20Disorders%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeuJm4RwkAptZnP89i%2Fbayes-for-schizophrenics-reasoning-in-delusional-disorders", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeuJm4RwkAptZnP89i%2Fbayes-for-schizophrenics-reasoning-in-delusional-disorders", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3267, "htmlBody": "<p><strong>Related to: </strong><a href=\"/lw/20/the_apologist_and_the_revolutionary/\">The Apologist and the Revolutionary</a>, <a href=\"/lw/13b/dreams_with_damaged_priors/\">Dreams with Damaged Priors</a></p>\n<p>Several years ago, <a href=\"/lw/20/the_apologist_and_the_revolutionary/\">I posted</a> about <a href=\"http://psych.utoronto.ca/~peterson/psy430s2001/Ramachandran%20VS%20Evolution%20of%20self-deception%20Med%20Hypoth%201996.pdf\">V.S. Ramachandran's 1996 theory</a> explaining anosognosia through an \"apologist\" and a \"revolutionary\". <br /><br />Anosognosia, a condition in which extremely sick patients mysteriously deny their sickness, occurs during right-sided brain injury but not left-sided brain injury. It can be extraordinarily strange: for example, in one case, a woman whose left arm was paralyzed insisted she could move her left arm just fine, and when her doctor pointed out her immobile arm, she claimed that was her daughter's arm even though it was obviously attached to her own shoulder. Anosognosia can be temporarily alleviated by squirting cold water into the patient's left ear canal, after which the patient suddenly realizes her condition but later loses awareness again and reverts back to the bizarre excuses and confabulations.<br /><br />Ramachandran suggested that the left brain is an \"apologist\", trying to justify existing theories, and the right brain is a \"revolutionary\" which changes existing theories when conditions warrant. If the right brain is damaged, patients are unable to change their beliefs; so when a patient's arm works fine until a right-brain stroke, the patient cannot discard the hypothesis that their arm is functional, and can only use the left brain to try to fit the facts to their belief.<br /><br />In the almost twenty years since Ramachandran's theory was published, new research has kept some of the general outline while changing many of the specifics in the hopes of explaining a wider range of delusions in neurological and psychiatric patients. The newer model acknowledges the left-brain/right-brain divide, but adds some new twists based on the Mind Projection Fallacy and the brain as a Bayesian reasoner.</p>\n<p><a id=\"more\"></a><br /><strong>INTRODUCTION TO DELUSIONS</strong><br /><br />Strange as anosognosia is, it's only one of several types of delusions, which are broadly categorized into polythematic and monothematic. Patients with polythematic delusions have multiple unconnected odd ideas: for example, the famous schizophrenic <a href=\"/lw/dc7/nash_equilibria_and_schelling_points/\">game theorist</a> John Nash believed that he was defending the Earth from alien attack, that he was the Emperor of Antarctica, <em>and</em> that he was the left foot of God. A patient with a monothematic delusion, on the other hand, usually only has one odd idea. Monothematic delusions vary less than polythematic ones: there are a few that are relatively common across multiple patients. For example:<br /><br />In the Capgras delusion, the patient, usually a victim of brain injury but sometimes a schizophrenic, believes that one or more people close to her has been replaced by an identical imposter. For example, one male patient expressed the worry that his wife was actually someone else, who had somehow contrived to exactly copy his wife's appearance and mannerisms. This delusion sounds harmlessly hilarious, but it can get very ugly: in at least one case, a patient got so upset with the deceit that he murdered the hypothesized imposter - actually his wife.<br /><br />The Fregoli delusion is the opposite: here the patient thinks that random strangers she meets are actually her friends and family members in disguise. Sometimes everyone may be the same person, who must be as masterful at quickly changing costumes as the famous Italian actor Fregoli (inspiring the condition's name).<br /><br />In the Cotard delusion, the patient believes she is dead. Cotard patients will neglect personal hygiene, social relationships, and planning for the future - as the dead have no need to worry about such things. Occasionally they will be able to describe in detail the \"decomposition\" they believe they are undergoing.<br /><br />Patients with all these types of delusions<sup>1</sup> - as well as anosognosiacs - share a common feature: they usually have damage to the right frontal lobe of the brain (including in schizophrenia, where the brain damage is of unknown origin and usually generalized, but where it is still possible to analyze which areas are the most abnormal). It would be nice if a theory of anosognosia also offered us a place to start explaining these other conditions, but this Ramachandran's idea fails to do. He posits a problem with belief shift: going from the originally correct but now obsolete \"my arm is healthy\" to the updated \"my arm is paralyzed\". But these other delusions cannot be explained by simple failure to update: delusions like \"the person who appears to be my wife is an identical imposter\" <em>never </em>made sense. We will have to look harder.<br /><br /><strong>ABNORMAL PERCEPTION: THE FIRST FACTOR</strong><br /><br />Coltheart, Langdon, and McKay <a href=\"http://www.docin.com/p-228080268.html\">posit what they call the \"two-factor theory\" of delusion</a>. In the two-factor theory, one problem causes an abnormal perception, and a second problem causes the brain to come up with a bizarre instead of a reasonable explanation.<br /><br />Abnormal perception has been best studied in the Capgras delusion. A series of experiments, including some by Ramachandran himself, demonstrate that Capgras patients lack a skin conductance response (usually used as a proxy of emotional reaction) to familiar faces. This meshes nicely with the brain damage pattern in Capgras, which seems to involve the connection between the face recognition areas in the temporal lobe and the emotional areas in the limibic system. So although the patient can recognize faces, and can feel emotions, the patient cannot feel emotions related to recognizing faces.<br /><br />The older \"one-factor\" theories of delusion stopped here. The patient, they said, knows that his wife looks like his wife, but he doesn't feel any emotional reaction to her. If it was really his wife, he would feel something - love, irritation, whatever - but he feels only the same blankness that would accompany seeing a stranger. Therefore (the one-factor theory says) his brain gropes for an explanation and decides that she really is a stranger. Why does this stranger look like his wife? Well, she must be wearing a very good disguise.<br /><br />One-factor theories also do a pretty good job of explaining many of the remaining monothematic delusions. A 1998 experiment shows that Cotard delusion sufferers have a globally decreased autonomic response: that is, nothing really makes them feel much of anything - a state consistent with being dead. And anosognosiacs have lost not only the nerve connections that would allow them to move their limbs, but the nerve connections that would send distress signals and even the connections that would send back \"error messages\" if the limb failed to move correctly - so the brain gets data that everything is fine.<br /><br />The basic principle behind the first factor is \"Assume that reality is such that my mental states are justified\", a sort of Super Mind Projection Fallacy.<br /><br />Although I have yet to find an official paper that says so, I think this same principle also explains many of the more typical schizophrenic delusions, of which two of the most common are delusions of grandeur and delusions of persecution. Delusions of grandeur are the belief that one is extremely important. In pop culture, they are typified by the psychiatric patient who believes he is Jesus or Napoleon - I've never met any Napoleons, but I know several Jesuses and recently worked with a man who thought he was Jesus and John Lennon at the same time. Here the first factor is probably an elevated mood (working through a miscalibrated <a href=\"http://www.psychwiki.com/wiki/Sociometer_Theory\">sociometer</a>). \"Wow, I feel like I'm really awesome. In what case would I be justified in thinking so highly of myself? Only if I were Jesus and John Lennon at the same time!\" A similar mechanism explains delusions of persecution, the classic \"the CIA is after me\" form of disease. We apply the Super Mind Projection Fallacy to a garden-variety anxiety disorder: \"In what case would I be justified in feeling this anxious? Only if people were constantly watching me and plotting to kill me. Who could do that? The CIA.\"<br /><br />But despite the explanatory power of the Super Mind Projection Fallacy, the one-factor model isn't enough.<br /><br /><strong>ABNORMAL BELIEF EVALUATION: THE SECOND FACTOR</strong><br /><br />The one-factor model requires people to be really stupid. Many Capgras patients were normal intelligent people before their injuries. Surely they wouldn't leap straight from \"I don't feel affection when I see my wife's face\" to \"And therefore this is a stranger who has managed to look exactly like my wife, sounds exactly like my wife, owns my wife's clothes and wedding ring and so on, and knows enough of my wife's secrets to answer any question I put to her exactly like my wife would.\" The lack of affection vaguely supports the stranger hypothesis, but the prior for the stranger hypothesis is so low that it should never even enter consideration (remember this phrasing: it will become important later.) Likewise, we've all felt really awesome at one point or another, but it's never occurred to most of us that maybe we are simultaneously Jesus and John Lennon.<br /><br />Further, most psychiatric patients with the deficits involved don't develop delusions. People with damage to the ventromedial area suffer the same disconnection between face recognition and emotional processing as Capgras patients, but they don't draw any unreasonable conclusions from it. Most people who get paralyzed don't come down with anosognosia, and most people with mania or anxiety don't think they're Jesus or persecuted by the CIA. What's the difference between these people and the delusional patients?<br /><br />The difference is the right dorsolateral prefrontal cortex, an area of the brain strongly associated with delusions. If whatever brain damage broke your emotional reactions to faces or paralyzed you or whatever spared the RDPC, you are unlikely to develop delusions. If your brain damage also damaged this area, you are correspondingly more likely to come up with a weird explanation.<br /><br />In his first papers on the subject, Coltheart vaguely refers to the RDPC as a \"belief evaluation\" center. Later, he gets more specific and talks about its role in Bayesian updating. In his chronology, a person damages the connection between face recognition and emotion, and \"rationally\" concludes the Capgras hypothesis. In his model, even if there's only a 1% prior of your spouse being an imposter, if there's a 1000 times greater likelihood of you not feeling anything toward an imposter than to your real spouse, you can \"rationally\" come to believe in the delusion. In normal people, this rational belief then gets worn away by updating based on evidence: the imposter seems to know your spouse's personal details, her secrets, her email passwords. In most patients, this is sufficient to have them update back to the idea that it is really their spouse. In Capgras patients, the damage to the RDPC prevents updating on \"exogenous evidence\" (for some reason, the endogenous evidence of the lack of emotion itself still gets through) and so they maintain their delusion.<br /><br />This theory has some trouble explaining why patients are still able to update about other situations, but Coltheart speculates that maybe the belief evaluation system is weakened but not totally broken, and can deal with anything except the ceaseless stream of contradictory endogenous information.<br /><strong></strong></p>\n<p><strong>EXPLANATORY ADEQUACY BIAS</strong><br /><br />McKay <a href=\"http://pure.rhul.ac.uk/portal/files/4679994/Delusional_Inference_McKay.pdf\">makes an excellent critique</a> of several questionable assumptions of this theory.<br /><br />First, is the Capgras hypothesis <em>ever </em>plausible? Coltheart et al pretend that the prior is 1/100, but this implies that there is a base rate of your spouse being an imposter one out of every hundred times you see her (or perhaps one out of every hundred people has a fake spouse) either of which is preposterous. No reasonable person could entertain the Capgras hypothesis even for a second, let alone for long enough that it becomes their working hypothesis and develops immunity to further updating from the broken RDPC.<br /><br />Second, there's no evidence that the ventromedial patients - the ones who lose face-related emotions but don't develop the Capgras delusion - once had the Capgras delusion but then successfully updated their way out of it. They just never develop the delusion to begin with. <br /><br />McKay keeps the Bayesian model, but for him the second factor is not a deficit in updating in general, but a deficit in the use of priors. He lists two important criteria for reasonable belief: \"explanatory adequacy\" (what standard Bayesians call the likelihood ratio; the new data must be more likely if the new belief is true than if it is false) and \"doxastic conservativism\" (what standard Bayesians call the prior; the new belief must be reasonably likely to begin with given everything else the patient knows about the world).</p>\n<p>Delusional patients with damage to their RDPC lose their ability to work with priors and so abandon all doxastic conservativism, essentially falling into a what we might term the Super Base Rate Fallacy. For them the only important criterion for a belief is explanatory adequacy. So when they notice their spouse's face no longer elicits any emotion, they decide that their spouse is not really their spouse at all. This does a great job of explaining the observed data - maybe the best job it's possible for an explanation to do. Its only minor problem is that it has a stupendously low prior, and this doesn't matter because they are no longer able to take priors into account.</p>\n<p>This also explains why the delusional belief is impervious to new evidence. Suppose the patient's spouse tells personal details of their honeymoon that no one else could possibly know. There are several possible explanations: the patient's spouse really is the patient's spouse, or (says the left-brain Apologist) the patient's spouse is an alien who was able to telepathically extract the relevant details from the patient's mind. The telepathic alien imposter hypothesis has great explanatory adequacy: it explains why the person looks like the spouse (the alien is a very good imposter), why the spouse produces no emotional response (it's not the spouse at all) and why the spouse knows the details of the honeymoon (the alien is telepathic). The \"it's really your spouse\" explanation only explains the first and the third observations. Of course, we as sane people know that the telepathic alien hypothesis has a very low base rate plausibility because of its high complexity and violation of Occam's Razor, but these are exactly the factors that the RDPC-damaged<sup>2</sup> patient can't take into account. Therefore, the seemingly convincing new evidence of the spouse's apparent memories only suffices to help the delusional patient infer that the imposter is telepathic.<br /><br />The Super Base Rate Fallacy can explain the other delusional states as well. I recently met a patient who was, indeed, convinced the CIA were after her; of note she also had extreme anxiety to the point where her arms were constantly shaking and she was hiding under the covers of her bed. CIA pursuit is probably the best possible reason to be anxious; the only reason we don't use it more often is how few people are really pursued by the CIA (well, as far as we know). My mentor warned me not to try to argue with the patient or convince her that the CIA wasn't really after her, as (she said from long experience) it would just make her think I was in on the conspiracy. This makes sense. \"The CIA is after you and your doctor is in on it\" explains both anxiety and the doctor's denial of the CIA very well; \"The CIA is not after you\" explains only the doctor's denial of the CIA. For anyone with a pathological inability to handle Occam's Razor, the best solution to a challenge to your hypothesis is always to make your hypothesis more elaborate.<br /><strong><br />OPEN QUESTIONS</strong><br /><br />Although I think McKay's model is a serious improvement over its predecessors, there are a few loose ends that continue to bother me.<br /><br />\"You have brain damage\" is also a theory with perfect explanatory adequacy. If one were to explain the Capgras delusion to Capgras patients, it would provide just as good an explanation for their odd reactions as the imposter hypothesis. Although the patient might not be able to appreciate its decreased complexity, they should at least remain indifferent between the two hypotheses. I've never read of any formal study of this, but given that someone must have tried explaining the Capgras delusion to Capgras patients I'm going to assume it doesn't work. Why not?<br /><br />Likewise, how come delusions are so specific? It's impossible to convince someone who thinks he is Napoleon that he's really just a random non-famous mental patient, but it's also impossible to convince him he's Alexander the Great (at least I think so; I don't know if it's ever been tried). But him being Alexander the Great is also consistent with his observed data and his deranged inference abilities. Why decide it's the CIA who's after you, and not the KGB or Bavarian Illuminati?<br /><br />Why is the failure so often <a href=\"http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0034771\">limited to failed inference from mental states</a>? That is, if a Capgras patient sees it is raining outside, the same process of base rate avoidance that made her fall for the Capgras delusion ought to make her think she's been transported to ther rainforest or something. This happens in polythematic delusion patients, where anything at all can generate a new delusion, but not those with monothematic delusions like Capgras. There must be some fundamental difference between how one draws inferences from mental states versus everything else.<br /><br />This work also raises the question of whether one can one consciously use System II Bayesian reasoning to argue oneself out of a delusion. It seems improbable, but I recently heard about an n=1 personal experiment of a rationalist with schizophrenia who used successfully used Bayes to convince themselves that a delusion (or possibly hallucination; the story was unclear) was false. I don't have their permission to post their story here, but I hope they'll appear in the comments.<br /><strong><br />FOOTNOTES</strong><br /><br /><strong>1:</strong> I left out discussion of the <a href=\"http://en.wikipedia.org/wiki/Alien_Hand_Syndrome\">Alien Hand Syndrome</a>, even though it was in my sources, because I believe it's more complicated than a simple delusion. There's some evidence that the alien hand actually does move independently; for example it will sometimes attempt to thwart tasks that the patient performs voluntarily with their good hand. Some sort of \"split brain\" issues seem like a better explanation than simple Mind Projection.<br /><br /><strong>2:</strong> The right dorsolateral prefrontal cortex <a href=\"http://www.mpg.de/5925490/meta-consciousness-brain\">also shows up in dream research</a>, where it tends to be one of the parts of the brain shut down during dreaming. This provides a reasonable explanation of why we don't notice our dreams' implausibility while we're dreaming them - and Eliezer specifically mentions he <a href=\"/lw/13b/dreams_with_damaged_priors/\">can't use priors correctly in his dreams</a>. It also highlights some interesting parallels between dreams and the monothematic delusions. For example, the typical \"And then I saw my mother, but she was also somehow my fourth grade teacher at the same time\" effect seems sort of like Capgras and Fregoli. Even more interestingly, the RDPC gets switched on during lucid dreaming, providing an explanation of why lucid dreamers are able to reason normally in dreams. Because lucid dreaming also involves a sudden \"switching on\" of \"awareness\", this makes the RDPC a good target area for consciousness research.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 2, "xHjy88N2uJvGdgzfw": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "euJm4RwkAptZnP89i", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 106, "baseScore": 143, "extendedScore": null, "score": 0.000303, "legacy": true, "legacyId": "18221", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 145, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 155, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZiQqsgGX6a42Sfpii", "8z2Fm2yaHpQz8rr5B", "yJfBzcDL9fBHJfZ6P"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-13T21:19:06.671Z", "modifiedAt": null, "url": null, "title": "Cynical explanations of FAI critics (including myself)", "slug": "cynical-explanations-of-fai-critics-including-myself", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:27.755Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Cqo5uKA6r8jickEhA/cynical-explanations-of-fai-critics-including-myself", "pageUrlRelative": "/posts/Cqo5uKA6r8jickEhA/cynical-explanations-of-fai-critics-including-myself", "linkUrl": "https://www.lesswrong.com/posts/Cqo5uKA6r8jickEhA/cynical-explanations-of-fai-critics-including-myself", "postedAtFormatted": "Monday, August 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cynical%20explanations%20of%20FAI%20critics%20(including%20myself)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACynical%20explanations%20of%20FAI%20critics%20(including%20myself)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCqo5uKA6r8jickEhA%2Fcynical-explanations-of-fai-critics-including-myself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cynical%20explanations%20of%20FAI%20critics%20(including%20myself)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCqo5uKA6r8jickEhA%2Fcynical-explanations-of-fai-critics-including-myself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCqo5uKA6r8jickEhA%2Fcynical-explanations-of-fai-critics-including-myself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 360, "htmlBody": "<p><strong>Related Posts: </strong><a href=\"/lw/dy8/a_cynical_explanation_for_why_rationalists_worry/\">A cynical explanation for why rationalists worry about FAI</a>,&nbsp;<a href=\"/lw/bl2/a_belief_propagation_graph/\">A belief propagation graph</a></p>\n<p>Lately I've been pondering the fact that while there are many critics of SIAI and its plan to form a team to build FAI, few of us seem to agree on what SIAI or we should do instead. Here are some of the alternative suggestions offered so far:</p>\n<ul>\n<li>work on computer security</li>\n<li>work to improve laws and institutions</li>\n<li>work on mind uploading</li>\n<li>work on intelligence amplification</li>\n<li>work on non-autonomous AI (e.g., Oracle AI, \"Tool AI\", automated formal reasoning systems, etc.)</li>\n<li>work on academically \"mainstream\" AGI approaches or trust that those researchers know what they are doing</li>\n<li>stop worrying about the Singularity and work on more mundane goals</li>\n</ul>\n<div>Given that ideal reasoners are not supposed to disagree, it seems likely that most if not all of these alternative suggestions can also be explained by their proponents being less than rational. Looking at myself and <a href=\"/lw/6mi/some_thoughts_on_singularity_strategies/\">my suggestion</a> to work on IA or uploading, I've noticed that I have a tendency to be initially over-optimistic about some technology and then become gradually more&nbsp;pessimistic&nbsp;as I learn more details about it, so that I end up being more optimistic about technologies that I'm less familiar with than the ones that I've studied in detail. (Another example of this is me being initially enamoured with Cypherpunk ideas and then giving up on them after inventing some key pieces of the necessary technology and seeing in more detail how it would actually have to work.)</div>\n<div>I'll skip giving explanations for other critics to avoid offending them, but it shouldn't be too hard for the reader to come up with their own explanations. It seems that I can't trust any of the FAI critics, including myself, nor do I think Eliezer and company are much better at reasoning or intuiting their way to a correct conclusion about how we should face the apparent threat and opportunity that is the Singularity. What <em>useful</em>&nbsp;implications can I draw from this? I don't know, but it seems like it can't hurt to pose the question to LessWrong.&nbsp;</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Cqo5uKA6r8jickEhA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 31, "extendedScore": null, "score": 9.63630413693727e-07, "legacy": true, "legacyId": "18236", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xDMdiiTYcZWvhjdoe", "Ce3kKTc5PAqLdWpfL", "73SotZnDbsYpxfnuQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-13T22:27:35.875Z", "modifiedAt": null, "url": null, "title": "Meetup : Meetup: Southwestern Ohio", "slug": "meetup-meetup-southwestern-ohio", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:35.641Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "dugancm", "createdAt": "2011-04-18T22:16:25.992Z", "isAdmin": false, "displayName": "dugancm"}, "userId": "EWqaPbouAnkiYDumx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bpkXmG57SgJjoRNK8/meetup-meetup-southwestern-ohio", "pageUrlRelative": "/posts/bpkXmG57SgJjoRNK8/meetup-meetup-southwestern-ohio", "linkUrl": "https://www.lesswrong.com/posts/bpkXmG57SgJjoRNK8/meetup-meetup-southwestern-ohio", "postedAtFormatted": "Monday, August 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Meetup%3A%20Southwestern%20Ohio&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Meetup%3A%20Southwestern%20Ohio%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbpkXmG57SgJjoRNK8%2Fmeetup-meetup-southwestern-ohio%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Meetup%3A%20Southwestern%20Ohio%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbpkXmG57SgJjoRNK8%2Fmeetup-meetup-southwestern-ohio", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbpkXmG57SgJjoRNK8%2Fmeetup-meetup-southwestern-ohio", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 88, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/cv'>Meetup: Southwestern Ohio</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 September 2012 04:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">21 Greene Boulevard, Beavercreek, Ohio 45440, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>New location! Dinner at <a href=\"http://www.thewineloftdayton.com/\" rel=\"nofollow\">The Wine Loft</a> near Dayton, Ohio. Reservation is under Dugan.\nAfter dinner activities may include:</p>\n\n<p>Presentation and discussion of preliminary material for a \"Rational Mattress Buying\" article (inspired by <a href=\"http://lesswrong.com/lw/cqz/rational_toothpaste_a_case_study/\">Rational Toothpaste: A Case Study</a>).</p>\n\n<p>Exploring and appraising <a href=\"http://www.thegreene.com/portals/greene/pdf/the-greene-directory-map.pdf\" rel=\"nofollow\">The Greene</a> as a location to hold future meetups.</p>\n\n<p>Splitting off into small groups for rejection therapy in unfamiliar surroundings.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/cv'>Meetup: Southwestern Ohio</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bpkXmG57SgJjoRNK8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 9.636645001887303e-07, "legacy": true, "legacyId": "18237", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Meetup__Southwestern_Ohio\">Discussion article for the meetup : <a href=\"/meetups/cv\">Meetup: Southwestern Ohio</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 September 2012 04:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">21 Greene Boulevard, Beavercreek, Ohio 45440, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>New location! Dinner at <a href=\"http://www.thewineloftdayton.com/\" rel=\"nofollow\">The Wine Loft</a> near Dayton, Ohio. Reservation is under Dugan.\nAfter dinner activities may include:</p>\n\n<p>Presentation and discussion of preliminary material for a \"Rational Mattress Buying\" article (inspired by <a href=\"http://lesswrong.com/lw/cqz/rational_toothpaste_a_case_study/\">Rational Toothpaste: A Case Study</a>).</p>\n\n<p>Exploring and appraising <a href=\"http://www.thegreene.com/portals/greene/pdf/the-greene-directory-map.pdf\" rel=\"nofollow\">The Greene</a> as a location to hold future meetups.</p>\n\n<p>Splitting off into small groups for rejection therapy in unfamiliar surroundings.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Meetup__Southwestern_Ohio1\">Discussion article for the meetup : <a href=\"/meetups/cv\">Meetup: Southwestern Ohio</a></h2>", "sections": [{"title": "Discussion article for the meetup : Meetup: Southwestern Ohio", "anchor": "Discussion_article_for_the_meetup___Meetup__Southwestern_Ohio", "level": 1}, {"title": "Discussion article for the meetup : Meetup: Southwestern Ohio", "anchor": "Discussion_article_for_the_meetup___Meetup__Southwestern_Ohio1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NHuLAS3oKZWr2X9hP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-13T23:13:04.545Z", "modifiedAt": null, "url": null, "title": "Meetup : Longmont Colorado Meetup", "slug": "meetup-longmont-colorado-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CwJ9SajwKQSz37eJT/meetup-longmont-colorado-meetup-0", "pageUrlRelative": "/posts/CwJ9SajwKQSz37eJT/meetup-longmont-colorado-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/CwJ9SajwKQSz37eJT/meetup-longmont-colorado-meetup-0", "postedAtFormatted": "Monday, August 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Longmont%20Colorado%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Longmont%20Colorado%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCwJ9SajwKQSz37eJT%2Fmeetup-longmont-colorado-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Longmont%20Colorado%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCwJ9SajwKQSz37eJT%2Fmeetup-longmont-colorado-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCwJ9SajwKQSz37eJT%2Fmeetup-longmont-colorado-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 86, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/cw'>Longmont Colorado Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 August 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Ziggis, 400 Main Street, Longmont, CO</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The move south worked well, so we're planning on cycling back and forth for a while. Next week, back to Fort Collins.</p>\n\n<p>I listened to this podcast this week:\n<a href=\"http://www.econtalk.org/archives/2012/01/david_rose_on_t.html\" rel=\"nofollow\">http://www.econtalk.org/archives/2012/01/david_rose_on_t.html</a></p>\n\n<p>Humans are primed for making ethical decisions based on the impact on small groups.</p>\n\n<p>How do we change our decision making to scale with much larger groups, and more complexity?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/cw'>Longmont Colorado Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CwJ9SajwKQSz37eJT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.636868759073655e-07, "legacy": true, "legacyId": "18238", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Longmont_Colorado_Meetup\">Discussion article for the meetup : <a href=\"/meetups/cw\">Longmont Colorado Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 August 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Ziggis, 400 Main Street, Longmont, CO</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The move south worked well, so we're planning on cycling back and forth for a while. Next week, back to Fort Collins.</p>\n\n<p>I listened to this podcast this week:\n<a href=\"http://www.econtalk.org/archives/2012/01/david_rose_on_t.html\" rel=\"nofollow\">http://www.econtalk.org/archives/2012/01/david_rose_on_t.html</a></p>\n\n<p>Humans are primed for making ethical decisions based on the impact on small groups.</p>\n\n<p>How do we change our decision making to scale with much larger groups, and more complexity?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Longmont_Colorado_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/cw\">Longmont Colorado Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Longmont Colorado Meetup", "anchor": "Discussion_article_for_the_meetup___Longmont_Colorado_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Longmont Colorado Meetup", "anchor": "Discussion_article_for_the_meetup___Longmont_Colorado_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-13T23:34:43.406Z", "modifiedAt": null, "url": null, "title": "Why Don't People Help Others More?", "slug": "why-don-t-people-help-others-more", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:37.623Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jisCHmxwmKoNwrRst/why-don-t-people-help-others-more", "pageUrlRelative": "/posts/jisCHmxwmKoNwrRst/why-don-t-people-help-others-more", "linkUrl": "https://www.lesswrong.com/posts/jisCHmxwmKoNwrRst/why-don-t-people-help-others-more", "postedAtFormatted": "Monday, August 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20Don't%20People%20Help%20Others%20More%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20Don't%20People%20Help%20Others%20More%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjisCHmxwmKoNwrRst%2Fwhy-don-t-people-help-others-more%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20Don't%20People%20Help%20Others%20More%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjisCHmxwmKoNwrRst%2Fwhy-don-t-people-help-others-more", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjisCHmxwmKoNwrRst%2Fwhy-don-t-people-help-others-more", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2875, "htmlBody": "<p>As Peter Singer writes in his book <a href=\"http://www.amazon.com/dp/1400067103\"><strong>The Life You Can Save</strong></a>: \"[t]he world would be a much simpler place if one could bring about social change merely by making a logically consistent moral argument\". Many people one encounters might agree that a social change movement is noble yet not want to do anything to promote it, or want to give more money to a charity yet refrain from doing so. Additional moralizing doesn't seem to do the trick. ...So what does?</p>\n<p>Motivating people to altruism is relevant for the <a href=\"/lw/6py/optimal_philanthropy_for_human_beings/\">optimal philanthropy movement</a>. &nbsp;For a start on the answer, like many things, I turn to psychology. Specifically, the psychology Peter Singer catalogues in his book.</p>\n<p>&nbsp;</p>\n<h2>A Single, Identifiable Victim</h2>\n<p>One of the most well-known motivations behind helping others is a personal connection, which triggers empathy. When psychologists researching generosity paid participants to join a psychological experiment and then later gave these participants the opportunity to donate to a global poverty fighting organization <a href=\"http://www.savethechildren.org/site/c.8rKLIXMGIpI4E/b.6115947/k.8D6E/Official_Site.htm\">Save the Children</a>, two different kinds of information were given.</p>\n<p>One random group of participants were told \"Food shortages in Malawi are affecting more than three million children\" and some additional information about how the need for donations was very strong, and these donations could help stop the food shortages.</p>\n<p>Another random group of participants were instead shown the photo of Rokia, a seven-year-old Malawian girl who is desperately poor. The participants were told that \"her life will be changed for the better by your gift\".</p>\n<p>Furthermore, a third random group of participants were shown the photo of Rokia, told about who she is and that \"her life will be changed for the better\", but ALSO told about the general information about the famine and told the same \"food shortages [...] are affecting more than three million\" -- a combination of both the previous groups.</p>\n<p>Lastly, a fourth random group was shown the photo of Rokia, informed about her the same as the other groups, and then given information about another child, identified by name, and told that their donation would also affect this child too for the better.</p>\n<p><strong><br />It's All About the Person</strong></p>\n<p>Interestingly, the group who was told ONLY about Rokia gave the most money. The group who was told about both children reported feeling less overal emotion than those who only saw Rokia, and gave less money. The group who was told about both Rokia and the general famine information gave even less than that, followed by the group that only got the general famine information.<sup>1,2</sup> &nbsp;It turns out that information about a single person was the most salient for creating an empathetic response to trigger a willingness to donate.<sup>1,2</sup></p>\n<p>This continues through additional studies. In another generosity experiment, one group of people was told that a single child needed a lifesaving medical treatment that costs $300K, and was given the opportunity to contribute towards this fund. A second random group of people was told that eight children needed a lifesaving treatment, and all of them would die unless $300K could be provided, and was given an opportunity to contribute. More people opted to donate toward the single child.<sup>3,4</sup></p>\n<p>This is the basis for why we're so willing to chase after <a href=\"http://en.wikipedia.org/wiki/2010_Copiap%C3%B3_mining_accident\">lost miners</a> or <a href=\"http://en.wikipedia.org/wiki/Jessica_McClure\">Baby Jessica</a> no matter the monetary cost, but turn a blind eye to the mass unknown starving in the developing world. &nbsp;Indeed, the person doesn't even need to be particularly identified, though it does help. In another experiment, people asked by researchers to make a donation to Habitat for Humanity were more likely to do so if they were told that the family \"has been selected\" rather than that they \"will be selected\" -- even though all other parts of the pitch were the same, and the participants got no information about who the families actually were<sup>5</sup>.</p>\n<p><strong><br />The Deliberative and The Affective</strong></p>\n<p>Why is this the case? Researcher Paul Slovic thinks that humans have two different processes for deciding what to do. The first is an <strong>affective system</strong> that responds to emotion, rapidly processing images and stories and generating an intuitive feeling that leads to immediate action. The second is a <strong>deliberative system</strong> that draws on reasoning, and operates on words, numbers, and abstractions, which is much slower to generate action.<sup>6</sup></p>\n<p>To follow up, the Rokia experiment was done again, except yet another twist was added -- there were two groups, one told only about Rokia exactly as before, and one told only the generic famine information exactly as before. Within each group, half the group took a survey designed to arouse their emotions by asking them things like \"When you hear the word 'baby' how do you feel?\" The other half of both groups was given emotionally neutral questions, like math puzzles.</p>\n<p>This time, the Rokia group gave far more, but those in the group who randomly had their emotions aroused gave even more than those who heard about Rokia but had finished math problems. On the other side, those who heard the generic famine information showed no increase in donation regardless of how heightened their emotions were.<sup>1</sup></p>\n<p>&nbsp;</p>\n<h2>Futility and Making a Difference</h2>\n<p>Imagine you're told that there are 3000 refugees at risk in a camp in Rwanda, and you could donate towards aid that would save 1500 of them. Would you do it? And how much would you donate?</p>\n<p>Now this time imagine that you can still save 1500 refugees with the same amount of money, but the camp has 10000 refugees. In an experiment where these two scenarios were presented not as a thought experiment but as realities to two separate random groups, the group that heard of only 3000 refugees were more likely to donate, and donated larger amounts.<sup>7,8</sup></p>\n<p>Enter another quirk of our giving psychology, right or wrong: <strong>futility thinking</strong>. We think that if we're not making a sizable difference, it's not worth making the difference at all -- it will only be a drop in the ocean and the problem will keep raging on.</p>\n<p>&nbsp;</p>\n<h2>Am I Responsible?</h2>\n<p>People are also far less likely to help if they're with other people. In this experiment, students were invited to participate in a market research survey. However, when the researcher gave the students their questionnaire to fill out, she went into a back room separated from the office only by a curtain. A few minutes later, noises strongly suggested that she had got on a chair to get something from a high shelf, and then fell off it, loudly complaining that she couldn't feel or move her foot.</p>\n<p>With only one student taking the survey, 70% of them stopped what they were doing and offered assistance. However, when there were two students taking the survey, this number dropped down dramatically. Most noticeably, when the group was two students -- but one of the students was a stooge who was in on it and would always not respond, the response rate of the non-stooge participant was only 7%.<sup>9</sup></p>\n<p>This one is known as <strong>diffusion of responsibility</strong>, better known as the <strong>bystander effect</strong> -- we help more often when we think it is our responsibility to do so, and -- again for right or for wrong -- we naturally look to others to see if they're helping before doing so ourselves.</p>\n<p>&nbsp;</p>\n<h2>What's Fair In Help?</h2>\n<p>It's clear that people value fairness, even to their own detriment. In a game called \"the Ultimatum Game\", one participant is given a sum of money by the researcher, say $10, and told they can split this money with an anonymous second player in any proportion they choose -- give them $10, give them $7, give them $5, give them nothing, everything is fair game. The catch is, however, the second player, after hearing of the split anonymously, gets to vote to accept it or reject it. Should the split be accepted, both players walk away with the agreed amount. But should the split be rejected, both players walk away with nothing.</p>\n<p><strong><br />A Fair Split</strong></p>\n<p>The economist, expecting ideally rational and perfectly self-interested players, predicts that the second player would accept any split that gets them money, since anything is better than nothing. And the first player, understanding this, would naturally offer $1 and keep $9 for himself. At no point are identities revealed, so reputation and retribution are no issue.</p>\n<p>But the results turn out to be quite different -- the vast majority offer an equal split. Yet, when an offer comes around that offers $2 or less, it is almost always rejected, even though $2 is better than nothing.<sup>10</sup> &nbsp;And this effect persists even when played for thousands of dollars and persists across nearly all cultures.</p>\n<p><strong><br />Splitting and Anchoring in Charity</strong></p>\n<p>This sense of fairness persists into helping as well -- people generally have a strong tendency not to want to help more than the other people around them, and if they find themselves the only ones helping on a frequent basis, they start to feel a \"sucker\". On the flipside, if others are doing more, they will follow suit.<sup>11,12,13</sup></p>\n<p>Those told the average donation to a charity nearly always tend to give that amount, even if the average told to them is a lie, having secretly been increased or decreased. And it can be replicated even without lying -- those told about an above average gift were far more likely to donate more, even attempting to match that gift.<sup>14,15</sup> &nbsp;Overall, we tend to match the behavior of our reference class -- those people we identify with -- and this includes how much we help. We donate more when we believe others are donating more, and donate less when we believe others are doing so.</p>\n<p>&nbsp;</p>\n<h2>Challenging the Self-Interest Norm</h2>\n<p>But there's a way to break this cycle of futility, responsibility, and fairness -- challenge the norm by openly communicating about helping others. While many religious and secular values insist that the best giving is anonymous giving, this turns out to not always be the case. While there may be other reasons to give anonymously, don't forget the benefits of giving openly -- being open about helping inspires others to help, and can help challenge the norms of the culture.</p>\n<p>Indeed, many organizations now exist to help challenge the norms of donations and try to create a culture where they give more. <a href=\"http://www.givingwhatwecan.org/\">GivingWhatWeCan</a> is a community of 230 people (<a href=\"http://www.greatplay.net/essays/joining-givingwhatwecan\">including me!</a>) who have all pledged to donate at least 10% of their income to organizations working on ending extreme poverty, and submit statements proving so. <a href=\"http://boldergiving.org/stories.php\">BolderGiving</a> has a bunch of inspiring stories of over 100 people who all give at least 20% of their income, with a dozen giving over 90%! And these aren't all rich people, some of them are even ordinary students.</p>\n<p><strong><br />Who's Willing to Be Altruistic?</strong></p>\n<p>While people are not saints, experiments have shown that people tend to grossly overestimate how self-interested other people are -- for one example, people estimated that males would overwhelmingly favor a piece of legislation to \"slash research funding to a disease that affects only women\", even while -- being male -- they themselves do not support such legislation.<sup>16</sup></p>\n<p>This also manifests itself in an expectation that people be \"self-interested\" in their philanthropic cause -- suggesting much stronger support for volunteers in Students Against Drunk Driving who themselves knew people killed in drunk driving accidents versus those people who had no such personal experiences but just thought it to be \"a very important cause\".<sup>17</sup></p>\n<p>Alex de Tocqueville, echoing the early economists who expected $9/$1 splits in the Ultimatum Game, wrote in 1835 that \"Americans enjoy explaining almost every act of their lives on the principle of self-interest\".<sup>18</sup> &nbsp;But this isn't always the case, and in challenging the norm, people make it more acceptable to be altruistic. It's not just \"goody two-shoes\", and it's praiseworthy to be \"too charitable\".</p>\n<p>&nbsp;</p>\n<h2>A Bit of a Nudge</h2>\n<p>A somewhat pressing problem in getting people to help was in organ donation -- surely no one was inconvenienced by having their organs donated after they had died. Yet, why would people not sign up? &nbsp;And how could we get more people to sign up?</p>\n<p>In Germany, only 12% of the population are registered organ donors. In nearby Austria, that number is 99.98%. Are people in Austria just less worried about what will happen to them after they die, or just that more altruistic? It turns out the answer is far more simple -- in Germany you must put yourself on the register to become a potential donor (opt-in), whereas in Austria you are a potential donor unless you object (opt-out). While people may be, for right or for wrong, worried about the fate of their body after it is dead, they appear less likely to express these reservations in opt-out systems.<sup>19</sup></p>\n<p>While Richard Thaler and Cass Sunstein argue in their book <a href=\"http://www.amazon.com/dp/0300122233\"><strong>Nudge: Improving Decisions About Health, Wellness, and Happiness</strong></a> that we sometimes suck at making decisions in our own interest and all could do better with more favorable \"defaults\", such defaults are also pressing in helping people.</p>\n<p>While opt-out organ donation is a huge deal, there's another similar idea -- <strong>opt-out philanthropy</strong>. Back before 2008 when the investment bank Bear Stearns still existed, Bear Stearns listed their guiding principle as philanthropy as fostering good citizenship and well-rounded individuals. To this effect, <a href=\"http://query.nytimes.com/gst/fullpage.html?res=9905E4DB1530F931A25752C1A9619C8B63&amp;pagewanted=all\">they required the top 1000 most highest paid employees to donate 4% of their salary and bonuses to non-profits</a>, and prove it with their tax returns. This resulted in more than $45 million in donations during 2006. Many employees described the requirement as \"getting themselves to do what they wanted to do anyway\".</p>\n<p>&nbsp;</p>\n<h2>Conclusions</h2>\n<p>So, according to this bit of psychology, what could we do to get other people to help more, besides moralize? Well, we have five key take-aways:</p>\n<p>(1) present these people with a single and highly identifiable victim that they can help<br />(2) nudge them with a default of opt-out philanthropy<br />(3) be more open about our willingness to be altruistic and encourage other people to help<br />(4) make sure people understand the average level of helping around them, and<br />(5) instill a responsibility to help and an understanding that doing so is not futile.</p>\n<p>Hopefully, with these tips and more, helping people more can be come just one of those things we do.</p>\n<p>&nbsp;</p>\n<h2>References</h2>\n<h5><span style=\"font-weight: normal;\"><em>(Note: Links are to PDF files.)<br /><br /></em>1: D. A. Small, G. Loewenstein, and P. Slovic. 2007. <a href=\"http://sds.hss.cmu.edu/media/pdfs/loewenstein/SympathyCallousness.pdf\">\"Sympathy and Callousness: The Impact of Deliberative Thought on Donations to Identifiable and Statistical Victims\"</a>. <em>Organizational Behavior and Human Decision Processes </em>102: p143-53<br /></span><span style=\"font-weight: normal; \"><br />2: Paul Slovic. 2007. </span><a style=\"font-weight: normal; \" href=\"http://www18.homepage.villanova.edu/diego.fernandezduque/Teaching/CognitivePsychology/Lectures_and_Labs/sssAppliedIssues/If_I_Look_At_The_Mass.pdf\">\"If I Look at the Mass I Will Never Act: Psychic Numbing and Genocide\"</a><span style=\"font-weight: normal; \">. </span><em style=\"font-weight: normal; \">Judgment and Decision Making </em><span style=\"font-weight: normal; \"><span style=\"font-weight: normal;\">2(2): p79-95.</span><br /><br /><span style=\"font-weight: normal;\">3: T. Kogut and I. Ritov. 2005. </span><a style=\"font-weight: normal; \" href=\"http://pluto.mscc.huji.ac.il/~msiritov/KogutRitovIdentified.pdf\">\"The 'Identified Victim' Effect: An Identified Group, or Just a Single Individual?\"</a><span style=\"font-weight: normal;\">. </span><em style=\"font-weight: normal; \">Journal of Behavioral Decision Making</em><span style=\"font-weight: normal;\">&nbsp;18: p157-67.</span><br /><br /><span style=\"font-weight: normal;\">4: T. Kogut and I. Ritov. 2005. </span><a style=\"font-weight: normal; \" href=\"https://www.unitn.it/files/download/14235/singularity_of_identified_victim.pdf\">\"The Singularity of Identified Victims in Separate and Joint Evaluations\"</a><span style=\"font-weight: normal;\">. </span><em style=\"font-weight: normal; \">Organizational Behavior and Human Decision Processes</em><span style=\"font-weight: normal;\">&nbsp;97: p106-116.</span><br /><br /><span style=\"font-weight: normal;\">5: D. A. Small and G. Lowenstein. 2003. </span><a style=\"font-weight: normal; \" href=\"http://sds.hss.cmu.edu/media/pdfs/loewenstein/HelpingVictimAltruism.pdf\">\"Helping the Victim or Helping a Victim: Altruism and Identifiability\"</a><span style=\"font-weight: normal;\">. </span><em style=\"font-weight: normal; \">Journal of Risk and Uncertainty</em><span style=\"font-weight: normal;\">&nbsp;26(1): p5-16.</span><br /><br /><span style=\"font-weight: normal;\">6: Singer cites this from Paul Slovic, who in turn cites it from: Seymour Epstein. 1994. \"Integration of the Cognitive and the Psychodynamic&nbsp;Unconscious\". <em>American Psychologist </em>49: p709-24. &nbsp;Slovic refers to the affective system as \"experiential\" and the deliberative system as \"analytic\". &nbsp;This is also related to Daniel Kahneman's popular book </span><a href=\"http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637\">Thinking Fast and Slow</a><span style=\"font-weight: normal;\"><span style=\"font-weight: normal;\">.</span><br /><br /><span style=\"font-weight: normal;\">7: D. Fetherstonhaugh, P. Slovic, S. M. Johnson, and J. Friedrich. 1997. </span><a style=\"font-weight: normal;\" href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Fetherstonhaugh-et-al-Insensitivity-to-the-value-of-human-life-A-study-of-psychophysical-numbing.pdf\">\"Insensitivity to the Value of Human Life: A Study of Psychophysical Numbing\"</a><span style=\"font-weight: normal;\">. &nbsp;</span><em style=\"font-weight: normal;\">Journal of Risk and Uncertainty</em><span style=\"font-weight: normal;\">&nbsp;14: p283-300.</span><br /><br /><span style=\"font-weight: normal;\">8: Daniel Kahneman and Amos Tversky. 1979. </span><a style=\"font-weight: normal;\" href=\"http://www.hss.caltech.edu/~camerer/Ec101/ProspectTheory.pdf\">\"Prospect Theory: An Analysis of Decision Under Risk.\"</a> <em style=\"font-weight: normal;\">Econometrica</em><span style=\"font-weight: normal;\">&nbsp;47: p263-91.</span><br /><br /><span style=\"font-weight: normal;\">9: Bib Lantan&eacute; and John Darley. 1970.&nbsp;</span><a href=\"http://www.amazon.com/The-Unresponsive-Bystander-Doesnt-Help/dp/0139386130\">The Unresponsive Bystander: Why Doesn't He Help?</a><span style=\"font-weight: normal;\">. New York: Appleton-Century-Crofts, p58.</span><br /><br /><span style=\"font-weight: normal;\">10: Martin Nowak, Karen Page, and Karl Sigmund. 2000. </span><a style=\"font-weight: normal;\" href=\"http://homepage.univie.ac.at/karl.sigmund/UltimatumScience00.pdf\">\"Fairness Versus Reason in the Ultimatum Game\"</a><span style=\"font-weight: normal;\">. </span><em style=\"font-weight: normal;\">Science</em><span style=\"font-weight: normal;\"> 289: p1183-75.</span><br /><br /><span style=\"font-weight: normal;\">11: Lee Ross and Richard E. Nisbett. 1991. </span><a href=\"http://www.amazon.com/The-Person-Situation-Perspectives-Psychology/dp/0877228515\">The Person and the Situation: Perspectives of Social Psychology</a><span style=\"font-weight: normal;\">.&nbsp;Philadelphia: Temple University Press, p27-46.</span><br /><br /><span style=\"font-weight: normal;\">12: Robert Cialdini. 2001. </span><a href=\"http://www.amazon.com/Influence-Science-Practice-4th-Edition/dp/0321011473\">Influence: Science and Practice, 4th Edition</a><span style=\"font-weight: normal;\">. Boston: Allyn and Bacon.</span><br /><br /><span style=\"font-weight: normal;\">13: Judith Lichtenberg. 2004. \"Absence and the Unfond Heart: Why People Are Less Giving Than They Might Be\". in Deen Chatterjee, ed. </span><a href=\"http://www.amazon.com/The-Ethics-Assistance-Cambridge-Philosophy/dp/0521527422\">The Ethics of Assistance: Morality and the Distant Needy</a><span style=\"font-weight: normal;\">. Cambridge, UK: Cambridge University Press.</span><br /><br /><span style=\"font-weight: normal;\">14: Jen Shang and Rachel Croson. Forthcoming. <a href=\"http://www.fieldexperiments.com/uploads/[70].pdf\">\"Field Experiments in Charitable Contribution: The Impact of Social Influence on the Voluntary Provision of Public Goods\"</a>. <em>The Economic Journal</em>.</span><br /><br /><span style=\"font-weight: normal;\">15: Rachel Croson and Jen Shang. 2008. <a href=\"http://www.iu.edu/~spea/pubs/faculty/Croson_Shang_2008.pdf\">\"The Impact of Downward Social Information on Contribution Decision\"</a>. <em>Experimental Economics </em>11: p221-33.<br /><br />16: Dale Miller. 199. <a href=\"http://www.cepr.org/meets/wkcn/3/3509/papers/Miller.pdf\">\"The Norm of Self-Interest\"</a>. <em>American Psychologist</em> 54: 1053-60.<br /><br />17: Rebecca Ratner and Jennifer Clarke. Unpublished. \"Negativity Conveyed to Social Actors Who Lack a Personal Connection to the Cause\".<br /><br />18: Alexis de Tocqueville in J.P. Mayer ed., G. Lawrence, trans. 1969. <a href=\"http://www.amazon.com/Democracy-America-Alexis-Tocqueville/dp/0060915226\">Democracy in America</a>. Garden City, N.Y.: Anchor, p546.<br /><br />19: Eric Johnson and Daniel Goldstein. 2003. <a href=\"http://webs.wofford.edu/pechwj/Do%20Defaults%20Save%20Lives.pdf\">\"Do Defaults Save Lives?\"</a>. Science 302: p1338-39.</span></span></span></h5>\n<p>&nbsp;</p>\n<p><span style=\"font-weight: normal; \"><span style=\"font-weight: normal;\"><span style=\"font-weight: normal;\"><span style=\"font-size: 10px;\">(This is an updated version of an&nbsp;</span><a style=\"font-size: 10px;\" href=\"http://www.greatplay.net/essays/why-dont-people-help-others-more\">earlier draft from my blog</a><span style=\"font-size: 10px;\">.)</span></span></span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JsJPrdgRGRqnci8cZ": 1, "4R8JYu4QF2FqzJxE5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jisCHmxwmKoNwrRst", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 54, "extendedScore": null, "score": 0.000135, "legacy": true, "legacyId": "18239", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 54, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>As Peter Singer writes in his book <a href=\"http://www.amazon.com/dp/1400067103\"><strong>The Life You Can Save</strong></a>: \"[t]he world would be a much simpler place if one could bring about social change merely by making a logically consistent moral argument\". Many people one encounters might agree that a social change movement is noble yet not want to do anything to promote it, or want to give more money to a charity yet refrain from doing so. Additional moralizing doesn't seem to do the trick. ...So what does?</p>\n<p>Motivating people to altruism is relevant for the <a href=\"/lw/6py/optimal_philanthropy_for_human_beings/\">optimal philanthropy movement</a>. &nbsp;For a start on the answer, like many things, I turn to psychology. Specifically, the psychology Peter Singer catalogues in his book.</p>\n<p>&nbsp;</p>\n<h2 id=\"A_Single__Identifiable_Victim\">A Single, Identifiable Victim</h2>\n<p>One of the most well-known motivations behind helping others is a personal connection, which triggers empathy. When psychologists researching generosity paid participants to join a psychological experiment and then later gave these participants the opportunity to donate to a global poverty fighting organization <a href=\"http://www.savethechildren.org/site/c.8rKLIXMGIpI4E/b.6115947/k.8D6E/Official_Site.htm\">Save the Children</a>, two different kinds of information were given.</p>\n<p>One random group of participants were told \"Food shortages in Malawi are affecting more than three million children\" and some additional information about how the need for donations was very strong, and these donations could help stop the food shortages.</p>\n<p>Another random group of participants were instead shown the photo of Rokia, a seven-year-old Malawian girl who is desperately poor. The participants were told that \"her life will be changed for the better by your gift\".</p>\n<p>Furthermore, a third random group of participants were shown the photo of Rokia, told about who she is and that \"her life will be changed for the better\", but ALSO told about the general information about the famine and told the same \"food shortages [...] are affecting more than three million\" -- a combination of both the previous groups.</p>\n<p>Lastly, a fourth random group was shown the photo of Rokia, informed about her the same as the other groups, and then given information about another child, identified by name, and told that their donation would also affect this child too for the better.</p>\n<p><strong id=\"It_s_All_About_the_Person\"><br>It's All About the Person</strong></p>\n<p>Interestingly, the group who was told ONLY about Rokia gave the most money. The group who was told about both children reported feeling less overal emotion than those who only saw Rokia, and gave less money. The group who was told about both Rokia and the general famine information gave even less than that, followed by the group that only got the general famine information.<sup>1,2</sup> &nbsp;It turns out that information about a single person was the most salient for creating an empathetic response to trigger a willingness to donate.<sup>1,2</sup></p>\n<p>This continues through additional studies. In another generosity experiment, one group of people was told that a single child needed a lifesaving medical treatment that costs $300K, and was given the opportunity to contribute towards this fund. A second random group of people was told that eight children needed a lifesaving treatment, and all of them would die unless $300K could be provided, and was given an opportunity to contribute. More people opted to donate toward the single child.<sup>3,4</sup></p>\n<p>This is the basis for why we're so willing to chase after <a href=\"http://en.wikipedia.org/wiki/2010_Copiap%C3%B3_mining_accident\">lost miners</a> or <a href=\"http://en.wikipedia.org/wiki/Jessica_McClure\">Baby Jessica</a> no matter the monetary cost, but turn a blind eye to the mass unknown starving in the developing world. &nbsp;Indeed, the person doesn't even need to be particularly identified, though it does help. In another experiment, people asked by researchers to make a donation to Habitat for Humanity were more likely to do so if they were told that the family \"has been selected\" rather than that they \"will be selected\" -- even though all other parts of the pitch were the same, and the participants got no information about who the families actually were<sup>5</sup>.</p>\n<p><strong id=\"The_Deliberative_and_The_Affective\"><br>The Deliberative and The Affective</strong></p>\n<p>Why is this the case? Researcher Paul Slovic thinks that humans have two different processes for deciding what to do. The first is an <strong>affective system</strong> that responds to emotion, rapidly processing images and stories and generating an intuitive feeling that leads to immediate action. The second is a <strong>deliberative system</strong> that draws on reasoning, and operates on words, numbers, and abstractions, which is much slower to generate action.<sup>6</sup></p>\n<p>To follow up, the Rokia experiment was done again, except yet another twist was added -- there were two groups, one told only about Rokia exactly as before, and one told only the generic famine information exactly as before. Within each group, half the group took a survey designed to arouse their emotions by asking them things like \"When you hear the word 'baby' how do you feel?\" The other half of both groups was given emotionally neutral questions, like math puzzles.</p>\n<p>This time, the Rokia group gave far more, but those in the group who randomly had their emotions aroused gave even more than those who heard about Rokia but had finished math problems. On the other side, those who heard the generic famine information showed no increase in donation regardless of how heightened their emotions were.<sup>1</sup></p>\n<p>&nbsp;</p>\n<h2 id=\"Futility_and_Making_a_Difference\">Futility and Making a Difference</h2>\n<p>Imagine you're told that there are 3000 refugees at risk in a camp in Rwanda, and you could donate towards aid that would save 1500 of them. Would you do it? And how much would you donate?</p>\n<p>Now this time imagine that you can still save 1500 refugees with the same amount of money, but the camp has 10000 refugees. In an experiment where these two scenarios were presented not as a thought experiment but as realities to two separate random groups, the group that heard of only 3000 refugees were more likely to donate, and donated larger amounts.<sup>7,8</sup></p>\n<p>Enter another quirk of our giving psychology, right or wrong: <strong>futility thinking</strong>. We think that if we're not making a sizable difference, it's not worth making the difference at all -- it will only be a drop in the ocean and the problem will keep raging on.</p>\n<p>&nbsp;</p>\n<h2 id=\"Am_I_Responsible_\">Am I Responsible?</h2>\n<p>People are also far less likely to help if they're with other people. In this experiment, students were invited to participate in a market research survey. However, when the researcher gave the students their questionnaire to fill out, she went into a back room separated from the office only by a curtain. A few minutes later, noises strongly suggested that she had got on a chair to get something from a high shelf, and then fell off it, loudly complaining that she couldn't feel or move her foot.</p>\n<p>With only one student taking the survey, 70% of them stopped what they were doing and offered assistance. However, when there were two students taking the survey, this number dropped down dramatically. Most noticeably, when the group was two students -- but one of the students was a stooge who was in on it and would always not respond, the response rate of the non-stooge participant was only 7%.<sup>9</sup></p>\n<p>This one is known as <strong>diffusion of responsibility</strong>, better known as the <strong>bystander effect</strong> -- we help more often when we think it is our responsibility to do so, and -- again for right or for wrong -- we naturally look to others to see if they're helping before doing so ourselves.</p>\n<p>&nbsp;</p>\n<h2 id=\"What_s_Fair_In_Help_\">What's Fair In Help?</h2>\n<p>It's clear that people value fairness, even to their own detriment. In a game called \"the Ultimatum Game\", one participant is given a sum of money by the researcher, say $10, and told they can split this money with an anonymous second player in any proportion they choose -- give them $10, give them $7, give them $5, give them nothing, everything is fair game. The catch is, however, the second player, after hearing of the split anonymously, gets to vote to accept it or reject it. Should the split be accepted, both players walk away with the agreed amount. But should the split be rejected, both players walk away with nothing.</p>\n<p><strong id=\"A_Fair_Split\"><br>A Fair Split</strong></p>\n<p>The economist, expecting ideally rational and perfectly self-interested players, predicts that the second player would accept any split that gets them money, since anything is better than nothing. And the first player, understanding this, would naturally offer $1 and keep $9 for himself. At no point are identities revealed, so reputation and retribution are no issue.</p>\n<p>But the results turn out to be quite different -- the vast majority offer an equal split. Yet, when an offer comes around that offers $2 or less, it is almost always rejected, even though $2 is better than nothing.<sup>10</sup> &nbsp;And this effect persists even when played for thousands of dollars and persists across nearly all cultures.</p>\n<p><strong id=\"Splitting_and_Anchoring_in_Charity\"><br>Splitting and Anchoring in Charity</strong></p>\n<p>This sense of fairness persists into helping as well -- people generally have a strong tendency not to want to help more than the other people around them, and if they find themselves the only ones helping on a frequent basis, they start to feel a \"sucker\". On the flipside, if others are doing more, they will follow suit.<sup>11,12,13</sup></p>\n<p>Those told the average donation to a charity nearly always tend to give that amount, even if the average told to them is a lie, having secretly been increased or decreased. And it can be replicated even without lying -- those told about an above average gift were far more likely to donate more, even attempting to match that gift.<sup>14,15</sup> &nbsp;Overall, we tend to match the behavior of our reference class -- those people we identify with -- and this includes how much we help. We donate more when we believe others are donating more, and donate less when we believe others are doing so.</p>\n<p>&nbsp;</p>\n<h2 id=\"Challenging_the_Self_Interest_Norm\">Challenging the Self-Interest Norm</h2>\n<p>But there's a way to break this cycle of futility, responsibility, and fairness -- challenge the norm by openly communicating about helping others. While many religious and secular values insist that the best giving is anonymous giving, this turns out to not always be the case. While there may be other reasons to give anonymously, don't forget the benefits of giving openly -- being open about helping inspires others to help, and can help challenge the norms of the culture.</p>\n<p>Indeed, many organizations now exist to help challenge the norms of donations and try to create a culture where they give more. <a href=\"http://www.givingwhatwecan.org/\">GivingWhatWeCan</a> is a community of 230 people (<a href=\"http://www.greatplay.net/essays/joining-givingwhatwecan\">including me!</a>) who have all pledged to donate at least 10% of their income to organizations working on ending extreme poverty, and submit statements proving so. <a href=\"http://boldergiving.org/stories.php\">BolderGiving</a> has a bunch of inspiring stories of over 100 people who all give at least 20% of their income, with a dozen giving over 90%! And these aren't all rich people, some of them are even ordinary students.</p>\n<p><strong id=\"Who_s_Willing_to_Be_Altruistic_\"><br>Who's Willing to Be Altruistic?</strong></p>\n<p>While people are not saints, experiments have shown that people tend to grossly overestimate how self-interested other people are -- for one example, people estimated that males would overwhelmingly favor a piece of legislation to \"slash research funding to a disease that affects only women\", even while -- being male -- they themselves do not support such legislation.<sup>16</sup></p>\n<p>This also manifests itself in an expectation that people be \"self-interested\" in their philanthropic cause -- suggesting much stronger support for volunteers in Students Against Drunk Driving who themselves knew people killed in drunk driving accidents versus those people who had no such personal experiences but just thought it to be \"a very important cause\".<sup>17</sup></p>\n<p>Alex de Tocqueville, echoing the early economists who expected $9/$1 splits in the Ultimatum Game, wrote in 1835 that \"Americans enjoy explaining almost every act of their lives on the principle of self-interest\".<sup>18</sup> &nbsp;But this isn't always the case, and in challenging the norm, people make it more acceptable to be altruistic. It's not just \"goody two-shoes\", and it's praiseworthy to be \"too charitable\".</p>\n<p>&nbsp;</p>\n<h2 id=\"A_Bit_of_a_Nudge\">A Bit of a Nudge</h2>\n<p>A somewhat pressing problem in getting people to help was in organ donation -- surely no one was inconvenienced by having their organs donated after they had died. Yet, why would people not sign up? &nbsp;And how could we get more people to sign up?</p>\n<p>In Germany, only 12% of the population are registered organ donors. In nearby Austria, that number is 99.98%. Are people in Austria just less worried about what will happen to them after they die, or just that more altruistic? It turns out the answer is far more simple -- in Germany you must put yourself on the register to become a potential donor (opt-in), whereas in Austria you are a potential donor unless you object (opt-out). While people may be, for right or for wrong, worried about the fate of their body after it is dead, they appear less likely to express these reservations in opt-out systems.<sup>19</sup></p>\n<p>While Richard Thaler and Cass Sunstein argue in their book <a href=\"http://www.amazon.com/dp/0300122233\"><strong>Nudge: Improving Decisions About Health, Wellness, and Happiness</strong></a> that we sometimes suck at making decisions in our own interest and all could do better with more favorable \"defaults\", such defaults are also pressing in helping people.</p>\n<p>While opt-out organ donation is a huge deal, there's another similar idea -- <strong>opt-out philanthropy</strong>. Back before 2008 when the investment bank Bear Stearns still existed, Bear Stearns listed their guiding principle as philanthropy as fostering good citizenship and well-rounded individuals. To this effect, <a href=\"http://query.nytimes.com/gst/fullpage.html?res=9905E4DB1530F931A25752C1A9619C8B63&amp;pagewanted=all\">they required the top 1000 most highest paid employees to donate 4% of their salary and bonuses to non-profits</a>, and prove it with their tax returns. This resulted in more than $45 million in donations during 2006. Many employees described the requirement as \"getting themselves to do what they wanted to do anyway\".</p>\n<p>&nbsp;</p>\n<h2 id=\"Conclusions\">Conclusions</h2>\n<p>So, according to this bit of psychology, what could we do to get other people to help more, besides moralize? Well, we have five key take-aways:</p>\n<p>(1) present these people with a single and highly identifiable victim that they can help<br>(2) nudge them with a default of opt-out philanthropy<br>(3) be more open about our willingness to be altruistic and encourage other people to help<br>(4) make sure people understand the average level of helping around them, and<br>(5) instill a responsibility to help and an understanding that doing so is not futile.</p>\n<p>Hopefully, with these tips and more, helping people more can be come just one of those things we do.</p>\n<p>&nbsp;</p>\n<h2 id=\"References\">References</h2>\n<h5><span style=\"font-weight: normal;\"><em>(Note: Links are to PDF files.)<br><br></em>1: D. A. Small, G. Loewenstein, and P. Slovic. 2007. <a href=\"http://sds.hss.cmu.edu/media/pdfs/loewenstein/SympathyCallousness.pdf\">\"Sympathy and Callousness: The Impact of Deliberative Thought on Donations to Identifiable and Statistical Victims\"</a>. <em>Organizational Behavior and Human Decision Processes </em>102: p143-53<br></span><span style=\"font-weight: normal; \"><br>2: Paul Slovic. 2007. </span><a style=\"font-weight: normal; \" href=\"http://www18.homepage.villanova.edu/diego.fernandezduque/Teaching/CognitivePsychology/Lectures_and_Labs/sssAppliedIssues/If_I_Look_At_The_Mass.pdf\">\"If I Look at the Mass I Will Never Act: Psychic Numbing and Genocide\"</a><span style=\"font-weight: normal; \">. </span><em style=\"font-weight: normal; \">Judgment and Decision Making </em><span style=\"font-weight: normal; \"><span style=\"font-weight: normal;\">2(2): p79-95.</span><br><br><span style=\"font-weight: normal;\">3: T. Kogut and I. Ritov. 2005. </span><a style=\"font-weight: normal; \" href=\"http://pluto.mscc.huji.ac.il/~msiritov/KogutRitovIdentified.pdf\">\"The 'Identified Victim' Effect: An Identified Group, or Just a Single Individual?\"</a><span style=\"font-weight: normal;\">. </span><em style=\"font-weight: normal; \">Journal of Behavioral Decision Making</em><span style=\"font-weight: normal;\">&nbsp;18: p157-67.</span><br><br><span style=\"font-weight: normal;\">4: T. Kogut and I. Ritov. 2005. </span><a style=\"font-weight: normal; \" href=\"https://www.unitn.it/files/download/14235/singularity_of_identified_victim.pdf\">\"The Singularity of Identified Victims in Separate and Joint Evaluations\"</a><span style=\"font-weight: normal;\">. </span><em style=\"font-weight: normal; \">Organizational Behavior and Human Decision Processes</em><span style=\"font-weight: normal;\">&nbsp;97: p106-116.</span><br><br><span style=\"font-weight: normal;\">5: D. A. Small and G. Lowenstein. 2003. </span><a style=\"font-weight: normal; \" href=\"http://sds.hss.cmu.edu/media/pdfs/loewenstein/HelpingVictimAltruism.pdf\">\"Helping the Victim or Helping a Victim: Altruism and Identifiability\"</a><span style=\"font-weight: normal;\">. </span><em style=\"font-weight: normal; \">Journal of Risk and Uncertainty</em><span style=\"font-weight: normal;\">&nbsp;26(1): p5-16.</span><br><br><span style=\"font-weight: normal;\">6: Singer cites this from Paul Slovic, who in turn cites it from: Seymour Epstein. 1994. \"Integration of the Cognitive and the Psychodynamic&nbsp;Unconscious\". <em>American Psychologist </em>49: p709-24. &nbsp;Slovic refers to the affective system as \"experiential\" and the deliberative system as \"analytic\". &nbsp;This is also related to Daniel Kahneman's popular book </span><a href=\"http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637\">Thinking Fast and Slow</a><span style=\"font-weight: normal;\"><span style=\"font-weight: normal;\">.</span><br><br><span style=\"font-weight: normal;\">7: D. Fetherstonhaugh, P. Slovic, S. M. Johnson, and J. Friedrich. 1997. </span><a style=\"font-weight: normal;\" href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Fetherstonhaugh-et-al-Insensitivity-to-the-value-of-human-life-A-study-of-psychophysical-numbing.pdf\">\"Insensitivity to the Value of Human Life: A Study of Psychophysical Numbing\"</a><span style=\"font-weight: normal;\">. &nbsp;</span><em style=\"font-weight: normal;\">Journal of Risk and Uncertainty</em><span style=\"font-weight: normal;\">&nbsp;14: p283-300.</span><br><br><span style=\"font-weight: normal;\">8: Daniel Kahneman and Amos Tversky. 1979. </span><a style=\"font-weight: normal;\" href=\"http://www.hss.caltech.edu/~camerer/Ec101/ProspectTheory.pdf\">\"Prospect Theory: An Analysis of Decision Under Risk.\"</a> <em style=\"font-weight: normal;\">Econometrica</em><span style=\"font-weight: normal;\">&nbsp;47: p263-91.</span><br><br><span style=\"font-weight: normal;\">9: Bib Lantan\u00e9 and John Darley. 1970.&nbsp;</span><a href=\"http://www.amazon.com/The-Unresponsive-Bystander-Doesnt-Help/dp/0139386130\">The Unresponsive Bystander: Why Doesn't He Help?</a><span style=\"font-weight: normal;\">. New York: Appleton-Century-Crofts, p58.</span><br><br><span style=\"font-weight: normal;\">10: Martin Nowak, Karen Page, and Karl Sigmund. 2000. </span><a style=\"font-weight: normal;\" href=\"http://homepage.univie.ac.at/karl.sigmund/UltimatumScience00.pdf\">\"Fairness Versus Reason in the Ultimatum Game\"</a><span style=\"font-weight: normal;\">. </span><em style=\"font-weight: normal;\">Science</em><span style=\"font-weight: normal;\"> 289: p1183-75.</span><br><br><span style=\"font-weight: normal;\">11: Lee Ross and Richard E. Nisbett. 1991. </span><a href=\"http://www.amazon.com/The-Person-Situation-Perspectives-Psychology/dp/0877228515\">The Person and the Situation: Perspectives of Social Psychology</a><span style=\"font-weight: normal;\">.&nbsp;Philadelphia: Temple University Press, p27-46.</span><br><br><span style=\"font-weight: normal;\">12: Robert Cialdini. 2001. </span><a href=\"http://www.amazon.com/Influence-Science-Practice-4th-Edition/dp/0321011473\">Influence: Science and Practice, 4th Edition</a><span style=\"font-weight: normal;\">. Boston: Allyn and Bacon.</span><br><br><span style=\"font-weight: normal;\">13: Judith Lichtenberg. 2004. \"Absence and the Unfond Heart: Why People Are Less Giving Than They Might Be\". in Deen Chatterjee, ed. </span><a href=\"http://www.amazon.com/The-Ethics-Assistance-Cambridge-Philosophy/dp/0521527422\">The Ethics of Assistance: Morality and the Distant Needy</a><span style=\"font-weight: normal;\">. Cambridge, UK: Cambridge University Press.</span><br><br><span style=\"font-weight: normal;\">14: Jen Shang and Rachel Croson. Forthcoming. <a href=\"http://www.fieldexperiments.com/uploads/[70].pdf\">\"Field Experiments in Charitable Contribution: The Impact of Social Influence on the Voluntary Provision of Public Goods\"</a>. <em>The Economic Journal</em>.</span><br><br><span style=\"font-weight: normal;\">15: Rachel Croson and Jen Shang. 2008. <a href=\"http://www.iu.edu/~spea/pubs/faculty/Croson_Shang_2008.pdf\">\"The Impact of Downward Social Information on Contribution Decision\"</a>. <em>Experimental Economics </em>11: p221-33.<br><br>16: Dale Miller. 199. <a href=\"http://www.cepr.org/meets/wkcn/3/3509/papers/Miller.pdf\">\"The Norm of Self-Interest\"</a>. <em>American Psychologist</em> 54: 1053-60.<br><br>17: Rebecca Ratner and Jennifer Clarke. Unpublished. \"Negativity Conveyed to Social Actors Who Lack a Personal Connection to the Cause\".<br><br>18: Alexis de Tocqueville in J.P. Mayer ed., G. Lawrence, trans. 1969. <a href=\"http://www.amazon.com/Democracy-America-Alexis-Tocqueville/dp/0060915226\">Democracy in America</a>. Garden City, N.Y.: Anchor, p546.<br><br>19: Eric Johnson and Daniel Goldstein. 2003. <a href=\"http://webs.wofford.edu/pechwj/Do%20Defaults%20Save%20Lives.pdf\">\"Do Defaults Save Lives?\"</a>. Science 302: p1338-39.</span></span></span></h5>\n<p>&nbsp;</p>\n<p><span style=\"font-weight: normal; \"><span style=\"font-weight: normal;\"><span style=\"font-weight: normal;\"><span style=\"font-size: 10px;\">(This is an updated version of an&nbsp;</span><a style=\"font-size: 10px;\" href=\"http://www.greatplay.net/essays/why-dont-people-help-others-more\">earlier draft from my blog</a><span style=\"font-size: 10px;\">.)</span></span></span></span></p>", "sections": [{"title": "A Single, Identifiable Victim", "anchor": "A_Single__Identifiable_Victim", "level": 1}, {"title": "It's All About the Person", "anchor": "It_s_All_About_the_Person", "level": 2}, {"title": "The Deliberative and The Affective", "anchor": "The_Deliberative_and_The_Affective", "level": 2}, {"title": "Futility and Making a Difference", "anchor": "Futility_and_Making_a_Difference", "level": 1}, {"title": "Am I Responsible?", "anchor": "Am_I_Responsible_", "level": 1}, {"title": "What's Fair In Help?", "anchor": "What_s_Fair_In_Help_", "level": 1}, {"title": "A Fair Split", "anchor": "A_Fair_Split", "level": 2}, {"title": "Splitting and Anchoring in Charity", "anchor": "Splitting_and_Anchoring_in_Charity", "level": 2}, {"title": "Challenging the Self-Interest Norm", "anchor": "Challenging_the_Self_Interest_Norm", "level": 1}, {"title": "Who's Willing to Be Altruistic?", "anchor": "Who_s_Willing_to_Be_Altruistic_", "level": 2}, {"title": "A Bit of a Nudge", "anchor": "A_Bit_of_a_Nudge", "level": 1}, {"title": "Conclusions", "anchor": "Conclusions", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "85 comments"}], "headingsCount": 15}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 85, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hEqsWLm5zQtsPevd3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-14T04:14:06.250Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - Career Optimization (Emphasis on Software)", "slug": "meetup-west-la-meetup-career-optimization-emphasis-on", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:04.570Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/atSdABnnfegSb3c8a/meetup-west-la-meetup-career-optimization-emphasis-on", "pageUrlRelative": "/posts/atSdABnnfegSb3c8a/meetup-west-la-meetup-career-optimization-emphasis-on", "linkUrl": "https://www.lesswrong.com/posts/atSdABnnfegSb3c8a/meetup-west-la-meetup-career-optimization-emphasis-on", "postedAtFormatted": "Tuesday, August 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20Career%20Optimization%20(Emphasis%20on%20Software)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20Career%20Optimization%20(Emphasis%20on%20Software)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FatSdABnnfegSb3c8a%2Fmeetup-west-la-meetup-career-optimization-emphasis-on%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20Career%20Optimization%20(Emphasis%20on%20Software)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FatSdABnnfegSb3c8a%2Fmeetup-west-la-meetup-career-optimization-emphasis-on", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FatSdABnnfegSb3c8a%2Fmeetup-west-la-meetup-career-optimization-emphasis-on", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/cx'>West LA Meetup - Career Optimization (Emphasis on Software)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 August 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, August 15th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> This week we will discuss how to find work you like, like work you find, and how to get to where you want to go. Almost everyone who has come to these seems to work with software, so realistically we will focus on that sort of career path.</p>\n\n<p>There will be general discussion too, and there are lots of interesting <a href=\"http://lesswrong.com/recentposts\">recent posts</a>. Don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/cx'>West LA Meetup - Career Optimization (Emphasis on Software)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "atSdABnnfegSb3c8a", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 9.638350092564871e-07, "legacy": true, "legacyId": "18243", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Career_Optimization__Emphasis_on_Software_\">Discussion article for the meetup : <a href=\"/meetups/cx\">West LA Meetup - Career Optimization (Emphasis on Software)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 August 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, August 15th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> This week we will discuss how to find work you like, like work you find, and how to get to where you want to go. Almost everyone who has come to these seems to work with software, so realistically we will focus on that sort of career path.</p>\n\n<p>There will be general discussion too, and there are lots of interesting <a href=\"http://lesswrong.com/recentposts\">recent posts</a>. Don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Career_Optimization__Emphasis_on_Software_1\">Discussion article for the meetup : <a href=\"/meetups/cx\">West LA Meetup - Career Optimization (Emphasis on Software)</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - Career Optimization (Emphasis on Software)", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Career_Optimization__Emphasis_on_Software_", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - Career Optimization (Emphasis on Software)", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Career_Optimization__Emphasis_on_Software_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-14T05:26:26.031Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne social meetup", "slug": "meetup-melbourne-social-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:15.634Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "toner", "createdAt": "2009-02-27T05:14:52.530Z", "isAdmin": false, "displayName": "toner"}, "userId": "XaYyFkpvhiiMscJJZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ivxCrtZHims8oxCNn/meetup-melbourne-social-meetup", "pageUrlRelative": "/posts/ivxCrtZHims8oxCNn/meetup-melbourne-social-meetup", "linkUrl": "https://www.lesswrong.com/posts/ivxCrtZHims8oxCNn/meetup-melbourne-social-meetup", "postedAtFormatted": "Tuesday, August 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20social%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20social%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FivxCrtZHims8oxCNn%2Fmeetup-melbourne-social-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20social%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FivxCrtZHims8oxCNn%2Fmeetup-melbourne-social-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FivxCrtZHims8oxCNn%2Fmeetup-melbourne-social-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 144, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/cy\">Melbourne social meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">24 August 2012 07:00:00PM (+1000)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Ben's house; see mailing list, Carlton VIC 3053, Australia</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Melbourne's next social meetup is on Friday 24th August, 6:30 for 7pm, at my house. If you have any trouble working out the location or getting in, you can call me on 0412 996 288 or Scott (shokwave) on 0432 862 932.</p>\n<p>We'll get some snacks and organise some form of take-away for dinner. BYO drinks and games.</p>\n<p>We always look forward to meeting new people!</p>\n<p>This meetup is usually on the third Friday of the month. We also have a regular meetup about practical rationality on the first Friday of each month.</p>\n<p>If you're free this Friday (the 17th), Adam suggests that you consider attending <a rel=\"nofollow\" href=\"http://2012.singularitysummit.com.au/other-activities/randal-koene-and-mark-pesce-at-embiggen-books/\">http://2012.singularitysummit.com.au/other-activities/randal-koene-and-mark-pesce-at-embiggen-books/</a> (but remember to book!), perhaps followed by dinner in the city.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/cy\">Melbourne social meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ivxCrtZHims8oxCNn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 9.638706080450462e-07, "legacy": true, "legacyId": "18244", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup\">Discussion article for the meetup : <a href=\"/meetups/cy\">Melbourne social meetup</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">24 August 2012 07:00:00PM (+1000)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Ben's house; see mailing list, Carlton VIC 3053, Australia</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Melbourne's next social meetup is on Friday 24th August, 6:30 for 7pm, at my house. If you have any trouble working out the location or getting in, you can call me on 0412 996 288 or Scott (shokwave) on 0432 862 932.</p>\n<p>We'll get some snacks and organise some form of take-away for dinner. BYO drinks and games.</p>\n<p>We always look forward to meeting new people!</p>\n<p>This meetup is usually on the third Friday of the month. We also have a regular meetup about practical rationality on the first Friday of each month.</p>\n<p>If you're free this Friday (the 17th), Adam suggests that you consider attending <a rel=\"nofollow\" href=\"http://2012.singularitysummit.com.au/other-activities/randal-koene-and-mark-pesce-at-embiggen-books/\">http://2012.singularitysummit.com.au/other-activities/randal-koene-and-mark-pesce-at-embiggen-books/</a> (but remember to book!), perhaps followed by dinner in the city.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup1\">Discussion article for the meetup : <a href=\"/meetups/cy\">Melbourne social meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-14T07:45:15.439Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Three Fallacies of Teleology", "slug": "seq-rerun-three-fallacies-of-teleology", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:09.095Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mJ8JkWJ7CYf43j8Ak/seq-rerun-three-fallacies-of-teleology", "pageUrlRelative": "/posts/mJ8JkWJ7CYf43j8Ak/seq-rerun-three-fallacies-of-teleology", "linkUrl": "https://www.lesswrong.com/posts/mJ8JkWJ7CYf43j8Ak/seq-rerun-three-fallacies-of-teleology", "postedAtFormatted": "Tuesday, August 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Three%20Fallacies%20of%20Teleology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Three%20Fallacies%20of%20Teleology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmJ8JkWJ7CYf43j8Ak%2Fseq-rerun-three-fallacies-of-teleology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Three%20Fallacies%20of%20Teleology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmJ8JkWJ7CYf43j8Ak%2Fseq-rerun-three-fallacies-of-teleology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmJ8JkWJ7CYf43j8Ak%2Fseq-rerun-three-fallacies-of-teleology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<p>Today's post, <a href=\"/lw/te/three_fallacies_of_teleology/\">Three Fallacies of Teleology</a> was originally published on 25 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Three_Fallacies_of_Teleology\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Theories of teleology have a few problems. First, theories of teleology often wind up drawing causal arrows from the future to the past. It also leads you to make predictions based on anthropomorphism. Finally, it opens you up to the Mind Projection Fallacy, assuming that the purpose of something is an inherent property of that thing, as opposed to a property of the agent or process that produced it.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/e2d/seq_rerun_magical_categories/\">Magical Categories</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mJ8JkWJ7CYf43j8Ak", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.639389397704903e-07, "legacy": true, "legacyId": "18255", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2HxAkCG7NWTrrn5R3", "yF2DiAAhJChWF7taf", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-14T13:40:27.305Z", "modifiedAt": null, "url": null, "title": "[LINK] The Cambridge Declaration on Consciousness", "slug": "link-the-cambridge-declaration-on-consciousness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:05.093Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FxTDFk7wgyJfYMPmn/link-the-cambridge-declaration-on-consciousness", "pageUrlRelative": "/posts/FxTDFk7wgyJfYMPmn/link-the-cambridge-declaration-on-consciousness", "linkUrl": "https://www.lesswrong.com/posts/FxTDFk7wgyJfYMPmn/link-the-cambridge-declaration-on-consciousness", "postedAtFormatted": "Tuesday, August 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20The%20Cambridge%20Declaration%20on%20Consciousness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20The%20Cambridge%20Declaration%20on%20Consciousness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFxTDFk7wgyJfYMPmn%2Flink-the-cambridge-declaration-on-consciousness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20The%20Cambridge%20Declaration%20on%20Consciousness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFxTDFk7wgyJfYMPmn%2Flink-the-cambridge-declaration-on-consciousness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFxTDFk7wgyJfYMPmn%2Flink-the-cambridge-declaration-on-consciousness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 133, "htmlBody": "<p>The <a href=\"http://fcmconference.org/\">Francis Crick Memorial Conference</a>, held in Cambridge last month, has come up with the <a href=\"http://fcmconference.org/img/CambridgeDeclarationOnConsciousness.pdf\">Cambridge Declaration on Consciousness</a> (PDF).</p>\n<p><strong>tl;dr</strong> humans still aren't special, consciousness seems to arise in quite a variety of nervous systems and working out what it is is a problem in neurology.</p>\n<p style=\"padding-left: 30px;\">We declare the following: &ldquo;The absence of a neocortex does not appear to preclude an organism from<br />experiencing affective states. Convergent evidence indicates that non-human animals have the<br />neuroanatomical, neurochemical, and neurophysiological substrates of conscious states along with<br />the capacity to exhibit intentional behaviors. Consequently, the weight of evidence indicates that<br />humans are not unique in possessing the neurological substrates that generate consciousness. Non-<br />human animals, including all mammals and birds, and many other creatures, including octopuses, also<br />possess these neurological substrates.&rdquo;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FxTDFk7wgyJfYMPmn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 7, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "18259", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-14T17:09:10.149Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley meetup: Discussion about startups", "slug": "meetup-berkeley-meetup-discussion-about-startups", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:08.173Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WAtEH9jJGsnDRGhix/meetup-berkeley-meetup-discussion-about-startups", "pageUrlRelative": "/posts/WAtEH9jJGsnDRGhix/meetup-berkeley-meetup-discussion-about-startups", "linkUrl": "https://www.lesswrong.com/posts/WAtEH9jJGsnDRGhix/meetup-berkeley-meetup-discussion-about-startups", "postedAtFormatted": "Tuesday, August 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%20meetup%3A%20Discussion%20about%20startups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%20meetup%3A%20Discussion%20about%20startups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWAtEH9jJGsnDRGhix%2Fmeetup-berkeley-meetup-discussion-about-startups%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%20meetup%3A%20Discussion%20about%20startups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWAtEH9jJGsnDRGhix%2Fmeetup-berkeley-meetup-discussion-about-startups", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWAtEH9jJGsnDRGhix%2Fmeetup-berkeley-meetup-discussion-about-startups", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 130, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/cz'>Berkeley meetup: Discussion about startups</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 August 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Berkeley, CA 94702</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Tomorrow's meetup will be a discussion about this Less Wrong post:</p>\n\n<p><a href=\"http://lesswrong.com/lw/e26/who_wants_to_start_an_important_startup/\" rel=\"nofollow\">http://lesswrong.com/lw/e26/who_wants_to_start_an_important_startup/</a></p>\n\n<p>The idea is that for-profit companies are a good way of accomplishing certain altruistic goals, and our community contains people with startup ideas and people who have the skills and energy to realize the ideas.</p>\n\n<p>Come to Zendo Wednesday to discuss your startup idea and/or get excited about someone else's project. Doors open at 7pm, and discussion starts at 7:30pm.</p>\n\n<p>(Also, there's an 80% chance that I'll have five clickers with which to play a fun operant conditioning game after the discussion.)</p>\n\n<p>For directions to Zendo, see the mailing list:</p>\n\n<p><a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a></p>\n\n<p>or call me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/cz'>Berkeley meetup: Discussion about startups</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WAtEH9jJGsnDRGhix", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 9.64216597173068e-07, "legacy": true, "legacyId": "18260", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley_meetup__Discussion_about_startups\">Discussion article for the meetup : <a href=\"/meetups/cz\">Berkeley meetup: Discussion about startups</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 August 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Berkeley, CA 94702</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Tomorrow's meetup will be a discussion about this Less Wrong post:</p>\n\n<p><a href=\"http://lesswrong.com/lw/e26/who_wants_to_start_an_important_startup/\" rel=\"nofollow\">http://lesswrong.com/lw/e26/who_wants_to_start_an_important_startup/</a></p>\n\n<p>The idea is that for-profit companies are a good way of accomplishing certain altruistic goals, and our community contains people with startup ideas and people who have the skills and energy to realize the ideas.</p>\n\n<p>Come to Zendo Wednesday to discuss your startup idea and/or get excited about someone else's project. Doors open at 7pm, and discussion starts at 7:30pm.</p>\n\n<p>(Also, there's an 80% chance that I'll have five clickers with which to play a fun operant conditioning game after the discussion.)</p>\n\n<p>For directions to Zendo, see the mailing list:</p>\n\n<p><a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a></p>\n\n<p>or call me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley_meetup__Discussion_about_startups1\">Discussion article for the meetup : <a href=\"/meetups/cz\">Berkeley meetup: Discussion about startups</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley meetup: Discussion about startups", "anchor": "Discussion_article_for_the_meetup___Berkeley_meetup__Discussion_about_startups", "level": 1}, {"title": "Discussion article for the meetup : Berkeley meetup: Discussion about startups", "anchor": "Discussion_article_for_the_meetup___Berkeley_meetup__Discussion_about_startups1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YbCc3NRrr5avvWSHT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-15T01:00:21.108Z", "modifiedAt": null, "url": null, "title": "Idle speculation about anchoring and the Facebook IPO", "slug": "idle-speculation-about-anchoring-and-the-facebook-ipo", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:05.413Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AQrCefMMit42yMaLg/idle-speculation-about-anchoring-and-the-facebook-ipo", "pageUrlRelative": "/posts/AQrCefMMit42yMaLg/idle-speculation-about-anchoring-and-the-facebook-ipo", "linkUrl": "https://www.lesswrong.com/posts/AQrCefMMit42yMaLg/idle-speculation-about-anchoring-and-the-facebook-ipo", "postedAtFormatted": "Wednesday, August 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Idle%20speculation%20about%20anchoring%20and%20the%20Facebook%20IPO&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIdle%20speculation%20about%20anchoring%20and%20the%20Facebook%20IPO%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAQrCefMMit42yMaLg%2Fidle-speculation-about-anchoring-and-the-facebook-ipo%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Idle%20speculation%20about%20anchoring%20and%20the%20Facebook%20IPO%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAQrCefMMit42yMaLg%2Fidle-speculation-about-anchoring-and-the-facebook-ipo", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAQrCefMMit42yMaLg%2Fidle-speculation-about-anchoring-and-the-facebook-ipo", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 209, "htmlBody": "<p>Facebook IPO'd at a price of 38 dollars a share, which apparently gave it a price-to-earnings ratio in the range of 100 - extremely, fantastically high. The price dropped pretty rapidly and is currently somewhere around 20 dollars; which still, presumably, gives it a very high P/E ratio somewhere in the forties. Now, suppose it had IPO'd at a more historically-reasonable P/E of, say, 20 - still high, but not stratospheric. That would put the initial share price somewhere around 10 or 12 dollars. Is there any strong reason to believe that the price would then have *risen* to where it is now? It is not obvious to me that the current price is supported by anything but the historical price - in other words, it's trading around 20 because it has recently traded around 25.&nbsp;</p>\n<p>My point: I can't help but wonder if someone connected to the IPO had read Kahneman on anchoring. Somebody, clearly, was buying the stock at 33, just as someone is still buying at 20; I wonder if the chain of thought had that apparently-arbitrary number \"38\" in it somewhere, making 33 look cheap - fundamentals be damned! And if this happened, who benefited, and what ought we to conclude about the efficiency of markets?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AQrCefMMit42yMaLg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 0, "extendedScore": null, "score": 9.644487053564808e-07, "legacy": true, "legacyId": "18261", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-15T03:25:30.869Z", "modifiedAt": null, "url": null, "title": "Open Thread, August 16-31, 2012", "slug": "open-thread-august-16-31-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:00.968Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qeZunxgFc4oqCbEcX/open-thread-august-16-31-2012", "pageUrlRelative": "/posts/qeZunxgFc4oqCbEcX/open-thread-august-16-31-2012", "linkUrl": "https://www.lesswrong.com/posts/qeZunxgFc4oqCbEcX/open-thread-august-16-31-2012", "postedAtFormatted": "Wednesday, August 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20August%2016-31%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20August%2016-31%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqeZunxgFc4oqCbEcX%2Fopen-thread-august-16-31-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20August%2016-31%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqeZunxgFc4oqCbEcX%2Fopen-thread-august-16-31-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqeZunxgFc4oqCbEcX%2Fopen-thread-august-16-31-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post, even in Discussion, it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qeZunxgFc4oqCbEcX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 9.645202335039595e-07, "legacy": true, "legacyId": "18262", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 316, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-15T04:02:43.977Z", "modifiedAt": null, "url": null, "title": "Let's be friendly to our allies", "slug": "let-s-be-friendly-to-our-allies", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:08.090Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JGWeissman", "createdAt": "2009-04-01T04:43:56.740Z", "isAdmin": false, "displayName": "JGWeissman"}, "userId": "Mw8rsM7m7E8nnEFEp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9F2A8pdPwR7u5eC57/let-s-be-friendly-to-our-allies", "pageUrlRelative": "/posts/9F2A8pdPwR7u5eC57/let-s-be-friendly-to-our-allies", "linkUrl": "https://www.lesswrong.com/posts/9F2A8pdPwR7u5eC57/let-s-be-friendly-to-our-allies", "postedAtFormatted": "Wednesday, August 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Let's%20be%20friendly%20to%20our%20allies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALet's%20be%20friendly%20to%20our%20allies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9F2A8pdPwR7u5eC57%2Flet-s-be-friendly-to-our-allies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Let's%20be%20friendly%20to%20our%20allies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9F2A8pdPwR7u5eC57%2Flet-s-be-friendly-to-our-allies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9F2A8pdPwR7u5eC57%2Flet-s-be-friendly-to-our-allies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 320, "htmlBody": "<p>Less Wrong was created to produce rationalists, <a href=\"/lw/66/rationality_common_interest_of_many_causes/\">so that many causes could benefit from the efforts of those rationalists</a>. The point is not just to have nice place to talk about rationality, but to <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">really make ourselves stronger</a>, to apply the lessons that we learn here to improve our own lives, and to <a href=\"/tag/philanthropy/\">improve the world</a>.</p>\n<p><a href=\"http://80000hours.org/\">80,000 Hours</a> is an organization created to provide direct domain specific help to people who want to support charitable causes, the same causes Less Wrong is supposed to produce rationalists to support. 80,000 Hours has goals clearly aligned with ours. Provided we think they are pursuing their aligned goals effectively, we should be excited about this. We should be happy when they reach out to us, to see how we can work together.</p>\n<p>So, I am very disappointed to see the negative reception of a Less Wrong post by 80,000 Hours member Benjamin Todd, <a href=\"/r/discussion/lw/dr1/what_are_your_questions_about_making_a_difference/\">asking us what questions we would like 80,000 Hours to answer for us</a>. They are basically offering to do free research for us on things that we care about, because our goals are aligned. And yet, as of this writing, that post has a score of -7, and it has received comments <a href=\"/r/discussion/lw/dr1/what_are_your_questions_about_making_a_difference/76vq\">complaining that it is an ad</a>. To be clear, ads of the sort that we want to avoid do not offer free services relevant to a core purpose of our community. I won't argue whether or not the post was an ad, but I will say that it belongs on Less Wrong and we should give it a good reception.</p>\n<p>I would like to thank Benjamin Todd and others at 80,000 hours for their work in helping people be more effective philanthropists and otherwise support important causes, and for engaging Less Wrong in this project. I also thank everyone who responded to post with their actual questions about making a difference.</p>\n<p>And, please, can we be nice to people who help us?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9F2A8pdPwR7u5eC57", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 68, "baseScore": 35, "extendedScore": null, "score": 9.645385742089173e-07, "legacy": true, "legacyId": "18263", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4PPE6D635iBcGPGRy", "DoLQN5ryZ9XkZjq5h", "bzShxYcRz7K7oNtsB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-15T04:06:01.216Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Dreams of AI Design", "slug": "seq-rerun-dreams-of-ai-design", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:05.099Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y6G6T4sigftTmgZGP/seq-rerun-dreams-of-ai-design", "pageUrlRelative": "/posts/Y6G6T4sigftTmgZGP/seq-rerun-dreams-of-ai-design", "linkUrl": "https://www.lesswrong.com/posts/Y6G6T4sigftTmgZGP/seq-rerun-dreams-of-ai-design", "postedAtFormatted": "Wednesday, August 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Dreams%20of%20AI%20Design&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Dreams%20of%20AI%20Design%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY6G6T4sigftTmgZGP%2Fseq-rerun-dreams-of-ai-design%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Dreams%20of%20AI%20Design%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY6G6T4sigftTmgZGP%2Fseq-rerun-dreams-of-ai-design", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY6G6T4sigftTmgZGP%2Fseq-rerun-dreams-of-ai-design", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<p>Today's post, <a href=\"/lw/tf/dreams_of_ai_design/\">Dreams of AI Design</a> was originally published on 26 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Dreams_of_AI_Design\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It can feel as though you understand how to build an AI, when really, you're still making all your predictions based on empathy. Your AI design will not work until you figure out a way to reduce the mental to the non-mental.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/e33/seq_rerun_three_fallacies_of_teleology/\">Three Fallacies of Teleology</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y6G6T4sigftTmgZGP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 9.64540194123128e-07, "legacy": true, "legacyId": "18264", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["p7ftQ6acRkgo6hqHb", "mJ8JkWJ7CYf43j8Ak", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-15T11:04:40.906Z", "modifiedAt": null, "url": null, "title": "The weakest arguments for and against human level AI", "slug": "the-weakest-arguments-for-and-against-human-level-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:05.876Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QSnZM5JZxCboMh8Qf/the-weakest-arguments-for-and-against-human-level-ai", "pageUrlRelative": "/posts/QSnZM5JZxCboMh8Qf/the-weakest-arguments-for-and-against-human-level-ai", "linkUrl": "https://www.lesswrong.com/posts/QSnZM5JZxCboMh8Qf/the-weakest-arguments-for-and-against-human-level-ai", "postedAtFormatted": "Wednesday, August 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20weakest%20arguments%20for%20and%20against%20human%20level%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20weakest%20arguments%20for%20and%20against%20human%20level%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQSnZM5JZxCboMh8Qf%2Fthe-weakest-arguments-for-and-against-human-level-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20weakest%20arguments%20for%20and%20against%20human%20level%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQSnZM5JZxCboMh8Qf%2Fthe-weakest-arguments-for-and-against-human-level-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQSnZM5JZxCboMh8Qf%2Fthe-weakest-arguments-for-and-against-human-level-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 405, "htmlBody": "<p>While going through the list of arguments for why to expect human level AI to happen or be impossible I was stuck by the same tremendously weak arguments that kept on coming up again and again. The weakest argument in favour of AI was the perenial:</p>\n<ul>\n<li>Moore's Law hence AI!</li>\n</ul>\n<p>Lest you think I'm exaggerating how weakly the argument was used, here are some random quotes:</p>\n<ul>\n<li>Progress in computer hardware has followed an amazingly steady curve in the last few decades [16]. Based largely on this trend, I believe that the creation of greater than human intelligence will occur during the next thirty years. (Vinge, 1993)</li>\n<li>Computers aren't terribly smart right now, but that's because the human brain has about a million times the raw power of todays' computers. [...] Since computer capacity doubles every two years or so, we expect that in about 40 years, the computers will be as powerful as human brains. (Eder 1994)</li>\n<li>Suppose my projections are correct, and the hardware requirements for human equivalence are available in 10 years for about the current price of a medium large computer. &nbsp;Suppose further that software development keeps pace (and it should be increasingly easy, because big computers are great programming aids), and machines able to think as well as humans begin to appear in 10 years. (Moravec, 1977) </li>\n</ul>\n<p>At least Moravec gives a glance towards software, even though it is merely to say that software \"keeps pace\" with hardware. What is the common scale for hardware and software that he seems to be using? I'd like to put Starcraft II, Excel 2003 and Cygwin on a hardware scale - do these&nbsp;correspond&nbsp;to Penitums, Ataris, and Colossus? I'm not particularly ripping into Moravec, but if you realise that software is important, then you should attempt to model software progress!</p>\n<p>But very rarely do any of these predictors try and show why having&nbsp;computers with say, the memory capacity or the FOPS of a human brain, will suddenly cause an AI to emerge.</p>\n<p>The weakest argument against AI was the standard:</p>\n<ul>\n<li>Free will (or creativity) hence no AI!</li>\n</ul>\n<p>Some of the more sophisticated go \"G&ouml;del, hence no AI!\". If the crux of your whole argument is that only humans can do X, then you need to show that only humans can do X - not assert it and spend the rest of your paper talking in great details about other things.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QSnZM5JZxCboMh8Qf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 22, "extendedScore": null, "score": 9.647465474120593e-07, "legacy": true, "legacyId": "18274", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-15T17:38:29.542Z", "modifiedAt": null, "url": null, "title": "Luke is doing an AMA on Reddit", "slug": "luke-is-doing-an-ama-on-reddit", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:08.384Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Spurlock", "createdAt": "2010-03-24T17:13:19.572Z", "isAdmin": false, "displayName": "Spurlock"}, "userId": "mK7rKWbkuoDsm3aQb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/C7ToLLwe2GyADoKtc/luke-is-doing-an-ama-on-reddit", "pageUrlRelative": "/posts/C7ToLLwe2GyADoKtc/luke-is-doing-an-ama-on-reddit", "linkUrl": "https://www.lesswrong.com/posts/C7ToLLwe2GyADoKtc/luke-is-doing-an-ama-on-reddit", "postedAtFormatted": "Wednesday, August 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Luke%20is%20doing%20an%20AMA%20on%20Reddit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALuke%20is%20doing%20an%20AMA%20on%20Reddit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC7ToLLwe2GyADoKtc%2Fluke-is-doing-an-ama-on-reddit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Luke%20is%20doing%20an%20AMA%20on%20Reddit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC7ToLLwe2GyADoKtc%2Fluke-is-doing-an-ama-on-reddit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC7ToLLwe2GyADoKtc%2Fluke-is-doing-an-ama-on-reddit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 30, "htmlBody": "<p>I'm sure most of us are used to just being able to badger him about things in the comments here on LW, but for anyone interested <a title=\"http://www.reddit.com/r/Futurology/comments/y9lm0/i_am_luke_muehlhauser_ceo_of_the_singularity/\" href=\"http://www.reddit.com/r/Futurology/http://www.reddit.com/r/Futurology/\">here's the link</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YgizoZqa7LEb3LEJn": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "C7ToLLwe2GyADoKtc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 26, "extendedScore": null, "score": 9.649407234176909e-07, "legacy": true, "legacyId": "18275", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-15T21:36:15.888Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC TED talks Meetup", "slug": "meetup-washington-dc-ted-talks-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:09.268Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ArLCSxp6cHiDXnLo8/meetup-washington-dc-ted-talks-meetup", "pageUrlRelative": "/posts/ArLCSxp6cHiDXnLo8/meetup-washington-dc-ted-talks-meetup", "linkUrl": "https://www.lesswrong.com/posts/ArLCSxp6cHiDXnLo8/meetup-washington-dc-ted-talks-meetup", "postedAtFormatted": "Wednesday, August 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20TED%20talks%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20TED%20talks%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FArLCSxp6cHiDXnLo8%2Fmeetup-washington-dc-ted-talks-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20TED%20talks%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FArLCSxp6cHiDXnLo8%2Fmeetup-washington-dc-ted-talks-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FArLCSxp6cHiDXnLo8%2Fmeetup-washington-dc-ted-talks-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 83, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/d0'>Washington DC TED talks Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 August 2012 04:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Washington DC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will be the second TED talks meetup, where we will watch and discuss TED talks. The last one was a lot of fun, and led to some quite interesting discussions, so I'm optimistic about this one too.</p>\n\n<p>The meetup is at a private apartment; please check the email list or PM me for details.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/d0'>Washington DC TED talks Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ArLCSxp6cHiDXnLo8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 9.650579952453686e-07, "legacy": true, "legacyId": "18276", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_TED_talks_Meetup\">Discussion article for the meetup : <a href=\"/meetups/d0\">Washington DC TED talks Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 August 2012 04:00:00PM (-0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Washington DC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will be the second TED talks meetup, where we will watch and discuss TED talks. The last one was a lot of fun, and led to some quite interesting discussions, so I'm optimistic about this one too.</p>\n\n<p>The meetup is at a private apartment; please check the email list or PM me for details.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_TED_talks_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/d0\">Washington DC TED talks Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC TED talks Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_TED_talks_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Washington DC TED talks Meetup", "anchor": "Discussion_article_for_the_meetup___Washington_DC_TED_talks_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-16T05:26:38.071Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Against Modal Logics", "slug": "seq-rerun-against-modal-logics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:07.420Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/chnwsFduQQ5hA7CNq/seq-rerun-against-modal-logics", "pageUrlRelative": "/posts/chnwsFduQQ5hA7CNq/seq-rerun-against-modal-logics", "linkUrl": "https://www.lesswrong.com/posts/chnwsFduQQ5hA7CNq/seq-rerun-against-modal-logics", "postedAtFormatted": "Thursday, August 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Against%20Modal%20Logics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Against%20Modal%20Logics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FchnwsFduQQ5hA7CNq%2Fseq-rerun-against-modal-logics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Against%20Modal%20Logics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FchnwsFduQQ5hA7CNq%2Fseq-rerun-against-modal-logics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FchnwsFduQQ5hA7CNq%2Fseq-rerun-against-modal-logics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>Today's post, <a href=\"/lw/tg/against_modal_logics/\">Against Modal Logics</a> was originally published on 27 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Against_Modal_Logics\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Unfortunately, very little of philosophy is actually helpful in AI research, for a few reasons.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/e3c/seq_rerun_dreams_of_ai_design/\">Dreams of AI Design</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "chnwsFduQQ5hA7CNq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 9.652900606926187e-07, "legacy": true, "legacyId": "18284", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vzLrQaGPa9DNCpuZz", "Y6G6T4sigftTmgZGP", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-16T06:53:50.980Z", "modifiedAt": null, "url": null, "title": "[LINK] SMBC comics: Existential Crisis Sally on \"Is forgotten torture real?\"", "slug": "link-smbc-comics-existential-crisis-sally-on-is-forgotten", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:07.258Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TBvmT6aTffJrB5mHN/link-smbc-comics-existential-crisis-sally-on-is-forgotten", "pageUrlRelative": "/posts/TBvmT6aTffJrB5mHN/link-smbc-comics-existential-crisis-sally-on-is-forgotten", "linkUrl": "https://www.lesswrong.com/posts/TBvmT6aTffJrB5mHN/link-smbc-comics-existential-crisis-sally-on-is-forgotten", "postedAtFormatted": "Thursday, August 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20SMBC%20comics%3A%20Existential%20Crisis%20Sally%20on%20%22Is%20forgotten%20torture%20real%3F%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20SMBC%20comics%3A%20Existential%20Crisis%20Sally%20on%20%22Is%20forgotten%20torture%20real%3F%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTBvmT6aTffJrB5mHN%2Flink-smbc-comics-existential-crisis-sally-on-is-forgotten%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20SMBC%20comics%3A%20Existential%20Crisis%20Sally%20on%20%22Is%20forgotten%20torture%20real%3F%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTBvmT6aTffJrB5mHN%2Flink-smbc-comics-existential-crisis-sally-on-is-forgotten", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTBvmT6aTffJrB5mHN%2Flink-smbc-comics-existential-crisis-sally-on-is-forgotten", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 22, "htmlBody": "<p><a href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=2705\">http://www.smbc-comics.com/index.php?db=comics&amp;id=2705</a></p>\n<p>Addresses questions like \"If I don't remember, but it definitely happened... who suffered?\" in a rather non-obvious way (non-obvious to me, anyway).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TBvmT6aTffJrB5mHN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 2, "extendedScore": null, "score": 9.6533310065536e-07, "legacy": true, "legacyId": "18290", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-16T08:53:21.431Z", "modifiedAt": null, "url": null, "title": "Towards Safe Robots: Approaching Asimov's 1st Law", "slug": "towards-safe-robots-approaching-asimov-s-1st-law", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:05.700Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Utopiah", "createdAt": "2012-03-10T14:57:03.414Z", "isAdmin": false, "displayName": "Utopiah"}, "userId": "gBoAdZcTffQnbsMAb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sdMZyifE5uSbQA6J9/towards-safe-robots-approaching-asimov-s-1st-law", "pageUrlRelative": "/posts/sdMZyifE5uSbQA6J9/towards-safe-robots-approaching-asimov-s-1st-law", "linkUrl": "https://www.lesswrong.com/posts/sdMZyifE5uSbQA6J9/towards-safe-robots-approaching-asimov-s-1st-law", "postedAtFormatted": "Thursday, August 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Towards%20Safe%20Robots%3A%20Approaching%20Asimov's%201st%20Law&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATowards%20Safe%20Robots%3A%20Approaching%20Asimov's%201st%20Law%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsdMZyifE5uSbQA6J9%2Ftowards-safe-robots-approaching-asimov-s-1st-law%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Towards%20Safe%20Robots%3A%20Approaching%20Asimov's%201st%20Law%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsdMZyifE5uSbQA6J9%2Ftowards-safe-robots-approaching-asimov-s-1st-law", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsdMZyifE5uSbQA6J9%2Ftowards-safe-robots-approaching-asimov-s-1st-law", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 661, "htmlBody": "<p>Towards Safe Robots: Approaching Asimov's 1st Law</p>\n<p>http://darwin.bth.rwth-aachen.de/opus3/volltexte/2011/3826/pdf/3826.pdf (via http://www.euron.org )</p>\n<p>Despite the title very little theory or philosophy but instead a focus on interaction (e.g. in a factory, between a human worker and a robot) and how to minimize risk: soft-robotics, crash-testing, collisions, ...</p>\n<blockquote>\n<p><strong>Abstract</strong><br />Up to now, state-of-the-art industrial robots played the most important role<br />in real-world applications and more advanced, highly sensorized robots were<br />usually kept in lab environments and remained in a prototypical stadium. Var-<br />ious factors like low robustness and the lack of computing power were large<br />hurdles in realizing robotic systems for highly demanding tasks in e.g. do-<br />mestic environments or as robotic co-workers. The recent increase in techno-<br />logy maturity finally made it possible to realize systems of high integration,<br />advanced sensorial capabilities and enhanced power to cross this barrier and<br />merge living spaces of humans and robot workspaces to at least a certain ex-<br />tent.</p>\n<p><br />In addition, the increasing effort various companies have invested to realize<br />first commercial service robotics products has made it necessary to properly<br />address one of the most fundamental questions of Human-Robot Interaction:</p>\n<p style=\"padding-left: 30px;\"><br />How to ensure safety in human-robot coexistence?</p>\n<p><br />Although the vision of coexistence itself has always been present, very little<br />effort has been made to actually enforce safety requirements, or to define safety<br />standards up to now.</p>\n<p><br />In this dissertation, the essential question about the necessary requirements<br />for a safe robot is addressed in depth and from various perspectives. The ap-<br />proach taken here focuses on the biomechanical level of injury assessment, ad-<br />dressing the physical evaluation of robot-human impacts and the definition of<br />the major factors that affect injuries during various worst-case scenarios. This<br />assessment is the basis for the design and exploration of various measures<br />to improve the safety in human-robot interaction. They range from control<br />schemes for collision detection, and reaction, to the investigation of novel joint<br />designs. An in-depth analysis of their contribution to safety in human-robot<br />coexistence is carried out.</p>\n<p><br />In addition to this &ldquo;on-contact&rdquo; treatment of human-robot interaction, the the-<br />sis proposes and discusses real-time collision avoidance methods, i.e. how to<br />design pre-collision strategies to prevent unintended contact. An additional<br />major outcome of this thesis is the development of a concept for a robotic co-<br />worker and its experimental verification in an industrially relevant real-world<br />scenario. In this context, a control architecture that enables a behavior based<br />access to the robot and provides an easy to parameterize interface to the safety<br />capabilities of the robot was developed. In addition, the architecture was ap-<br />plied in various other applications that deal with physical Human-Robot In<br />teraction as e.g. the first continuously brain controlled robot by a tetraplegic<br />person or an EMG2 controlled robot.</p>\n<p><br />Generally, all aspects discussed in this thesis are fully supported by a variety<br />of experiments and cross-verifications, leading to strong conclusions in this<br />sensitive and immanently important topic. Several surprising and gratifying<br />results, which were registered in the robotics community to great interest, were<br />obtained.</p>\n<p><br />In addition to the scientific output, the outcome of this thesis attracted also<br />significant public attention, confirming the importance of the topic for robotics<br />research.</p>\n<p><br />The major parts and contributions of this thesis are described hereafter in more<br />detail. Furthermore, the resulting publications which are an outcome of the<br />work are cited.\"</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sdMZyifE5uSbQA6J9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -12, "extendedScore": null, "score": 9.653920822305047e-07, "legacy": true, "legacyId": "18291", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-16T14:53:56.796Z", "modifiedAt": null, "url": null, "title": "Competence in experts: summary", "slug": "competence-in-experts-summary", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:07.035Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yrNW4ApXrpn2KMhxr/competence-in-experts-summary", "pageUrlRelative": "/posts/yrNW4ApXrpn2KMhxr/competence-in-experts-summary", "linkUrl": "https://www.lesswrong.com/posts/yrNW4ApXrpn2KMhxr/competence-in-experts-summary", "postedAtFormatted": "Thursday, August 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Competence%20in%20experts%3A%20summary&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACompetence%20in%20experts%3A%20summary%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyrNW4ApXrpn2KMhxr%2Fcompetence-in-experts-summary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Competence%20in%20experts%3A%20summary%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyrNW4ApXrpn2KMhxr%2Fcompetence-in-experts-summary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyrNW4ApXrpn2KMhxr%2Fcompetence-in-experts-summary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 128, "htmlBody": "<p>Just giving a short table-summary of an <a href=\"http://www.sciencedirect.com/science/article/pii/074959789290064E\">article</a> by James Shanteau on which areas and tasks experts developed a good intuition - and which ones they didn't. Though the article is old, the results seem to be in agreement with more recent summaries, such as&nbsp;<a href=\"http://psycnet.apa.org/journals/amp/64/6/515/\">Kahneman and Klein's</a>. The heart of the article was a decomposition of characteristics (for professions and for tasks within those professions) where we would expert experts to develop good performance:</p>\n<p><br /> \n<table style=\"color: #000000; line-height: 19px; text-align: justify;\" border=\"3\" cellpadding=\"10\" align=\"center\">\n<tbody>\n<tr>\n<th style=\"font-size: 15px;\" align=\"center\">Good performance</th> <th style=\"font-size: 15px;\" align=\"center\">Poor performance</th>\n</tr>\n<tr>\n<td style=\"font-size: 15px;\" align=\"center\">\n<p style=\"font-size: small; line-height: normal\">Static stimuli</p>\n<p style=\"font-size: small; line-height: normal\">Decisions about things</p>\n<p style=\"font-size: small; line-height: normal\">Experts agree on stimuli</p>\n<p style=\"font-size: small; line-height: normal\">More predictable problems</p>\n<p style=\"font-size: small; line-height: normal\">Some errors expected</p>\n<p style=\"font-size: small; line-height: normal\">Repetitive tasks</p>\n<p style=\"font-size: small; line-height: normal\">Feedback available</p>\n<p style=\"font-size: small; line-height: normal\">Objective analysis available</p>\n<p style=\"font-size: small; line-height: normal\">Problem decomposable</p>\n<p style=\"font-size: small; line-height: normal\">Decision aids common</p>\n</td>\n<td style=\"font-size: 15px;\" align=\"center\">\n<p style=\"font-size: small; line-height: normal\">Dynamic (changeable) stimuli</p>\n<p style=\"font-size: small; line-height: normal\">Decisions about behavior</p>\n<p style=\"font-size: small; line-height: normal\">Experts disagree on stimuli</p>\n<p style=\"font-size: small; line-height: normal\">Less predictable problems</p>\n<p style=\"font-size: small; line-height: normal\">Few errors expected</p>\n<p style=\"font-size: small; line-height: normal\">Unique tasks</p>\n<p style=\"font-size: small; line-height: normal\">Feedback unavailable</p>\n<p style=\"font-size: small; line-height: normal\">Subjective analysis only</p>\n<p style=\"font-size: small; line-height: normal\">Problem not decomposable</p>\n<p style=\"font-size: small; line-height: normal\">Decision aids rare</p>\n</td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>I do feel that this may go some way to explaining the expert's performance <a href=\"/lw/e36/ai_timeline_predictions_are_we_getting_better/\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"tRPnS4FoZeWjRfBxN": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yrNW4ApXrpn2KMhxr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 25, "extendedScore": null, "score": 3.9e-05, "legacy": true, "legacyId": "18294", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["47ci9ixyEbGKWENwR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-16T20:02:15.088Z", "modifiedAt": null, "url": null, "title": "Who Wants To Start An Important Startup?", "slug": "who-wants-to-start-an-important-startup", "viewCount": null, "lastCommentedAt": "2020-12-29T13:05:09.101Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ShannonFriedman", "createdAt": "2012-06-19T16:21:31.296Z", "isAdmin": false, "displayName": "ShannonFriedman"}, "userId": "yzRAjgwgXY3bbapsP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YbCc3NRrr5avvWSHT/who-wants-to-start-an-important-startup", "pageUrlRelative": "/posts/YbCc3NRrr5avvWSHT/who-wants-to-start-an-important-startup", "linkUrl": "https://www.lesswrong.com/posts/YbCc3NRrr5avvWSHT/who-wants-to-start-an-important-startup", "postedAtFormatted": "Thursday, August 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Who%20Wants%20To%20Start%20An%20Important%20Startup%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWho%20Wants%20To%20Start%20An%20Important%20Startup%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYbCc3NRrr5avvWSHT%2Fwho-wants-to-start-an-important-startup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Who%20Wants%20To%20Start%20An%20Important%20Startup%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYbCc3NRrr5avvWSHT%2Fwho-wants-to-start-an-important-startup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYbCc3NRrr5avvWSHT%2Fwho-wants-to-start-an-important-startup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1196, "htmlBody": "<p><em><strong>SUMMARY</strong>:&nbsp;</em><em><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 12.571428298950195px;\">Let's collect people who want to work on for-profit&nbsp;</span><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 12.571428298950195px;\">companies that have significant positive impacts on many people's lives.</span></em></p>\n<p>Google provides a huge service to the world - efficient search of a vast amount of data. I would really like to see more for-profit businesses like Google, especially in underserved areas like those explored by non-profits <a href=\"http://www.givewell.org/\">GiveWell</a>, <a href=\"http://intelligence.org/\">Singularity Institute </a>and <a href=\"http://appliedrationality.org/\">CFAR</a>. GiveWell is a nonprofit that is both working toward making humanity better, and thinking about leverage. Instead of hacking away at one branch of the problem of effective charity by working on one avenue for helping people, they've taken it meta. They're providing a huge service by helping people choose non-profits to donate to that give the most bang for your buck, and they're giving the non-profits feedback on how they can improve. I would love to see more problems taken meta like that, where people invest in high leverage things.</p>\n<p>Beyond these non-profits, I think there is a huge amount of low-hanging fruit for creating businesses that create a lot of good for humanity and make money. For-profit businesses that pay their employees and investors well have the advantage that they can entice very successful and comfortable people away from other jobs that are less beneficial to humanity. Unlike non-profits where people are often trying to scrape by, doing the good of their hearts, people doing for-profits can live easy lives with luxurious self care while improving the world at the same time.</p>\n<p>It's all well and good to appeal to altruistic motives, but a lot more people can be mobilzed if they don't have to sacrifice their own comfort. I have learned a great deal about this from Jesse and Sharla at <a href=\"http://www.rejuvenateyourpractice.com/index.html\">Rejuvenate</a><a href=\"http://www.rejuvenateyourpractice.com/index.html\"></a>. They train coaches and holistic practitioners in sales and marketing - enabling thousands of people to start businesses who are doing the sorts of things that advance their mission. They do this while also being multi-millionaires themselves, and maintaining a very comfortable lifestyle, taking the time for self-care and relaxation to recharge from long workdays.</p>\n<p>Less Wrong is read by thousands of people, many of whom are <a href=\"/lw/8p4/2011_survey_results/\">brilliant and talented</a>. In addition, Less Wrong readers include people who are interested in the future of the world and think about the big picture. They think about things like AI and the vast positive and negative consequences it could have. In general, they consider possibilities that are outside of their immediate sensory experience.</p>\n<p>I've run into a lot of people in this community with some really cool, unique, and interesting ideas, for high-impact ways to improve the world. I've also run into a lot of talent in this community, and I have concluded that we have the resources to implement a lot of these same ideas.</p>\n<p>Thus, I am opening up this post as a discussion for these possibilities. I believe that we can share and refine them on this blog, and that there are talented people who will execute them if we come up with something good. For instance, I have run into countless programmers who would love to be working on something more inspiring than what they're doing now. I've also personally talked to several smart organizational leader types, such as <a href=\"/user/jolly\">Jolly</a> and <a href=\"/user/evelynm\">Evelyn</a>, who are interested in helping with and/or leading inspiring projects And that's only the people I've met personally; I know there are a lot more folks like that, and people with talents and resources that haven't even occurred to me, who are going to be reading this.</p>\n<p><br /> <strong>Topics to consider when examining an idea: </strong></p>\n<ul>\n<li> Tradeoffs between optimizing for good effects on the world v. making a profit.</li>\n<li>Ways to improve both profitability and good effects on the world.</li>\n<li>Timespan - projects for 3 months, 1 year, 5 years, 10+ years</li>\n<li>Using resources efficiently (e.g. creating betting markets where a lot of people give opinions that they have enough confidence in to back with money, instead of having one individual trying to figure out probabilities)</li>\n<li>Opportunities for uber-programmers who can do anything quickly (they are reading and you just might interest and inspire them)</li>\n<li>Opportunities for newbies trying to get a foot in the door who will work for cheap</li>\n<li>What people/resources do we have at our disposal now, and what can we do with that?</li>\n<li>What people/resources are still needed?</li>\n<li>If you think of something else, make a comment about it in the <a href=\"/lw/e26/who_wants_to_start_an_important_startup/775t\">thread</a> for that, and it might get added to this list. </li>\n</ul>\n<p><br /> An example idea from <a href=\"http://www.history.com/shows/invention-usa/bios/reichart-von-wolfsheild\">Reichart Von Wolfsheild</a>:</p>\n<p style=\"padding-left: 30px;\"><em>A project to document the best advice we can muster into a single tome. It would inherently be something dynamic, that would grow and cover the topics important to humans that they normally seek refuge and comfort for in religion. A \"bible\" of sorts for the critical mind.</em></p>\n<p style=\"padding-left: 30px;\"><em> Before things like wikis, this was a difficult problem to take on. But, that has changed, and the best information we have available can in fact be filtered for, and simplified. The trick now, is to organize it in a way that helps humans. which is not how most information is organized.</em></p>\n<p><strong>Collaboration</strong></p>\n<ol>\n<li><strong>Please keep the mission in mind</strong> (let's have more for-profit companies working on goals that benefit people too!) when giving feedback. When you write a comment, consider whether it is contributing to that goal, or if it's counterproductive to motivation or idea-generation, and edit accordingly. </li>\n<li><strong>Give feedback, the more specific the better.</strong> Negative feedback is valuable because it tells us where to concentrate further work. It can also be a motivation-killer; it feels like punishment, and not just for the specific item criticized, so be charitable about the motives and intelligence of others, and stay mindful of how much and how aggressively you dole critiques out. (Do give critiques, they're essential - just be gentle!) Also, distribute positive feedback for the opposite effect. <a href=\"/lw/e26/who_wants_to_start_an_important_startup/773w\">More detail on giving the best possible feedback in this comment</a>.</li>\n<li><strong> Please point other people with resources such as business experience, intelligence, implementation skills, and funding capacity at this post.</strong> The more people with these resources who look at this and collaborate in the comments, the more likely it is for these ideas to get implemented. In addition to posting this to Less Wrong, I will be sending the link to a lot of friends with shrewd business skills, resources and talent, who might be interested in helping make projects happen, or possibly in finding people to work on their own projects since many of them are already working on projects to make the world better.</li>\n<li><strong>Please provide feedback</strong>. If anything good happens in your life as a result of this post or discussion, please comment about it and/or give me feedback. It inspires people, and I have bets going that I'd like to win. <a href=\"/lw/e26/who_wants_to_start_an_important_startup/7742\">Consider making bets of your own!</a> It is also important to let me know if you are going to use the ideas, so that we don't end up with needless duplication and competition.</li>\n</ol>\n<p><em>Finally: If this works right, there will be lots of information flying around. Check out the <a href=\"/lw/e26/who_wants_to_start_an_important_startup/7746\">organization thread</a>&nbsp;and the<a href=\"http://lw-ideas.herokuapp.com/\" target=\"_blank\"> wiki</a>.</em><strong><br /></strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4kQXps8dYsKJgaayN": 1, "qAvbtzdG2A2RBn7in": 1, "8sh6iLwYWDJ7z3fPo": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YbCc3NRrr5avvWSHT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 56, "extendedScore": null, "score": 0.000114, "legacy": true, "legacyId": "18222", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 45, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 410, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HAEPbGaMygJq8L59k"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-16T21:57:41.809Z", "modifiedAt": null, "url": null, "title": "Kelly Criteria and Two Envelopes", "slug": "kelly-criteria-and-two-envelopes", "viewCount": null, "lastCommentedAt": "2012-08-17T18:12:41.380Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZBq4H9tdkm5MgC8hL/kelly-criteria-and-two-envelopes", "pageUrlRelative": "/posts/ZBq4H9tdkm5MgC8hL/kelly-criteria-and-two-envelopes", "linkUrl": "https://www.lesswrong.com/posts/ZBq4H9tdkm5MgC8hL/kelly-criteria-and-two-envelopes", "postedAtFormatted": "Thursday, August 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Kelly%20Criteria%20and%20Two%20Envelopes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKelly%20Criteria%20and%20Two%20Envelopes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZBq4H9tdkm5MgC8hL%2Fkelly-criteria-and-two-envelopes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Kelly%20Criteria%20and%20Two%20Envelopes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZBq4H9tdkm5MgC8hL%2Fkelly-criteria-and-two-envelopes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZBq4H9tdkm5MgC8hL%2Fkelly-criteria-and-two-envelopes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 339, "htmlBody": "<p>(This post is motivated by <a href=\"/lw/dy9/solving_the_two_envelopes_problem/\">recent</a> <a href=\"/lw/e26/who_wants_to_start_an_important_startup/77e3\">discussions</a> here of the two titular topics.)</p>\n<p>Suppose someone hands you two envelopes and gives you some information that allows you to conclude either:</p>\n<ol>\n<li>The expected ratio of amount of money in the red envelope to the amount in the blue is &gt;1, or</li>\n<li>With probability close to 1 (say 0.999) the amount of money in the red envelope is greater than the amount in the blue.</li>\n</ol>\n<div>In either case, is the conclusion sufficient to imply that one should choose the red envelope over the blue? Obviously not, right? (Well, at least #2 should be obvious, and #1 was recently <a href=\"/lw/dy9/solving_the_two_envelopes_problem/75dh\">pointed out</a> by VincentYu.) In any case I will also give some simple counter-examples here:</div>\n<div><ol>\n<li>Suppose red envelope has $5 and blue envelope has even chance of $1 and $100. E(R/B) = .5(5/1)+.5(5/100) = 2.525 but one would want to choose the blue envelope assuming utility linear in money.</li>\n<li>Red envelope has $100, blue envelope has $99 with probability 0.999 and $1 million with probability 0.001.&nbsp;</li>\n</ol></div>\n<p>Notice that it's not sufficient to establish both conclusions at once either (my second example above actually satisfies both).</p>\n<p>A common argument for the Kelly Criteria being \"optimal\" (see page 10 of <a href=\"http://www.bf.uzh.ch/publikationen/pdf/publ_1967.pdf\">this review paper</a> recommended by Robin Hanson) is to mathematically establish conclusions 1 and 2, with Kelly Criteria in place of the red envelope and \"any other strategy\" in place of the blue envelope. However it turns out that \"optimal\" is not supposed to be normative, as the paper later explains:</p>\n<blockquote>\n<p>\n<p>In essence&nbsp;the critique is that you should maximize your utility function rather than to base&nbsp;your investment decision on some other criterion. This is certainly correct, but fails&nbsp;to appreciate that Kelly's results are not necessarily normative but rather descriptive.</p>\n</p>\n</blockquote>\n<p>So the upshot here is that unless your utility function is actually log in money and not, say, linear (or even <a href=\"/lw/12v/fair_division_of_blackhole_negentropy_an/\">superlinear</a>) in the amount of resources under your control, you may not want to adopt the Kelly Criteria even when the other commonly mentioned assumptions are&nbsp;satisfied.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HPZzE9XBy99RmbmQe": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZBq4H9tdkm5MgC8hL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "18296", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GezzauYzGTkcwgkA7", "z3W8PRHJM9ZanTDcx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-17T04:05:17.774Z", "modifiedAt": null, "url": null, "title": " Number of Members on LessWrong", "slug": "number-of-members-on-lesswrong-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:05.988Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Epiphany", "createdAt": "2012-08-12T03:33:21.256Z", "isAdmin": false, "displayName": "Epiphany"}, "userId": "BbbFp6hQzKF4YX8em", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9quuJDKBr7HWGkJMg/number-of-members-on-lesswrong-0", "pageUrlRelative": "/posts/9quuJDKBr7HWGkJMg/number-of-members-on-lesswrong-0", "linkUrl": "https://www.lesswrong.com/posts/9quuJDKBr7HWGkJMg/number-of-members-on-lesswrong-0", "postedAtFormatted": "Friday, August 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%20Number%20of%20Members%20on%20LessWrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%20Number%20of%20Members%20on%20LessWrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9quuJDKBr7HWGkJMg%2Fnumber-of-members-on-lesswrong-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%20Number%20of%20Members%20on%20LessWrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9quuJDKBr7HWGkJMg%2Fnumber-of-members-on-lesswrong-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9quuJDKBr7HWGkJMg%2Fnumber-of-members-on-lesswrong-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 504, "htmlBody": "<p>I wanted to know how many people have joined LessWrong but I couldn't find anything stating the number of members on LessWrong anywhere on the site or the internet, so I decided to MacGyver it out of Google:<br /><br />site:lesswrong.com/user -\"submitted by\" -\"comments by\"<br /><br />(Translation provided at the end.)<br /><br />This gets a similar result in Bing and Yahoo:<br /><br />\"lesswrong.com/user\"<br /><br />If this is correct, LessWrong has over 9,000 members, but my LessWrong population figure is likely to be low.&nbsp; Since it was so hard to find out how many users LessWrong has, I decided to post it.&nbsp; I can't be the only curious person.<br /><br />&nbsp;<br /><br />Why my figure is likely to be on the low side (and general inaccuracies):<br /><br />&nbsp; - Some users may not be included in Google's index yet.&nbsp; For instance, if they have never posted, there may be no link to their page (which is what I searched for - user pages), and the spider would not find them.&nbsp; This may be restricted to members that have actually commented, posted, or have been linked to in some way somewhere on the internet.<br /><br />&nbsp; - Search engine caches are not in real time.&nbsp; There can be a lag of up to months, depending on how much the search engine \"likes\" the page.<br /><br />&nbsp; - It has been reported by previous employees of a major search engine that they are using crazy old computer equipment to store their caches.&nbsp; I've been told that it is common for sections of cache to be down for that reason.<br /><br />&nbsp; - Some of the results in Bing and Yahoo were irrelevant, though I think I weeded them pretty thoroughly for Google if my random samples of results pages are a good indication of the whole.<br /><br />Go ahead and check it out - stick the code in Google and see how many LessWrong members it shows.&nbsp; You'll certainly get a more up-to-date total than I have posted here.&nbsp; ;)<br /><br />&nbsp;<br /><br />Translation for those of you that don't know Google's codes:<br /><br />site:lesswrong.com/user<br /><br />\"Search only lesswrong.com, only the user directory.\"<br /><br />(The user directory is where each user's home page is, so I'm essentially telling it \"find all the home page directories\".)<br /><br />-\"submitted by\" -\"comments by\"<br /><br />Exclude any page in that directory with the exact text \"submitted by\", exclude any page with the exact text \"comments by\"<br /><br />(The submissions and comments pages use a url in that directory, so they will show up in the results if I do not subtract them.&nbsp; Also, I used exact text specific to those pages, so that the text in the links on user home pages do not get user home pages omitted from the search.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9quuJDKBr7HWGkJMg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "18306", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-17T05:47:55.115Z", "modifiedAt": null, "url": null, "title": "Number of Members on LessWrong", "slug": "number-of-members-on-lesswrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:07.572Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Epiphany", "createdAt": "2012-08-12T03:33:21.256Z", "isAdmin": false, "displayName": "Epiphany"}, "userId": "BbbFp6hQzKF4YX8em", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SBAJ56QFiwYugRe6A/number-of-members-on-lesswrong", "pageUrlRelative": "/posts/SBAJ56QFiwYugRe6A/number-of-members-on-lesswrong", "linkUrl": "https://www.lesswrong.com/posts/SBAJ56QFiwYugRe6A/number-of-members-on-lesswrong", "postedAtFormatted": "Friday, August 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Number%20of%20Members%20on%20LessWrong&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANumber%20of%20Members%20on%20LessWrong%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSBAJ56QFiwYugRe6A%2Fnumber-of-members-on-lesswrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Number%20of%20Members%20on%20LessWrong%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSBAJ56QFiwYugRe6A%2Fnumber-of-members-on-lesswrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSBAJ56QFiwYugRe6A%2Fnumber-of-members-on-lesswrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1016, "htmlBody": "<p>I was excited to find this site, so I wanted to know how many people had joined LessWrong.&nbsp; Was it what it seemed - that a lot of people had actually gathered around the theme of rational thought - or was that just wishful thinking about a site that a guy with a neat idea and his buddies put together?&nbsp; I couldn't find anything stating the number of members on LessWrong anywhere on the site or the internet, so I decided it would be a fun test of my search engine knowledge to nail jello to a tree and make my own.</p>\n<p>Some argue that Google totals are completely meaningless, however, the real problem is that it's very complicated and if you don't know how search engines work, your likelihood of getting a usable number is low.&nbsp; I took into account the potential pitfalls when MacGyvering this figure out of Google.&nbsp; So far, no one has posted a significant flaw with my specific method.&nbsp; (I will change that statement if they do, once I've read their comment.)&nbsp; Also, <a href=\"/lw/ec2/preventing_discussion_from_being_watered_down_by/\">I was right (Find in page: total)</a>.</p>\n<p>Here is the query I constructed:</p>\n<pre>site:lesswrong.com/user -\"submitted by\" -\"comments by\"</pre>\n<p>(Translation provided at the end.)<br /><br />This gets a similar result in Bing and Yahoo:</p>\n<pre>\"lesswrong.com/user\"</pre>\n<p>If this is correct, LessWrong has over 9,000 members.&nbsp; That's my claim: \"LessWrong probably has over 9,000 members\" not \"LessWrong has exactly 9,000 members\".&nbsp; My LessWrong population figure is likely to be low.&nbsp; (I explain this below.)</p>\n<p>Why did I do this?&nbsp; I was really overjoyed to find this site and wanted to see whether it was somebody's personal site with just a few buddies, or if they actually managed to draw a significant gathering of people who are interested in rational thought.&nbsp; I was very happy to see that it looks much bigger than a personal site.&nbsp; Since it was so hard to find out how many users LessWrong has, I decided to share.</p>\n<p>I think a lot of people assume the hasty generalization that \"all search engine totals are meaningless\".&nbsp; If you're an average user just plugging in search terms with little understanding of how search engines work: yes, you should regard them as meaningless.&nbsp; However, if you know the limitations of a technique, what parts of the system your working within are consistent and what parts of it are not, I say it is possible to get some meaning within those limitations.&nbsp; Do I know <strong>all</strong> the limitations?&nbsp; Well, I assume I am unaware of things I don't know, so I won't say that.&nbsp; But I do know that so far nobody has proven this number or method wrong.&nbsp; If you want to prove me wrong, go for it.&nbsp; That would be fascinating.&nbsp; Remember that the claim is \"LessWrong probably has over 9,000 members\".&nbsp; The entire purpose of this was to get an \"at least this many\" figure for how many members LessWrong has.&nbsp; The inaccuracies I've already taken into consideration in order to compensate for the limits of this technique are listed below:</p>\n<p>&nbsp;</p>\n<p><strong>Why this is an \"at least this many\" figure, pitfalls I've avoided or addressed, and inaccuracies.</strong><br /><br />&nbsp; - Some users may not be included in Google's index yet.&nbsp; For instance, if they have never posted, there may be no link to their page (which is what I searched for - user pages), and the spider would not find them.&nbsp; This may be restricted to members that have actually commented, posted, or have been linked to in some way somewhere on the internet.&nbsp; <br /><br />&nbsp; - Search engine caches are not in real time.&nbsp; There can be a lag of up to months, depending on how much the search engine \"likes\" the page. <br /><br />&nbsp; - It has been reported by previous employees of a major search engine that they are using crazy old computer equipment to store their caches.&nbsp; I've been told that it is common for sections of cache to be down for that reason.</p>\n<p>&nbsp; - Search engines have restrictions in place to conserve resources.&nbsp; For instance, they won't let you peruse all of the results using the \"next\" button, and they don't total all of the results that they have when you first press \"search\" (you may see that number increase later if you continue to press \"next\" to see more pages of results.)</p>\n<p>&nbsp; - It has been argued that Google doesn't interpret search terms the way you'd think.&nbsp; I knew that before I started.&nbsp; The query&nbsp; was designed with that in mind.&nbsp; I explain that here: http://lesswrong.com/r/discussion/lw/e4j/number_of_members_on_lesswrong/780g</p>\n<p>&nbsp; - Some of the results in Bing and Yahoo were irrelevant, though I think I weeded them pretty thoroughly for Google if my random samples of results pages are a good indication of the whole.</p>\n<p>&nbsp; - When you go to your user page, if you have more than 10 comments, a next link shows at the bottom and clicking it makes more pages appear.&nbsp; My understanding is that Google doesn't index these types of links - and they don't seem to be getting included.&nbsp; http://lesswrong.com/lw/e4j/number_of_members_on_lesswrong/7839</p>\n<p>Go ahead and check it out - stick the query in Google and see how many LessWrong members it shows.&nbsp; You'll certainly get a more up-to-date total than I have posted here.&nbsp; ;)</p>\n<p>&nbsp;</p>\n<p><strong>Translation for those of you that don't know Google's codes:</strong></p>\n<pre>site:lesswrong.com/user</pre>\n<p>\"Search only lesswrong.com, only the user directory.\"</p>\n<p>(The user directory is where each user's home page is, so I'm essentially telling it \"find all the home page directories\".)</p>\n<pre>-\"submitted by\" -\"comments by\"</pre>\n<p>Exclude any page in that directory with the exact text \"submitted by\" or \"comments by\"</p>\n<p>(The submissions and comments pages use a url in that directory, so they will show up in the results if I do not subtract them.&nbsp; Also, I used exact text specific to those pages, so that the text in the links on user home pages do not get user home pages omitted from the search. )</p>\n<p>&nbsp;</p>\n<p><strong>Note:</strong></p>\n<p>I realize this number isn't scientific proof of anything, (we can't see Google's code so that would be foolish), which is why I'm not attempting to use it to convince anyone of anything important.&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SBAJ56QFiwYugRe6A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 4, "extendedScore": null, "score": 9.66011641877571e-07, "legacy": true, "legacyId": "18307", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I was excited to find this site, so I wanted to know how many people had joined LessWrong.&nbsp; Was it what it seemed - that a lot of people had actually gathered around the theme of rational thought - or was that just wishful thinking about a site that a guy with a neat idea and his buddies put together?&nbsp; I couldn't find anything stating the number of members on LessWrong anywhere on the site or the internet, so I decided it would be a fun test of my search engine knowledge to nail jello to a tree and make my own.</p>\n<p>Some argue that Google totals are completely meaningless, however, the real problem is that it's very complicated and if you don't know how search engines work, your likelihood of getting a usable number is low.&nbsp; I took into account the potential pitfalls when MacGyvering this figure out of Google.&nbsp; So far, no one has posted a significant flaw with my specific method.&nbsp; (I will change that statement if they do, once I've read their comment.)&nbsp; Also, <a href=\"/lw/ec2/preventing_discussion_from_being_watered_down_by/\">I was right (Find in page: total)</a>.</p>\n<p>Here is the query I constructed:</p>\n<pre>site:lesswrong.com/user -\"submitted by\" -\"comments by\"</pre>\n<p>(Translation provided at the end.)<br><br>This gets a similar result in Bing and Yahoo:</p>\n<pre>\"lesswrong.com/user\"</pre>\n<p>If this is correct, LessWrong has over 9,000 members.&nbsp; That's my claim: \"LessWrong probably has over 9,000 members\" not \"LessWrong has exactly 9,000 members\".&nbsp; My LessWrong population figure is likely to be low.&nbsp; (I explain this below.)</p>\n<p>Why did I do this?&nbsp; I was really overjoyed to find this site and wanted to see whether it was somebody's personal site with just a few buddies, or if they actually managed to draw a significant gathering of people who are interested in rational thought.&nbsp; I was very happy to see that it looks much bigger than a personal site.&nbsp; Since it was so hard to find out how many users LessWrong has, I decided to share.</p>\n<p>I think a lot of people assume the hasty generalization that \"all search engine totals are meaningless\".&nbsp; If you're an average user just plugging in search terms with little understanding of how search engines work: yes, you should regard them as meaningless.&nbsp; However, if you know the limitations of a technique, what parts of the system your working within are consistent and what parts of it are not, I say it is possible to get some meaning within those limitations.&nbsp; Do I know <strong>all</strong> the limitations?&nbsp; Well, I assume I am unaware of things I don't know, so I won't say that.&nbsp; But I do know that so far nobody has proven this number or method wrong.&nbsp; If you want to prove me wrong, go for it.&nbsp; That would be fascinating.&nbsp; Remember that the claim is \"LessWrong probably has over 9,000 members\".&nbsp; The entire purpose of this was to get an \"at least this many\" figure for how many members LessWrong has.&nbsp; The inaccuracies I've already taken into consideration in order to compensate for the limits of this technique are listed below:</p>\n<p>&nbsp;</p>\n<p><strong>Why this is an \"at least this many\" figure, pitfalls I've avoided or addressed, and inaccuracies.</strong><br><br>&nbsp; - Some users may not be included in Google's index yet.&nbsp; For instance, if they have never posted, there may be no link to their page (which is what I searched for - user pages), and the spider would not find them.&nbsp; This may be restricted to members that have actually commented, posted, or have been linked to in some way somewhere on the internet.&nbsp; <br><br>&nbsp; - Search engine caches are not in real time.&nbsp; There can be a lag of up to months, depending on how much the search engine \"likes\" the page. <br><br>&nbsp; - It has been reported by previous employees of a major search engine that they are using crazy old computer equipment to store their caches.&nbsp; I've been told that it is common for sections of cache to be down for that reason.</p>\n<p>&nbsp; - Search engines have restrictions in place to conserve resources.&nbsp; For instance, they won't let you peruse all of the results using the \"next\" button, and they don't total all of the results that they have when you first press \"search\" (you may see that number increase later if you continue to press \"next\" to see more pages of results.)</p>\n<p>&nbsp; - It has been argued that Google doesn't interpret search terms the way you'd think.&nbsp; I knew that before I started.&nbsp; The query&nbsp; was designed with that in mind.&nbsp; I explain that here: http://lesswrong.com/r/discussion/lw/e4j/number_of_members_on_lesswrong/780g</p>\n<p>&nbsp; - Some of the results in Bing and Yahoo were irrelevant, though I think I weeded them pretty thoroughly for Google if my random samples of results pages are a good indication of the whole.</p>\n<p>&nbsp; - When you go to your user page, if you have more than 10 comments, a next link shows at the bottom and clicking it makes more pages appear.&nbsp; My understanding is that Google doesn't index these types of links - and they don't seem to be getting included.&nbsp; http://lesswrong.com/lw/e4j/number_of_members_on_lesswrong/7839</p>\n<p>Go ahead and check it out - stick the query in Google and see how many LessWrong members it shows.&nbsp; You'll certainly get a more up-to-date total than I have posted here.&nbsp; ;)</p>\n<p>&nbsp;</p>\n<p><strong id=\"Translation_for_those_of_you_that_don_t_know_Google_s_codes_\">Translation for those of you that don't know Google's codes:</strong></p>\n<pre>site:lesswrong.com/user</pre>\n<p>\"Search only lesswrong.com, only the user directory.\"</p>\n<p>(The user directory is where each user's home page is, so I'm essentially telling it \"find all the home page directories\".)</p>\n<pre>-\"submitted by\" -\"comments by\"</pre>\n<p>Exclude any page in that directory with the exact text \"submitted by\" or \"comments by\"</p>\n<p>(The submissions and comments pages use a url in that directory, so they will show up in the results if I do not subtract them.&nbsp; Also, I used exact text specific to those pages, so that the text in the links on user home pages do not get user home pages omitted from the search. )</p>\n<p>&nbsp;</p>\n<p><strong id=\"Note_\">Note:</strong></p>\n<p>I realize this number isn't scientific proof of anything, (we can't see Google's code so that would be foolish), which is why I'm not attempting to use it to convince anyone of anything important.&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "Translation for those of you that don't know Google's codes:", "anchor": "Translation_for_those_of_you_that_don_t_know_Google_s_codes_", "level": 1}, {"title": "Note:", "anchor": "Note_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "32 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["MLaSGq6A6bLTdt6r8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-17T07:07:11.196Z", "modifiedAt": null, "url": null, "title": "AI timeline predictions: are we getting better?", "slug": "ai-timeline-predictions-are-we-getting-better", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:45.551Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/47ci9ixyEbGKWENwR/ai-timeline-predictions-are-we-getting-better", "pageUrlRelative": "/posts/47ci9ixyEbGKWENwR/ai-timeline-predictions-are-we-getting-better", "linkUrl": "https://www.lesswrong.com/posts/47ci9ixyEbGKWENwR/ai-timeline-predictions-are-we-getting-better", "postedAtFormatted": "Friday, August 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20timeline%20predictions%3A%20are%20we%20getting%20better%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20timeline%20predictions%3A%20are%20we%20getting%20better%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F47ci9ixyEbGKWENwR%2Fai-timeline-predictions-are-we-getting-better%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20timeline%20predictions%3A%20are%20we%20getting%20better%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F47ci9ixyEbGKWENwR%2Fai-timeline-predictions-are-we-getting-better", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F47ci9ixyEbGKWENwR%2Fai-timeline-predictions-are-we-getting-better", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1228, "htmlBody": "<p><strong>EDIT</strong>: <em>Thanks to <a href=\"/lw/e36/ai_timeline_predictions_are_we_getting_better/77gd\">Kaj's</a> work, we now have more rigorous evidence on the \"Maes-Garreau law\" (the idea that people will predict AI coming before they die). This post has been updated with extra information. The original data used for this analysis can now be found through <a href=\"/lw/e79/ai_timeline_prediction_data/\">here</a>.</em></p>\n<p>Thanks to some sterling work by <a href=\"http://kajsotala.fi/\">Kaj Sotala</a> and others (such as Jonathan Wang and Brian Potter - all paid for by the gracious <a href=\"http://intelligence.org/\">Singularity Institute</a>,&nbsp;a fine organisation that I recommend everyone look into), we've managed to put together a databases listing all AI predictions that we could find. The list is necessarily incomplete, but we found as much as we could, and collated the data so that we could have an overview of what people have been predicting in the field since Turing.</p>\n<p>We retained 257 predictions total, of various quality (in our expanded definition, philosophical arguments such as \"computers can't think because they don't have bodies\" count as predictions). Of these, 95 could be construed as giving timelines for the creation of human-level AIs. And \"construed\" is the operative word - very few were in a convenient \"By golly, I give a 50% chance that we will have human-level AIs by XXXX\" format. Some gave ranges; some were surveys of various experts; some predicted other things (such as child-like AIs, or superintelligent AIs).</p>\n<p>Where possible, I collapsed these down to single median estimate, making some somewhat arbitrary choices and judgement calls.&nbsp;When a range was given, I took the mid-point of that range. If a year was given with a 50% likelihood estimate, I took that year. If it was the collection of a variety of expert opinions, I took the prediction of the median expert. If the author predicted some sort of AI by a given date (partial AI or superintelligent AI), I took that date as their estimate rather than trying to correct it in one direction or the other (there were roughly the same number of subhuman AIs as suphuman AIs in the list, and not that many of either). I read extracts of the papers to make judgement calls when interpreting problematic statements like \"within thirty years\" or \"during this century\" (is that a range or an end-date?).</p>\n<p>So some biases will certainly have crept in during the process. That said, it's still probably the best data we have. So keeping all that in mind, let's have a look at what these guys said (and it was mainly guys).<a id=\"more\"></a></p>\n<p>There are two&nbsp;stereotypes&nbsp;about predictions in AI and similar technologies. The first is the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Maes-Garreau_Law\">Maes-Garreau law</a>: technologies as supposed to arrive... just within the lifetime of the predictor!</p>\n<p>The other stereotype is the informal 20-30 year range for any new technology: the predictor knows the technology isn't immediately available, but puts it in a range where people would still be likely to worry about it. And so the predictor gets kudos for&nbsp;addressing&nbsp;the problem or the potential, and is safely retired by the time it (doesn't) come to pass. Are either of these stereotypes born out by the data? Well, here is a histogram of the various \"time to AI\" predictions:</p>\n<p><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_e36_0.png?v=36ff71c36479ab581b62303b569d7257\" alt=\"Time to AI\" width=\"492\" height=\"417\" /></p>\n<p>As can be seen, the 20-30 year stereotype is not exactly born out - but a 15-25 one would be. Over a third of predictions are in this range. If we ignore predictions more than 75 years into the future, 40% are in the 15-25 range, and 50% are in the 15-30 range.</p>\n<p>Apart from that, there is a gradual tapering off, a slight increase at 50 years, and twelve predictions beyond three quarters of a century. Eyeballing this, there doesn't seem to much evidence for the&nbsp;Maes-Garreau law. Kaj&nbsp;<a href=\"/lw/e36/ai_timeline_predictions_are_we_getting_better/77gd\">looked</a>&nbsp;into this specifically, plotting (life expectancy)&nbsp;minus (time to AI) versus the age of the predictor; the&nbsp;Maes-Garreau law would expect the data to be&nbsp;clustered&nbsp;around the zero line:</p>\n<p><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_e36_5.png?v=746231896edf0f2063e0a428cd0274cd\" alt=\"Age versus time remaining to AI\" width=\"567\" height=\"495\" /></p>\n<p>Most of the data seems to be decades out from the zero point (note the scale on the y axis). You could argue, possibly, that fifty year olds are more likely to predict AI just within their lifetime, but this is a very weak effect. I see no evidence for the&nbsp;Maes-Garreau law - of the 37 prediction Kaj retained, only 6 predictions (16%) were within five years (in either direction) of the expected death date.</p>\n<p>But not all predictions are created equal. 62 of the predictors were labelled \"experts\" in the analysis - these had some degree of expertise in fields that were relevant to AI. The other 33 were amateurs - journalists, writers and such. Decomposing into these two groups showed very little difference, though:</p>\n<p><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_e36_1.png\" alt=\"Time to AI - Experts\" width=\"492\" height=\"417\" />&nbsp;<img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_e36_2.png\" alt=\"Time to AI - non-experts\" width=\"492\" height=\"417\" /></p>\n<p>The only noticeable difference is that amateurs lacked the upswing at 50 years, and were relatively more likely to push their predictions beyond 75 years. This does not look like good news for the experts - if their performance can't be distinguished from amateurs, what contributions is their expertise making?</p>\n<p>But I've been remiss so far - combining predictions that we know are false (because their deadline has come and gone) with those that could still be true. If we look at predictions that have failed, we get this interesting graph:</p>\n<p><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_e36_3.png\" alt=\"Time to AI - failed predictions\" width=\"492\" height=\"417\" /></p>\n<p>This looks very similar to the original graph. The main difference being the lack of very long range predictions. This is not, in fact, because there has not yet been enough time for these predictions to be proved false, but because prior to the 1990s, there were actually no predictions with a timeline greater than fifty years.&nbsp;This can best be seen on this scatter plot, which plots the time predicted to AI against the date the prediction was made:</p>\n<p><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_e36_4.png\" alt=\"Time to AI by prediction date\" width=\"481\" height=\"595\" /></p>\n<p>As can be seen, as time elapses, people become more willing to predict very long ranges. But this is something of an&nbsp;artefact&nbsp;- in the early days of computing, people were very willing to predict that AI was impossible. Since this didn't give a timeline, their \"predictions\" didn't show up on the graph. It recent times, people seem a little less likely to claim AI is impossible, replaced by&nbsp;these \"in a century or two\" timelines.</p>\n<p>Apart from that one difference, predictions look&nbsp;remarkably&nbsp;consistent over the span: modern predictors are claiming about the same time will elapse before AI arrives as their (incorrect) predecessors. This doesn't mean that the modern experts are wrong - maybe AI really is imminent this time round, maybe modern experts have more information and are making more finely calibrated guesses. But in a field like AI prediction, where experts lack feed back for their pronouncements, we should expect them to perform <a href=\"http://www.sciencedirect.com/science/article/pii/074959789290064E\">poorly</a>, and for biases to <a href=\"http://psycnet.apa.org/journals/amp/64/6/515.pdf\">dominate</a> their thinking. This seems the likely hypothesis - it would be extraordinarily unlikely that modern experts, free of biases and full of good information, would reach exactly the same prediction distribution as their biased and incorrect predecessors.</p>\n<p>In summary:</p>\n<ul>\n<li>Over a third of predictors claim AI will happen 16-25 years in the future.</li>\n<li>There is no evidence that predictors are predicting AI happening towards the end of their own life expectancy.</li>\n<li>There is little difference between experts and non-experts (some possible reasons for this can be found <a href=\"/r/discussion/lw/e46/competence_in_experts_summary/\">here</a>).</li>\n<li>There is little difference between current predictions, and those known to have been wrong previously.</li>\n<li>It is not unlikely that recent predictions are suffering from the same biases and errors as their predecessors.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zHjC29kkPmsdo7WTr": 7}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "47ci9ixyEbGKWENwR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 59, "baseScore": 79, "extendedScore": null, "score": 0.000187, "legacy": true, "legacyId": "18258", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 79, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 81, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Q6oWinLaKXmGNWGLy", "yrNW4ApXrpn2KMhxr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-17T08:41:27.895Z", "modifiedAt": null, "url": null, "title": "[LINK] Cryonics - without even trying", "slug": "link-cryonics-without-even-trying", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:08.916Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kawoomba", "createdAt": "2012-05-01T11:54:25.423Z", "isAdmin": false, "displayName": "Kawoomba"}, "userId": "FScr44PGNPbBCodRv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/r2maJPwrH9gWv2qeH/link-cryonics-without-even-trying", "pageUrlRelative": "/posts/r2maJPwrH9gWv2qeH/link-cryonics-without-even-trying", "linkUrl": "https://www.lesswrong.com/posts/r2maJPwrH9gWv2qeH/link-cryonics-without-even-trying", "postedAtFormatted": "Friday, August 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Cryonics%20-%20without%20even%20trying&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Cryonics%20-%20without%20even%20trying%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr2maJPwrH9gWv2qeH%2Flink-cryonics-without-even-trying%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Cryonics%20-%20without%20even%20trying%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr2maJPwrH9gWv2qeH%2Flink-cryonics-without-even-trying", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr2maJPwrH9gWv2qeH%2Flink-cryonics-without-even-trying", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 535, "htmlBody": "<p>(Title is tongue-in-cheek, \"preservation\" would've been more appropriate but less catchy)<br /><br />With [news like that](http://news.discovery.com/history/preserved-brain-bog-england-110406.html), how hard can it be when you actually do want to preserve a brain:<br /><br />&gt; A human skull dated to about 2,684 years ago with an \"exceptionally preserved\" human brain still inside of it was recently discovered in a waterlogged U.K. pit, according to a new Journal of Archaeological Science study.<br /><br />&gt; The brain is the oldest known intact human brain from Europe and Asia, according to the authors, who also believe it's one of the best-preserved ancient brains in the world. (...) Scientists believe that submersion in liquid, anoxic environments helps to preserve human brain tissue. <br /><br />Unfortunately for the poor guy / brain, we killed his survival prospects. He did go with the cheap option of just saving the head. Speculating, if he got found another few centuries from now, he might've been a patient, not \"archeological remains\".<br /><br />On a more serious note, I'd like the perspective of someone signed up for cryonics on this:</p>\n<p>With people signed up for cryonics nowadays - I hear it even comes with a necklace! - I wonder what role the signalling aspect (to others, more importantly to oneself, feeling safer from death) plays versus the actual permanent-death-evading.</p>\n<p>Having been present for (mouse) brain slice experiments done immediately after extraction, being confronted with the rapidly progressing tissue decay, the most important aspect that could easily be optimised - apart from research into other methods of preservation - was the time from the extraction to the experiments. Each minute made a tremendous difference. Not a surprise: as the aphorism in neurology (stroke therapy) goes, \"<strong>time is brain</strong>\".<br /><br />What leads me to somewhat doubt the seriousness of the actual belief in brain preservation, versus the belief in belief that's based on minimising existential angst, is that the obvious idea of \"when death is approaching with an ETA of less than X, commit suicide with cryonics on immediate standby\" is not an integral part of the discussion. X may be weeks, or even years, based on how serious you take cryonics.</p>\n<p>The above incidentally contains a way of betting to indicate the strength you assign to the actual prospects of cryonics, versus the role it plays for you psychologically. Isn't betting on your beliefs encouraged in this community? (NB: the \"suicide\" is just included to avoid legal ramifications.)</p>\n<p>Regardless of future technological advances, orders of magnitude less brain damage will certainly pose less of a problem than the delay caused even by a couple of hours. A couple of hours = your brain tissue is already a scorched battlefield! Both necrosis and apoptosis get started within minutes.<br /><br />Measuring your actual belief in the success of cryonics (for someone signed up for cryonics), waiting for death by natural causes doesn't indicate a lot of confidence when even a few weeks of life seem to be measured more highly than a tremendous increase in the actual prospects of cryonics working.</p>\n<p>Or do you have above mentioned plans in place for when your life expectancy is less than X months/years (for whatever reason)?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "r2maJPwrH9gWv2qeH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 9, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "18313", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-17T15:47:34.606Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Brussels, Dublin, Redmond WA, Washington DC", "slug": "weekly-lw-meetups-austin-brussels-dublin-redmond-wa", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5ryCjLt7GgKY6T4Qp/weekly-lw-meetups-austin-brussels-dublin-redmond-wa", "pageUrlRelative": "/posts/5ryCjLt7GgKY6T4Qp/weekly-lw-meetups-austin-brussels-dublin-redmond-wa", "linkUrl": "https://www.lesswrong.com/posts/5ryCjLt7GgKY6T4Qp/weekly-lw-meetups-austin-brussels-dublin-redmond-wa", "postedAtFormatted": "Friday, August 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Brussels%2C%20Dublin%2C%20Redmond%20WA%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Brussels%2C%20Dublin%2C%20Redmond%20WA%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ryCjLt7GgKY6T4Qp%2Fweekly-lw-meetups-austin-brussels-dublin-redmond-wa%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Brussels%2C%20Dublin%2C%20Redmond%20WA%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ryCjLt7GgKY6T4Qp%2Fweekly-lw-meetups-austin-brussels-dublin-redmond-wa", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ryCjLt7GgKY6T4Qp%2Fweekly-lw-meetups-austin-brussels-dublin-redmond-wa", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 429, "htmlBody": "<p><strong>This summary was posted to LW Main on August 10th, and has now been moved to discussion.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/c3\">Brussels meetup:&nbsp;<span class=\"date\">11 August 2012 12:00PM</span></a></li>\n<li><a href=\"/meetups/cs\">(Redmond WA) Board Games:&nbsp;<span class=\"date\">11 August 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/c1\">Dublin, Ireland Meetup:&nbsp;<span class=\"date\">12 August 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/cn\">Washington DC Social Meetup:&nbsp;<span class=\"date\">12 August 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/cm\">Bratislava, Slovakia - the first LW meetup:&nbsp;<span class=\"date\">18 August 2012 05:00PM</span></a></li>\n<li><a href=\"/meetups/cp\">Less Wrong Sydney: 20th August :&nbsp;<span class=\"date\">20 August 2012 06:30PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">11 August 2018 02:30PM</span></a></li>\n<li><a href=\"/meetups/cf\">Cambridge (MA) Meetup:&nbsp;<span class=\"date\">19 August 2012 02:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong><strong>&nbsp;</strong><strong></strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>, <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5ryCjLt7GgKY6T4Qp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 9.663080289905249e-07, "legacy": true, "legacyId": "18194", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-17T16:30:14.592Z", "modifiedAt": null, "url": null, "title": "Launch of the world's first animal charity evaluator: Effective Animal Activism", "slug": "launch-of-the-world-s-first-animal-charity-evaluator-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:06.185Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Lukas_Gloor", "createdAt": "2012-06-10T19:33:54.240Z", "isAdmin": false, "displayName": "Lukas_Gloor"}, "userId": "c8sYrDHyjxX8MNoxi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TCrnRdkk8FgCdBYs3/launch-of-the-world-s-first-animal-charity-evaluator-0", "pageUrlRelative": "/posts/TCrnRdkk8FgCdBYs3/launch-of-the-world-s-first-animal-charity-evaluator-0", "linkUrl": "https://www.lesswrong.com/posts/TCrnRdkk8FgCdBYs3/launch-of-the-world-s-first-animal-charity-evaluator-0", "postedAtFormatted": "Friday, August 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Launch%20of%20the%20world's%20first%20animal%20charity%20evaluator%3A%20Effective%20Animal%20Activism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALaunch%20of%20the%20world's%20first%20animal%20charity%20evaluator%3A%20Effective%20Animal%20Activism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTCrnRdkk8FgCdBYs3%2Flaunch-of-the-world-s-first-animal-charity-evaluator-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Launch%20of%20the%20world's%20first%20animal%20charity%20evaluator%3A%20Effective%20Animal%20Activism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTCrnRdkk8FgCdBYs3%2Flaunch-of-the-world-s-first-animal-charity-evaluator-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTCrnRdkk8FgCdBYs3%2Flaunch-of-the-world-s-first-animal-charity-evaluator-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 7, "htmlBody": "<p><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:HyphenationZone>21</w:HyphenationZone> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Normale Tabelle\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0cm 5.4pt 0cm 5.4pt; mso-para-margin-top:0cm; mso-para-margin-right:0cm; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0cm; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-GB; mso-fareast-language:EN-US;} --> <!--[endif] --></p>\n<p>Apologies, this post was posted by accident.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TCrnRdkk8FgCdBYs3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "18317", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-17T16:33:16.847Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup Thursday 7pm", "slug": "meetup-fort-collins-colorado-meetup-thursday-7pm", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HH57zw7BmfMNEcoS2/meetup-fort-collins-colorado-meetup-thursday-7pm", "pageUrlRelative": "/posts/HH57zw7BmfMNEcoS2/meetup-fort-collins-colorado-meetup-thursday-7pm", "linkUrl": "https://www.lesswrong.com/posts/HH57zw7BmfMNEcoS2/meetup-fort-collins-colorado-meetup-thursday-7pm", "postedAtFormatted": "Friday, August 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHH57zw7BmfMNEcoS2%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Thursday%207pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHH57zw7BmfMNEcoS2%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHH57zw7BmfMNEcoS2%2Fmeetup-fort-collins-colorado-meetup-thursday-7pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 52, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/d1'>Fort Collins, Colorado Meetup Thursday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 August 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1129 W. Elizabeth St. Fort Collins 80521</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Back to Fort Collins.</p>\n\n<p>What are the experiments you are running now? How do you track them?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/d1'>Fort Collins, Colorado Meetup Thursday 7pm</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HH57zw7BmfMNEcoS2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 9.66330625258772e-07, "legacy": true, "legacyId": "18318", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm\">Discussion article for the meetup : <a href=\"/meetups/d1\">Fort Collins, Colorado Meetup Thursday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 August 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1129 W. Elizabeth St. Fort Collins 80521</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Back to Fort Collins.</p>\n\n<p>What are the experiments you are running now? How do you track them?</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm1\">Discussion article for the meetup : <a href=\"/meetups/d1\">Fort Collins, Colorado Meetup Thursday 7pm</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Thursday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Thursday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Thursday_7pm1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-17T19:24:01.241Z", "modifiedAt": null, "url": null, "title": "TLP: Insight without Change", "slug": "tlp-insight-without-change", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:06.484Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FcasJj4ARzmpYwWYg/tlp-insight-without-change", "pageUrlRelative": "/posts/FcasJj4ARzmpYwWYg/tlp-insight-without-change", "linkUrl": "https://www.lesswrong.com/posts/FcasJj4ARzmpYwWYg/tlp-insight-without-change", "postedAtFormatted": "Friday, August 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20TLP%3A%20Insight%20without%20Change&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATLP%3A%20Insight%20without%20Change%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFcasJj4ARzmpYwWYg%2Ftlp-insight-without-change%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=TLP%3A%20Insight%20without%20Change%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFcasJj4ARzmpYwWYg%2Ftlp-insight-without-change", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFcasJj4ARzmpYwWYg%2Ftlp-insight-without-change", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 115, "htmlBody": "<p>An <a href=\"http://thelastpsychiatrist.com/2012/08/just_because_you_see_it_doesnt.html\">interesting read</a> by one of the blogs I read. It seems relevant to <a href=\"/lw/dxr/epiphany_addiction/\">epiphany addiction</a>.</p>\n<p>&nbsp;</p>\n<p>Once you've read it (or tried and decided you didn't like it): the primary thing I got out of it is that insights can only be communicated inside social relationships (counting self-self relationships), which is difficult and rare to optimize around correctly. The father saying \"your problem is that you seek validation from me\" can only be done the in context of the father-son relationship, and saying it directly is often not the optimal way to actually change the behavior of the son. (Inception's <a href=\"http://www.youtube.com/watch?v=T5gYLO_nw_Y\">climax</a> comes to mind, which I don't recommend watching before seeing the rest of the film.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FcasJj4ARzmpYwWYg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 0, "extendedScore": null, "score": 9.66415048114213e-07, "legacy": true, "legacyId": "18321", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CZridws5zBzwfjgef"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-17T21:37:08.624Z", "modifiedAt": null, "url": null, "title": "Any LessWrongers going to EMFCamp?", "slug": "any-lesswrongers-going-to-emfcamp", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kAiinLRMajB2zXNne/any-lesswrongers-going-to-emfcamp", "pageUrlRelative": "/posts/kAiinLRMajB2zXNne/any-lesswrongers-going-to-emfcamp", "linkUrl": "https://www.lesswrong.com/posts/kAiinLRMajB2zXNne/any-lesswrongers-going-to-emfcamp", "postedAtFormatted": "Friday, August 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Any%20LessWrongers%20going%20to%20EMFCamp%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAny%20LessWrongers%20going%20to%20EMFCamp%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkAiinLRMajB2zXNne%2Fany-lesswrongers-going-to-emfcamp%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Any%20LessWrongers%20going%20to%20EMFCamp%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkAiinLRMajB2zXNne%2Fany-lesswrongers-going-to-emfcamp", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkAiinLRMajB2zXNne%2Fany-lesswrongers-going-to-emfcamp", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<p><a href=\"http://emfcamp.org\">Electromagnetic Field</a> is a three-day hacker camp near Milton Keynes (UK), running from Friday 31st August to Sunday 2nd September. It seems like something that would interest a healthy fraction of the LW crowd; among other things, there are talks on biohacking, gene therapy and \"engineering tomorrow's healthcare, synthetic tissue generation\". So I figured I'd see if anyone else here is planning to go; and if so, are you interested in meeting?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kAiinLRMajB2zXNne", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 3, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "18322", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-17T22:41:57.843Z", "modifiedAt": null, "url": null, "title": "Proposed rewrites of LW home page, about page, and FAQ", "slug": "proposed-rewrites-of-lw-home-page-about-page-and-faq", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:08.504Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TD2bn6MNXmecW4YWY/proposed-rewrites-of-lw-home-page-about-page-and-faq", "pageUrlRelative": "/posts/TD2bn6MNXmecW4YWY/proposed-rewrites-of-lw-home-page-about-page-and-faq", "linkUrl": "https://www.lesswrong.com/posts/TD2bn6MNXmecW4YWY/proposed-rewrites-of-lw-home-page-about-page-and-faq", "postedAtFormatted": "Friday, August 17th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Proposed%20rewrites%20of%20LW%20home%20page%2C%20about%20page%2C%20and%20FAQ&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProposed%20rewrites%20of%20LW%20home%20page%2C%20about%20page%2C%20and%20FAQ%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTD2bn6MNXmecW4YWY%2Fproposed-rewrites-of-lw-home-page-about-page-and-faq%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Proposed%20rewrites%20of%20LW%20home%20page%2C%20about%20page%2C%20and%20FAQ%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTD2bn6MNXmecW4YWY%2Fproposed-rewrites-of-lw-home-page-about-page-and-faq", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTD2bn6MNXmecW4YWY%2Fproposed-rewrites-of-lw-home-page-about-page-and-faq", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 829, "htmlBody": "<p><strong>Proposed rewrites can be found <a href=\"http://wiki.lesswrong.com/wiki/User:John_Maxwell_IV\">here</a>. &nbsp;Please suggest specific improvements in the comments!</strong></p>\n<p>Although long-time Less Wrong users don't pay much attention to the home page, about page, and FAQ, I suspect new users pay lots of attention to them. &nbsp;A few times, elsewhere on the internet, I've seen people describe their impression of Less Wrong that seemed primarily gleaned from these pages--they made generalizations about Less Wrong that didn't seem true to me, but might <em>appear to be true</em> if all one did was read the about page and FAQ.</p>\n<p>The about page, in particular, is called out to every new visitor. &nbsp;Try visiting Less Wrong in incognito mode or private browsing (i.e. without your current cookies) to see&nbsp;<a href=\"http://i.imgur.com/AjkiS.png\">what I'm referring to</a>.</p>\n<p>But the current set of \"newcomer pages\" isn't very good, in my opinion:</p>\n<ul>\n<li>Text is duplicated between the home page and the about page. &nbsp;There's plenty to say and link to without repeating ourselves.</li>\n<li>The first paragraph of the home page text has four links to Wikipedia articles and none to Less Wrong posts. These may be very good Wikipedia articles, but I tend to think that linking to <em>actual Less Wrong posts</em> is generally a better way to communicate what kind of site Less Wrong is than linking to Wikipedia.</li>\n<li>The home page text also makes references to the blog, discussion section, and meetups, which are already highlighted plenty in the brain image.</li>\n<li>I think the primary purpose of the about page should be to describe and link to lots of interesting Less Wrong posts. &nbsp;I think <em>reading posts</em>&nbsp;is probably best way to figure out what Less Wrong is about. &nbsp;If the smorgasboard of posts linked to from the about page is sufficiently varied and high-quality, I think that most users will be able to find at least a couple posts they really like. &nbsp;Right now this purpose isn't given much real estate. &nbsp;There <em>is</em> a sentence starting with the words \"If you want a sampling of the content on the main blog...\", but this sentence does little to describe the posts it links to aside from providing a few related keywords.</li>\n<li>There's also a lot of instruction on the about page regarding how to do basic stuff like create posts. &nbsp;Facebook and Youtube don't seem to think it's necessary to provide instructions on how to do basic stuff, so I don't think we need it either. &nbsp;(Just in case, though, it's mostly still all there in my rewrite of the FAQ.)</li>\n<li>Some of the answers in the FAQ make us look very close-minded (when in fact we're only a little close-minded). See <a href=\"http://wiki.lesswrong.com/wiki/FAQ#Why_is_almost_everyone_here_an_atheist.3F\">Why is almost everyone here an atheist?</a>&nbsp;and <a href=\"http://wiki.lesswrong.com/wiki/FAQ#Why_do_you_all_agree_on_so_much.3F_Am_I_joining_a_cult.3F\">Why do you all agree on so much? &nbsp;Am I joining a cult?</a>. &nbsp;I think it's possible to answer these questions in a way that's less obnoxious and gives a more accurate impression of what LW is like:&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/User:John_Maxwell_IV/FAQ#Why_does_everyone_on_Less_Wrong_seem_to_be_an_atheist.3F\">1</a>, <a href=\"http://wiki.lesswrong.com/wiki/User:John_Maxwell_IV/FAQ#Why_do_Less_Wrong_users_seem_to_agree_on_so_much.3F\">2</a>.</li>\n</ul>\n<div>These pages also have the general appearance of being woefully under-optimized. &nbsp;For example, the home page links to an&nbsp;<a href=\"/lw/4wm/rationality_boot_camp/\">announcement</a>&nbsp;for an event that&nbsp;ended&nbsp;almost a year ago.</div>\n<div><strong>Hence my proposed rewrites</strong>:&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/User:John_Maxwell_IV/Homepage\">home page</a>, <a href=\"http://wiki.lesswrong.com/wiki/User:John_Maxwell_IV/Aboutpage\">about page</a>, and <a href=\"http://wiki.lesswrong.com/wiki/User:John_Maxwell_IV/FAQ\">FAQ</a>. &nbsp;A few notes:</div>\n<div>\n<ul>\n<li>I tried to link to various posts that are explicitly targeted at newcomers, like \"<a href=\"/lw/34m/what_ive_learned_from_less_wrong/\">What I've Learned from Less Wrong</a>\" and \"<a href=\"/lw/1to/what_is_bayesianism/\">What is Bayesianism?</a>\", but weren't being shown on the existing&nbsp;newcomer pages.</li>\n<li>I put a lot more stuff in the FAQ, on the theory that a long FAQ doesn't hurt much since folks can just read the answers to the questions that interest them.</li>\n<li>I deliberately avoided looking at the existing pages at first when writing my alternatives, to avoid contamination. My thinking was that being different for its own sake was good if we could reliably figure out which version was better in each case (e.g. overcome <a href=\"http://en.wikipedia.org/wiki/Status_quo_bias\">status quo bias</a>). &nbsp;Please comment on nitty-gritty differences between the two versions, e.g. if you think I left an important sentence from the originals out or if one of the posts I linked to seems rather weak.</li>\n</ul>\n</div>\n<p>I certainly don't claim to speak for all Less Wrong users. &nbsp;If you have any thoughts, please comment here, <a href=\"/message/compose/?to=John_Maxwell_IV\">send me a private message</a>, or log in to the wiki and edit <a href=\"http://wiki.lesswrong.com/mediawiki/index.php?title=User:John_Maxwell_IV/Homepage&amp;action=edit\">the</a> <a href=\"http://wiki.lesswrong.com/mediawiki/index.php?title=User:John_Maxwell_IV/Aboutpage&amp;action=edit\">candidate</a> <a href=\"http://wiki.lesswrong.com/wiki/User:John_Maxwell_IV/FAQ\">pages</a> directly.</p>\n<p>I'm especially interested in getting feedback on the FAQ, because I took the liberty of codifying some social norms that were previously implicit: see the section <a href=\"http://wiki.lesswrong.com/wiki/User:John_Maxwell_IV/FAQ#Site_Etiquette_and_Social_Norms\">Site Etiquette and Social Norms</a>, especially the bits about <a href=\"http://wiki.lesswrong.com/wiki/User:John_Maxwell_IV/FAQ#When_should_I_post_in_Discussion.2C_and_when_should_I_post_in_Main.3F\">Discussion vs Main</a>, <a href=\"http://wiki.lesswrong.com/wiki/User:John_Maxwell_IV/FAQ#Is_it_OK_to_talk_about_politics.3F\">politics</a>, and \"<a href=\"http://wiki.lesswrong.com/wiki/User:John_Maxwell_IV/FAQ#Why_did_I_get_voted_down.3F\">if you never get voted down, you're not posting enough</a>\".</p>\n<p>If you think I codified the social norms incorrectly, or you've been thinking they really should be different, please comment! &nbsp;The FAQ seems like a good way to broadcast preferred norms, so I suspect this is an ideal thread to discuss them.</p>\n<p>If you've got a suggested change that's nontrivial, I encourage you to create a poll for it here using comments as poll options or <a href=\"http://www.makefoil.com/poll.php\">HonoreDB's system</a>.</p>\n<p>(<a href=\"/lw/dwb/open_thread_august_115_2012/75hg\">Previous discussion</a>.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TD2bn6MNXmecW4YWY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 24, "extendedScore": null, "score": 9.665129381082067e-07, "legacy": true, "legacyId": "18323", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["s887k4Hcqj28cchYo", "qGEqpy7J78bZh3awf", "AN2cBr6xKWCB8dRQG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-18T00:20:07.779Z", "modifiedAt": null, "url": null, "title": "Enjoy solving \"impossible\" problems? Group project!", "slug": "enjoy-solving-impossible-problems-group-project", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:36.236Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Epiphany", "createdAt": "2012-08-12T03:33:21.256Z", "isAdmin": false, "displayName": "Epiphany"}, "userId": "BbbFp6hQzKF4YX8em", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HWsuGLyTJr5iHBWLy/enjoy-solving-impossible-problems-group-project", "pageUrlRelative": "/posts/HWsuGLyTJr5iHBWLy/enjoy-solving-impossible-problems-group-project", "linkUrl": "https://www.lesswrong.com/posts/HWsuGLyTJr5iHBWLy/enjoy-solving-impossible-problems-group-project", "postedAtFormatted": "Saturday, August 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Enjoy%20solving%20%22impossible%22%20problems%3F%20Group%20project!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEnjoy%20solving%20%22impossible%22%20problems%3F%20Group%20project!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHWsuGLyTJr5iHBWLy%2Fenjoy-solving-impossible-problems-group-project%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Enjoy%20solving%20%22impossible%22%20problems%3F%20Group%20project!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHWsuGLyTJr5iHBWLy%2Fenjoy-solving-impossible-problems-group-project", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHWsuGLyTJr5iHBWLy%2Fenjoy-solving-impossible-problems-group-project", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 896, "htmlBody": "<p>In the <a href=\"/lw/di6/muehlhauserhibbard_dialogue_on_agi/\">Muehlhauser-Hibbard Dialogue on AGI</a>, Hibbard states it will be \"impossible to decelerate AI capabilities\" but Luke counters with \"Persuade key AGI researchers of the importance of safety ... If we can change the minds of a few key AGI scientists, it may be that key insights into AGI are delayed by years or decades.\" and before I read that dialogue, I had come up with <a href=\"/r/discussion/lw/e7a/heading_off_a_nearterm_agi_arms_race/798f\">three additional ideas</a> on <a href=\"/lw/e7a/heading_off_a_nearterm_agi_arms_race/\">Heading off a near-term AGI arms race.</a> Bill Hibbard may be right that \"any effort expended on that goal could be better applied to the political and technical problems of AI safety\" but I doubt he's right that it's impossible.</p>\n<p>How do you prove something is impossible?&nbsp; You might prove that a specific METHOD of getting to the goal does not work, but that doesn't mean there's not another method.&nbsp; You might prove that all the methods you know about do not work.&nbsp; That doesn't prove there's not some other option you don't see.&nbsp; \"I don't see an option, therefore it's impossible.\" is only an appeal to ignorance.&nbsp; It's a common one but it's incorrect reasoning regardless.&nbsp; Think about it.&nbsp; Can you think of a way to prove that a method that does work isn't out there waiting to be discovered without saying the equivalent of \"I don't see any evidence for this.\" We can say \"I don't see it, I don't see it, I don't see it!\" all day long.&nbsp;</p>\n<p>I say: \"Then Look!\"</p>\n<p>How often do we push past this feeling to keep thinking of ideas that might work?&nbsp; For many, the answer is \"never\" or \"only if it's needed\".&nbsp; The sense that something is impossible is subjective and fallible.&nbsp; If we don't have a way of proving something is impossible, but yet believe it to be impossible anyway, this is a belief.&nbsp; What distinguishes this from bias?&nbsp;</p>\n<p>I think it's a common fear that you may waste your entire life on doing something that is, in fact, impossible.&nbsp; This is valid, but it's completely missing the obvious:&nbsp; As soon as you think of a plan to do the impossible, you'll be able to guess whether it will work.&nbsp; The hard part is THINKING of a plan to do the impossible.&nbsp; I'm suggesting that if we put our heads together, we can think of a plan to make an impossible thing into a possible one.&nbsp; Not only that, I think we're capable of doing this on a worthwhile topic.&nbsp; An idea that's not only going to benefit humanity, but is a good enough idea that the amount of time and effort and risk required to accomplish the task is worth it.</p>\n<p>Here's how I am going to proceed:&nbsp;</p>\n<p>Step 1: Come up with a bunch of impossible project ideas.&nbsp;</p>\n<p>Step 2: Figure out which one appeals to the most people.&nbsp;</p>\n<p>Step 3: Invent the methodology by which we are going to accomplish said project.&nbsp;</p>\n<p>Step 4: Improve the method as needed until we're convinced it's likely to work.</p>\n<p>Step 5: Get the project done.</p>\n<p>&nbsp;</p>\n<p><strong>Impossible Project Ideas</strong></p>\n<ul>\n<li>Decelerate AI Capabilities Research: If we develop AI before we've figured out the political and technical safety measures, we could have a disaster.&nbsp; <a href=\"/lw/di6/muehlhauserhibbard_dialogue_on_agi/\">Luke's Ideas</a> (Starts with \"Persuade key AGI researchers of the importance of safety\").&nbsp; <a href=\"/r/discussion/lw/e7a/heading_off_a_nearterm_agi_arms_race/798f\">My ideas.</a></li>\n<li><a href=\"/r/discussion/lw/e50/enjoy_challenging_yourself_to_do_the_impossible/78ao\">Solve Violent Crime:</a> Testosterone may be the root cause of the vast majority of violent crime, but there are obstacles in treating it.&nbsp; </li>\n<li><a href=\"/r/discussion/lw/e50/lets_do_the_impossible_group_project/783w\">Syntax/static Analysis Checker for Laws</a>: Automatically look for conflicting/inconsistent definitions, logical conflicts, and other possible problems or ambiguities.&nbsp; </li>\n</ul>\n<ul>\n<li>\n<p><a href=\"/r/discussion/lw/e50/lets_do_the_impossible_group_project/783s\">Understand the psychology of money</a></p>\n</li>\n<li><a href=\"/r/discussion/lw/e50/lets_do_the_impossible_group_project/782b\">Rational Agreement Software</a>:&nbsp; If rationalists should ideally always agree, why not make an organized information resource designed to get us all to agree?&nbsp; This would track the arguments for and against ideas in such a way where each piece can be verified logically and challenged, make the entire collection of arguments available in an organized manner where none are repeated and no useless information is included, and it would need to be such that anybody can edit it like a wiki, resulting in the most rational outcome being displayed prominently at the top.&nbsp; This is especially hard because it would be our responsibility to make something SO good, it convinces one another to agree, and it would have to be structured well enough that we actually manage to <a href=\"/r/discussion/lw/e50/lets_do_the_impossible_group_project/784d\">distinguish between opinions and facts</a>. Also, Gwern mentions in <a href=\"/lw/dhe/to_learn_critical_thinking_study_critical_thinking/\">a post about critical thinking</a> that argument maps increase critical thinking skills.</li>\n<li><a href=\"/r/discussion/lw/e50/lets_do_the_impossible_group_project/782c\">Discover unrecognized bias</a>:&nbsp; This is especially hard since we'll be using our biased brains to try and detect it.&nbsp; We'd have to hack our own way of imagining around the corners, peeking behind our own minds.</li>\n</ul>\n<ul>\n<li><a href=\"/r/discussion/lw/e50/lets_do_the_impossible_group_project/782e\">Logic checking AI</a>: Build an AI that checks your logic for logical fallacies and other methods of poor reasoning. </li>\n</ul>\n<p>Add your own ideas below (one idea per comment, so we can vote them up and down), make sure to describe your vision, then I'll list them here.</p>\n<p>&nbsp;</p>\n<p><strong>Figure out which one appeals to the most people.</strong></p>\n<p>Assuming each idea is put into a separate comment, we can vote them up or down.&nbsp; If they begin with the word \"Idea\" I'll be able to find them and put them on the list.&nbsp; If your idea is getting enough attention obviously, it will at some point make sense to create a new discussion for it.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HWsuGLyTJr5iHBWLy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -2, "extendedScore": null, "score": 9.665614909881495e-07, "legacy": true, "legacyId": "18324", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>In the <a href=\"/lw/di6/muehlhauserhibbard_dialogue_on_agi/\">Muehlhauser-Hibbard Dialogue on AGI</a>, Hibbard states it will be \"impossible to decelerate AI capabilities\" but Luke counters with \"Persuade key AGI researchers of the importance of safety ... If we can change the minds of a few key AGI scientists, it may be that key insights into AGI are delayed by years or decades.\" and before I read that dialogue, I had come up with <a href=\"/r/discussion/lw/e7a/heading_off_a_nearterm_agi_arms_race/798f\">three additional ideas</a> on <a href=\"/lw/e7a/heading_off_a_nearterm_agi_arms_race/\">Heading off a near-term AGI arms race.</a> Bill Hibbard may be right that \"any effort expended on that goal could be better applied to the political and technical problems of AI safety\" but I doubt he's right that it's impossible.</p>\n<p>How do you prove something is impossible?&nbsp; You might prove that a specific METHOD of getting to the goal does not work, but that doesn't mean there's not another method.&nbsp; You might prove that all the methods you know about do not work.&nbsp; That doesn't prove there's not some other option you don't see.&nbsp; \"I don't see an option, therefore it's impossible.\" is only an appeal to ignorance.&nbsp; It's a common one but it's incorrect reasoning regardless.&nbsp; Think about it.&nbsp; Can you think of a way to prove that a method that does work isn't out there waiting to be discovered without saying the equivalent of \"I don't see any evidence for this.\" We can say \"I don't see it, I don't see it, I don't see it!\" all day long.&nbsp;</p>\n<p>I say: \"Then Look!\"</p>\n<p>How often do we push past this feeling to keep thinking of ideas that might work?&nbsp; For many, the answer is \"never\" or \"only if it's needed\".&nbsp; The sense that something is impossible is subjective and fallible.&nbsp; If we don't have a way of proving something is impossible, but yet believe it to be impossible anyway, this is a belief.&nbsp; What distinguishes this from bias?&nbsp;</p>\n<p>I think it's a common fear that you may waste your entire life on doing something that is, in fact, impossible.&nbsp; This is valid, but it's completely missing the obvious:&nbsp; As soon as you think of a plan to do the impossible, you'll be able to guess whether it will work.&nbsp; The hard part is THINKING of a plan to do the impossible.&nbsp; I'm suggesting that if we put our heads together, we can think of a plan to make an impossible thing into a possible one.&nbsp; Not only that, I think we're capable of doing this on a worthwhile topic.&nbsp; An idea that's not only going to benefit humanity, but is a good enough idea that the amount of time and effort and risk required to accomplish the task is worth it.</p>\n<p>Here's how I am going to proceed:&nbsp;</p>\n<p>Step 1: Come up with a bunch of impossible project ideas.&nbsp;</p>\n<p>Step 2: Figure out which one appeals to the most people.&nbsp;</p>\n<p>Step 3: Invent the methodology by which we are going to accomplish said project.&nbsp;</p>\n<p>Step 4: Improve the method as needed until we're convinced it's likely to work.</p>\n<p>Step 5: Get the project done.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Impossible_Project_Ideas\">Impossible Project Ideas</strong></p>\n<ul>\n<li>Decelerate AI Capabilities Research: If we develop AI before we've figured out the political and technical safety measures, we could have a disaster.&nbsp; <a href=\"/lw/di6/muehlhauserhibbard_dialogue_on_agi/\">Luke's Ideas</a> (Starts with \"Persuade key AGI researchers of the importance of safety\").&nbsp; <a href=\"/r/discussion/lw/e7a/heading_off_a_nearterm_agi_arms_race/798f\">My ideas.</a></li>\n<li><a href=\"/r/discussion/lw/e50/enjoy_challenging_yourself_to_do_the_impossible/78ao\">Solve Violent Crime:</a> Testosterone may be the root cause of the vast majority of violent crime, but there are obstacles in treating it.&nbsp; </li>\n<li><a href=\"/r/discussion/lw/e50/lets_do_the_impossible_group_project/783w\">Syntax/static Analysis Checker for Laws</a>: Automatically look for conflicting/inconsistent definitions, logical conflicts, and other possible problems or ambiguities.&nbsp; </li>\n</ul>\n<ul>\n<li>\n<p><a href=\"/r/discussion/lw/e50/lets_do_the_impossible_group_project/783s\">Understand the psychology of money</a></p>\n</li>\n<li><a href=\"/r/discussion/lw/e50/lets_do_the_impossible_group_project/782b\">Rational Agreement Software</a>:&nbsp; If rationalists should ideally always agree, why not make an organized information resource designed to get us all to agree?&nbsp; This would track the arguments for and against ideas in such a way where each piece can be verified logically and challenged, make the entire collection of arguments available in an organized manner where none are repeated and no useless information is included, and it would need to be such that anybody can edit it like a wiki, resulting in the most rational outcome being displayed prominently at the top.&nbsp; This is especially hard because it would be our responsibility to make something SO good, it convinces one another to agree, and it would have to be structured well enough that we actually manage to <a href=\"/r/discussion/lw/e50/lets_do_the_impossible_group_project/784d\">distinguish between opinions and facts</a>. Also, Gwern mentions in <a href=\"/lw/dhe/to_learn_critical_thinking_study_critical_thinking/\">a post about critical thinking</a> that argument maps increase critical thinking skills.</li>\n<li><a href=\"/r/discussion/lw/e50/lets_do_the_impossible_group_project/782c\">Discover unrecognized bias</a>:&nbsp; This is especially hard since we'll be using our biased brains to try and detect it.&nbsp; We'd have to hack our own way of imagining around the corners, peeking behind our own minds.</li>\n</ul>\n<ul>\n<li><a href=\"/r/discussion/lw/e50/lets_do_the_impossible_group_project/782e\">Logic checking AI</a>: Build an AI that checks your logic for logical fallacies and other methods of poor reasoning. </li>\n</ul>\n<p>Add your own ideas below (one idea per comment, so we can vote them up and down), make sure to describe your vision, then I'll list them here.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Figure_out_which_one_appeals_to_the_most_people_\">Figure out which one appeals to the most people.</strong></p>\n<p>Assuming each idea is put into a separate comment, we can vote them up or down.&nbsp; If they begin with the word \"Idea\" I'll be able to find them and put them on the list.&nbsp; If your idea is getting enough attention obviously, it will at some point make sense to create a new discussion for it.</p>\n<p>&nbsp;</p>", "sections": [{"title": "Impossible Project Ideas", "anchor": "Impossible_Project_Ideas", "level": 1}, {"title": "Figure out which one appeals to the most people.", "anchor": "Figure_out_which_one_appeals_to_the_most_people_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "72 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cFP6iBezD4KCfXkpW", "ADyGe6rMMoCYMTxhg", "MGtKNd5GBXN5oNj7p"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-18T06:11:42.676Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Harder Choices Matter Less", "slug": "seq-rerun-harder-choices-matter-less", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:06.916Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5mCFeS6bYf2Ktkmsy/seq-rerun-harder-choices-matter-less", "pageUrlRelative": "/posts/5mCFeS6bYf2Ktkmsy/seq-rerun-harder-choices-matter-less", "linkUrl": "https://www.lesswrong.com/posts/5mCFeS6bYf2Ktkmsy/seq-rerun-harder-choices-matter-less", "postedAtFormatted": "Saturday, August 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Harder%20Choices%20Matter%20Less&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Harder%20Choices%20Matter%20Less%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5mCFeS6bYf2Ktkmsy%2Fseq-rerun-harder-choices-matter-less%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Harder%20Choices%20Matter%20Less%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5mCFeS6bYf2Ktkmsy%2Fseq-rerun-harder-choices-matter-less", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5mCFeS6bYf2Ktkmsy%2Fseq-rerun-harder-choices-matter-less", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<p>Today's post, <a href=\"/lw/th/harder_choices_matter_less/\">Harder Choices Matter Less</a> was originally published on 29 August 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Harder_Choices_Matter_Less\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If a choice is hard, that means the alternatives are around equally balanced, right?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/e3w/seq_rerun_against_modal_logics/\">Against Modal Logics</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5mCFeS6bYf2Ktkmsy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 9.667354191699835e-07, "legacy": true, "legacyId": "18336", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["85LY7zQhTkWo4PmRc", "chnwsFduQQ5hA7CNq", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-18T12:08:32.126Z", "modifiedAt": null, "url": null, "title": "An angle of attack on Open Problem #1", "slug": "an-angle-of-attack-on-open-problem-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:08.032Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benja", "createdAt": "2009-02-27T04:37:47.476Z", "isAdmin": false, "displayName": "Benya"}, "userId": "3vZZP8TBXvozbe5Cv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DDJr5fuR5jeD47k9g/an-angle-of-attack-on-open-problem-1", "pageUrlRelative": "/posts/DDJr5fuR5jeD47k9g/an-angle-of-attack-on-open-problem-1", "linkUrl": "https://www.lesswrong.com/posts/DDJr5fuR5jeD47k9g/an-angle-of-attack-on-open-problem-1", "postedAtFormatted": "Saturday, August 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20angle%20of%20attack%20on%20Open%20Problem%20%231&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20angle%20of%20attack%20on%20Open%20Problem%20%231%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDDJr5fuR5jeD47k9g%2Fan-angle-of-attack-on-open-problem-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20angle%20of%20attack%20on%20Open%20Problem%20%231%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDDJr5fuR5jeD47k9g%2Fan-angle-of-attack-on-open-problem-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDDJr5fuR5jeD47k9g%2Fan-angle-of-attack-on-open-problem-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2082, "htmlBody": "<p><del><strong>There is <a href=\"/lw/e4e/an_angle_of_attack_on_open_problem_1/7858\">a problem with the proof here</a> and I have to think about whether I can fix it.</strong> Thanks to <span class=\"comment-author\"><span class=\"author\">vi21maobk9vp for <a href=\"/lw/e4e/an_angle_of_attack_on_open_problem_1/7854\">pointing me in the right direction</a>!</span></span></del><em> <strong>I have posted <a href=\"/r/discussion/lw/e5j/how_to_cheat_l%C3%B6bs_theorem_my_second_try/\">a new and hopefully correct proof attempt</a>.</strong> Thanks again to vi21maobk9vp!<br /></em></p>\n<p><em>In his talk on <a href=\"https://www.youtube.com/watch?v=MwriJqBZyoM\">open problems in Friendly AI</a></em>,<em> Eliezer's first question is how, given L&ouml;b's theorem, an AI can replace itself with a better expected utility maximizer that believes in as much mathematics as the original AI. I know exactly one trick for that sort of problem, so I decided to try that on a toy variant. To my surprise, it more or less just worked. Therefore:</em></p>\n<p>Professor Quirrell proposes a game. You start with a score of one. Professor Quirrell moves first, by choosing a computer program and showing you its source code. You then have three options: Take your winnings; double down; or self-destruct.</p>\n<p>If you take your winnings, the game ends, and your score is converted to Quirrell points.</p>\n<p>If you self-destruct, the game ends, your score is lost, you'll be sent to bed without dinner, you'll lose 150 House points, Rita Skeeter will write a feature alleging that you're a Death Eater, and Professor Quirrell will publicly critique your performance. You are advised not to pick this option.</p>\n<p>If you double down, your score doubles, and you advance to the next round. Professor Quirrell again moves first by choosing a computer program. Then, it's your turn&mdash;except that this time, you don't get to choose your move yourself: instead, it'll be chosen by Professor Quirrell's program from the previous round.</p>\n<p>Professor Quirrell will endeavor to present an <em>educational</em> sequence of programs.</p>\n<p><a id=\"more\"></a></p>\n<p align=\"center\">*</p>\n<p>The idea is that Quirrell will make you self-destruct if he possibly can, so you must only accept programs that don't self-destruct, that accept only programs that don't self-destruct, that accept only programs that only accept&mdash;etc. That's supposed to capture one essential aspect of Eliezer's problem, namely how to make sure that a proposed rewrite doesn't destroy your values, while ignoring the complications due to a different aspect, namely comparing the expected values before and after the rewrite. In Quirrell's game, there are safe and unsafe rewrites, and you should always double down on a safe one and take your winnings when presented with an unsafe one. Let's look at some interesting programs that we could recognize as safe. [<del>And to deal with the possibility of an infinite sequence of double-downs, let's stipulate a small but positive chance each round that Quirrell will end the game and pay you even if your program chose to double down.</del> <strong>ETA:</strong> Luke A. Somers <a href=\"/lw/e4e/an_angle_of_attack_on_open_problem_1/78nk\">points out that this provision isn't necessary</a>.]</p>\n<p>Let PA(0) := Peano Arithmetic, and PA(n+1) := PA(n) + for all formulas 'C': \"if PA(n) proves 'C', then C\". Define AI(n) by</p>\n<pre>def AI(n)(p):\n&nbsp;&nbsp;&nbsp;&nbsp;Look at the first 3^^^3 theorems of PA(n).\n&nbsp;&nbsp;&nbsp;&nbsp;if (one of them says \"p is safe\"):\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;double down\n&nbsp;&nbsp;&nbsp;&nbsp;else:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;take winnings</pre>\n<p>If the current round is controlled by AI(7) and Quirrell hands it AI(6) as input, then AI(7) will double down: PA(7) will conclude that AI(6) will only double down on programs that PA(6) proves to be safe, which implies that they <em>are</em> safe. But if Quirrell <em>hands AI(7) to itself</em>, this reasoning doesn't go through, because PA(7) cannot use \"PA(7) proves p is safe\" to conclude \"p is safe\". So even if Quirrell is nice enough to choose AI(n)'s in a decreasing sequence, our windfall will end after a constant number rounds, because (<strong>*sinister drumroll*</strong>) AI(0) has run out of math to believe in, so to speak. That's precisely the problem Eliezer explains in his talk.</p>\n<p>By quining, we could write an AI<sub>Q</sub> which will recognize itself as safe, so if Quirrell chooses AI<sub>Q</sub> over and over again, we'll be doubling down for longer and longer times. But that insight won't help us with writing interesting self-modifying AIs. [<strong>ETA:</strong> Wei Dai has managed to <a href=\"/r/discussion/lw/e4e/an_angle_of_attack_on_open_problem_1/78jo\">push the limits of using quining into interesting territory</a>.] Is there something that can use non-specialized reasoning, like the AI(n), to recognize an indefinitely long sequence of variants of itself as safe?</p>\n<p align=\"center\">*</p>\n<p>Define \"p is safe for n steps\" (p a program) to mean that there is no m, 0 &lt; m &lt;= n, and sequence of programs p=p<sub>0</sub>, p<sub>1</sub>, ..., p<sub>m</sub> such that (a) for 0 &lt; i &lt; m, p<sub>i-1</sub> will double down when fed p<sub>i</sub>, and (b) p<sub>m-1</sub> will <em><strong>self-destruct</strong></em> when fed p<sub>m</sub>. Clearly this can be formalized in the language of Peano Arithmetic.</p>\n<p>Now let's extend the language of PA by a constant symbol K, and define PA<sub>K</sub> := Peano Arithmetic (actually, the natural extension of PA to this new language) + for all formulas 'C' of the base language: \"if PA(K) proves 'C', then C\". Finally, define AI<sub>K</sub> by</p>\n<pre>def AI<sub>K</sub>(p):\n&nbsp;&nbsp;&nbsp;&nbsp;Look at the first 3^^^3 theorems of PA<sub>K</sub>.\n&nbsp;&nbsp;&nbsp;&nbsp;if (one of them says \"p is safe for K steps\"):\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;double down\n&nbsp;&nbsp;&nbsp;&nbsp;else:\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;take winnings</pre>\n<p>I claim that AI<sub>K</sub> is safe, and furthermore, AI<sub>K</sub>(AI<sub>K</sub>) will double down, and so will AI<sub>K</sub>(p) where p is some trivial modification of AI<sub>K</sub> like changing 3^^^3 to 4^^^^4.</p>\n<p align=\"center\">*</p>\n<p>Fair warning: now comes the technical section of this post.</p>\n<p><em>Proof sketch. </em>Let inst(n,x) be the meta-level function that takes a sentence or proof x in the extended language and substitutes the numeral n (i.e., the unary number in the language of PA that encodes n) for all occurrences of the constant symbol K. Note that if x is a valid proof in PA<sub>K</sub>, then inst(n,x) will be a valid proof in PA(n+1). Of course, what statement it is a proof <em>of</em> will depend on n. In particular, if x proves \"p is safe for K steps\", then inst(n,x) is a PA(n+1) proof of \"p is safe for n steps\". Since this is argument works for all n, p will in fact be safe [and we can formalize this in PA<span class=\"st\"><sub>&omega;</sub>, if we want to</span>]. This shows that AI<sub>K</sub> is safe, as claimed.</p>\n<p>Formalizing the above argument in Peano Arithmetic, and writing inst<sub>PA</sub>(n,x) for the object-level encoding of the meta-level function, we can prove: \"For all extended-language sentences 'C', if PA<sub>K</sub> proves 'C', then for all n, PA(n+1) proves inst<sub>PA</sub>(n,'C').\" By substituting K-1 for n, it follows that for all exteded-language sentences 'C', PA<sub>K</sub> proves \"If PA<sub>K</sub> proves 'C', and K&gt;0, then PA(K) proves inst<sub>PA</sub>(K-1,'C')\". Now, for 'D' := inst(K-1,'C'), PA can prove that \"inst<sub>PA</sub>(K-1,'C') = 'D'\"; thus, by virtue of its extra axioms, PA<sub>K</sub> concludes: \"If PA<sub>K</sub> proves 'C', and K&gt;0, then inst(K-1,C)\".</p>\n<p>What we must show is that PA<sub>K</sub> proves \"AI<sub>K</sub> is safe for K steps\" through some argument that doesn't actually involve simulating AI<sub>K</sub>(AI<sub>K</sub>) (because in that case, we wouldn't expect to find the proof among the first 3^^^3 theorems). First, note that PA already (\"quickly\") proves \"If AI<sub>K</sub>(p) doubles down, then PA<sub>K</sub> proves 'p is safe for K steps'\". In PA<sub>K</sub>, it follows that \"If AI<sub>K</sub>(p) doubles down, and K&gt;0, then inst(K-1, 'p is safe for K steps')\". But this is just a meta-level expression for the sentence \"If AI<sub>K</sub>(p) doubles down, and K&gt;0, then p is safe for K-1 steps\"&mdash;and at that point, the reflective magic is over, and we only need to establish by \"ordinary\" PA reasoning that if AI<sub>K</sub> will only accept programs that are safe for K-1 steps, then AI<sub>K</sub> is safe for K steps.</p>\n<p align=\"right\"><span class=\"Unicode\">Q.E.D.</span></p>\n<p>Note that this argument does not depend on the number of theorems AI<sub>K</sub> looks at, because it only depends on the fact that if AI<sub>K</sub>(p) <em>does</em> double down, then there is <em>some</em> PA<sub>K</sub> proof of p's (K-1)-step safety.</p>\n<p align=\"center\">*</p>\n<p>The real question is whether this is just a hack or can tell us something about how to approach a solution to the real thing. It could very well be that this is one of the cheap tricks Eliezer and Marcello tried that don't solve the core problem (as Eliezer explains in his talk). Certainly the proof seems to be more tangled with the rounds structure of Quirrell's game than I find elegant. Also, I'm not at all sure that the key proof idea noted in the previous paragraph still works when we go from Quirrell's game to expected utility maximization.</p>\n<p>However, as I said at the beginning of this post, I know exactly one trick for dealing with problems of this sort&mdash;by which I mean, trying to do something that seems impossible due to a diagonalization proof. It's well-appreciated that you can usually avoid diagonalization by passing from a single collection of things to a hierarchy of things (we can avoid Cantor by concluding that there are multiple infinite cardinalities; G&ouml;del and L&ouml;b, by the PA(n) hierarchy; Turing, by a hierarchy of halting oracles; Tarski, by a hierarchy of truth predicates; and so on). It's less well appreciated, but I think true (though fuzzy), that many fields manage to circumvent the effects of diagonalization a bit further by considering objects that in some sense live on multiple levels of the hierarchy at the same time. I'd call that \"the parametric polymorphism trick\", perhaps.</p>\n<p>In this post, we met PA<sub>K</sub>, which can be interpreted as PA(n), for any n. I haven't tried it in detail, but something very similar should be possible for Tarski's truth predicates. A sufficiently powerful total programming language cannot have a self-interpreter, but you should be able to have a constant symbol K, a single interpreter code file, and a hierarchy of semantics such that according to the K=n+1 semantics, the interpreter implements the K=n semantics. In Church's simple theory of types, you can't apply a function to itself, but in polymorphic variants, you can instantiate (id : <span class=\"st\">&alpha; -&gt; </span><span class=\"st\">&alpha;</span>) to (id : (<span class=\"st\">&alpha; -&gt; </span><span class=\"st\">&alpha;) -&gt; (</span><span class=\"st\">&alpha; -&gt; </span><span class=\"st\">&alpha;)), so that id(id) makes sense.</span> Set-theoretic <span class=\"st\">models of the untyped lambda calculus need to deal with the fact that if a set S is isomorphic to the function space (S -&gt; T), then S is either empty or has only one element; the usual solution would take me a bit more than one sentence to explain, but it's always struck me as related to what happens in the polymorphic type theories. Looking a bit farther afield, if you're a set theory platonist, if </span><span class=\"st\">&alpha;&lt;</span><span class=\"st\">&beta;&lt;</span><span class=\"st\">&gamma; and if the von Neumann levels <strong>V</strong></span><sub><span class=\"st\">&alpha;</span></sub><span class=\"st\">, </span><span class=\"st\"><strong>V</strong></span><sub><span class=\"st\">&beta;</span></sub><span class=\"st\"> and&nbsp;</span><span class=\"st\"><strong>V</strong></span><sub><span class=\"st\">&gamma;</span></sub><span class=\"st\"> are all models of ZFC, then the ZFC proof that \"if there is a set model of ZFC, then ZFC is consistent\" can be interpreted in </span><span class=\"st\"><strong>V</strong></span><sub><span class=\"st\">&beta;</span></sub><span class=\"st\">, where it applies to </span><span class=\"st\"><strong>V</strong></span><sub><span class=\"st\">&alpha;</span></sub><span class=\"st\">, and it can also be interpreted in </span><span class=\"st\"><strong>V</strong></span><sub><span class=\"st\">&gamma;</span></sub><span class=\"st\">,</span> where it applies to both <span class=\"st\"><strong>V</strong></span><sub><span class=\"st\">&alpha;</span></sub><span class=\"st\"> and </span><span class=\"st\"><strong>V</strong></span><sub><span class=\"st\">&beta;</span></sub><span class=\"st\">. And so on. It may be that there's a good solution to the AI rewrite problem and it has nothing to do with this type of trick at all, but it seems at least worthwhile to look in that direction.<br /></span></p>\n<p>[Actually, come to think of it, I <em>do</em> know a second trick for circumventing diagonalization, exemplified by passing from total to partial recursive functions and from two- to three-valued logics, but in logic, that one usually makes the resulting systems too weak to be interesting.]</p>\n<p align=\"center\">*</p>\n<p>Three more points in closing. First, sorry for the informality of the proof sketch! It would be very much appreciated if people would go through it and point out unclear things/problems/corrections. Also, I'm hoping to make a post at some point that gives a more intuitive explanation for <em>why</em> this works.</p>\n<p>[<strong>ETA:</strong><span class=\"comment-author\"><span class=\"author\"> vi21maobk9vp <a href=\"/lw/e4e/an_angle_of_attack_on_open_problem_1/78el\">points out</a></span> that the following note may lead to unhelpful confusion; perhaps best skip this or read our discussion in the thread above that comment.</span>] Second, provability in PA<sub>K</sub> in some sense says that a sentence is provable in PA(n), for all n. In particular, PA<sub>K</sub> is conservative over PA(1), since if PA<sub>K</sub> proves a sentence C in the base language, then PA(1) proves inst(1,C), which is just C; in some sense, this makes PA<sub>K</sub> rather weak. If we don't want this, we could make a variant where provability implies says there is some m such that the sentence is provable in PA(n) for all n&gt;m. To do this, we'd use a trick usually employed to show that there are non-standard models of the natural numbers: Add to PA<sub>K</sub> the axioms \"K&gt;1\", \"K&gt;2\", \"K&gt;3\", and so on. This is consistent by the <a href=\"http://en.wikipedia.org/wiki/Compactness_theorem\">Compactness Theorem</a>, because any concrete proof can only use a finite number of these axioms. But in this extended system, we can prove anything that we can prove in any PA(n).</p>\n<p>Third, I chose the particular definition of PA<sub>K</sub> because it made my particular proof simpler to write. Looking only at the definitions, I would find it more natural to make PA<sub>K</sub> conservative over PA(0) by using \"if K&gt;0 and PA(K-1) proves 'C', then C\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "wBoHTJs9iQzczNtW3": 1, "Pa2SdZsLFmqhs42Do": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DDJr5fuR5jeD47k9g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 47, "extendedScore": null, "score": 9.66911997612015e-07, "legacy": true, "legacyId": "18302", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 85, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EqCxSMZoZPmdARCTm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-18T15:17:01.302Z", "modifiedAt": null, "url": null, "title": "[Link] Reddit, help me find some peace I'm dying young", "slug": "link-reddit-help-me-find-some-peace-i-m-dying-young", "viewCount": null, "lastCommentedAt": "2013-01-03T03:05:04.757Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fLaKKRZckYtrrcz7m/link-reddit-help-me-find-some-peace-i-m-dying-young", "pageUrlRelative": "/posts/fLaKKRZckYtrrcz7m/link-reddit-help-me-find-some-peace-i-m-dying-young", "linkUrl": "https://www.lesswrong.com/posts/fLaKKRZckYtrrcz7m/link-reddit-help-me-find-some-peace-i-m-dying-young", "postedAtFormatted": "Saturday, August 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Reddit%2C%20help%20me%20find%20some%20peace%20I'm%20dying%20young&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Reddit%2C%20help%20me%20find%20some%20peace%20I'm%20dying%20young%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfLaKKRZckYtrrcz7m%2Flink-reddit-help-me-find-some-peace-i-m-dying-young%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Reddit%2C%20help%20me%20find%20some%20peace%20I'm%20dying%20young%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfLaKKRZckYtrrcz7m%2Flink-reddit-help-me-find-some-peace-i-m-dying-young", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfLaKKRZckYtrrcz7m%2Flink-reddit-help-me-find-some-peace-i-m-dying-young", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1496, "htmlBody": "<p>Saw this on reddit.</p>\n<p><a href=\"http://www.reddit.com/r/atheism/comments/ydsy5/reddit_help_me_find_some_peace_in_dying_young_im/\">http://www.reddit.com/r/atheism/comments/ydsy5/reddit_help_me_find_some_peace_in_dying_young_im/</a></p>\n<div class=\"usertext-body\">\n<div class=\"md\">\n<blockquote>\n<p>Hey Reddit,</p>\n<p>I'm a 23 year old girl with recurrent Glioblastoma multiforme (GBM), a highly aggressive type of brain cancer. <a href=\"http://www.reddit.com/r/AskReddit/comments/uvaqe/today_is_my_23rd_birthday_and_probably_my_last/\">I posted a couple of months ago asking for suggestions for things I should try before I die</a> (life expectancy is 3-6 months) and got a lot of great ideas (many of which I've fulfilled).</p>\n<p>At the time of my last post, my treatment was undecided. I ended up participating in a phase I trial at Dana-Farber, but I progressed after two months of treatment. There are not many great treatment options left for me, but my next move will be five radiosurgery treatments at Duke University next week. My prognosis looks pretty bleak at this point, and though I am hoping to exceed the 6-10 month median survival, I have to prepare to die. <strong>In a way, I am fortunate because the lesion is primarily in my brain stem (controls things like breathing), so I will likely die before the tumor spreads to the areas central to who I am.</strong></p>\n<p><strong><br /></strong></p>\n<p><strong>I'm back on Reddit again, mostly to ask for help because I want to be cryogenically preserved upon my death. I've been interested in cryonics since long before I was even diagnosed, but I never thought that I would have to secure the finances so fast, and without a career or savings to stand on.</strong> As weird as it feels to ask for help here, I feel I should just give it a shot and sees what happens.</p>\n<p><strong>I caused a lot of family controversy last week by breaking the news to my parents. I can tell I've alienated them quite a bit as they are Christian and don't see why I'd want to be preserved; in their mind, I am going to heaven and my \"soul\" will forever leave my body when I die anyway. </strong>I clearly upset both of them with the implication that I was agnostic (I didn't say this outright, but it's true). My mom is fairly supportive of my plans to be preserved, but unfortunately, my dad isn't a fan of the idea, and he's really the only family I have that could offer financial help (my parents are divorced and not on good terms). The company I'm looking into, Cryonics Institute, costs $30,000-35,000 with transportation to the facility accounted for. <strong>My boyfriend is fully supportive, but like me, he's broke and barely out of college.</strong></p>\n<p>I know this is a big thing to ask for, and I'm sure many people are doubtful that preservation is plausible with cryonics. I'm far from convinced, but I would rather take the chance with preservation than rot in the ground or get cremated. The company I'm looking into, Cryonics Institute, has a good intro on their FAQ page that offers a hopeful outlook on future technology: <a href=\"http://cryonics.org/prod.html\">http://cryonics.org/prod.html</a></p>\n<p><strong>A lot of people on reddit wanted to start a fundraiser for me awhile ago to aid in doing fun things before I die. I am hoping that redditors will still have some interest in helping me even if it's not going towards vacation or skydiving and shit like that. Cryopreservation is sincerely what will bring me the most peace in death.</strong></p>\n<p>I wish I could give a particularly compelling reason why I deserve another chance at life, but there's not much to say. I'm still just a kid, and hadn't even finished college when I was diagnosed. Unfortunately the most interesting thing I have yet to do is get a terminal disease at a young age.</p>\n<p>If you guys can help me out, I would be grateful to a degree I can't possibly describe. I'm desperate. <strong>If you care to donate to the cause, the link to my blog and fundraiser is <a href=\"http://suozlogs.wordpress.com/\">HERE</a>.</strong> Anything, and I mean ANYTHING, you can do to help would be endlessly appreciated. If you don't want to look at my dumb cancer blog, the direct link to the preservation fund can be found <strong><a href=\"https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=4WR8KS68RC6YY\">HERE</a></strong></p>\n<p>On a lighter note, I'm open to the idea of trading donations for anything you might want in exchange (within legal limits). This could be fun!</p>\n<p>Proof can be found on my earlier post, but here's a pic from today: <a href=\"http://i.imgur.com/Qdkzn.jpg?1\">http://i.imgur.com/Qdkzn.jpg?1</a></p>\n<p>I'm also open to any questions about brain cancer, or my rationale for wanting to be preserved.</p>\n<p><strong>EDIT:</strong></p>\n<p>I want to explain in a little more detail why I think cryopreservation is worth a try. (Even an expensive try).</p>\n<p>First, I want to make it clear that I'm not betting my life on cryopreservation. I am aware of the problems with the current state of cryonics, but I have the hope that technology might come up with a solution in the future. No one knows what technology will be available in 50 years. Yes, it takes \"faith\" in technology, but it takes faith to assume that technology won't be sufficient to reverse these problems someday.</p>\n<p>The main point I want to make here is that it's a better shot at living again than if I were decomposing somewhere or cooked into ash. The relative value of even a slight chance at living again is a huge payoff for what seems like a lot of money to me now, but probably would be an easy decision for me if I had a steady job. Compare the cost of preservation to the cost of traveling overseas to pursue experimental treatments; I think the current state of glioblastoma treatment is just as bleak (if not more), but it doesn't seem so crazy to pursue those routes.</p>\n<p>I'm trying to be preserved because I've done everything else in my power to help me extend my life. I've looked at essentially every diet, supplement, clinical trial, and \"miracle treatment\" out there. This is the last thing I can possibly do to fight for another chance, and if does happen to work, it will be incredible.</p>\n<p>Live again or die trying.</p>\n<p><strong>EDIT 2: A cool quote</strong></p>\n<p><strong>\"The correct scientific answer to the question \"Does cryonics work?\" is: \"The clinical trials are in progress. Come back in a century and we'll give you a reliable answer.\" The relevant question for those of us who don't expect to survive that long is: \"Would I rather be in the control group, or the experimental group?\" We are forced by circumstances to answer that question without the benefit of knowing the results of the clinical trials.\" - Dr. Ralph Merkle</strong></p>\n<p><strong>TLDR; I want to be cryogenically preserved when I die from brain cancer but can't afford it. I am literally begging for financial help.</strong></p>\n</blockquote>\n<p>I couldn't help be moved by this. I felt a very <em>strong sense that she is one of us, </em>whoever \"us\" is. Looking at some of the negative comments and worst of all <em>bad arguments</em> people are using as reasons not to donate made me more upset.</p>\n<p><strong>I hope some here might join me in dismantling them. </strong>I'd also encourage those like me for who this buys a lot of warm fuzzies to donate. <a href=\"/lw/e5d/link_reddit_help_me_find_some_peace_im_dying_young/7888\">Though it might be wise to wait until we hear from CI</a> or some other third party on the matter.</p>\n<p><strong>Edit: </strong><strong><a href=\"/lw/e5d/link_reddit_help_me_find_some_peace_im_dying_young/78g1\">She has since made a comment on LW!</a></strong><strong> </strong>The provided information has made me pretty much certain that this is a genuine plight.<strong><br /></strong></p>\n<p>redditors where willing to give her money to go skydiving, they don't want to give her money to buy cryonics. <a href=\"/lw/65b/scientific_misconduct_misdiagnosed_because_of/4bu1\">Sometimes I can only weep. </a></p>\n<p>I think it pretty clear that promoting efficient charity in that particular thread is very unlikely to result in people giving money to better causes. Also I just plain want her to be rewarded in some small way! Note the part starting in the second paragraph that I bolded, not only did she realized what she really was, but she stepped over the entire set of pro-death rationalizations and faced the social pressure people she loved exerted on her because they think <em>she might go to heaven</em> ... its not her fault that a few cells in her brain went haywire before she could afford an insurance policy, I just don't want people like that not having something to show after getting so much stuff <em>right</em>.</p>\n<p><strong>2n Edit:</strong></p>\n<p><span>For anyone who just realized the universe sucks and wishes to do something about that whole people dying thing, they are welcome to engage in some optimal death defeating philanthropy by donating to <a href=\"/r/discussion/lw/e6j/the_brain_preservation_foundation_still_needs/\">The Brain Preservation Prize</a> that has been endorsed by both Robin Hanson and Eliezer Yudkowsky.&nbsp; </span></p>\n<blockquote>\n<p>I know that there are more than 17 other people like me in the world, who really want to see the results of these attempts. A world in which brains can be cheaply preserved indefinitely is a world I want to live in - and it would just be <em>sad</em> if this project fizzled because it lacked the funds to verify the already-existing results.</p>\n</blockquote>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"JsJPrdgRGRqnci8cZ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fLaKKRZckYtrrcz7m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 53, "baseScore": 28, "extendedScore": null, "score": 6.4e-05, "legacy": true, "legacyId": "18337", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Saw this on reddit.</p>\n<p><a href=\"http://www.reddit.com/r/atheism/comments/ydsy5/reddit_help_me_find_some_peace_in_dying_young_im/\">http://www.reddit.com/r/atheism/comments/ydsy5/reddit_help_me_find_some_peace_in_dying_young_im/</a></p>\n<div class=\"usertext-body\">\n<div class=\"md\">\n<blockquote>\n<p>Hey Reddit,</p>\n<p>I'm a 23 year old girl with recurrent Glioblastoma multiforme (GBM), a highly aggressive type of brain cancer. <a href=\"http://www.reddit.com/r/AskReddit/comments/uvaqe/today_is_my_23rd_birthday_and_probably_my_last/\">I posted a couple of months ago asking for suggestions for things I should try before I die</a> (life expectancy is 3-6 months) and got a lot of great ideas (many of which I've fulfilled).</p>\n<p>At the time of my last post, my treatment was undecided. I ended up participating in a phase I trial at Dana-Farber, but I progressed after two months of treatment. There are not many great treatment options left for me, but my next move will be five radiosurgery treatments at Duke University next week. My prognosis looks pretty bleak at this point, and though I am hoping to exceed the 6-10 month median survival, I have to prepare to die. <strong>In a way, I am fortunate because the lesion is primarily in my brain stem (controls things like breathing), so I will likely die before the tumor spreads to the areas central to who I am.</strong></p>\n<p><strong><br></strong></p>\n<p><strong>I'm back on Reddit again, mostly to ask for help because I want to be cryogenically preserved upon my death. I've been interested in cryonics since long before I was even diagnosed, but I never thought that I would have to secure the finances so fast, and without a career or savings to stand on.</strong> As weird as it feels to ask for help here, I feel I should just give it a shot and sees what happens.</p>\n<p><strong>I caused a lot of family controversy last week by breaking the news to my parents. I can tell I've alienated them quite a bit as they are Christian and don't see why I'd want to be preserved; in their mind, I am going to heaven and my \"soul\" will forever leave my body when I die anyway. </strong>I clearly upset both of them with the implication that I was agnostic (I didn't say this outright, but it's true). My mom is fairly supportive of my plans to be preserved, but unfortunately, my dad isn't a fan of the idea, and he's really the only family I have that could offer financial help (my parents are divorced and not on good terms). The company I'm looking into, Cryonics Institute, costs $30,000-35,000 with transportation to the facility accounted for. <strong>My boyfriend is fully supportive, but like me, he's broke and barely out of college.</strong></p>\n<p>I know this is a big thing to ask for, and I'm sure many people are doubtful that preservation is plausible with cryonics. I'm far from convinced, but I would rather take the chance with preservation than rot in the ground or get cremated. The company I'm looking into, Cryonics Institute, has a good intro on their FAQ page that offers a hopeful outlook on future technology: <a href=\"http://cryonics.org/prod.html\">http://cryonics.org/prod.html</a></p>\n<p><strong id=\"A_lot_of_people_on_reddit_wanted_to_start_a_fundraiser_for_me_awhile_ago_to_aid_in_doing_fun_things_before_I_die__I_am_hoping_that_redditors_will_still_have_some_interest_in_helping_me_even_if_it_s_not_going_towards_vacation_or_skydiving_and_shit_like_that__Cryopreservation_is_sincerely_what_will_bring_me_the_most_peace_in_death_\">A lot of people on reddit wanted to start a fundraiser for me awhile ago to aid in doing fun things before I die. I am hoping that redditors will still have some interest in helping me even if it's not going towards vacation or skydiving and shit like that. Cryopreservation is sincerely what will bring me the most peace in death.</strong></p>\n<p>I wish I could give a particularly compelling reason why I deserve another chance at life, but there's not much to say. I'm still just a kid, and hadn't even finished college when I was diagnosed. Unfortunately the most interesting thing I have yet to do is get a terminal disease at a young age.</p>\n<p>If you guys can help me out, I would be grateful to a degree I can't possibly describe. I'm desperate. <strong>If you care to donate to the cause, the link to my blog and fundraiser is <a href=\"http://suozlogs.wordpress.com/\">HERE</a>.</strong> Anything, and I mean ANYTHING, you can do to help would be endlessly appreciated. If you don't want to look at my dumb cancer blog, the direct link to the preservation fund can be found <strong><a href=\"https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=4WR8KS68RC6YY\">HERE</a></strong></p>\n<p>On a lighter note, I'm open to the idea of trading donations for anything you might want in exchange (within legal limits). This could be fun!</p>\n<p>Proof can be found on my earlier post, but here's a pic from today: <a href=\"http://i.imgur.com/Qdkzn.jpg?1\">http://i.imgur.com/Qdkzn.jpg?1</a></p>\n<p>I'm also open to any questions about brain cancer, or my rationale for wanting to be preserved.</p>\n<p><strong id=\"EDIT_\">EDIT:</strong></p>\n<p>I want to explain in a little more detail why I think cryopreservation is worth a try. (Even an expensive try).</p>\n<p>First, I want to make it clear that I'm not betting my life on cryopreservation. I am aware of the problems with the current state of cryonics, but I have the hope that technology might come up with a solution in the future. No one knows what technology will be available in 50 years. Yes, it takes \"faith\" in technology, but it takes faith to assume that technology won't be sufficient to reverse these problems someday.</p>\n<p>The main point I want to make here is that it's a better shot at living again than if I were decomposing somewhere or cooked into ash. The relative value of even a slight chance at living again is a huge payoff for what seems like a lot of money to me now, but probably would be an easy decision for me if I had a steady job. Compare the cost of preservation to the cost of traveling overseas to pursue experimental treatments; I think the current state of glioblastoma treatment is just as bleak (if not more), but it doesn't seem so crazy to pursue those routes.</p>\n<p>I'm trying to be preserved because I've done everything else in my power to help me extend my life. I've looked at essentially every diet, supplement, clinical trial, and \"miracle treatment\" out there. This is the last thing I can possibly do to fight for another chance, and if does happen to work, it will be incredible.</p>\n<p>Live again or die trying.</p>\n<p><strong id=\"EDIT_2__A_cool_quote\">EDIT 2: A cool quote</strong></p>\n<p><strong id=\"_The_correct_scientific_answer_to_the_question__Does_cryonics_work___is___The_clinical_trials_are_in_progress__Come_back_in_a_century_and_we_ll_give_you_a_reliable_answer___The_relevant_question_for_those_of_us_who_don_t_expect_to_survive_that_long_is___Would_I_rather_be_in_the_control_group__or_the_experimental_group___We_are_forced_by_circumstances_to_answer_that_question_without_the_benefit_of_knowing_the_results_of_the_clinical_trials_____Dr__Ralph_Merkle\">\"The correct scientific answer to the question \"Does cryonics work?\" is: \"The clinical trials are in progress. Come back in a century and we'll give you a reliable answer.\" The relevant question for those of us who don't expect to survive that long is: \"Would I rather be in the control group, or the experimental group?\" We are forced by circumstances to answer that question without the benefit of knowing the results of the clinical trials.\" - Dr. Ralph Merkle</strong></p>\n<p><strong id=\"TLDR__I_want_to_be_cryogenically_preserved_when_I_die_from_brain_cancer_but_can_t_afford_it__I_am_literally_begging_for_financial_help_\">TLDR; I want to be cryogenically preserved when I die from brain cancer but can't afford it. I am literally begging for financial help.</strong></p>\n</blockquote>\n<p>I couldn't help be moved by this. I felt a very <em>strong sense that she is one of us, </em>whoever \"us\" is. Looking at some of the negative comments and worst of all <em>bad arguments</em> people are using as reasons not to donate made me more upset.</p>\n<p><strong>I hope some here might join me in dismantling them. </strong>I'd also encourage those like me for who this buys a lot of warm fuzzies to donate. <a href=\"/lw/e5d/link_reddit_help_me_find_some_peace_im_dying_young/7888\">Though it might be wise to wait until we hear from CI</a> or some other third party on the matter.</p>\n<p><strong>Edit: </strong><strong><a href=\"/lw/e5d/link_reddit_help_me_find_some_peace_im_dying_young/78g1\">She has since made a comment on LW!</a></strong><strong> </strong>The provided information has made me pretty much certain that this is a genuine plight.<strong><br></strong></p>\n<p>redditors where willing to give her money to go skydiving, they don't want to give her money to buy cryonics. <a href=\"/lw/65b/scientific_misconduct_misdiagnosed_because_of/4bu1\">Sometimes I can only weep. </a></p>\n<p>I think it pretty clear that promoting efficient charity in that particular thread is very unlikely to result in people giving money to better causes. Also I just plain want her to be rewarded in some small way! Note the part starting in the second paragraph that I bolded, not only did she realized what she really was, but she stepped over the entire set of pro-death rationalizations and faced the social pressure people she loved exerted on her because they think <em>she might go to heaven</em> ... its not her fault that a few cells in her brain went haywire before she could afford an insurance policy, I just don't want people like that not having something to show after getting so much stuff <em>right</em>.</p>\n<p><strong id=\"2n_Edit_\">2n Edit:</strong></p>\n<p><span>For anyone who just realized the universe sucks and wishes to do something about that whole people dying thing, they are welcome to engage in some optimal death defeating philanthropy by donating to <a href=\"/r/discussion/lw/e6j/the_brain_preservation_foundation_still_needs/\">The Brain Preservation Prize</a> that has been endorsed by both Robin Hanson and Eliezer Yudkowsky.&nbsp; </span></p>\n<blockquote>\n<p>I know that there are more than 17 other people like me in the world, who really want to see the results of these attempts. A world in which brains can be cheaply preserved indefinitely is a world I want to live in - and it would just be <em>sad</em> if this project fizzled because it lacked the funds to verify the already-existing results.</p>\n</blockquote>\n</div>\n</div>", "sections": [{"title": "A lot of people on reddit wanted to start a fundraiser for me awhile ago to aid in doing fun things before I die. I am hoping that redditors will still have some interest in helping me even if it's not going towards vacation or skydiving and shit like that. Cryopreservation is sincerely what will bring me the most peace in death.", "anchor": "A_lot_of_people_on_reddit_wanted_to_start_a_fundraiser_for_me_awhile_ago_to_aid_in_doing_fun_things_before_I_die__I_am_hoping_that_redditors_will_still_have_some_interest_in_helping_me_even_if_it_s_not_going_towards_vacation_or_skydiving_and_shit_like_that__Cryopreservation_is_sincerely_what_will_bring_me_the_most_peace_in_death_", "level": 1}, {"title": "EDIT:", "anchor": "EDIT_", "level": 1}, {"title": "EDIT 2: A cool quote", "anchor": "EDIT_2__A_cool_quote", "level": 1}, {"title": "\"The correct scientific answer to the question \"Does cryonics work?\" is: \"The clinical trials are in progress. Come back in a century and we'll give you a reliable answer.\" The relevant question for those of us who don't expect to survive that long is: \"Would I rather be in the control group, or the experimental group?\" We are forced by circumstances to answer that question without the benefit of knowing the results of the clinical trials.\" - Dr. Ralph Merkle", "anchor": "_The_correct_scientific_answer_to_the_question__Does_cryonics_work___is___The_clinical_trials_are_in_progress__Come_back_in_a_century_and_we_ll_give_you_a_reliable_answer___The_relevant_question_for_those_of_us_who_don_t_expect_to_survive_that_long_is___Would_I_rather_be_in_the_control_group__or_the_experimental_group___We_are_forced_by_circumstances_to_answer_that_question_without_the_benefit_of_knowing_the_results_of_the_clinical_trials_____Dr__Ralph_Merkle", "level": 1}, {"title": "TLDR; I want to be cryogenically preserved when I die from brain cancer but can't afford it. I am literally begging for financial help.", "anchor": "TLDR__I_want_to_be_cryogenically_preserved_when_I_die_from_brain_cancer_but_can_t_afford_it__I_am_literally_begging_for_financial_help_", "level": 1}, {"title": "2n Edit:", "anchor": "2n_Edit_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "182 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 182, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iSm7uNHBByxKPEF5j"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2012-08-18T15:17:01.302Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-18T17:57:19.120Z", "modifiedAt": "2022-04-07T03:03:16.074Z", "url": null, "title": "How to get cryocrastinators to actually sign up for cryonics", "slug": "how-to-get-cryocrastinators-to-actually-sign-up-for-cryonics", "viewCount": null, "lastCommentedAt": "2020-05-18T05:13:38.791Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JGWeissman", "createdAt": "2009-04-01T04:43:56.740Z", "isAdmin": false, "displayName": "JGWeissman"}, "userId": "Mw8rsM7m7E8nnEFEp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/E4RXMhsXyagTW2qKf/how-to-get-cryocrastinators-to-actually-sign-up-for-cryonics", "pageUrlRelative": "/posts/E4RXMhsXyagTW2qKf/how-to-get-cryocrastinators-to-actually-sign-up-for-cryonics", "linkUrl": "https://www.lesswrong.com/posts/E4RXMhsXyagTW2qKf/how-to-get-cryocrastinators-to-actually-sign-up-for-cryonics", "postedAtFormatted": "Saturday, August 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20get%20cryocrastinators%20to%20actually%20sign%20up%20for%20cryonics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20get%20cryocrastinators%20to%20actually%20sign%20up%20for%20cryonics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE4RXMhsXyagTW2qKf%2Fhow-to-get-cryocrastinators-to-actually-sign-up-for-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20get%20cryocrastinators%20to%20actually%20sign%20up%20for%20cryonics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE4RXMhsXyagTW2qKf%2Fhow-to-get-cryocrastinators-to-actually-sign-up-for-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE4RXMhsXyagTW2qKf%2Fhow-to-get-cryocrastinators-to-actually-sign-up-for-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 659, "htmlBody": "<p>At the end of CFAR's July Rationality Minicamp, we had a party with people from the LW/SIAI/CFAR community in the San Francisco Bay area. During this party, I had a conversation with the girlfriend of a participant in a previous minicamp, who was not signed up for cryonics (her boyfriend was). The conversation went like this:</p>\n<p>me: So, you know what cryonics is?</p>\n<p>her: Yes</p>\n<p>me: And you think it's a good idea?</p>\n<p>her: Yes</p>\n<p>me: And you are not signed up yet?</p>\n<p>her: Yes</p>\n<p>me: And you would like to be?</p>\n<p>her: Yes</p>\n<p>me: Wait a minute while I get my laptop.</p>\n<p>And I got my laptop, pointed my browser at <a href=\"http://rudihoffman.com/form_request.html\">Rudi Hoffman's quote request form</a><sup>1</sup>, and said, \"Here, fill out this form\". And she did.</p>\n<p><a id=\"more\"></a></p>\n<p>The hard part of all that was identifying a cryocrastinator, by which I mean someone who believes they should be signed up for cryonics, but for whatever reason, hasn't actually signed up. Once I know that I am talking to such a person, just giving them an actionable first step to do right now gets them to do that step.</p>\n<p>Previously to the party, I had held an \"unconference\" seminar for cryocrastinating minicampers in which I did a scaled up version of the same thing. For this I told everyone in advance to bring their own laptops, and I gave them the URL. (There was some confusion about the target audience of this seminar, and some people who were not yet convinced it was a good idea for them came expecting more of a discussion. They had no trouble expressing this, and were not required to fill out the form.) At the party, I did this for one other person<sup>2</sup>.</p>\n<p>What I have observed to work so far is that people will take the first step of filling out the quote request form when I make it easy for them. I am counting on Rudi to get them through the rest of the process, so they end up actually signed up. Rudi has agreed to track success rates of these people getting through the whole process, and I plan to check in with him in early December, and report back.</p>\n<p>I was planning to write this up when I had the full results, but seeing <a href=\"/r/discussion/lw/e5d/link_reddit_help_me_find_some_peace_im_dying_young/\">this story of a young woman with brain cancer forced to beg to raise funds at the last minute</a> reminded me that cryocrastinators are running out of time (even though getting brain cancer young is rare, there are cryocrastinators of all ages who aren't aware of when life insurance will become unaffordable). So I thought it would be good to let people know now how easy it is to get that cryocrastinator you know to get started signing up.</p>\n<p>Again, all you have to do is establish that they want to be signed up for cryonics but aren't, and put <a href=\"http://rudihoffman.com/form_request.html\">this form</a> in front of them and tell them that filling it out is the first step. Rudi will take them through the rest of it. And if you yourself are cryocrastinating, take a few minutes for your first step in signing up by filling out <a href=\"http://rudihoffman.com/form_request.html\">the form</a>.</p>\n<hr />\n<p>&nbsp;</p>\n<p>(If you do not already think cryonics is a good idea, I do not expect you to follow any of the advice in this article. I wrote this for the benefit of all the people who do think cryonics is a good idea, but are having trouble actually signing up. You may be interested in trying to generalize the technique for other forms of procrastination, however.)</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>1. Yes, Rudi Hoffman will make some money off of this. He should, as he is putting in professional hours to provide a valuable service. But the motivation behind this article is to get people to sign up for cryonics. Other paths with other first steps are welcome, as is any advice for people outside the United States.</p>\n<p>2. I am not naming the other people involved. They can opt in to identifying themselves if they want.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "E4RXMhsXyagTW2qKf", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 29, "extendedScore": null, "score": 5.8e-05, "legacy": true, "legacyId": "18338", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 95, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fLaKKRZckYtrrcz7m"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-08-18T17:57:19.120Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-08-19T02:35:52.503Z", "modifiedAt": null, "url": null, "title": "Scott Aaronson's cautious optimism for the MWI", "slug": "scott-aaronson-s-cautious-optimism-for-the-mwi", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:06.933Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "calef", "createdAt": "2011-01-26T21:41:24.522Z", "isAdmin": false, "displayName": "calef"}, "userId": "voRPyQHqt6FHmXkZe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/woFmg927nZiyzbzTd/scott-aaronson-s-cautious-optimism-for-the-mwi", "pageUrlRelative": "/posts/woFmg927nZiyzbzTd/scott-aaronson-s-cautious-optimism-for-the-mwi", "linkUrl": "https://www.lesswrong.com/posts/woFmg927nZiyzbzTd/scott-aaronson-s-cautious-optimism-for-the-mwi", "postedAtFormatted": "Sunday, August 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Scott%20Aaronson's%20cautious%20optimism%20for%20the%20MWI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScott%20Aaronson's%20cautious%20optimism%20for%20the%20MWI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwoFmg927nZiyzbzTd%2Fscott-aaronson-s-cautious-optimism-for-the-mwi%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Scott%20Aaronson's%20cautious%20optimism%20for%20the%20MWI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwoFmg927nZiyzbzTd%2Fscott-aaronson-s-cautious-optimism-for-the-mwi", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwoFmg927nZiyzbzTd%2Fscott-aaronson-s-cautious-optimism-for-the-mwi", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 75, "htmlBody": "<p>http://www.scottaaronson.com/blog/?p=1103</p>\n<p>Eliezer's gung-ho attitude about the realism of the Many Worlds Interpretation always rubbed me the wrong way, especially in the podcast between both him and Scott (around 8:43 in http://bloggingheads.tv/videos/2220).&nbsp; I've seen a similar sentiment expressed before about the MWI sequences.&nbsp; And I say that still believing it to be the most seemingly correct of the available interpretations.&nbsp;</p>\n<p>&nbsp;</p>\n<p>I feel Scott's post does an excellent job grounding it as a possibly correct, and in-principle falsifiable interpretation.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "woFmg927nZiyzbzTd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 8, "extendedScore": null, "score": 9.673414483713348e-07, "legacy": true, "legacyId": "18339", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 70, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}